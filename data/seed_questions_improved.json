{
  "questions": [
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A retail company has developed a REST API which is deployed in an Auto Scaling group behind an Application Load Balancer. The REST API stores the user data in Amazon DynamoDB and any static content, such as images, are served via Amazon Simple Storage Service (Amazon S3). On analyzing the usage trends, it is found that 90% of the read requests are for commonly accessed data across all users. As a Solutions Architect, which of the following would you suggest as the MOST efficient solution to improve the application performance?",
      "options": [
        "A. Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and Amazon CloudFront for Amazon S3",
        "B. Enable ElastiCache Redis for DynamoDB and Amazon CloudFront for Amazon S3",
        "C. Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and ElastiCache Memcached for Amazon S3",
        "D. Enable ElastiCache Redis for DynamoDB and ElastiCache Memcached for Amazon S3"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and Amazon CloudFront for Amazon S3 Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for Amazon DynamoDB that delivers up to a 10 times performance improvement—from milliseconds to microseconds—even at millions of requests per second. Amazon DynamoDB Accelerator (DAX) is tightly integrated with Amazon DynamoDB—you simply provision a DAX cluster, use the DAX client SDK to point your existing Amazon DynamoDB API calls at the DAX cluster, and let DAX handle the rest. Because DAX is API-compatible with Amazon DynamoDB, you don't have to make any functional application code changes. DAX is used to natively cache Amazon DynamoDB reads. Amazon CloudFront is a content delivery network (CDN) service that delivers static and dynamic web content, video streams, and APIs around the world, securely and at scale. By design, delivering data out of Amazon CloudFront can be more cost-effective than delivering it from S3 directly to your users. When a user requests content that you serve with CloudFront, their request is routed to a nearby Edge Location. If CloudFront has a cached copy of the requested file, CloudFront delivers it to the user, providing a fast (low-latency) response. If the file they’ve requested isn’t yet cached, CloudFront retrieves it from your origin – for example, the Amazon S3 bucket where you’ve stored your content. So, you can use Amazon CloudFront to improve application performance to serve static content from Amazon S3. Incorrect options: Enable ElastiCache Redis for DynamoDB and Amazon CloudFront for Amazon S3 Amazon ElastiCache for Redis is a blazing fast in-memory data store that provides sub-millisecond latency to power internet-scale real-time applications. Amazon ElastiCache for Redis is a great choice for real-time transactional and analytical processing use cases such as caching, chat/messaging, gaming leaderboards, geospatial, machine learning, media streaming, queues, real-time analytics, and session store. Amazon ElastiCache for Redis Overview: via - https://aws.amazon.com/elasticache/redis/ Although you can integrate Redis with DynamoDB, it's much more involved than using DAX which is a much better fit. Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and ElastiCache Memcached for Amazon S3 Enable ElastiCache Redis for DynamoDB and ElastiCache Memcached for Amazon S3 Amazon ElastiCache for Memcached is a Memcached-compatible in-memory key-value store service that can be used as a cache or a data store. Amazon ElastiCache for Memcached is a great choice for implementing an in-memory cache to decrease access latency, increase throughput, and ease the load off your relational or NoSQL database. Amazon ElastiCache Memcached cannot be used as a cache to serve static content from Amazon S3, so both these options are incorrect. References: https://aws.amazon.com/dynamodb/dax/ https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/ https://aws.amazon.com/elasticache/redis/",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An e-commerce company is looking for a solution with high availability, as it plans to migrate its flagship application to a fleet of Amazon Elastic Compute Cloud (Amazon EC2) instances. The solution should allow for content-based routing as part of the architecture. As a Solutions Architect, which of the following will you suggest for the company?",
      "options": [
        "A. Use an Application Load Balancer for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure Auto Scaling group to mask any failure of an instance",
        "B. Use a Network Load Balancer for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure a Private IP address to mask any failure of an instance",
        "C. Use an Auto Scaling group for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure an elastic IP address (EIP) to mask any failure of an instance",
        "D. Use an Auto Scaling group for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure a Public IP address to mask any failure of an instance"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Use an Application Load Balancer for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure Auto Scaling group to mask any failure of an instance The Application Load Balancer (ALB) is best suited for load balancing HTTP and HTTPS traffic and provides advanced request routing targeted at the delivery of modern application architectures, including microservices and containers. Operating at the individual request level (Layer 7), the Application Load Balancer routes traffic to targets within Amazon Virtual Private Cloud (Amazon VPC) based on the content of the request. This is the correct option since the question has a specific requirement for content-based routing which can be configured via the Application Load Balancer. Different Availability Zones (AZs) provide high availability to the overall architecture and Auto Scaling group will help mask any instance failures. More info on Application Load Balancer: via - https://aws.amazon.com/blogs/aws/new-aws-application-load-balancer/ Incorrect options: Use a Network Load Balancer for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure a Private IP address to mask any failure of an instance - Network Load Balancer cannot facilitate content-based routing so this option is incorrect. Use an Auto Scaling group for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure an elastic IP address (EIP) to mask any failure of an instance Use an Auto Scaling group for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure a Public IP address to mask any failure of an instance Both these options are incorrect as you cannot use the Auto Scaling group to distribute traffic to the Amazon EC2 instances. An elastic IP address (EIP) is a static, public, IPv4 address allocated to your AWS account. With an Elastic IP address, you can mask the failure of an instance or software by rapidly remapping the address to another instance in your account. Elastic IPs do not change and remain allocated to your account until you delete them. More info on Elastic Load Balancing (ELB): via - https://docs.aws.amazon.com/whitepapers/latest/fault-tolerant-components/fault-tolerant-components.pdf You can span your Auto Scaling group across multiple Availability Zones (AZs) within an AWS Region and then attaching a load balancer to distribute incoming traffic across those zones. via - https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html References: https://aws.amazon.com/blogs/aws/new-aws-application-load-balancer/ https://docs.aws.amazon.com/whitepapers/latest/fault-tolerant-components/fault-tolerant-components.pdf https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A healthcare company uses its on-premises infrastructure to run legacy applications that require specialized customizations to the underlying Oracle database as well as its host operating system (OS). The company also wants to improve the availability of the Oracle database layer. The company has hired you as an AWS Certified Solutions Architect – Associate to build a solution on AWS that meets these requirements while minimizing the underlying infrastructure maintenance effort. Which of the following options represents the best solution for this use case?",
      "options": [
        "A. Deploy the Oracle database layer on multiple Amazon EC2 instances spread across two Availability Zones (AZs). This deployment configuration guarantees high availability and also allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system",
        "B. Leverage multi-AZ configuration of Amazon RDS Custom for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system",
        "C. Leverage multi-AZ configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system",
        "D. Leverage cross AZ read-replica configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Leverage multi-AZ configuration of Amazon RDS Custom for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system Amazon RDS is a managed service that makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while managing time-consuming database administration tasks. Amazon RDS can automatically back up your database and keep your database software up to date with the latest version. However, RDS does not allow you to access the host OS of the database. For the given use-case, you need to use Amazon RDS Custom for Oracle as it allows you to access and customize your database server host and operating system, for example by applying special patches and changing the database software settings to support third-party applications that require privileged access. Amazon RDS Custom for Oracle facilitates these functionalities with minimum infrastructure maintenance effort. You need to set up the RDS Custom for Oracle in multi-AZ configuration for high availability. Amazon RDS Custom for Oracle: via - https://aws.amazon.com/blogs/aws/amazon-rds-custom-for-oracle-new-control-capabilities-in-database-environment/ Incorrect options: Leverage multi-AZ configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system Leverage cross AZ read-replica configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system Amazon RDS for Oracle does not allow you to access and customize your database server host and operating system. Therefore, both these options are incorrect. Deploy the Oracle database layer on multiple Amazon EC2 instances spread across two Availability Zones (AZs). This deployment configuration guarantees high availability and also allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system - The use case requires that the best solution should involve minimum infrastructure maintenance effort. When you use Amazon EC2 instances to host the databases, you need to manage the server health, server maintenance, server patching, and database maintenance tasks yourself. In addition, you will also need to manage the multi-AZ configuration by deploying Amazon EC2 instances across two Availability Zones (AZs), perhaps by using an Auto Scaling group. These steps entail significant maintenance effort. Hence this option is incorrect. References: https://aws.amazon.com/blogs/aws/amazon-rds-custom-for-oracle-new-control-capabilities-in-database-environment/ https://aws.amazon.com/rds/faqs/",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A financial services company recently launched an initiative to improve the security of its AWS resources and it had enabled AWS Shield Advanced across multiple AWS accounts owned by the company. Upon analysis, the company has found that the costs incurred are much higher than expected. Which of the following would you attribute as the underlying reason for the unexpectedly high costs for AWS Shield Advanced service?",
      "options": [
        "A. Consolidated billing has not been enabled. All the AWS accounts should fall under a single consolidated billing for the monthly fee to be charged only once",
        "B. AWS Shield Advanced is being used for custom servers, that are not part of AWS Cloud, thereby resulting in increased costs",
        "C. AWS Shield Advanced also covers AWS Shield Standard plan, thereby resulting in increased costs",
        "D. Savings Plans has not been enabled for the AWS Shield Advanced service across all the AWS accounts"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Consolidated billing has not been enabled. All the AWS accounts should fall under a single consolidated billing for the monthly fee to be charged only once If your organization has multiple AWS accounts, then you can subscribe multiple AWS Accounts to AWS Shield Advanced by individually enabling it on each account using the AWS Management Console or API. You will pay the monthly fee once as long as the AWS accounts are all under a single consolidated billing, and you own all the AWS accounts and resources in those accounts. Incorrect options: AWS Shield Advanced is being used for custom servers, that are not part of AWS Cloud, thereby resulting in increased costs - AWS Shield Advanced does offer protection to resources outside of AWS. This should not cause unexpected spike in billing costs. AWS Shield Advanced also covers AWS Shield Standard plan, thereby resulting in increased costs - AWS Shield Standard is automatically enabled for all AWS customers at no additional cost. AWS Shield Advanced is an optional paid service. Savings Plans has not been enabled for the AWS Shield Advanced service across all the AWS accounts - This option has been added as a distractor. Savings Plans is a flexible pricing model that offers low prices on Amazon EC2 instances, AWS Lambda, and AWS Fargate usage, in exchange for a commitment to a consistent amount of usage (measured in $/hour) for a 1 or 3 year term. Savings Plans is not applicable for the AWS Shield Advanced service. References: https://aws.amazon.com/shield/faqs/ https://aws.amazon.com/savingsplans/faq/",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A Big Data analytics company wants to set up an AWS cloud architecture that throttles requests in case of sudden traffic spikes. The company is looking for AWS services that can be used for buffering or throttling to handle such traffic variations. Which of the following services can be used to support this requirement?",
      "options": [
        "A. Amazon Simple Queue Service (Amazon SQS), Amazon Simple Notification Service (Amazon SNS) and AWS Lambda",
        "B. Amazon API Gateway, Amazon Simple Queue Service (Amazon SQS) and Amazon Kinesis",
        "C. Amazon Gateway Endpoints, Amazon Simple Queue Service (Amazon SQS) and Amazon Kinesis",
        "D. Elastic Load Balancer, Amazon Simple Queue Service (Amazon SQS), AWS Lambda"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Throttling is the process of limiting the number of requests an authorized program can submit to a given operation in a given amount of time. Amazon API Gateway, Amazon Simple Queue Service (Amazon SQS) and Amazon Kinesis To prevent your API from being overwhelmed by too many requests, Amazon API Gateway throttles requests to your API using the token bucket algorithm, where a token counts for a request. Specifically, API Gateway sets a limit on a steady-state rate and a burst of request submissions against all APIs in your account. In the token bucket algorithm, the burst is the maximum bucket size. Amazon Simple Queue Service (Amazon SQS) - Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. Amazon SQS offers buffer capabilities to smooth out temporary volume spikes without losing messages or increasing latency. Amazon Kinesis - Amazon Kinesis is a fully managed, scalable service that can ingest, buffer, and process streaming data in real-time. Incorrect options: Amazon Simple Queue Service (Amazon SQS), Amazon Simple Notification Service (Amazon SNS) and AWS Lambda - Amazon SQS has the ability to buffer its messages. Amazon Simple Notification Service (SNS) cannot buffer messages and is generally used with SQS to provide the buffering facility. When requests come in faster than your Lambda function can scale, or when your function is at maximum concurrency, additional requests fail as the Lambda throttles those requests with error code 429 status code. So, this combination of services is incorrect. Amazon Gateway Endpoints, Amazon Simple Queue Service (Amazon SQS) and Amazon Kinesis - A Gateway Endpoint is a gateway that you specify as a target for a route in your route table for traffic destined to a supported AWS service. This cannot help in throttling or buffering of requests. Amazon SQS and Kinesis can buffer incoming data. Since Gateway Endpoint is an incorrect service for throttling or buffering, this option is incorrect. Elastic Load Balancer, Amazon Simple Queue Service (Amazon SQS), AWS Lambda - Elastic Load Balancer cannot throttle requests. Amazon SQS can be used to buffer messages. When requests come in faster than your Lambda function can scale, or when your function is at maximum concurrency, additional requests fail as the Lambda throttles those requests with error code 429 status code. So, this combination of services is incorrect. References: https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-request-throttling.html https://aws.amazon.com/sqs/features/",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A media company runs a photo-sharing web application that is accessed across three different countries. The application is deployed on several Amazon Elastic Compute Cloud (Amazon EC2) instances running behind an Application Load Balancer. With new government regulations, the company has been asked to block access from two countries and allow access only from the home country of the company. Which configuration should be used to meet this changed requirement?",
      "options": [
        "A. Configure the security group on the Application Load Balancer",
        "B. Configure AWS Web Application Firewall (AWS WAF) on the Application Load Balancer in a Amazon Virtual Private Cloud (Amazon VPC)",
        "C. Use Geo Restriction feature of Amazon CloudFront in a Amazon Virtual Private Cloud (Amazon VPC)",
        "D. Configure the security group for the Amazon EC2 instances"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: AWS Web Application Firewall (AWS WAF) is a web application firewall service that lets you monitor web requests and protect your web applications from malicious requests. Use AWS WAF to block or allow requests based on conditions that you specify, such as the IP addresses. You can also use AWS WAF preconfigured protections to block common attacks like SQL injection or cross-site scripting. Configure AWS Web Application Firewall (AWS WAF) on the Application Load Balancer in a Amazon Virtual Private Cloud (Amazon VPC) You can use AWS WAF with your Application Load Balancer to allow or block requests based on the rules in a web access control list (web ACL). Geographic (Geo) Match Conditions in AWS WAF allows you to use AWS WAF to restrict application access based on the geographic location of your viewers. With geo match conditions you can choose the countries from which AWS WAF should allow access. Geo match conditions are important for many customers. For example, legal and licensing requirements restrict some customers from delivering their applications outside certain countries. These customers can configure a whitelist that allows only viewers in those countries. Other customers need to prevent the downloading of their encrypted software by users in certain countries. These customers can configure a blacklist so that end-users from those countries are blocked from downloading their software. Incorrect options: Use Geo Restriction feature of Amazon CloudFront in a Amazon Virtual Private Cloud (Amazon VPC) - Geo Restriction feature of Amazon CloudFront helps in restricting traffic based on the user's geographic location. But, CloudFront works from edge locations and doesn't belong to a VPC. Hence, this option itself is incorrect and given only as a distractor. Configure the security group on the Application Load Balancer Configure the security group for the Amazon EC2 instances Security Groups cannot restrict access based on the user's geographic location. References: https://aws.amazon.com/about-aws/whats-new/2017/10/aws-waf-now-supports-geographic-match/ https://aws.amazon.com/blogs/aws/aws-web-application-firewall-waf-for-application-load-balancers/ https://aws.amazon.com/about-aws/whats-new/2016/12/AWS-WAF-now-available-on-Application-Load-Balancer/",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "The flagship application for a gaming company connects to an Amazon Aurora database and the entire technology stack is currently deployed in the United States. Now, the company has plans to expand to Europe and Asia for its operations. It needs the games table to be accessible globally but needs the users and games_played tables to be regional only. How would you implement this with minimal application refactoring?",
      "options": [
        "A. Use an Amazon Aurora Global Database for the games table and use Amazon Aurora for the users and games_played tables",
        "B. Use an Amazon Aurora Global Database for the games table and use Amazon DynamoDB tables for the users and games_played tables",
        "C. Use a Amazon DynamoDB global table for the games table and use Amazon Aurora for the users and games_played tables",
        "D. Use a Amazon DynamoDB global table for the games table and use Amazon DynamoDB tables for the users and games_played tables"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Use an Amazon Aurora Global Database for the games table and use Amazon Aurora for the users and games_played tables Amazon Aurora is a MySQL and PostgreSQL-compatible relational database built for the cloud, that combines the performance and availability of traditional enterprise databases with the simplicity and cost-effectiveness of open source databases. Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 128TB per database instance. Aurora is not an in-memory database. Amazon Aurora Global Database is designed for globally distributed applications, allowing a single Amazon Aurora database to span multiple AWS regions. It replicates your data with no impact on database performance, enables fast local reads with low latency in each region, and provides disaster recovery from region-wide outages. Amazon Aurora Global Database is the correct choice for the given use-case. For the given use-case, we, therefore, need to have two Aurora clusters, one for the global table (games table) and the other one for the local tables (users and games_played tables). Incorrect options: Use an Amazon Aurora Global Database for the games table and use Amazon DynamoDB tables for the users and games_played tables Use a Amazon DynamoDB global table for the games table and use Amazon Aurora for the users and games_played tables Use a Amazon DynamoDB global table for the games table and use Amazon DynamoDB tables for the users and games_played tables Here, we want minimal application refactoring. Amazon DynamoDB and Amazon Aurora have a completely different APIs, due to Amazon Aurora being SQL and Amazon DynamoDB being NoSQL. So all three options are incorrect, as they have Amazon DynamoDB as one of the components. Reference: https://aws.amazon.com/rds/aurora/faqs/",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A media agency stores its re-creatable assets on Amazon Simple Storage Service (Amazon S3) buckets. The assets are accessed by a large number of users for the first few days and the frequency of access falls down drastically after a week. Although the assets would be accessed occasionally after the first week, but they must continue to be immediately accessible when required. The cost of maintaining all the assets on Amazon S3 storage is turning out to be very expensive and the agency is looking at reducing costs as much as possible. As an AWS Certified Solutions Architect – Associate, can you suggest a way to lower the storage costs while fulfilling the business requirements?",
      "options": [
        "A. Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days",
        "B. Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 7 days",
        "C. Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days",
        "D. Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 7 days"
      ],
      "correct_answer": "C",
      "explanation": "Correct option: Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days Amazon S3 One Zone-IA is for data that is accessed less frequently, but requires rapid access when needed. Unlike other S3 Storage Classes which store data in a minimum of three Availability Zones (AZs), Amazon S3 One Zone-IA stores data in a single Availability Zone (AZ) and costs 20% less than Amazon S3 Standard-IA. Amazon S3 One Zone-IA is ideal for customers who want a lower-cost option for infrequently accessed and re-creatable data but do not require the availability and resilience of Amazon S3 Standard or Amazon S3 Standard-IA. The minimum storage duration is 30 days before you can transition objects from Amazon S3 Standard to Amazon S3 One Zone-IA. Amazon S3 One Zone-IA offers the same high durability, high throughput, and low latency of Amazon S3 Standard, with a low per GB storage price and per GB retrieval fee. S3 Storage Classes can be configured at the object level, and a single bucket can contain objects stored across Amazon S3 Standard, Amazon S3 Intelligent-Tiering, Amazon S3 Standard-IA, and Amazon S3 One Zone-IA. You can also use S3 Lifecycle policies to automatically transition objects between storage classes without any application changes. Constraints for Lifecycle storage class transitions: via - https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html Supported Amazon S3 lifecycle transitions: via - https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html Incorrect options: Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 7 days Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 7 days As mentioned earlier, the minimum storage duration is 30 days before you can transition objects from Amazon S3 Standard to Amazon S3 One Zone-IA or Amazon S3 Standard-IA, so both these options are added as distractors. Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days - Amazon S3 Standard-IA is for data that is accessed less frequently, but requires rapid access when needed. S3 Standard-IA offers the high durability, high throughput, and low latency of Amazon S3 Standard, with a low per GB storage price and per GB retrieval fee. This combination of low cost and high performance makes Amazon S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files. But, it costs more than Amazon S3 One Zone-IA because of the redundant storage across Availability Zones (AZs). As the data is re-creatable, so you don't need to incur this additional cost. References: https://aws.amazon.com/s3/storage-classes/ https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A financial services company uses Amazon GuardDuty for analyzing its AWS account metadata to meet the compliance guidelines. However, the company has now decided to stop using Amazon GuardDuty service. All the existing findings have to be deleted and cannot persist anywhere on AWS Cloud. Which of the following techniques will help the company meet this requirement?",
      "options": [
        "A. Suspend the service in the general settings",
        "B. De-register the service under services tab",
        "C. Raise a service request with Amazon to completely delete the data from all their backups",
        "D. Disable the service in the general settings"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: Amazon GuardDuty offers threat detection that enables you to continuously monitor and protect your AWS accounts, workloads, and data stored in Amazon S3. GuardDuty analyzes continuous streams of meta-data generated from your account and network activity found in AWS CloudTrail Events, Amazon VPC Flow Logs, and DNS Logs. It also uses integrated threat intelligence such as known malicious IP addresses, anomaly detection, and machine learning to identify threats more accurately. Disable the service in the general settings Disabling the service will delete all remaining data, including your findings and configurations before relinquishing the service permissions and resetting the service. So, this is the correct option for our use case. Incorrect options: Suspend the service in the general settings - You can stop Amazon GuardDuty from analyzing your data sources at any time by choosing to suspend the service in the general settings. This will immediately stop the service from analyzing data, but does not delete your existing findings or configurations. De-register the service under services tab - This is a made-up option, used only as a distractor. Raise a service request with Amazon to completely delete the data from all their backups - There is no need to create a service request as you can delete the existing findings by disabling the service. Reference: https://aws.amazon.com/guardduty/faqs/",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A retail company uses Amazon Elastic Compute Cloud (Amazon EC2) instances, Amazon API Gateway, Amazon RDS, Elastic Load Balancer and Amazon CloudFront services. To improve the security of these services, the Risk Advisory group has suggested a feasibility check for using the Amazon GuardDuty service. Which of the following would you identify as data sources supported by Amazon GuardDuty?",
      "options": [
        "A. VPC Flow Logs, Amazon API Gateway logs, Amazon S3 access logs",
        "B. VPC Flow Logs, Domain Name System (DNS) logs, AWS CloudTrail events",
        "C. Elastic Load Balancing logs, Domain Name System (DNS) logs, AWS CloudTrail events",
        "D. Amazon CloudFront logs, Amazon API Gateway logs, AWS CloudTrail events"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: VPC Flow Logs, Domain Name System (DNS) logs, AWS CloudTrail events Amazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts, workloads, and data stored in Amazon S3. With the cloud, the collection and aggregation of account and network activities is simplified, but it can be time-consuming for security teams to continuously analyze event log data for potential threats. With GuardDuty, you now have an intelligent and cost-effective option for continuous threat detection in AWS. The service uses machine learning, anomaly detection, and integrated threat intelligence to identify and prioritize potential threats. Amazon GuardDuty analyzes tens of billions of events across multiple AWS data sources, such as AWS CloudTrail events, Amazon VPC Flow Logs, and DNS logs. With a few clicks in the AWS Management Console, GuardDuty can be enabled with no software or hardware to deploy or maintain. By integrating with Amazon EventBridge Events, GuardDuty alerts are actionable, easy to aggregate across multiple accounts, and straightforward to push into existing event management and workflow systems. How Amazon GuardDuty works: via - https://aws.amazon.com/guardduty/ Incorrect options: VPC Flow Logs, Amazon API Gateway logs, Amazon S3 access logs Elastic Load Balancing logs, Domain Name System (DNS) logs, AWS CloudTrail events Amazon CloudFront logs, Amazon API Gateway logs, AWS CloudTrail events These three options contradict the explanation provided above, so these options are incorrect. Reference: https://aws.amazon.com/guardduty/",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A telecom company operates thousands of hardware devices like switches, routers, cables, etc. The real-time status data for these devices must be fed into a communications application for notifications. Simultaneously, another analytics application needs to read the same real-time status data and analyze all the connecting lines that may go down because of any device failures. As an AWS Certified Solutions Architect – Associate, which of the following solutions would you suggest, so that both the applications can consume the real-time status data concurrently?",
      "options": [
        "A. Amazon Simple Notification Service (SNS)",
        "B. Amazon Simple Queue Service (SQS) with Amazon Simple Notification Service (SNS)",
        "C. Amazon Kinesis Data Streams",
        "D. Amazon Simple Queue Service (SQS) with Amazon Simple Email Service (Amazon SES)"
      ],
      "correct_answer": "C",
      "explanation": "Correct option: Amazon Kinesis Data Streams Amazon Kinesis Data Streams enables real-time processing of streaming big data. It provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications. The Amazon Kinesis Client Library (KCL) delivers all records for a given partition key to the same record processor, making it easier to build multiple applications reading from the same Amazon Kinesis data stream (for example, to perform counting, aggregation, and filtering). AWS recommends Amazon Kinesis Data Streams for use cases with requirements that are similar to the following: Routing related records to the same record processor (as in streaming MapReduce). For example, counting and aggregation are simpler when all records for a given key are routed to the same record processor. Ordering of records. For example, you want to transfer log data from the application host to the processing/archival host while maintaining the order of log statements. Ability for multiple applications to consume the same stream concurrently. For example, you have one application that updates a real-time dashboard and another that archives data to Amazon Redshift. You want both applications to consume data from the same stream concurrently and independently. Ability to consume records in the same order a few hours later. For example, you have a billing application and an audit application that runs a few hours behind the billing application. Because Amazon Kinesis Data Streams stores data for up to 365 days, you can run the audit application up to 365 days behind the billing application. Incorrect options: Amazon Simple Notification Service (SNS) - Amazon Simple Notification Service (SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. Amazon SNS provides topics for high-throughput, push-based, many-to-many messaging. Amazon SNS is a notification service and cannot be used for real-time processing of data. Amazon Simple Queue Service (SQS) with Amazon Simple Notification Service (SNS) - Amazon Simple Queue Service (Amazon SQS) offers a reliable, highly scalable hosted queue for storing messages as they travel between computers. Amazon SQS lets you easily move data between distributed application components and helps you build applications in which messages are processed independently (with message-level ack/fail semantics), such as automated workflows. Since multiple applications need to consume the same data stream concurrently, Kinesis is a better choice when compared to the combination of SQS with SNS. Amazon Simple Queue Service (SQS) with Amazon Simple Email Service (Amazon SES) - As discussed above, Amazon Kinesis is a better option for this use case in comparison to Amazon SQS. Also, Amazon SES does not fit this use-case. Hence, this option is an incorrect answer. Reference: https://aws.amazon.com/kinesis/data-streams/faqs/",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A company manages a multi-tier social media application that runs on Amazon Elastic Compute Cloud (Amazon EC2) instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones (AZs) and use an Amazon Aurora database. As an AWS Certified Solutions Architect – Associate, you have been tasked to make the application more resilient to periodic spikes in request rates. Which of the following solutions would you recommend for the given use-case? (Select two)",
      "options": [
        "A. Use Amazon Aurora Replica",
        "B. Use AWS Shield",
        "C. Use AWS Global Accelerator",
        "D. Use AWS Direct Connect",
        "E. Use Amazon CloudFront distribution in front of the Application Load Balancer"
      ],
      "correct_answer": "A",
      "explanation": "Correct options: You can use Amazon Aurora replicas and Amazon CloudFront distribution to make the application more resilient to spikes in request rates. Use Amazon Aurora Replica Amazon Aurora Replicas have two main purposes. You can issue queries to them to scale the read operations for your application. You typically do so by connecting to the reader endpoint of the cluster. That way, Aurora can spread the load for read-only connections across as many Aurora Replicas as you have in the cluster. Amazon Aurora Replicas also help to increase availability. If the writer instance in a cluster becomes unavailable, Aurora automatically promotes one of the reader instances to take its place as the new writer. Up to 15 Aurora Replicas can be distributed across the Availability Zones (AZs) that a DB cluster spans within an AWS Region. Use Amazon CloudFront distribution in front of the Application Load Balancer Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment. CloudFront points of presence (POPs) (edge locations) make sure that popular content can be served quickly to your viewers. Amazon CloudFront also has regional edge caches that bring more of your content closer to your viewers, even when the content is not popular enough to stay at a POP, to help improve performance for that content. Amazon CloudFront offers an origin failover feature to help support your data resiliency needs. Amazon CloudFront is a global service that delivers your content through a worldwide network of data centers called edge locations or points of presence (POPs). If your content is not already cached in an edge location, Amazon CloudFront retrieves it from an origin that you've identified as the source for the definitive version of the content. Incorrect options: Use AWS Shield - AWS Shield is a managed Distributed Denial of Service (DDoS) protection service that safeguards applications running on AWS. AWS Shield provides always-on detection and automatic inline mitigations that minimize application downtime and latency. There are two tiers of AWS Shield - Standard and Advanced. AWS Shield cannot be used to improve application resiliency to handle spikes in traffic. Use AWS Global Accelerator - AWS Global Accelerator is a service that improves the availability and performance of your applications with local or global users. It provides static IP addresses that act as a fixed entry point to your application endpoints in a single or multiple AWS Regions, such as your Application Load Balancers, Network Load Balancers or Amazon EC2 instances. Amazon Global Accelerator is a good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or Voice over IP, as well as for HTTP use cases that specifically require static IP addresses or deterministic, fast regional failover. Since Amazon CloudFront is better for improving application resiliency to handle spikes in traffic, so this option is ruled out. Use AWS Direct Connect - AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations. Using industry-standard 802.1q VLANs, this dedicated connection can be partitioned into multiple virtual interfaces. AWS Direct Connect does not involve the Internet; instead, it uses dedicated, private network connections between your intranet and Amazon VPC. AWS Direct Connect cannot be used to improve application resiliency to handle spikes in traffic. References: https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/disaster-recovery-resiliency.html https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html https://aws.amazon.com/global-accelerator/faqs/ https://docs.aws.amazon.com/global-accelerator/latest/dg/disaster-recovery-resiliency.html",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A geological research agency maintains the seismological data for the last 100 years. The data has a velocity of 1GB per minute. You would like to store the data with only the most relevant attributes to build a predictive model for earthquakes. What AWS services would you use to build the most cost-effective solution with the LEAST amount of infrastructure maintenance?",
      "options": [
        "A. Ingest the data in Amazon Kinesis Data Firehose and use an intermediary AWS Lambda function to filter and transform the incoming stream before the output is dumped on Amazon S3",
        "B. Ingest the data in Amazon Kinesis Data Analytics and use SQL queries to filter and transform the data before writing to Amazon S3",
        "C. Ingest the data in Amazon Kinesis Data Streams and use an intermediary AWS Lambda function to filter and transform the incoming stream before the output is dumped on Amazon S3",
        "D. Ingest the data in a Spark Streaming Cluster on Amazon EMR and use Spark Streaming transformations before writing to Amazon S3"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Ingest the data in Amazon Kinesis Data Firehose and use an intermediary AWS Lambda function to filter and transform the incoming stream before the output is dumped on Amazon S3 Amazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon OpenSearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards you’re already using today. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security. Amazon Kinesis Data Firehose Overview via - https://aws.amazon.com/kinesis/data-firehose/ The correct option is to ingest the data in Amazon Kinesis Data Firehose and use a AWS Lambda function to filter and transform the incoming data before the output is dumped on Amazon S3. This way you only need to store a sliced version of the data with only the relevant data attributes required for your model. Also it should be noted that this solution is entirely serverless and requires no infrastructure maintenance. Amazon Kinesis Data Firehose to Amazon S3: via - https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html Incorrect options: Ingest the data in Amazon Kinesis Data Analytics and use SQL queries to filter and transform the data before writing to Amazon S3 - Amazon Kinesis Data Analytics is the easiest way to analyze streaming data in real-time. Kinesis Data Analytics enables you to easily and quickly build queries and sophisticated streaming applications in three simple steps: setup your streaming data sources, write your queries or streaming applications, and set up your destination for processed data. Kinesis Data Analytics cannot directly ingest data from the source as it ingests data either from Kinesis Data Streams or Kinesis Data Firehose, so this option is ruled out. Ingest the data in Amazon Kinesis Data Streams and use an intermediary AWS Lambda function to filter and transform the incoming stream before the output is dumped on Amazon S3 - Amazon Kinesis Data Streams (KDS) is a massively scalable, highly durable data ingestion and processing service optimized for streaming data. Amazon Kinesis Data Streams is integrated with a number of AWS services, including Amazon Kinesis Data Firehose for near real-time transformation. Kinesis Data Streams cannot directly write the output to Amazon S3. Unlike Amazon Kinesis Data Firehose, KDS does not offer a ready-made integration via an intermediary AWS Lambda function to reliably dump data into Amazon S3. You will need to do a lot of custom coding to get the AWS Lambda function to process the incoming stream and then store the transformed output to Amazon S3 with the constraint that the buffer is maintained reliably and no transformed data is lost. So this option is incorrect. Ingest the data in a Spark Streaming Cluster on Amazon EMR and use Spark Streaming transformations before writing to Amazon S3 - Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. Amazon EMR uses Hadoop, an open-source framework, to distribute your data and processing across a resizable cluster of Amazon EC2 instances. Using an EMR cluster would imply managing the underlying infrastructure so it’s ruled out because the correct solution for the given use-case should require the least amount of infrastructure maintenance. Reference: https://aws.amazon.com/kinesis/data-firehose/",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A leading social media analytics company is contemplating moving its dockerized application stack into AWS Cloud. The company is not sure about the pricing for using Amazon Elastic Container Service (Amazon ECS) with the EC2 launch type compared to the Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type. Which of the following is correct regarding the pricing for these two services?",
      "options": [
        "A. Amazon ECS with EC2 launch type is charged based on EC2 instances and EBS volumes used. Amazon ECS with Fargate launch type is charged based on vCPU and memory resources that the containerized application requests",
        "B. Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are just charged based on Elastic Container Service used per hour",
        "C. Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on vCPU and memory resources that the containerized application requests",
        "D. Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on Amazon EC2 instances and Amazon EBS Elastic Volumes used"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Amazon ECS with EC2 launch type is charged based on EC2 instances and EBS volumes used. Amazon ECS with Fargate launch type is charged based on vCPU and memory resources that the containerized application requests Amazon Elastic Container Service (Amazon ECS) is a fully managed container orchestration service. ECS allows you to easily run, scale, and secure Docker container applications on AWS. Amazon ECS Overview: via - https://aws.amazon.com/ecs/ With the Fargate launch type, you pay for the amount of vCPU and memory resources that your containerized application requests. vCPU and memory resources are calculated from the time your container images are pulled until the Amazon ECS Task terminates, rounded up to the nearest second. With the EC2 launch type, there is no additional charge for the EC2 launch type. You pay for AWS resources (e.g. EC2 instances or EBS volumes) you create to store and run your application. Incorrect options: Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on vCPU and memory resources that the containerized application requests Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on Amazon EC2 instances and Amazon EBS Elastic Volumes used As mentioned above - with the Fargate launch type, you pay for the amount of vCPU and memory resources. With EC2 launch type, you pay for AWS resources (e.g. EC2 instances or EBS volumes). Hence both these options are incorrect. Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are just charged based on Elastic Container Service used per hour This is a made-up option and has been added as a distractor. References: https://aws.amazon.com/ecs/pricing/",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An audit department generates and accesses the audit reports only twice in a financial year. The department uses AWS Step Functions to orchestrate the report creating process that has failover and retry scenarios built into the solution. The underlying data to create these audit reports is stored on Amazon S3, runs into hundreds of Terabytes and should be available with millisecond latency. As an AWS Certified Solutions Architect – Associate, which is the MOST cost-effective storage class that you would recommend to be used for this use-case?",
      "options": [
        "A. Amazon S3 Intelligent-Tiering (S3 Intelligent-Tiering)",
        "B. Amazon S3 Standard-Infrequent Access (S3 Standard-IA)",
        "C. Amazon S3 Standard",
        "D. Amazon S3 Glacier Deep Archive"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Amazon S3 Standard-Infrequent Access (S3 Standard-IA) Since the data is accessed only twice in a financial year but needs rapid access when required, the most cost-effective storage class for this use-case is Amazon S3 Standard-IA. S3 Standard-IA storage class is for data that is accessed less frequently but requires rapid access when needed. S3 Standard-IA matches the high durability, high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval fee. Amazon Standard-IA is designed for 99.9% availability compared to 99.99% availability of Amazon S3 Standard. However, the report creation process has failover and retry scenarios built into the workflow, so in case the data is not available owing to the 99.9% availability of Amazon S3 Standard-IA, the job will be auto re-invoked till data is successfully retrieved. Therefore this is the correct option. Amazon S3 Storage Classes Overview: via - https://aws.amazon.com/s3/storage-classes/ Incorrect options: Amazon S3 Standard - Amazon S3 Standard offers high durability, availability, and performance object storage for frequently accessed data. As described above, Amazon S3 Standard-IA storage is a better fit than Amazon S3 Standard, hence using S3 standard is ruled out for the given use-case. Amazon S3 Intelligent-Tiering (S3 Intelligent-Tiering) - For a small monthly object monitoring and automation charge, Amazon S3 Intelligent-Tiering monitors access patterns and automatically moves objects that have not been accessed to lower-cost access tiers. The Amazon S3 Intelligent-Tiering storage class is designed to optimize costs by automatically moving data to the most cost-effective access tier, without performance impact or operational overhead. S3 Standard-IA matches the high durability, high throughput, and low latency of S3 Intelligent-Tiering, with a low per GB storage price and per GB retrieval fee. Moreover, Standard-IA has the same availability as that of Amazon S3 Intelligent-Tiering. So, it's cost-efficient to use S3 Standard-IA instead of S3 Intelligent-Tiering. Amazon S3 Glacier Deep Archive - Amazon S3 Glacier Deep Archive is a secure, durable, and low-cost storage class for data archiving. Amazon S3 Glacier Deep Archive does not support millisecond latency, so this option is ruled out. For more details on the durability, availability, cost and access latency - please review this reference link: https://aws.amazon.com/s3/storage-classes",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A new DevOps engineer has joined a large financial services company recently. As part of his onboarding, the IT department is conducting a review of the checklist for tasks related to AWS Identity and Access Management (AWS IAM). As an AWS Certified Solutions Architect – Associate, which best practices would you recommend (Select two)?",
      "options": [
        "A. Create a minimum number of accounts and share these account credentials among employees",
        "B. Grant maximum privileges to avoid assigning privileges again",
        "C. Enable AWS Multi-Factor Authentication (AWS MFA) for privileged users",
        "D. Use user credentials to provide access specific permissions for Amazon EC2 instances",
        "E. Configure AWS CloudTrail to log all AWS Identity and Access Management (AWS IAM) actions"
      ],
      "correct_answer": "C",
      "explanation": "Correct options: Enable AWS Multi-Factor Authentication (AWS MFA) for privileged users As per the AWS best practices, it is better to enable Multi Factor Authentication (MFA) for privileged users via an MFA-enabled mobile device or hardware MFA token. Configure AWS CloudTrail to log all AWS Identity and Access Management (AWS IAM) actions AWS recommends to turn on AWS CloudTrail to log all IAM actions for monitoring and audit purposes. Incorrect options: Create a minimum number of accounts and share these account credentials among employees - AWS recommends that user account credentials should not be shared between users. So, this option is incorrect. Grant maximum privileges to avoid assigning privileges again - AWS recommends granting the least privileges required to complete a certain job and avoid giving excessive privileges which can be misused. So, this option is incorrect. Use user credentials to provide access specific permissions for Amazon EC2 instances - It is highly recommended to use roles to grant access permissions for EC2 instances working on different AWS services. So, this option is incorrect. References: https://aws.amazon.com/iam/ https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html https://aws.amazon.com/cloudtrail/faqs/",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An organization wants to delegate access to a set of users from the development environment so that they can access some resources in the production environment which is managed under another AWS account. As a solutions architect, which of the following steps would you recommend?",
      "options": [
        "A. Create new IAM user credentials for the production environment and share these credentials with the set of users from the development environment",
        "B. Create a new IAM role with the required permissions to access the resources in the production environment. The users can then assume this IAM role while accessing the resources from the production environment",
        "C. It is not possible to access cross-account resources",
        "D. Both IAM roles and IAM users can be used interchangeably for cross-account access"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Create a new IAM role with the required permissions to access the resources in the production environment. The users can then assume this IAM role while accessing the resources from the production environment IAM roles allow you to delegate access to users or services that normally don't have access to your organization's AWS resources. IAM users or AWS services can assume a role to obtain temporary security credentials that can be used to make AWS API calls. Consequently, you don't have to share long-term credentials for access to a resource. Using IAM roles, it is possible to access cross-account resources. Incorrect options: Create new IAM user credentials for the production environment and share these credentials with the set of users from the development environment - There is no need to create new IAM user credentials for the production environment, as you can use IAM roles to access cross-account resources. It is not possible to access cross-account resources - You can use IAM roles to access cross-account resources. Both IAM roles and IAM users can be used interchangeably for cross-account access - IAM roles and IAM users are separate IAM entities and should not be mixed. Only IAM roles can be used to access cross-account resources. Reference: https://aws.amazon.com/iam/features/manage-roles/",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A news network uses Amazon Simple Storage Service (Amazon S3) to aggregate the raw video footage from its reporting teams across the US. The news network has recently expanded into new geographies in Europe and Asia. The technical teams at the overseas branch offices have reported huge delays in uploading large video files to the destination Amazon S3 bucket. Which of the following are the MOST cost-effective options to improve the file upload speed into Amazon S3 (Select two)",
      "options": [
        "A. Create multiple AWS Site-to-Site VPN connections between the AWS Cloud and branch offices in Europe and Asia. Use these VPN connections for faster file uploads into Amazon S3",
        "B. Create multiple AWS Direct Connect connections between the AWS Cloud and branch offices in Europe and Asia. Use the direct connect connections for faster file uploads into Amazon S3",
        "C. Use Amazon S3 Transfer Acceleration (Amazon S3TA) to enable faster file uploads into the destination S3 bucket",
        "D. Use multipart uploads for faster file uploads into the destination Amazon S3 bucket",
        "E. Use AWS Global Accelerator for faster file uploads into the destination Amazon S3 bucket"
      ],
      "correct_answer": "C",
      "explanation": "Correct options: Use Amazon S3 Transfer Acceleration (Amazon S3TA) to enable faster file uploads into the destination S3 bucket Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket. Amazon S3TA takes advantage of Amazon CloudFront’s globally distributed edge locations. As the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path. Use multipart uploads for faster file uploads into the destination Amazon S3 bucket Multipart upload allows you to upload a single object as a set of parts. Each part is a contiguous portion of the object's data. You can upload these object parts independently and in any order. If transmission of any part fails, you can retransmit that part without affecting other parts. After all parts of your object are uploaded, Amazon S3 assembles these parts and creates the object. In general, when your object size reaches 100 MB, you should consider using multipart uploads instead of uploading the object in a single operation. Multipart upload provides improved throughput, therefore it facilitates faster file uploads. Incorrect options: Create multiple AWS Direct Connect connections between the AWS Cloud and branch offices in Europe and Asia. Use the direct connect connections for faster file uploads into Amazon S3 - AWS Direct Connect is a cloud service solution that makes it easy to establish a dedicated network connection from your premises to AWS. AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations. Direct connect takes significant time (several months) to be provisioned and is an overkill for the given use-case. Create multiple AWS Site-to-Site VPN connections between the AWS Cloud and branch offices in Europe and Asia. Use these VPN connections for faster file uploads into Amazon S3 - AWS Site-to-Site VPN enables you to securely connect your on-premises network or branch office site to your Amazon Virtual Private Cloud (Amazon VPC). You can securely extend your data center or branch office network to the cloud with an AWS Site-to-Site VPN connection. A VPC VPN Connection utilizes IPSec to establish encrypted network connectivity between your intranet and Amazon VPC over the Internet. VPN Connections are a good solution if you have low to modest bandwidth requirements and can tolerate the inherent variability in Internet-based connectivity. Site-to-site VPN will not help in accelerating the file transfer speeds into S3 for the given use-case. Use AWS Global Accelerator for faster file uploads into the destination Amazon S3 bucket - AWS Global Accelerator is a service that improves the availability and performance of your applications with local or global users. It provides static IP addresses that act as a fixed entry point to your application endpoints in a single or multiple AWS Regions, such as your Application Load Balancers, Network Load Balancers or Amazon EC2 instances. AWS Global Accelerator will not help in accelerating the file transfer speeds into S3 for the given use-case. References: https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "The IT department at a consulting firm is conducting a training workshop for new developers. As part of an evaluation exercise on Amazon S3, the new developers were asked to identify the invalid storage class lifecycle transitions for objects stored on Amazon S3. Can you spot the INVALID lifecycle transitions from the options below? (Select two)",
      "options": [
        "A. Amazon S3 Intelligent-Tiering => Amazon S3 Standard",
        "B. Amazon S3 One Zone-IA => Amazon S3 Standard-IA",
        "C. Amazon S3 Standard => Amazon S3 Intelligent-Tiering",
        "D. Amazon S3 Standard-IA => Amazon S3 Intelligent-Tiering",
        "E. Amazon S3 Standard-IA => Amazon S3 One Zone-IA"
      ],
      "correct_answer": "A",
      "explanation": "Correct options: As the question wants to know about the INVALID lifecycle transitions, the following options are the correct answers - Amazon S3 Intelligent-Tiering => Amazon S3 Standard Amazon S3 One Zone-IA => Amazon S3 Standard-IA Following are the unsupported life cycle transitions for S3 storage classes - Any storage class to the Amazon S3 Standard storage class. Any storage class to the Reduced Redundancy storage class. The Amazon S3 Intelligent-Tiering storage class to the Amazon S3 Standard-IA storage class. The Amazon S3 One Zone-IA storage class to the Amazon S3 Standard-IA or Amazon S3 Intelligent-Tiering storage classes. Incorrect options: Amazon S3 Standard => Amazon S3 Intelligent-Tiering Amazon S3 Standard-IA => Amazon S3 Intelligent-Tiering Amazon S3 Standard-IA => Amazon S3 One Zone-IA Here are the supported life cycle transitions for S3 storage classes - The S3 Standard storage class to any other storage class. Any storage class to the S3 Glacier or S3 Glacier Deep Archive storage classes. The S3 Standard-IA storage class to the S3 Intelligent-Tiering or S3 One Zone-IA storage classes. The S3 Intelligent-Tiering storage class to the S3 One Zone-IA storage class. The S3 Glacier storage class to the S3 Glacier Deep Archive storage class. Amazon S3 supports a waterfall model for transitioning between storage classes, as shown in the diagram below: via - https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html Reference: https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A healthcare startup needs to enforce compliance and regulatory guidelines for objects stored in Amazon S3. One of the key requirements is to provide adequate protection against accidental deletion of objects. As a solutions architect, what are your recommendations to address these guidelines? (Select two) ?",
      "options": [
        "A. Establish a process to get managerial approval for deleting Amazon S3 objects",
        "B. Create an event trigger on deleting any Amazon S3 object. The event invokes an Amazon Simple Notification Service (Amazon SNS) notification via email to the IT manager",
        "C. Enable versioning on the Amazon S3 bucket",
        "D. Change the configuration on Amazon S3 console so that the user needs to provide additional confirmation while deleting any Amazon S3 object",
        "E. Enable multi-factor authentication (MFA) delete on the Amazon S3 bucket"
      ],
      "correct_answer": "C",
      "explanation": "Correct options: Enable versioning on the Amazon S3 bucket Versioning is a means of keeping multiple variants of an object in the same bucket. You can use versioning to preserve, retrieve, and restore every version of every object stored in your Amazon S3 bucket. Versioning-enabled buckets enable you to recover objects from accidental deletion or overwrite. For example: If you overwrite an object, it results in a new object version in the bucket. You can always restore the previous version. If you delete an object, instead of removing it permanently, Amazon S3 inserts a delete marker, which becomes the current object version. You can always restore the previous version. Hence, this is the correct option. Versioning Overview: via - https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html Enable multi-factor authentication (MFA) delete on the Amazon S3 bucket To provide additional protection, multi-factor authentication (MFA) delete can be enabled. MFA delete requires secondary authentication to take place before objects can be permanently deleted from an Amazon S3 bucket. Hence, this is the correct option. Incorrect options: Create an event trigger on deleting any Amazon S3 object. The event invokes an Amazon Simple Notification Service (Amazon SNS) notification via email to the IT manager - Sending an event trigger after object deletion does not meet the objective of preventing object deletion by mistake because the object has already been deleted. So, this option is incorrect. Establish a process to get managerial approval for deleting Amazon S3 objects - This option for getting managerial approval is just a distractor. Change the configuration on Amazon S3 console so that the user needs to provide additional confirmation while deleting any Amazon S3 object - There is no provision to set up Amazon S3 configuration to ask for additional confirmation before deleting an object. This option is incorrect. References: https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMFADelete.html",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A company runs a data processing workflow that takes about 60 minutes to complete. The workflow can withstand disruptions and it can be started and stopped multiple times. Which is the most cost-effective solution to build a solution for the workflow?",
      "options": [
        "A. Use AWS Lambda function to run the workflow processes",
        "B. Use Amazon EC2 on-demand instances to run the workflow processes",
        "C. Use Amazon EC2 reserved instances to run the workflow processes",
        "D. Use Amazon EC2 spot instances to run the workflow processes"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: Use Amazon EC2 spot instances to run the workflow processes Amazon EC2 instance types: via - https://aws.amazon.com/ec2/pricing/ Amazon EC2 Spot instances allow you to request spare Amazon EC2 computing capacity for up to 90% off the On-Demand price. Spot instances are recommended for: Applications that have flexible start and end times Applications that are feasible only at very low compute prices Users with urgent computing needs for large amounts of additional capacity For the given use case, spot instances offer the most cost-effective solution as the workflow can withstand disruptions and can be started and stopped multiple times. For example, considering a process that runs for an hour and needs about 1024 MB of memory, spot instance pricing for a t2.micro instance (having 1024 MB of RAM) is $0.0035 per hour. Spot instance pricing: via - https://aws.amazon.com/ec2/spot/pricing/ Contrast this with the pricing of a Lambda function (having 1024 MB of allocated memory), which comes out to $0.0000000167 per 1ms or $0.06 per hour ($0.0000000167 * 1000 * 60 * 60 per hour). AWS Lambda function pricing: via - https://aws.amazon.com/lambda/pricing/ Thus, a spot instance turns out to be about 20 times cost effective than a Lambda function to meet the requirements of the given use case. Incorrect options: Use AWS Lambda function to run the workflow processes - As mentioned in the explanation above, a Lambda function turns out to be 20 times more expensive than a spot instance to meet the workflow requirements of the given use case, so this option is incorrect. You should also note that the maximum execution time of a Lambda function is 15 minutes, so the workflow process would be disrupted for sure. On the other hand, it is certainly possible that the workflow process can be completed in a single run on the spot instance (the average frequency of stop instance interruption across all Regions and instance types is Use Amazon EC2 on-demand instances to run the workflow processes Use Amazon EC2 reserved instances to run the workflow processes You should note that both on-demand and reserved instances are more expensive than spot instances. In addition, reserved instances have a term of 1 year or 3 years, so they are not suited for the given workflow. Therefore, both these options are incorrect. References: https://aws.amazon.com/ec2/pricing/ https://aws.amazon.com/ec2/spot/pricing/ https://aws.amazon.com/lambda/pricing/ https://aws.amazon.com/ec2/spot/instance-advisor/",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A US-based healthcare startup is building an interactive diagnostic tool for COVID-19 related assessments. The users would be required to capture their personal health records via this tool. As this is sensitive health information, the backup of the user data must be kept encrypted in Amazon Simple Storage Service (Amazon S3). The startup does not want to provide its own encryption keys but still wants to maintain an audit trail of when an encryption key was used and by whom. Which of the following is the BEST solution for this use-case?",
      "options": [
        "A. Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the user data on Amazon S3",
        "B. Use server-side encryption with AWS Key Management Service keys (SSE-KMS) to encrypt the user data on Amazon S3",
        "C. Use server-side encryption with customer-provided keys (SSE-C) to encrypt the user data on Amazon S3",
        "D. Use client-side encryption with client provided keys and then upload the encrypted user data to Amazon S3"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Use server-side encryption with AWS Key Management Service keys (SSE-KMS) to encrypt the user data on Amazon S3 AWS Key Management Service (AWS KMS) is a service that combines secure, highly available hardware and software to provide a key management system scaled for the cloud. When you use server-side encryption with AWS KMS (SSE-KMS), you can specify a customer-managed CMK that you have already created. SSE-KMS provides you with an audit trail that shows when your CMK was used and by whom. Therefore SSE-KMS is the correct solution for this use-case. Server Side Encryption in S3: via - https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html Incorrect options: Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the user data on Amazon S3 - When you use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key. However this option does not provide the ability to audit trail the usage of the encryption keys. Use server-side encryption with customer-provided keys (SSE-C) to encrypt the user data on Amazon S3 - With Server-Side Encryption with Customer-Provided Keys (SSE-C), you manage the encryption keys and Amazon S3 manages the encryption, as it writes to disks, and decryption when you access your objects. However this option does not provide the ability to audit trail the usage of the encryption keys. Use client-side encryption with client provided keys and then upload the encrypted user data to Amazon S3 - Using client-side encryption is ruled out as the startup does not want to provide the encryption keys. References: https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingClientSideEncryption.html https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An ivy-league university is assisting NASA to find potential landing sites for exploration vehicles of unmanned missions to our neighboring planets. The university uses High Performance Computing (HPC) driven application architecture to identify these landing sites. Which of the following Amazon EC2 instance topologies should this application be deployed on?",
      "options": [
        "A. The Amazon EC2 instances should be deployed in a spread placement group so that there are no correlated failures",
        "B. The Amazon EC2 instances should be deployed in a partition placement group so that distributed workloads can be handled effectively",
        "C. The Amazon EC2 instances should be deployed in a cluster placement group so that the underlying workload can benefit from low network latency and high network throughput",
        "D. The Amazon EC2 instances should be deployed in an Auto Scaling group so that application meets high availability requirements"
      ],
      "correct_answer": "C",
      "explanation": "Correct option: The Amazon EC2 instances should be deployed in a cluster placement group so that the underlying workload can benefit from low network latency and high network throughput The key thing to understand in this question is that HPC workloads need to achieve low-latency network performance necessary for tightly-coupled node-to-node communication that is typical of HPC applications. Cluster placement groups pack instances close together inside an Availability Zone. These are recommended for applications that benefit from low network latency, high network throughput, or both. Therefore this option is the correct answer. Cluster Placement Group: via - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html Incorrect options: The Amazon EC2 instances should be deployed in a partition placement group so that distributed workloads can be handled effectively - A partition placement group spreads your instances across logical partitions such that groups of instances in one partition do not share the underlying hardware with groups of instances in different partitions. This strategy is typically used by large distributed and replicated workloads, such as Hadoop, Cassandra, and Kafka. A partition placement group can have a maximum of seven partitions per Availability Zone. Since a partition placement group can have partitions in multiple Availability Zones in the same region, therefore instances will not have low-latency network performance. Hence the partition placement group is not the right fit for HPC applications. Partition Placement Group: via - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html The Amazon EC2 instances should be deployed in a spread placement group so that there are no correlated failures - A spread placement group is a group of instances that are each placed on distinct racks, with each rack having its own network and power source. The instances are placed across distinct underlying hardware to reduce correlated failures. You can have a maximum of seven running instances per Availability Zone per group. Since a spread placement group can span multiple Availability Zones in the same Region, therefore instances will not have low-latency network performance. Hence spread placement group is not the right fit for HPC applications. Spread Placement Group: via - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html The Amazon EC2 instances should be deployed in an Auto Scaling group so that application meets high availability requirements - An Auto Scaling group contains a collection of Amazon EC2 instances that are treated as a logical grouping for the purposes of automatic scaling. You do not use Auto Scaling groups per se to meet HPC requirements. Reference: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An Electronic Design Automation (EDA) application produces massive volumes of data that can be divided into two categories. The 'hot data' needs to be both processed and stored quickly in a parallel and distributed fashion. The 'cold data' needs to be kept for reference with quick access for reads and updates at a low cost. Which of the following AWS services is BEST suited to accelerate the aforementioned chip design process?",
      "options": [
        "A. Amazon FSx for Windows File Server",
        "B. Amazon EMR",
        "C. Amazon FSx for Lustre",
        "D. AWS Glue"
      ],
      "correct_answer": "C",
      "explanation": "Correct option: Amazon FSx for Lustre Amazon FSx for Lustre makes it easy and cost-effective to launch and run the world’s most popular high-performance file system. It is used for workloads such as machine learning, high-performance computing (HPC), video processing, and financial modeling. The open-source Lustre file system is designed for applications that require fast storage – where you want your storage to keep up with your compute. FSx for Lustre integrates with Amazon S3, making it easy to process data sets with the Lustre file system. When linked to an S3 bucket, an FSx for Lustre file system transparently presents S3 objects as files and allows you to write changed data back to S3. FSx for Lustre provides the ability to both process the 'hot data' in a parallel and distributed fashion as well as easily store the 'cold data' on Amazon S3. Therefore this option is the BEST fit for the given problem statement. Incorrect options: Amazon FSx for Windows File Server - Amazon FSx for Windows File Server provides fully managed, highly reliable file storage that is accessible over the industry-standard Service Message Block (SMB) protocol. It is built on Windows Server, delivering a wide range of administrative features such as user quotas, end-user file restore, and Microsoft Active Directory (AD) integration. FSx for Windows does not allow you to present S3 objects as files and does not allow you to write changed data back to S3. Therefore you cannot reference the \"cold data\" with quick access for reads and updates at low cost. Hence this option is not correct. Amazon EMR - Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. Amazon EMR uses Hadoop, an open-source framework, to distribute your data and processing across a resizable cluster of Amazon EC2 instances. EMR does not offer the same storage and processing speed as FSx for Lustre. So it is not the right fit for the given high-performance workflow scenario. AWS Glue - AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. AWS Glue job is meant to be used for batch ETL data processing. AWS Glue does not offer the same storage and processing speed as FSx for Lustre. So it is not the right fit for the given high-performance workflow scenario. References: https://aws.amazon.com/fsx/lustre/ https://aws.amazon.com/fsx/windows/faqs/",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An IT company wants to review its security best-practices after an incident was reported where a new developer on the team was assigned full access to Amazon DynamoDB. The developer accidentally deleted a couple of tables from the production environment while building out a new feature. Which is the MOST effective way to address this issue so that such incidents do not recur?",
      "options": [
        "A. The CTO should review the permissions for each new developer's IAM user so that such incidents don't recur",
        "B. Remove full database access for all IAM users in the organization",
        "C. Only root user should have full database access in the organization",
        "D. Use permissions boundary to control the maximum permissions employees can grant to the IAM principals"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: Use permissions boundary to control the maximum permissions employees can grant to the IAM principals A permissions boundary can be used to control the maximum permissions employees can grant to the IAM principals (that is, users and roles) that they create and manage. As the IAM administrator, you can define one or more permissions boundaries using managed policies and allow your employee to create a principal with this boundary. The employee can then attach a permissions policy to this principal. However, the effective permissions of the principal are the intersection of the permissions boundary and permissions policy. As a result, the new principal cannot exceed the boundary that you defined. Therefore, using the permissions boundary offers the right solution for this use-case. Permission Boundary Example: via - https://aws.amazon.com/blogs/security/delegate-permission-management-to-developers-using-iam-permissions-boundaries/ Incorrect options: Remove full database access for all IAM users in the organization - It is not practical to remove full access for all IAM users in the organization because a select set of users need this access for database administration. So this option is not correct. The CTO should review the permissions for each new developer's IAM user so that such incidents don't recur - Likewise the CTO is not expected to review the permissions for each new developer's IAM user, as this is best done via an automated procedure. This option has been added as a distractor. Only root user should have full database access in the organization - As a best practice, the root user should not access the AWS account to carry out any administrative procedures. So this option is not correct. Reference: https://aws.amazon.com/blogs/security/delegate-permission-management-to-developers-using-iam-permissions-boundaries/",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "As part of a pilot program, a biotechnology company wants to integrate data files from its on-premises analytical application with AWS Cloud via an NFS interface. Which of the following AWS service is the MOST efficient solution for the given use-case?",
      "options": [
        "A. AWS Storage Gateway - File Gateway",
        "B. AWS Storage Gateway - Volume Gateway",
        "C. AWS Site-to-Site VPN",
        "D. AWS Storage Gateway - Tape Gateway"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: AWS Storage Gateway - File Gateway AWS Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage. The service provides three different types of gateways – Tape Gateway, File Gateway, and Volume Gateway – that seamlessly connect on-premises applications to cloud storage, caching data locally for low-latency access. AWS Storage Gateway's file interface, or file gateway, offers you a seamless way to connect to the cloud in order to store application data files and backup images as durable objects on Amazon S3 cloud storage. File gateway offers SMB or NFS-based access to data in Amazon S3 with local caching. As the company wants to integrate data files from its analytical instruments into AWS via an NFS interface, therefore AWS Storage Gateway - File Gateway is the correct answer. File Gateway Overview: via - https://docs.aws.amazon.com/storagegateway/latest/userguide/StorageGatewayConcepts.html Incorrect options: AWS Storage Gateway - Volume Gateway - You can configure the AWS Storage Gateway service as a Volume Gateway to present cloud-based iSCSI block storage volumes to your on-premises applications. Volume Gateway does not support NFS interface, so this option is not correct. AWS Storage Gateway - Tape Gateway - AWS Storage Gateway - Tape Gateway allows moving tape backups to the cloud. Tape Gateway does not support NFS interface, so this option is not correct. AWS Site-to-Site VPN - AWS Site-to-Site VPN enables you to securely connect your on-premises network or branch office site to your Amazon Virtual Private Cloud (Amazon VPC). You can securely extend your data center or branch office network to the cloud with an AWS Site-to-Site VPN (Site-to-Site VPN) connection. It uses internet protocol security (IPSec) communications to create encrypted VPN tunnels between two locations. You cannot use AWS Site-to-Site VPN to integrate data files via the NFS interface, so this option is not correct. References: https://aws.amazon.com/storagegateway/ https://aws.amazon.com/storagegateway/volume/ https://aws.amazon.com/storagegateway/file/ https://aws.amazon.com/storagegateway/vtl/",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A video analytics organization has been acquired by a leading media company. The analytics organization has 10 independent applications with an on-premises data footprint of about 70 Terabytes for each application. The CTO of the media company has set a timeline of two weeks to carry out the data migration from on-premises data center to AWS Cloud and establish connectivity. Which of the following are the MOST cost-effective options for completing the data transfer and establishing connectivity? (Select two)",
      "options": [
        "A. Order 10 AWS Snowball Edge Storage Optimized devices to complete the one-time data transfer",
        "B. Setup AWS Direct Connect to establish connectivity between the on-premises data center and AWS Cloud",
        "C. Setup AWS Site-to-Site VPN to establish on-going connectivity between the on-premises data center and AWS Cloud",
        "D. Order 1 AWS Snowmobile to complete the one-time data transfer",
        "E. Order 70 AWS Snowball Edge Storage Optimized devices to complete the one-time data transfer"
      ],
      "correct_answer": "A",
      "explanation": "Correct options: Order 10 AWS Snowball Edge Storage Optimized devices to complete the one-time data transfer AWS Snowball Edge Storage Optimized is the optimal choice if you need to securely and quickly transfer dozens of terabytes to petabytes of data to AWS. It provides up to 80 Terabytes of usable HDD storage, 40 vCPUs, 1 TB of SATA SSD storage, and up to 40 Gigabytes network connectivity to address large scale data transfer and pre-processing use cases. As each Snowball Edge Storage Optimized device can handle 80 Terabytes of data, you can order 10 such devices to take care of the data transfer for all applications. Exam Alert: The original Snowball devices were transitioned out of service and Snowball Edge Storage Optimized are now the primary devices used for data transfer. You may see the Snowball device on the exam, just remember that the original Snowball device had 80 Terabytes of storage space. Setup AWS Site-to-Site VPN to establish on-going connectivity between the on-premises data center and AWS Cloud AWS Site-to-Site VPN enables you to securely connect your on-premises network or branch office site to your Amazon Virtual Private Cloud (Amazon VPC). You can securely extend your data center or branch office network to the cloud with an AWS Site-to-Site VPN connection. A VPC VPN Connection utilizes IPSec to establish encrypted network connectivity between your intranet and Amazon VPC over the Internet. VPN Connections can be configured in minutes and are a good solution if you have an immediate need, have low to modest bandwidth requirements, and can tolerate the inherent variability in Internet-based connectivity. Therefore this option is the right fit for the given use-case as the connectivity can be easily established within the given timeframe. Incorrect options: Order 1 AWS Snowmobile to complete the one-time data transfer - Each AWS Snowmobile has a total capacity of up to 100 petabytes. To migrate large datasets of 10 petabytes or more in a single location, you should use AWS Snowmobile. For datasets less than 10 petabytes or distributed in multiple locations, you should use Snowball. So AWS Snowmobile is not the right fit for this use-case. Setup AWS Direct Connect to establish connectivity between the on-premises data center and AWS Cloud - AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations. Using industry-standard 802.1q VLANs, this dedicated connection can be partitioned into multiple virtual interfaces. AWS Direct Connect does not involve the Internet; instead, it uses dedicated, private network connections between your intranet and Amazon VPC. Direct Connect involves significant monetary investment and takes at least a month to set up, therefore it's not the correct fit for this use-case. Order 70 AWS Snowball Edge Storage Optimized devices to complete the one-time data transfer - As the data-transfer can be completed with just 10 AWS Snowball Edge Storage Optimized devices, there is no need to order 70 devices. References: https://aws.amazon.com/snowball/faqs/ https://aws.amazon.com/vpn/ https://aws.amazon.com/snowmobile/faqs/ https://aws.amazon.com/directconnect/",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A leading video streaming service delivers billions of hours of content from Amazon Simple Storage Service (Amazon S3) to customers around the world. Amazon S3 also serves as the data lake for its big data analytics solution. The data lake has a staging zone where intermediary query results are kept only for 24 hours. These results are also heavily referenced by other parts of the analytics pipeline. Which of the following is the MOST cost-effective strategy for storing this intermediary query data?",
      "options": [
        "A. Store the intermediary query results in Amazon S3 Standard storage class",
        "B. Store the intermediary query results in Amazon S3 Glacier Instant Retrieval storage class",
        "C. Store the intermediary query results in Amazon S3 Standard-Infrequent Access storage class",
        "D. Store the intermediary query results in Amazon S3 One Zone-Infrequent Access storage class"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Store the intermediary query results in Amazon S3 Standard storage class Amazon S3 Standard offers high durability, availability, and performance object storage for frequently accessed data. Because it delivers low latency and high throughput, S3 Standard is appropriate for a wide variety of use cases, including cloud applications, dynamic websites, content distribution, mobile and gaming applications, and big data analytics. As there is no minimum storage duration charge and no retrieval fee (remember that intermediary query results are heavily referenced by other parts of the analytics pipeline), this is the MOST cost-effective storage class amongst the given options. Incorrect options: Store the intermediary query results in Amazon S3 Glacier Instant Retrieval storage class - Amazon S3 Glacier Instant Retrieval delivers the fastest access to archive storage, with the same throughput and milliseconds access as the S3 Standard and S3 Standard-IA storage classes. S3 Glacier Instant Retrieval is ideal for archive data that needs immediate access, such as medical images, news media assets, or user-generated content archives. The minimum storage duration charge is 90 days, so this option is NOT cost-effective because intermediary query results need to be kept only for 24 hours. Hence this option is not correct. Store the intermediary query results in Amazon S3 Standard-Infrequent Access storage class - Amazon S3 Standard-IA is for data that is accessed less frequently but requires rapid access when needed. S3 Standard-IA offers high durability, high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval fee. This combination of low cost and high performance makes S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files. The minimum storage duration charge is 30 days, so this option is NOT cost-effective because intermediary query results need to be kept only for 24 hours. Hence this option is not correct. Store the intermediary query results in Amazon S3 One Zone-Infrequent Access storage class - Amazon S3 One Zone-IA is for data that is accessed less frequently but requires rapid access when needed. Unlike other S3 Storage Classes which store data in a minimum of three Availability Zones (AZs), S3 One Zone-IA stores data in a single AZ and costs 20% less than S3 Standard-IA. The minimum storage duration charge is 30 days, so this option is NOT cost-effective because intermediary query results need to be kept only for 24 hours. Hence this option is not correct. To summarize again, S3 Standard-IA and S3 One Zone-IA have a minimum storage duration charge of 30 days (so instead of 24 hours, you end up paying for 30 days). S3 Standard-IA and S3 One Zone-IA also have retrieval charges (as the results are heavily referenced by other parts of the analytics pipeline, so the retrieval costs would be pretty high). Therefore, these storage classes are not cost optimal for the given use-case. Reference: https://aws.amazon.com/s3/storage-classes/",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "The engineering team at an in-home fitness company is evaluating multiple in-memory data stores with the ability to power its on-demand, live leaderboard. The company's leaderboard requires high availability, low latency, and real-time processing to deliver customizable user data for the community of users working out together virtually from the comfort of their home. As a solutions architect, which of the following solutions would you recommend? (Select two)",
      "options": [
        "A. Power the on-demand, live leaderboard using Amazon DynamoDB as it meets the in-memory, high availability, low latency requirements",
        "B. Power the on-demand, live leaderboard using Amazon ElastiCache for Redis as it meets the in-memory, high availability, low latency requirements",
        "C. Power the on-demand, live leaderboard using Amazon DynamoDB with DynamoDB Accelerator (DAX) as it meets the in-memory, high availability, low latency requirements",
        "D. Power the on-demand, live leaderboard using Amazon Neptune as it meets the in-memory, high availability, low latency requirements",
        "E. Power the on-demand, live leaderboard using Amazon RDS for Aurora as it meets the in-memory, high availability, low latency requirements"
      ],
      "correct_answer": "B",
      "explanation": "Correct options: Power the on-demand, live leaderboard using Amazon ElastiCache for Redis as it meets the in-memory, high availability, low latency requirements Amazon ElastiCache for Redis is a blazing fast in-memory data store that provides sub-millisecond latency to power internet-scale real-time applications. Amazon ElastiCache for Redis is a great choice for real-time transactional and analytical processing use cases such as caching, chat/messaging, gaming leaderboards, geospatial, machine learning, media streaming, queues, real-time analytics, and session store. ElastiCache for Redis can be used to power the live leaderboard, so this option is correct. Amazon ElastiCache for Redis Overview: Power the on-demand, live leaderboard using Amazon DynamoDB with DynamoDB Accelerator (DAX) as it meets the in-memory, high availability, low latency requirements Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multiregion, multimaster, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications. DAX is a DynamoDB-compatible caching service that enables you to benefit from fast in-memory performance for demanding applications. So DynamoDB with DAX can be used to power the live leaderboard. Incorrect options: Power the on-demand, live leaderboard using Amazon Neptune as it meets the in-memory, high availability, low latency requirements - Amazon Neptune is a fast, reliable, fully-managed graph database service that makes it easy to build and run applications that work with highly connected datasets. Neptune is not an in-memory database, so this option is not correct. Power the on-demand, live leaderboard using Amazon DynamoDB as it meets the in-memory, high availability, low latency requirements - DynamoDB is not an in-memory database, so this option is not correct. Power the on-demand, live leaderboard using Amazon RDS for Aurora as it meets the in-memory, high availability, low latency requirements - Amazon Aurora is a MySQL and PostgreSQL-compatible relational database built for the cloud, that combines the performance and availability of traditional enterprise databases with the simplicity and cost-effectiveness of open source databases. Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 128TB per database instance. Aurora is not an in-memory database, so this option is not correct. References: https://aws.amazon.com/elasticache/ https://aws.amazon.com/elasticache/redis/ https://aws.amazon.com/dynamodb/dax/",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "The engineering team at a data analytics company has observed that its flagship application functions at its peak performance when the underlying Amazon Elastic Compute Cloud (Amazon EC2) instances have a CPU utilization of about 50%. The application is built on a fleet of Amazon EC2 instances managed under an Auto Scaling group. The workflow requests are handled by an internal Application Load Balancer that routes the requests to the instances. As a solutions architect, what would you recommend so that the application runs near its peak performance state?",
      "options": [
        "A. Configure the Auto Scaling group to use target tracking policy and set the CPU utilization as the target metric with a target value of 50%",
        "B. Configure the Auto Scaling group to use step scaling policy and set the CPU utilization as the target metric with a target value of 50%",
        "C. Configure the Auto Scaling group to use simple scaling policy and set the CPU utilization as the target metric with a target value of 50%",
        "D. Configure the Auto Scaling group to use a Amazon Cloudwatch alarm triggered on a CPU utilization threshold of 50%"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Configure the Auto Scaling group to use target tracking policy and set the CPU utilization as the target metric with a target value of 50% An Auto Scaling group contains a collection of Amazon EC2 instances that are treated as a logical grouping for the purposes of automatic scaling and management. An Auto Scaling group also enables you to use Amazon EC2 Auto Scaling features such as health check replacements and scaling policies. With target tracking scaling policies, you select a scaling metric and set a target value. Amazon EC2 Auto Scaling creates and manages the CloudWatch alarms that trigger the scaling policy and calculates the scaling adjustment based on the metric and the target value. The scaling policy adds or removes capacity as required to keep the metric at, or close to, the specified target value. For example, you can use target tracking scaling to: Configure a target tracking scaling policy to keep the average aggregate CPU utilization of your Auto Scaling group at 50 percent. This meets the requirements specified in the given use-case and therefore, this is the correct option. Target Tracking Policy Overview: via - https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html Incorrect options: Configure the Auto Scaling group to use step scaling policy and set the CPU utilization as the target metric with a target value of 50% Configure the Auto Scaling group to use simple scaling policy and set the CPU utilization as the target metric with a target value of 50% With step scaling and simple scaling, you choose scaling metrics and threshold values for the Amazon CloudWatch alarms that trigger the scaling process. Neither step scaling nor simple scaling can be configured to use a target metric for CPU utilization, hence both these options are incorrect. Configure the Auto Scaling group to use a Amazon Cloudwatch alarm triggered on a CPU utilization threshold of 50% - An Auto Scaling group cannot directly use a Cloudwatch alarm as the source for a scale-in or scale-out event, hence this option is incorrect. References: https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-simple-step.html",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An IT consultant is helping the owner of a medium-sized business set up an AWS account. What are the security recommendations he must follow while creating the AWS account root user? (Select two)",
      "options": [
        "A. Create a strong password for the AWS account root user",
        "B. Encrypt the access keys and save them on Amazon S3",
        "C. Create AWS account root user access keys and share those keys only with the business owner",
        "D. Send an email to the business owner with details of the login username and password for the AWS root user. This will help the business owner to troubleshoot any login issues in future",
        "E. Enable Multi Factor Authentication (MFA) for the AWS account root user account"
      ],
      "correct_answer": "A",
      "explanation": "Correct options: Create a strong password for the AWS account root user Enable Multi Factor Authentication (MFA) for the AWS account root user account Here are some of the best practices while creating an AWS account root user: 1) Use a strong password to help protect account-level access to the AWS Management Console. 2) Never share your AWS account root user password or access keys with anyone. 3) If you do have an access key for your AWS account root user, delete it. If you must keep it, rotate (change) the access key regularly. You should not encrypt the access keys and save them on Amazon S3. 4) If you don't already have an access key for your AWS account root user, don't create one unless you absolutely need to. 5) Enable AWS multi-factor authentication (MFA) on your AWS account root user account. AWS Root Account Security Best Practices: via - https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html Incorrect options: Encrypt the access keys and save them on Amazon S3 - AWS recommends that if you don't already have an access key for your AWS account root user, don't create one unless you absolutely need to. Even an encrypted access key for the root user poses a significant security risk. Therefore, this option is incorrect. Create AWS account root user access keys and share those keys only with the business owner - AWS recommends that if you don't already have an access key for your AWS account root user, don't create one unless you absolutely need to. Hence, this option is incorrect. Send an email to the business owner with details of the login username and password for the AWS root user. This will help the business owner to troubleshoot any login issues in future - AWS recommends that you should never share your AWS account root user password or access keys with anyone. Sending an email with AWS account root user credentials creates a security risk as it can be misused by anyone reading the email. Hence, this option is incorrect. Reference: https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#create-iam-users",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "The DevOps team at an e-commerce company wants to perform some maintenance work on a specific Amazon EC2 instance that is part of an Auto Scaling group using a step scaling policy. The team is facing a maintenance challenge - every time the team deploys a maintenance patch, the instance health check status shows as out of service for a few minutes. This causes the Auto Scaling group to provision another replacement instance immediately. As a solutions architect, which are the MOST time/resource efficient steps that you would recommend so that the maintenance work can be completed at the earliest? (Select two)",
      "options": [
        "A. Put the instance into the Standby state and then update the instance by applying the maintenance patch. Once the instance is ready, you can exit the Standby state and then return the instance to service",
        "B. Take a snapshot of the instance, create a new Amazon Machine Image (AMI) and then launch a new instance using this AMI. Apply the maintenance patch to this new instance and then add it back to the Auto Scaling Group by using the manual scaling policy. Terminate the earlier instance that had the maintenance issue",
        "C. Delete the Auto Scaling group and apply the maintenance fix to the given instance. Create a new Auto Scaling group and add all the instances again using the manual scaling policy",
        "D. Suspend the ReplaceUnhealthy process type for the Auto Scaling group and apply the maintenance patch to the instance. Once the instance is ready, you can manually set the instance's health status back to healthy and activate the ReplaceUnhealthy process type again",
        "E. Suspend the ScheduledActions process type for the Auto Scaling group and apply the maintenance patch to the instance. Once the instance is ready, you can you can manually set the instance's health status back to healthy and activate the ScheduledActions process type again"
      ],
      "correct_answer": "A",
      "explanation": "Correct options: Put the instance into the Standby state and then update the instance by applying the maintenance patch. Once the instance is ready, you can exit the Standby state and then return the instance to service - You can put an instance that is in the InService state into the Standby state, update some software or troubleshoot the instance, and then return the instance to service. Instances that are on standby are still part of the Auto Scaling group, but they do not actively handle application traffic. How Standby State Works: via - https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-enter-exit-standby.html Suspend the ReplaceUnhealthy process type for the Auto Scaling group and apply the maintenance patch to the instance. Once the instance is ready, you can manually set the instance's health status back to healthy and activate the ReplaceUnhealthy process type again - The ReplaceUnhealthy process terminates instances that are marked as unhealthy and then creates new instances to replace them. Amazon EC2 Auto Scaling stops replacing instances that are marked as unhealthy. Instances that fail EC2 or Elastic Load Balancing health checks are still marked as unhealthy. As soon as you resume the ReplaceUnhealthly process, Amazon EC2 Auto Scaling replaces instances that were marked unhealthy while this process was suspended. Incorrect options: Take a snapshot of the instance, create a new Amazon Machine Image (AMI) and then launch a new instance using this AMI. Apply the maintenance patch to this new instance and then add it back to the Auto Scaling Group by using the manual scaling policy. Terminate the earlier instance that had the maintenance issue - Taking the snapshot of the existing instance to create a new AMI and then creating a new instance in order to apply the maintenance patch is not time/resource optimal, hence this option is ruled out. Delete the Auto Scaling group and apply the maintenance fix to the given instance. Create a new Auto Scaling group and add all the instances again using the manual scaling policy - It's not recommended to delete the Auto Scaling group just to apply a maintenance patch on a specific instance. Suspend the ScheduledActions process type for the Auto Scaling group and apply the maintenance patch to the instance. Once the instance is ready, you can you can manually set the instance's health status back to healthy and activate the ScheduledActions process type again - Amazon EC2 Auto Scaling does not execute scaling actions that are scheduled to run during the suspension period. This option is not relevant to the given use-case. References: https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-enter-exit-standby.html https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-suspend-resume-processes.html https://docs.aws.amazon.com/autoscaling/ec2/userguide/health-checks-overview.html",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A leading carmaker would like to build a new car-as-a-sensor service by leveraging fully serverless components that are provisioned and managed automatically by AWS. The development team at the carmaker does not want an option that requires the capacity to be manually provisioned, as it does not want to respond manually to changing volumes of sensor data. Given these constraints, which of the following solutions is the BEST fit to develop this car-as-a-sensor service?",
      "options": [
        "A. Ingest the sensor data in an Amazon Simple Queue Service (Amazon SQS) standard queue, which is polled by an AWS Lambda function in batches and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing",
        "B. Ingest the sensor data in an Amazon Simple Queue Service (Amazon SQS) standard queue, which is polled by an application running on an Amazon EC2 instance and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing",
        "C. Ingest the sensor data in Amazon Kinesis Data Firehose, which directly writes the data into an auto-scaled Amazon DynamoDB table for downstream processing",
        "D. Ingest the sensor data in Amazon Kinesis Data Streams, which is polled by an application running on an Amazon EC2 instance and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Ingest the sensor data in an Amazon Simple Queue Service (Amazon SQS) standard queue, which is polled by an AWS Lambda function in batches and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume. Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent. AWS manages all ongoing operations and underlying infrastructure needed to provide a highly available and scalable message queuing service. With SQS, there is no upfront cost, no need to acquire, install, and configure messaging software, and no time-consuming build-out and maintenance of supporting infrastructure. SQS queues are dynamically created and scale automatically so you can build and grow applications quickly and efficiently. As there is no need to manually provision the capacity, so this is the correct option. Incorrect options: Ingest the sensor data in Amazon Kinesis Data Firehose, which directly writes the data into an auto-scaled Amazon DynamoDB table for downstream processing -Amazon Kinesis Data Firehose is a fully managed service for delivering real-time streaming data to destinations such as Amazon Simple Storage Service (Amazon S3), Amazon Redshift, Amazon OpenSearch Service, Splunk, and any custom HTTP endpoint or HTTP endpoints owned by supported third-party service providers, including Datadog, Dynatrace, LogicMonitor, MongoDB, New Relic, and Sumo Logic. Firehose cannot directly write into a DynamoDB table, so this option is incorrect. Ingest the sensor data in an Amazon Simple Queue Service (Amazon SQS) standard queue, which is polled by an application running on an Amazon EC2 instance and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing Ingest the sensor data in Amazon Kinesis Data Streams, which is polled by an application running on an Amazon EC2 instance and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing Using an application on an Amazon EC2 instance is ruled out as the carmaker wants to use fully serverless components. So both these options are incorrect. References: https://aws.amazon.com/sqs/ https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html https://aws.amazon.com/kinesis/data-streams/faqs/",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A major bank is using Amazon Simple Queue Service (Amazon SQS) to migrate several core banking applications to the cloud to ensure high availability and cost efficiency while simplifying administrative complexity and overhead. The development team at the bank expects a peak rate of about 1000 messages per second to be processed via SQS. It is important that the messages are processed in order. Which of the following options can be used to implement this system?",
      "options": [
        "A. Use Amazon SQS standard queue to process the messages",
        "B. Use Amazon SQS FIFO (First-In-First-Out) queue to process the messages",
        "C. Use Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 4 messages per operation to process the messages at the peak rate",
        "D. Use Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 2 messages per operation to process the messages at the peak rate"
      ],
      "correct_answer": "C",
      "explanation": "Correct option: Use Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 4 messages per operation to process the messages at the peak rate Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues - Standard queues vs FIFO queues. For FIFO queues, the order in which messages are sent and received is strictly preserved (i.e. First-In-First-Out). On the other hand, the standard SQS queues offer best-effort ordering. This means that occasionally, messages might be delivered in an order different from which they were sent. By default, FIFO queues support up to 300 messages per second (300 send, receive, or delete operations per second). When you batch 10 messages per operation (maximum), FIFO queues can support up to 3,000 messages per second. Therefore you need to process 4 messages per operation so that the FIFO queue can support up to 1200 messages per second, which is well within the peak rate. FIFO Queues Overview: via - https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html Incorrect options: Use Amazon SQS standard queue to process the messages - As messages need to be processed in order, therefore standard queues are ruled out. Use Amazon SQS FIFO (First-In-First-Out) queue to process the messages - By default, FIFO queues support up to 300 messages per second and this is not sufficient to meet the message processing throughput per the given use-case. Hence this option is incorrect. Use Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 2 messages per operation to process the messages at the peak rate - As mentioned earlier in the explanation, you need to use FIFO queues in batch mode and process 4 messages per operation, so that the FIFO queue can support up to 1200 messages per second. With 2 messages per operation, you can only support up to 600 messages per second. References: https://aws.amazon.com/sqs/ https://aws.amazon.com/sqs/features/",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A social photo-sharing company uses Amazon Simple Storage Service (Amazon S3) to store the images uploaded by the users. These images are kept encrypted in Amazon S3 by using AWS Key Management Service (AWS KMS) and the company manages its own AWS KMS keys for encryption. A member of the DevOps team accidentally deleted the AWS KMS key a day ago, thereby rendering the user's photo data unrecoverable. You have been contacted by the company to consult them on possible solutions to this crisis. As a solutions architect, which of the following steps would you recommend to solve this issue?",
      "options": [
        "A. Contact AWS support to retrieve the AWS KMS key from their backup",
        "B. The company should issue a notification on its web application informing the users about the loss of their data",
        "C. As the AWS KMS key was deleted a day ago, it must be in the 'pending deletion' status and hence you can just cancel the KMS key deletion and recover the key",
        "D. The AWS KMS key can be recovered by the AWS root account user"
      ],
      "correct_answer": "C",
      "explanation": "Correct option: As the AWS KMS key was deleted a day ago, it must be in the 'pending deletion' status and hence you can just cancel the KMS key deletion and recover the key AWS Key Management Service (KMS) makes it easy for you to create and manage cryptographic keys and control their use across a wide range of AWS services and in your applications. AWS KMS is a secure and resilient service that uses hardware security modules that have been validated under FIPS 140-2. Deleting an AWS KMS key in AWS Key Management Service (AWS KMS) is destructive and potentially dangerous. Therefore, AWS KMS enforces a waiting period. To delete a KMS key in AWS KMS you schedule key deletion. You can set the waiting period from a minimum of 7 days up to a maximum of 30 days. The default waiting period is 30 days. During the waiting period, the KMS key status and key state is Pending deletion. To recover the KMS key, you can cancel key deletion before the waiting period ends. After the waiting period ends you cannot cancel key deletion, and AWS KMS deletes the KMS key. How Deleting AWS KMS keys Works: via - https://docs.aws.amazon.com/kms/latest/developerguide/deleting-keys.html Incorrect options: Contact AWS support to retrieve the AWS KMS key from their backup The AWS KMS key can be recovered by the AWS root account user The AWS root account user cannot recover the AWS KMS key and the AWS support does not have access to KMS keys via any backups. Both these options just serve as distractors. The company should issue a notification on its web application informing the users about the loss of their data - This option is not required as the data can be recovered via the cancel key deletion feature. Reference: https://docs.aws.amazon.com/kms/latest/developerguide/deleting-keys.html",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An IT security consultancy is working on a solution to protect data stored in Amazon S3 from any malicious activity as well as check for any vulnerabilities on Amazon EC2 instances. As a solutions architect, which of the following solutions would you suggest to help address the given requirement?",
      "options": [
        "A. Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon Inspector to check for vulnerabilities on Amazon EC2 instances",
        "B. Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon GuardDuty to check for vulnerabilities on Amazon EC2 instances",
        "C. Use Amazon Inspector to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon Inspector to check for vulnerabilities on Amazon EC2 instances",
        "D. Use Amazon Inspector to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon GuardDuty to check for vulnerabilities on Amazon EC2 instances"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon Inspector to check for vulnerabilities on Amazon EC2 instances Amazon GuardDuty offers threat detection that enables you to continuously monitor and protect your AWS accounts, workloads, and data stored in Amazon S3. GuardDuty analyzes continuous streams of meta-data generated from your account and network activity found in AWS CloudTrail Events, Amazon VPC Flow Logs, and DNS Logs. It also uses integrated threat intelligence such as known malicious IP addresses, anomaly detection, and machine learning to identify threats more accurately. How Amazon GuardDuty works: via - https://aws.amazon.com/guardduty/ Amazon Inspector security assessments help you check for unintended network accessibility of your Amazon EC2 instances and for vulnerabilities on those EC2 instances. Amazon Inspector assessments are offered to you as pre-defined rules packages mapped to common security best practices and vulnerability definitions. Incorrect options: Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon GuardDuty to check for vulnerabilities on Amazon EC2 instances Use Amazon Inspector to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon Inspector to check for vulnerabilities on Amazon EC2 instances Use Amazon Inspector to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon GuardDuty to check for vulnerabilities on Amazon EC2 instances These three options contradict the explanation provided above, so these options are incorrect. References: https://aws.amazon.com/guardduty/ https://aws.amazon.com/inspector/",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "The engineering team at an e-commerce company wants to establish a dedicated, encrypted, low latency, and high throughput connection between its data center and AWS Cloud. The engineering team has set aside sufficient time to account for the operational overhead of establishing this connection. As a solutions architect, which of the following solutions would you recommend to the company?",
      "options": [
        "A. Use AWS Direct Connect to establish a connection between the data center and AWS Cloud",
        "B. Use AWS site-to-site VPN to establish a connection between the data center and AWS Cloud",
        "C. Use AWS Direct Connect plus virtual private network (VPN) to establish a connection between the data center and AWS Cloud",
        "D. Use AWS Transit Gateway to establish a connection between the data center and AWS Cloud"
      ],
      "correct_answer": "C",
      "explanation": "Correct option: Use AWS Direct Connect plus virtual private network (VPN) to establish a connection between the data center and AWS Cloud AWS Direct Connect is a cloud service solution that makes it easy to establish a dedicated network connection from your premises to AWS. AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations. With AWS Direct Connect plus VPN, you can combine one or more AWS Direct Connect dedicated network connections with the Amazon VPC VPN. This combination provides an IPsec-encrypted private connection that also reduces network costs, increases bandwidth throughput, and provides a more consistent network experience than internet-based VPN connections. This solution combines the AWS managed benefits of the VPN solution with low latency, increased bandwidth, more consistent benefits of the AWS Direct Connect solution, and an end-to-end, secure IPsec connection. Therefore, AWS Direct Connect plus VPN is the correct solution for this use-case. AWS Direct Connect Plus VPN: via - https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-vpn.html Incorrect options: Use AWS site-to-site VPN to establish a connection between the data center and AWS Cloud - AWS Site-to-Site VPN enables you to securely connect your on-premises network or branch office site to your Amazon Virtual Private Cloud (Amazon VPC). A VPC VPN Connection utilizes IPSec to establish encrypted network connectivity between your intranet and Amazon VPC over the Internet. VPN Connections are a good solution if you have an immediate need, have low to modest bandwidth requirements, and can tolerate the inherent variability in Internet-based connectivity. However, Site-to-site VPN cannot provide low latency and high throughput connection, therefore this option is ruled out. Use AWS Transit Gateway to establish a connection between the data center and AWS Cloud - AWS Transit Gateway is a network transit hub that you can use to interconnect your virtual private clouds (VPC) and on-premises networks. AWS Transit Gateway by itself cannot establish a low latency and high throughput connection between a data center and AWS Cloud. Hence this option is incorrect. Use AWS Direct Connect to establish a connection between the data center and AWS Cloud - AWS Direct Connect by itself cannot provide an encrypted connection between a data center and AWS Cloud, so this option is ruled out. References: https://aws.amazon.com/directconnect/ https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-plus-vpn-network-to-amazon.html",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "One of the biggest football leagues in Europe has granted the distribution rights for live streaming its matches in the USA to a silicon valley based streaming services company. As per the terms of distribution, the company must make sure that only users from the USA are able to live stream the matches on their platform. Users from other countries in the world must be denied access to these live-streamed matches. Which of the following options would allow the company to enforce these streaming restrictions? (Select two)",
      "options": [
        "A. Use Amazon Route 53 based latency-based routing policy to restrict distribution of content to only the locations in which you have distribution rights",
        "B. Use Amazon Route 53 based weighted routing policy to restrict distribution of content to only the locations in which you have distribution rights",
        "C. Use Amazon Route 53 based failover routing policy to restrict distribution of content to only the locations in which you have distribution rights",
        "D. Use Amazon Route 53 based geolocation routing policy to restrict distribution of content to only the locations in which you have distribution rights",
        "E. Use georestriction to prevent users in specific geographic locations from accessing content that you're distributing through a Amazon CloudFront web distribution"
      ],
      "correct_answer": "D",
      "explanation": "Correct options: Use Amazon Route 53 based geolocation routing policy to restrict distribution of content to only the locations in which you have distribution rights Geolocation routing lets you choose the resources that serve your traffic based on the geographic location of your users, meaning the location that DNS queries originate from. For example, you might want all queries from Europe to be routed to an ELB load balancer in the Frankfurt region. You can also use geolocation routing to restrict the distribution of content to only the locations in which you have distribution rights. Use georestriction to prevent users in specific geographic locations from accessing content that you're distributing through a Amazon CloudFront web distribution You can use georestriction, also known as geo-blocking, to prevent users in specific geographic locations from accessing content that you're distributing through a Amazon CloudFront web distribution. When a user requests your content, Amazon CloudFront typically serves the requested content regardless of where the user is located. If you need to prevent users in specific countries from accessing your content, you can use the CloudFront geo restriction feature to do one of the following: Allow your users to access your content only if they're in one of the countries on a whitelist of approved countries. Prevent your users from accessing your content if they're in one of the countries on a blacklist of banned countries. So this option is also correct. Amazon Route 53 Routing Policy Overview: via - https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html Incorrect options: Use Amazon Route 53 based latency-based routing policy to restrict distribution of content to only the locations in which you have distribution rights - Use latency-based routing when you have resources in multiple AWS Regions and you want to route traffic to the region that provides the lowest latency. To use latency-based routing, you create latency records for your resources in multiple AWS Regions. When Amazon Route 53 receives a DNS query for your domain or subdomain (example.com or acme.example.com), it determines which AWS Regions you've created latency records for, determines which region gives the user the lowest latency, and then selects a latency record for that region. Route 53 responds with the value from the selected record, such as the IP address for a web server. Use Amazon Route 53 based weighted routing policy to restrict distribution of content to only the locations in which you have distribution rights - Weighted routing lets you associate multiple resources with a single domain name (example.com) or subdomain name (acme.example.com) and choose how much traffic is routed to each resource. This can be useful for a variety of purposes, including load balancing and testing new versions of the software. Use Amazon Route 53 based failover routing policy to restrict distribution of content to only the locations in which you have distribution rights - Failover routing lets you route traffic to a resource when the resource is healthy or to a different resource when the first resource is unhealthy. The primary and secondary records can route traffic to anything from an Amazon S3 bucket that is configured as a website to a complex tree of records. Weighted routing or failover routing or latency routing cannot be used to restrict the distribution of content to only the locations in which you have distribution rights. So all three options above are incorrect. References: https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-geo https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-geo https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-geo",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "Which of the following feature of an Amazon S3 bucket can only be suspended and not disabled once it have been enabled?",
      "options": [
        "A. Server Access Logging",
        "B. Static Website Hosting",
        "C. Requester Pays",
        "D. Versioning"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: Versioning Once you version-enable a bucket, it can never return to an unversioned state. Versioning can only be suspended once it has been enabled. Versioning Overview: via - https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html Incorrect options: Server Access Logging Static Website Hosting Requester Pays Server Access Logging, Static Website Hosting and Requester Pays features can be disabled even after they have been enabled. Reference: https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A junior scientist working with the Deep Space Research Laboratory at NASA is trying to upload a high-resolution image of a nebula into Amazon S3. The image size is approximately 3 gigabytes. The junior scientist is using Amazon S3 Transfer Acceleration (Amazon S3TA) for faster image upload. It turns out that Amazon S3TA did not result in an accelerated transfer. Given this scenario, which of the following is correct regarding the charges for this image transfer?",
      "options": [
        "A. The junior scientist does not need to pay any transfer charges for the image upload",
        "B. The junior scientist only needs to pay S3TA transfer charges for the image upload",
        "C. The junior scientist only needs to pay Amazon S3 transfer charges for the image upload",
        "D. The junior scientist needs to pay both S3 transfer charges and S3TA transfer charges for the image upload"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: The junior scientist does not need to pay any transfer charges for the image upload There are no S3 data transfer charges when data is transferred in from the internet. Also with S3TA, you pay only for transfers that are accelerated. Therefore the junior scientist does not need to pay any transfer charges for the image upload because S3TA did not result in an accelerated transfer. Amazon S3 Transfer Acceleration (S3TA) Overview: via - https://aws.amazon.com/s3/transfer-acceleration/ Incorrect options: The junior scientist only needs to pay S3TA transfer charges for the image upload - Since S3TA did not result in an accelerated transfer, there are no S3TA transfer charges to be paid. The junior scientist only needs to pay Amazon S3 transfer charges for the image upload - There are no S3 data transfer charges when data is transferred in from the internet. So this option is incorrect. The junior scientist needs to pay both S3 transfer charges and S3TA transfer charges for the image upload - There are no Amazon S3 data transfer charges when data is transferred in from the internet. Since S3TA did not result in an accelerated transfer, there are no S3TA transfer charges to be paid. References: https://aws.amazon.com/s3/transfer-acceleration/ https://aws.amazon.com/s3/pricing/",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "The solo founder at a tech startup has just created a brand new AWS account. The founder has provisioned an Amazon EC2 instance 1A which is running in AWS Region A. Later, he takes a snapshot of the instance 1A and then creates a new Amazon Machine Image (AMI) in Region A from this snapshot. This AMI is then copied into another Region B. The founder provisions an instance 1B in Region B using this new AMI in Region B. At this point in time, what entities exist in Region B?",
      "options": [
        "A. 1 Amazon EC2 instance, 1 AMI and 1 snapshot exist in Region B",
        "B. 1 Amazon EC2 instance and 1 AMI exist in Region B",
        "C. 1 Amazon EC2 instance and 1 snapshot exist in Region B",
        "D. 1 Amazon EC2 instance and 2 AMIs exist in Region B"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: 1 Amazon EC2 instance, 1 AMI and 1 snapshot exist in Region B An Amazon Machine Image (AMI) provides the information required to launch an instance. You must specify an AMI when you launch an instance. When the new AMI is copied from Region A into Region B, it automatically creates a snapshot in Region B because AMIs are based on the underlying snapshots. Further, an instance is created from this AMI in Region B. Hence, we have 1 Amazon EC2 instance, 1 AMI and 1 snapshot in Region B. Amazon Machine Image (AMI) Overview: via - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html Incorrect options: 1 Amazon EC2 instance and 1 AMI exist in Region B 1 Amazon EC2 instance and 1 snapshot exist in Region B 1 Amazon EC2 instance and 2 AMIs exist in Region B As mentioned earlier in the explanation, when the new AMI is copied from Region A into Region B, it also creates a snapshot in Region B because AMIs are based on the underlying snapshots. In addition, an instance is created from this AMI in Region B. So, we have 1 Amazon EC2 instance, 1 AMI and 1 snapshot in Region B. Hence all three options are incorrect. Reference: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "The sourcing team at the US headquarters of a global e-commerce company is preparing a spreadsheet of the new product catalog. The spreadsheet is saved on an Amazon Elastic File System (Amazon EFS) created in us-east-1 region. The sourcing team counterparts from other AWS regions such as Asia Pacific and Europe also want to collaborate on this spreadsheet. As a solutions architect, what is your recommendation to enable this collaboration with the LEAST amount of operational overhead?",
      "options": [
        "A. The spreadsheet on the Amazon Elastic File System (Amazon EFS) can be accessed in other AWS regions by using an inter-region VPC peering connection",
        "B. The spreadsheet will have to be copied into Amazon EFS file systems of other AWS regions as Amazon EFS is a regional service and it does not allow access from other AWS regions",
        "C. The spreadsheet will have to be copied in Amazon S3 which can then be accessed from any AWS region",
        "D. The spreadsheet data will have to be moved into an Amazon RDS for MySQL database which can then be accessed from any AWS region"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: The spreadsheet on the Amazon Elastic File System (Amazon EFS) can be accessed in other AWS regions by using an inter-region VPC peering connection Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. Amazon EFS is a regional service storing data within and across multiple Availability Zones (AZs) for high availability and durability. Amazon EC2 instances can access your file system across AZs, regions, and VPCs, while on-premises servers can access using AWS Direct Connect or AWS VPN. You can connect to Amazon EFS file systems from EC2 instances in other AWS regions using an inter-region VPC peering connection, and from on-premises servers using an AWS VPN connection. So this is the correct option. Incorrect options: The spreadsheet will have to be copied in Amazon S3 which can then be accessed from any AWS region The spreadsheet data will have to be moved into an Amazon RDS for MySQL database which can then be accessed from any AWS region Copying the spreadsheet into Amazon S3 or Amazon RDS for MySQL database is not the correct solution as it involves a lot of operational overhead. For Amazon RDS, one would need to write custom code to replicate the spreadsheet functionality running off of the database. S3 does not allow in-place edit of an object. Additionally, it's also not POSIX compliant. So one would need to develop a custom application to \"simulate in-place edits\" to support collabaration as per the use-case. So both these options are ruled out. The spreadsheet will have to be copied into Amazon EFS file systems of other AWS regions as Amazon EFS is a regional service and it does not allow access from other AWS regions - Creating copies of the spreadsheet into Amazon EFS file systems of other AWS regions would mean no collaboration would be possible between the teams. In this case, each team would work on \"its own file\" instead of a single file accessed and updated by all teams. Hence this option is incorrect. Reference: https://aws.amazon.com/efs/",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A research group runs its flagship application on a fleet of Amazon EC2 instances for a specialized task that must deliver high random I/O performance. Each instance in the fleet would have access to a dataset that is replicated across the instances by the application itself. Because of the resilient application architecture, the specialized task would continue to be processed even if any instance goes down, as the underlying application would ensure the replacement instance has access to the required dataset. Which of the following options is the MOST cost-optimal and resource-efficient solution to build this fleet of Amazon EC2 instances?",
      "options": [
        "A. Use Amazon Elastic Block Store (Amazon EBS) based EC2 instances",
        "B. Use Amazon EC2 instances with Amazon EFS mount points",
        "C. Use Instance Store based Amazon EC2 instances",
        "D. Use Amazon EC2 instances with access to Amazon S3 based storage"
      ],
      "correct_answer": "C",
      "explanation": "Correct option: Use Instance Store based Amazon EC2 instances An instance store provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host instance. Instance store is ideal for the temporary storage of information that changes frequently such as buffers, caches, scratch data, and other temporary content, or for data that is replicated across a fleet of instances, such as a load-balanced pool of web servers. Instance store volumes are included as part of the instance's usage cost. As Instance Store based volumes provide high random I/O performance at low cost (as the storage is part of the instance's usage cost) and the resilient architecture can adjust for the loss of any instance, therefore you should use Instance Store based Amazon EC2 instances for this use-case. Amazon EC2 Instance Store Overview: via - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html Incorrect options: Use Amazon Elastic Block Store (Amazon EBS) based EC2 instances - Amazon Elastic Block Store (Amazon EBS) based volumes would need to use provisioned IOPS (io1) as the storage type and that would incur additional costs. As we are looking for the most cost-optimal solution, this option is ruled out. Use Amazon EC2 instances with Amazon EFS mount points - Using Amazon Elastic File System (Amazon EFS) implies that extra resources would have to be provisioned (compared to using instance store where the storage is located on disks that are physically attached to the host instance itself). As we are looking for the most resource-efficient solution, this option is also ruled out. Use Amazon EC2 instances with access to Amazon S3 based storage - Using Amazon EC2 instances with access to Amazon S3 based storage does not deliver high random I/O performance, this option is just added as a distractor. Reference: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A software engineering intern at an e-commerce company is documenting the process flow to provision Amazon EC2 instances via the Amazon EC2 API. These instances are to be used for an internal application that processes Human Resources payroll data. He wants to highlight those volume types that cannot be used as a boot volume. Can you help the intern by identifying those storage volume types that CANNOT be used as boot volumes while creating the instances? (Select two)",
      "options": [
        "A. General Purpose Solid State Drive (gp2)",
        "B. Throughput Optimized Hard disk drive (st1)",
        "C. Provisioned IOPS Solid state drive (io1)",
        "D. Instance Store",
        "E. Cold Hard disk drive (sc1)"
      ],
      "correct_answer": "B",
      "explanation": "Correct options: Throughput Optimized Hard disk drive (st1) Cold Hard disk drive (sc1) The Amazon EBS volume types fall into two categories: Solid state drive (SSD) backed volumes optimized for transactional workloads involving frequent read/write operations with small I/O size, where the dominant performance attribute is IOPS. Hard disk drive (HDD) backed volumes optimized for large streaming workloads where throughput (measured in MiB/s) is a better performance measure than IOPS. Throughput Optimized HDD (st1) and Cold HDD (sc1) volume types CANNOT be used as a boot volume, so these two options are correct. Please see this detailed overview of the volume types for Amazon EBS volumes. via - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html Incorrect options: General Purpose Solid State Drive (gp2) Provisioned IOPS Solid state drive (io1) Instance Store General Purpose SSD (gp2), Provisioned IOPS SSD (io1), and Instance Store can be used as a boot volume. References: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/RootDeviceStorage.html",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "The payroll department at a company initiates several computationally intensive workloads on Amazon EC2 instances at a designated hour on the last day of every month. The payroll department has noticed a trend of severe performance lag during this hour. The engineering team has figured out a solution by using Auto Scaling Group for these Amazon EC2 instances and making sure that 10 Amazon EC2 instances are available during this peak usage hour. For normal operations only 2 Amazon EC2 instances are enough to cater to the workload. As a solutions architect, which of the following steps would you recommend to implement the solution?",
      "options": [
        "A. Configure your Auto Scaling group by creating a scheduled action that kicks-off at the designated hour on the last day of the month. Set the min count as well as the max count of instances to 10. This causes the scale-out to happen before peak traffic kicks in at the designated hour",
        "B. Configure your Auto Scaling group by creating a scheduled action that kicks-off at the designated hour on the last day of the month. Set the desired capacity of instances to 10. This causes the scale-out to happen before peak traffic kicks in at the designated hour",
        "C. Configure your Auto Scaling group by creating a target tracking policy and setting the instance count to 10 at the designated hour. This causes the scale-out to happen before peak traffic kicks in at the designated hour",
        "D. Configure your Auto Scaling group by creating a simple tracking policy and setting the instance count to 10 at the designated hour. This causes the scale-out to happen before peak traffic kicks in at the designated hour"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Configure your Auto Scaling group by creating a scheduled action that kicks-off at the designated hour on the last day of the month. Set the desired capacity of instances to 10. This causes the scale-out to happen before peak traffic kicks in at the designated hour Scheduled scaling allows you to set your own scaling schedule. For example, let's say that every week the traffic to your web application starts to increase on Wednesday, remains high on Thursday, and starts to decrease on Friday. You can plan your scaling actions based on the predictable traffic patterns of your web application. Scaling actions are performed automatically as a function of time and date. A scheduled action sets the minimum, maximum, and desired sizes to what is specified by the scheduled action at the time specified by the scheduled action. For the given use case, the correct solution is to set the desired capacity to 10. When we want to specify a range of instances, then we must use min and max values. Incorrect options: Configure your Auto Scaling group by creating a scheduled action that kicks-off at the designated hour on the last day of the month. Set the min count as well as the max count of instances to 10. This causes the scale-out to happen before peak traffic kicks in at the designated hour - As mentioned earlier in the explanation, only when we want to specify a range of instances, then we must use min and max values. As the given use-case requires exactly 10 instances to be available during the peak hour, so we must set the desired capacity to 10. Hence this option is incorrect. Configure your Auto Scaling group by creating a target tracking policy and setting the instance count to 10 at the designated hour. This causes the scale-out to happen before peak traffic kicks in at the designated hour Configure your Auto Scaling group by creating a simple tracking policy and setting the instance count to 10 at the designated hour. This causes the scale-out to happen before peak traffic kicks in at the designated hour Target tracking policy or simple tracking policy cannot be used to effect a scaling action at a certain designated hour. Both these options have been added as distractors. Reference: https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A technology blogger wants to write a review on the comparative pricing for various storage types available on AWS Cloud. The blogger has created a test file of size 1 gigabytes with some random data. Next he copies this test file into AWS S3 Standard storage class, provisions an Amazon EBS volume (General Purpose SSD (gp2)) with 100 gigabytes of provisioned storage and copies the test file into the Amazon EBS volume, and lastly copies the test file into an Amazon EFS Standard Storage filesystem. At the end of the month, he analyses the bill for costs incurred on the respective storage types for the test file. What is the correct order of the storage charges incurred for the test file on these three storage types?",
      "options": [
        "A. Cost of test file storage on Amazon S3 Standard",
        "B. Cost of test file storage on Amazon S3 Standard",
        "C. Cost of test file storage on Amazon EFS",
        "D. Cost of test file storage on Amazon EBS"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Cost of test file storage on Amazon S3 Standard With Amazon EBS Elastic Volumes, you pay only for the resources that you use. The Amazon EFS Standard Storage pricing is $0.30 per GB per month. Therefore the cost for storing the test file on EFS is $0.30 for the month. For Amazon EBS General Purpose SSD (gp2) volumes, the charges are $0.10 per GB-month of provisioned storage. Therefore, for a provisioned storage of 100GB for this use-case, the monthly cost on EBS is $0.10*100 = $10. This cost is irrespective of how much storage is actually consumed by the test file. For S3 Standard storage, the pricing is $0.023 per GB per month. Therefore, the monthly storage cost on S3 for the test file is $0.023. Therefore this is the correct option. Incorrect options: Cost of test file storage on Amazon S3 Standard Cost of test file storage on Amazon EFS Cost of test file storage on Amazon EBS Following the computations shown earlier in the explanation, these three options are incorrect. References: https://aws.amazon.com/ebs/pricing/ https://aws.amazon.com/s3/pricing/(https://aws.amazon.com/s3/pricing/) https://aws.amazon.com/efs/pricing/",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A retail company runs a customer management system backed by a Microsoft SQL Server database. The system is tightly integrated with applications that rely on T-SQL queries. The company wants to modernize its infrastructure by migrating to Amazon Aurora PostgreSQL, but it needs to avoid major modifications to the existing application logic. Which combination of actions should the company take to achieve this goal with minimal application refactoring? (Select two)",
      "options": [
        "A. Configure Amazon Aurora PostgreSQL with a custom endpoint that emulates Microsoft SQL Server behavior",
        "B. Use Amazon Aurora Global Database to replicate data across regions for compatibility",
        "C. Deploy Babelfish for Aurora PostgreSQL to enable support for T-SQL commands",
        "D. Use AWS Schema Conversion Tool (AWS SCT) along with AWS Database Migration Service (AWS DMS) to migrate the schema and data",
        "E. Use AWS Glue to convert T-SQL queries to PostgreSQL-compatible SQL during the migration"
      ],
      "correct_answer": "C",
      "explanation": "Correct options: Deploy Babelfish for Aurora PostgreSQL to enable support for T-SQL commands Babelfish allows Aurora PostgreSQL to understand T-SQL (Microsoft SQL Server's query language) and SQL Server wire protocol, enabling applications to communicate with Aurora using SQL Server-style queries with minimal code changes. This is ideal for minimizing application code refactoring. via - https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/babelfish.html Use AWS Schema Conversion Tool (AWS SCT) along with AWS Database Migration Service (AWS DMS) to migrate the schema and data The AWS Schema Conversion Tool (SCT) helps convert the SQL Server schema to PostgreSQL-compatible syntax, and AWS Database Migration Service (DMS) can move the actual data with minimal downtime. These tools are designed for database migration and are essential for schema and data transfer. Incorrect options: Configure Amazon Aurora PostgreSQL with a custom endpoint that emulates Microsoft SQL Server behavior - Aurora endpoints do not provide protocol-level emulation of SQL Server unless Babelfish is explicitly enabled. There is no feature to make Aurora natively emulate SQL Server behavior without Babelfish. Use Amazon Aurora Global Database to replicate data across regions for compatibility - Aurora Global Database helps with cross-region disaster recovery and read scalability, but it does not assist with SQL Server compatibility or reduce application code changes. Use AWS Glue to convert T-SQL queries to PostgreSQL-compatible SQL during the migration - AWS Glue is primarily used for ETL and data transformation, not for application SQL query conversion. It cannot translate T-SQL into PostgreSQL syntax. References: https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/babelfish.html https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/babelfish-compatibility.html",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "The development team at an e-commerce startup has set up multiple microservices running on Amazon EC2 instances under an Application Load Balancer. The team wants to route traffic to multiple back-end services based on the URL path of the HTTP header. So it wants requests for https://www.example.com/orders to go to a specific microservice and requests for https://www.example.com/products to go to another microservice. Which of the following features of Application Load Balancers can be used for this use-case?",
      "options": [
        "A. Query string parameter-based routing",
        "B. HTTP header-based routing",
        "C. Host-based Routing",
        "D. Path-based Routing"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: Path-based Routing Elastic Load Balancing automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and AWS Lambda functions. If your application is composed of several individual services, an Application Load Balancer can route a request to a service based on the content of the request. Here are the different types - Host-based Routing: You can route a client request based on the Host field of the HTTP header allowing you to route to multiple domains from the same load balancer. Path-based Routing: You can route a client request based on the URL path of the HTTP header. HTTP header-based routing: You can route a client request based on the value of any standard or custom HTTP header. HTTP method-based routing: You can route a client request based on any standard or custom HTTP method. Query string parameter-based routing: You can route a client request based on the query string or query parameters. Source IP address CIDR-based routing: You can route a client request based on source IP address CIDR from where the request originates. Path-based Routing Overview: You can use path conditions to define rules that route requests based on the URL in the request (also known as path-based routing). The path pattern is applied only to the path of the URL, not to its query parameters. via - https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html#path-conditions Incorrect options: Query string parameter-based routing HTTP header-based routing Host-based Routing As mentioned earlier in the explanation, none of these three types of routing support requests based on the URL path of the HTTP header. Hence these three are incorrect. Reference: https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "The product team at a startup has figured out a market need to support both stateful and stateless client-server communications via the application programming interface (APIs) developed using its platform. You have been hired by the startup as a solutions architect to build a solution to fulfill this market need using Amazon API Gateway. Which of the following would you identify as correct?",
      "options": [
        "A. Amazon API Gateway creates RESTful APIs that enable stateless client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateful, full-duplex communication between client and server",
        "B. Amazon API Gateway creates RESTful APIs that enable stateful client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateful, full-duplex communication between client and server",
        "C. Amazon API Gateway creates RESTful APIs that enable stateless client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateless, full-duplex communication between client and server",
        "D. Amazon API Gateway creates RESTful APIs that enable stateful client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateless, full-duplex communication between client and server"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Amazon API Gateway creates RESTful APIs that enable stateless client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateful, full-duplex communication between client and server Amazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the front door for applications to access data, business logic, or functionality from your backend services. Using API Gateway, you can create RESTful APIs and WebSocket APIs that enable real-time two-way communication applications. How Amazon API Gateway Works: via - https://aws.amazon.com/api-gateway/ Amazon API Gateway creates RESTful APIs that: Are HTTP-based. Enable stateless client-server communication. Implement standard HTTP methods such as GET, POST, PUT, PATCH, and DELETE. Amazon API Gateway creates WebSocket APIs that: Adhere to the WebSocket protocol, which enables stateful, full-duplex communication between client and server. Route incoming messages based on message content. So Amazon API Gateway supports stateless RESTful APIs as well as stateful WebSocket APIs. Therefore this option is correct. Incorrect options: Amazon API Gateway creates RESTful APIs that enable stateful client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateful, full-duplex communication between client and server Amazon API Gateway creates RESTful APIs that enable stateless client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateless, full-duplex communication between client and server Amazon API Gateway creates RESTful APIs that enable stateful client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateless, full-duplex communication between client and server These three options contradict the earlier details provided in the explanation. To summarize, Amazon API Gateway supports stateless RESTful APIs and stateful WebSocket APIs. Hence these options are incorrect. Reference: https://docs.aws.amazon.com/apigateway/latest/developerguide/welcome.html",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "The engineering team at a Spanish professional football club has built a notification system for its website using Amazon Simple Notification Service (Amazon SNS) notifications which are then handled by an AWS Lambda function for end-user delivery. During the off-season, the notification systems need to handle about 100 requests per second. During the peak football season, the rate touches about 5000 requests per second and it is noticed that a significant number of the notifications are not being delivered to the end-users on the website. As a solutions architect, which of the following would you suggest as the BEST possible solution to this issue?",
      "options": [
        "A. Amazon SNS has hit a scalability limit, so the team needs to contact AWS support to raise the account limit",
        "B. Amazon SNS message deliveries to AWS Lambda have crossed the account concurrency quota for AWS Lambda, so the team needs to contact AWS support to raise the account limit",
        "C. The engineering team needs to provision more servers running the Amazon SNS service",
        "D. The engineering team needs to provision more servers running the AWS Lambda service"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Amazon SNS message deliveries to AWS Lambda have crossed the account concurrency quota for AWS Lambda, so the team needs to contact AWS support to raise the account limit Amazon Simple Notification Service (Amazon SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. How Amazon SNS Works: via - https://aws.amazon.com/sns/ With AWS Lambda, you can run code without provisioning or managing servers. You pay only for the compute time that you consume—there’s no charge when your code isn’t running. AWS Lambda currently supports 1000 concurrent executions per AWS account per region. If your Amazon SNS message deliveries to AWS Lambda contribute to crossing these concurrency quotas, your Amazon SNS message deliveries will be throttled. You need to contact AWS support to raise the account limit. Therefore this option is correct. Incorrect options: Amazon SNS has hit a scalability limit, so the team needs to contact AWS support to raise the account limit - Amazon SNS leverages the proven AWS cloud to dynamically scale with your application. You don't need to contact AWS support, as SNS is a fully managed service, taking care of the heavy lifting related to capacity planning, provisioning, monitoring, and patching. Therefore, this option is incorrect. The engineering team needs to provision more servers running the Amazon SNS service The engineering team needs to provision more servers running the AWS Lambda service As both AWS Lambda and Amazon SNS are serverless and fully managed services, the engineering team cannot provision more servers. Both of these options are incorrect. References: https://aws.amazon.com/sns/ https://aws.amazon.com/sns/faqs/",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A file-hosting service uses Amazon Simple Storage Service (Amazon S3) under the hood to power its storage offerings. Currently all the customer files are uploaded directly under a single Amazon S3 bucket. The engineering team has started seeing scalability issues where customer file uploads have started failing during the peak access hours with more than 5000 requests per second. Which of the following is the MOST resource efficient and cost-optimal way of addressing this issue?",
      "options": [
        "A. Change the application architecture to create a new Amazon S3 bucket for each customer and then upload each customer's files directly under the respective buckets",
        "B. Change the application architecture to create a new Amazon S3 bucket for each day's data and then upload the daily files directly under that day's bucket",
        "C. Change the application architecture to use Amazon Elastic File System (Amazon EFS) instead of Amazon S3 for storing the customers' uploaded files",
        "D. Change the application architecture to create customer-specific custom prefixes within the single Amazon S3 bucket and then upload the daily files into those prefixed locations"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: Change the application architecture to create customer-specific custom prefixes within the single Amazon S3 bucket and then upload the daily files into those prefixed locations Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. Your applications can easily achieve thousands of transactions per second in request performance when uploading and retrieving storage from Amazon S3. Amazon S3 automatically scales to high request rates. For example, your application can achieve at least 3,500 PUT/COPY/POST/DELETE or 5,500 GET/HEAD requests per second per prefix in a bucket. There are no limits to the number of prefixes in a bucket. You can increase your read or write performance by parallelizing reads. For example, if you create 10 prefixes in an Amazon S3 bucket to parallelize reads, you could scale your read performance to 55,000 read requests per second. Please see this example for more clarity on prefixes: if you have a file f1 stored in an S3 object path like so s3://your_bucket_name/folder1/sub_folder_1/f1, then /folder1/sub_folder_1/ becomes the prefix for file f1. Some data lake applications on Amazon S3 scan millions or billions of objects for queries that run over petabytes of data. These data lake applications achieve single-instance transfer rates that maximize the network interface used for their Amazon EC2 instance, which can be up to 100 Gb/s on a single instance. These applications then aggregate throughput across multiple instances to get multiple terabits per second. Therefore creating customer-specific custom prefixes within the single bucket and then uploading the daily files into those prefixed locations is the BEST solution for the given constraints. Optimizing Amazon S3 Performance: via - https://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance.html Incorrect options: Change the application architecture to create a new Amazon S3 bucket for each customer and then upload each customer's files directly under the respective buckets - Creating a new Amazon S3 bucket for each new customer is an inefficient way of handling resource availability (S3 buckets need to be globally unique) as some customers may use the service sparingly but the bucket name is locked for them forever. Moreover, this is really not required as we can use S3 prefixes to improve the performance. Change the application architecture to create a new Amazon S3 bucket for each day's data and then upload the daily files directly under that day's bucket - Creating a new Amazon S3 bucket for each new day's data is also an inefficient way of handling resource availability (S3 buckets need to be globally unique) as some of the bucket names may not be available for daily data processing. Moreover, this is really not required as we can use S3 prefixes to improve the performance. Change the application architecture to use Amazon Elastic File System (Amazon EFS) instead of Amazon S3 for storing the customers' uploaded files - Amazon EFS is a costlier storage option compared to Amazon S3, so it is ruled out. Reference: https://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance.html",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A new DevOps engineer has just joined a development team and wants to understand the replication capabilities for Amazon RDS Multi-AZ deployment as well as Amazon RDS Read-replicas. Which of the following correctly summarizes these capabilities for the given database?",
      "options": [
        "A. Multi-AZ follows asynchronous replication and spans one Availability Zone (AZ) within a single region. Read replicas follow synchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region",
        "B. Multi-AZ follows synchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow asynchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region",
        "C. Multi-AZ follows asynchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow synchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region",
        "D. Multi-AZ follows asynchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow asynchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Multi-AZ follows synchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow asynchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region Amazon RDS Multi-AZ deployments provide enhanced availability and durability for RDS database (DB) instances, making them a natural fit for production database workloads. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). Multi-AZ spans at least two Availability Zones (AZs) within a single region. Amazon RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. For the MySQL, MariaDB, PostgreSQL, Oracle, and SQL Server database engines, Amazon RDS creates a second DB instance using a snapshot of the source DB instance. It then uses the engines' native asynchronous replication to update the read replica whenever there is a change to the source DB instance. Amazon RDS replicates all databases in the source DB instance. Read replicas can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region. Exam Alert: Please review this comparison vis-a-vis Multi-AZ vs Read Replica for Amazon RDS: via - https://aws.amazon.com/rds/features/multi-az/ Incorrect Options: Multi-AZ follows asynchronous replication and spans one Availability Zone (AZ) within a single region. Read replicas follow synchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region Multi-AZ follows asynchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow synchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region Multi-AZ follows asynchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow asynchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region These three options contradict the earlier details provided in the explanation. To summarize, Multi-AZ deployment follows synchronous replication for Amazon RDS. Hence these options are incorrect. References: https://aws.amazon.com/rds/features/multi-az/ https://aws.amazon.com/rds/features/read-replicas/",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A gaming company uses Amazon Aurora as its primary database service. The company has now deployed 5 multi-AZ read replicas to increase the read throughput and for use as failover target. The replicas have been assigned the following failover priority tiers and corresponding instance sizes are given in parentheses: tier-1 (16 terabytes), tier-1 (32 terabytes), tier-10 (16 terabytes), tier-15 (16 terabytes), tier-15 (32 terabytes). In the event of a failover, Amazon Aurora will promote which of the following read replicas?",
      "options": [
        "A. Tier-15 (32 terabytes)",
        "B. Tier-1 (16 terabytes)",
        "C. Tier-10 (16 terabytes)",
        "D. Tier-1 (32 terabytes)"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: Tier-1 (32 terabytes) Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 128TB per database instance. It delivers high performance and availability with up to 15 low-latency read replicas, point-in-time recovery, continuous backup to Amazon S3, and replication across three Availability Zones (AZs). For Amazon Aurora, each Read Replica is associated with a priority tier (0-15). In the event of a failover, Amazon Aurora will promote the Read Replica that has the highest priority (the lowest numbered tier). If two or more Aurora Replicas share the same priority, then Amazon RDS promotes the replica that is largest in size. If two or more Aurora Replicas share the same priority and size, then Amazon Aurora promotes an arbitrary replica in the same promotion tier. Therefore, for this problem statement, the Tier-1 (32 terabytes) replica will be promoted. Incorrect options: Tier-15 (32 terabytes) Tier-1 (16 terabytes) Tier-10 (16 terabytes) Given the failover rules discussed earlier in the explanation, these three options are incorrect. References: https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.AuroraHighAvailability.html",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A company uses Amazon DynamoDB as a data store for various kinds of customer data, such as user profiles, user events, clicks, and visited links. Some of these use-cases require a high request rate (millions of requests per second), low predictable latency, and reliability. The company now wants to add a caching layer to support high read volumes. As a solutions architect, which of the following AWS services would you recommend as a caching layer for this use-case? (Select two)",
      "options": [
        "A. Amazon DynamoDB Accelerator (DAX)",
        "B. Amazon ElastiCache",
        "C. Amazon Relational Database Service (Amazon RDS)",
        "D. Amazon OpenSearch Service",
        "E. Amazon Redshift"
      ],
      "correct_answer": "A",
      "explanation": "Correct options: Amazon DynamoDB Accelerator (DAX) Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement – from milliseconds to microseconds – even at millions of requests per second. DAX does all the heavy lifting required to add in-memory acceleration to your DynamoDB tables, without requiring developers to manage cache invalidation, data population, or cluster management. Therefore, this is a correct option. DAX Overview: via - https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.concepts.html Amazon ElastiCache Amazon ElastiCache for Memcached is an ideal front-end for data stores like Amazon RDS or Amazon DynamoDB, providing a high-performance middle tier for applications with extremely high request rates and/or low latency requirements. Therefore, this is also a correct option. Incorrect options: Amazon Relational Database Service (Amazon RDS) - Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching, and backups. Amazon RDS cannot be used as a caching layer for Amazon DynamoDB. Amazon OpenSearch Service - Amazon OpenSearch Service is a managed service that makes it easy for you to perform interactive log analytics, real-time application monitoring, website search, and more. OpenSearch is an open source, distributed search and analytics suite derived from Elasticsearch. It cannot be used as a caching layer for Amazon DynamoDB. Amazon Redshift - Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large scale data set storage and analysis. It cannot be used as a caching layer for Amazon DynamoDB. References: https://aws.amazon.com/dynamodb/dax/ https://aws.amazon.com/elasticache/faqs/",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "Amazon CloudFront offers a multi-tier cache in the form of regional edge caches that improve latency. However, there are certain content types that bypass the regional edge cache, and go directly to the origin. Which of the following content types skip the regional edge cache? (Select two)",
      "options": [
        "A. Dynamic content, as determined at request time (cache-behavior configured to forward all headers)",
        "B. Proxy methods PUT/POST/PATCH/OPTIONS/DELETE go directly to the origin",
        "C. E-commerce assets such as product photos",
        "D. User-generated videos",
        "E. Static content such as style sheets, JavaScript files"
      ],
      "correct_answer": "A",
      "explanation": "Correct options: Dynamic content, as determined at request time (cache-behavior configured to forward all headers) Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment. CloudFront points of presence (POPs) (edge locations) make sure that popular content can be served quickly to your viewers. CloudFront also has regional edge caches that bring more of your content closer to your viewers, even when the content is not popular enough to stay at a POP, to help improve performance for that content. Dynamic content, as determined at request time (cache-behavior configured to forward all headers), does not flow through regional edge caches, but goes directly to the origin. So this option is correct. Proxy methods PUT/POST/PATCH/OPTIONS/DELETE go directly to the origin Proxy methods PUT/POST/PATCH/OPTIONS/DELETE go directly to the origin from the POPs and do not proxy through the regional edge caches. So this option is also correct. How Amazon CloudFront Delivers Content: via - https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/HowCloudFrontWorks.html Incorrect Options: E-commerce assets such as product photos User-generated videos Static content such as style sheets, JavaScript files The following type of content flows through the regional edge caches - user-generated content, such as video, photos, or artwork; e-commerce assets such as product photos and videos and static content such as style sheets, JavaScript files. Hence these three options are not correct. Reference: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/HowCloudFrontWorks.html",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A company uses Amazon S3 buckets for storing sensitive customer data. The company has defined different retention periods for different objects present in the Amazon S3 buckets, based on the compliance requirements. But, the retention rules do not seem to work as expected. Which of the following options represent a valid configuration for setting up retention periods for objects in Amazon S3 buckets? (Select two)",
      "options": [
        "A. When you apply a retention period to an object version explicitly, you specify a Retain Until Date for the object version",
        "B. You cannot place a retention period on an object version through a bucket default setting",
        "C. When you use bucket default settings, you specify a Retain Until Date for the object version",
        "D. Different versions of a single object can have different retention modes and periods",
        "E. The bucket default settings will override any explicit retention mode or period you request on an object version"
      ],
      "correct_answer": "A",
      "explanation": "Correct options: When you apply a retention period to an object version explicitly, you specify a Retain Until Date for the object version You can place a retention period on an object version either explicitly or through a bucket default setting. When you apply a retention period to an object version explicitly, you specify a Retain Until Date for the object version. Amazon S3 stores the Retain Until Date setting in the object version's metadata and protects the object version until the retention period expires. Different versions of a single object can have different retention modes and periods Like all other Object Lock settings, retention periods apply to individual object versions. Different versions of a single object can have different retention modes and periods. For example, suppose that you have an object that is 15 days into a 30-day retention period, and you PUT an object into Amazon S3 with the same name and a 60-day retention period. In this case, your PUT succeeds, and Amazon S3 creates a new version of the object with a 60-day retention period. The older version maintains its original retention period and becomes deletable in 15 days. Incorrect options: You cannot place a retention period on an object version through a bucket default setting - You can place a retention period on an object version either explicitly or through a bucket default setting. When you use bucket default settings, you specify a Retain Until Date for the object version - When you use bucket default settings, you don't specify a Retain Until Date. Instead, you specify a duration, in either days or years, for which every object version placed in the bucket should be protected. The bucket default settings will override any explicit retention mode or period you request on an object version - If your request to place an object version in a bucket contains an explicit retention mode and period, those settings override any bucket default settings for that object version. Reference: https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lock-overview.html",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A logistics company is building a multi-tier application to track the location of its trucks during peak operating hours. The company wants these data points to be accessible in real-time in its analytics platform via a REST API. The company has hired you as an AWS Certified Solutions Architect Associate to build a multi-tier solution to store and retrieve this location data for analysis. Which of the following options addresses the given use case?",
      "options": [
        "A. Leverage Amazon Athena with Amazon S3",
        "B. Leverage Amazon QuickSight with Amazon Redshift",
        "C. Leverage Amazon API Gateway with Amazon Kinesis Data Analytics",
        "D. Leverage Amazon API Gateway with AWS Lambda"
      ],
      "correct_answer": "C",
      "explanation": "Correct option: Leverage Amazon API Gateway with Amazon Kinesis Data Analytics You can use Kinesis Data Analytics to transform and analyze streaming data in real-time with Apache Flink. Kinesis Data Analytics enables you to quickly build end-to-end stream processing applications for log analytics, clickstream analytics, Internet of Things (IoT), ad tech, gaming, etc. The four most common use cases are streaming extract-transform-load (ETL), continuous metric generation, responsive real-time analytics, and interactive querying of data streams. Kinesis Data Analytics for Apache Flink applications provides your application 50 GB of running application storage per Kinesis Processing Unit (KPU). Amazon API Gateway is a fully managed service that allows you to publish, maintain, monitor, and secure APIs at any scale. Amazon API Gateway offers two options to create RESTful APIs, HTTP APIs and REST APIs, as well as an option to create WebSocket APIs. Amazon API Gateway: via - https://aws.amazon.com/blogs/aws/amazon-rds-custom-for-oracle-new-control-capabilities-in-database-environment/ For the given use case, you can use Amazon API Gateway to create a REST API that handles incoming requests having location data from the trucks and sends it to the Kinesis Data Analytics application on the back end. Amazon Kinesis Data Analytics: via - https://aws.amazon.com/kinesis/data-analytics/ Incorrect options: Leverage Amazon Athena with Amazon S3 - Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena cannot be used to build a REST API to consume data from the source. So this option is incorrect. Leverage Amazon QuickSight with Amazon Redshift - QuickSight is a cloud-native, serverless business intelligence service. Quicksight cannot be used to build a REST API to consume data from the source. Redshift is a fully managed AWS cloud data warehouse. So this option is incorrect. Leverage Amazon API Gateway with AWS Lambda - You cannot use Lambda to store and retrieve the location data for analysis, so this option is incorrect. References: https://docs.aws.amazon.com/apigateway/latest/developerguide/integrating-api-with-aws-services-kinesis.html https://aws.amazon.com/kinesis/data-analytics/ https://aws.amazon.com/kinesis/data-analytics/faqs/",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A development team requires permissions to list an Amazon S3 bucket and delete objects from that bucket. A systems administrator has created the following IAM policy to provide access to the bucket and applied that policy to the group. The group is not able to delete objects in the bucket. The company follows the principle of least privilege. \"Version\": \"2021-10-17\", \"Statement\": [ { \"Action\": [ \"s3:ListBucket\", \"s3:DeleteObject\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket\" ], \"Effect\": \"Allow\" } ] Which statement should a solutions architect add to the policy to address this issue?",
      "options": [
        "A. { \"Action\": [ \"s3:*Object\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket/*\" ], \"Effect\": \"Allow\" }",
        "B. { \"Action\": [ \"s3:DeleteObject\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket/*\" ], \"Effect\": \"Allow\" }",
        "C. { \"Action\": [ \"s3:DeleteObject\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket*\" ], \"Effect\": \"Allow\" }",
        "D. { \"Action\": [ \"s3:*\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket/*\" ], \"Effect\": \"Allow\" }"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: ** { \"Action\": [ \"s3:DeleteObject\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket/*\" ], \"Effect\": \"Allow\" } ** The main elements of a policy statement are: Effect: Specifies whether the statement will Allow or Deny an action (Allow is the effect defined here). Action: Describes a specific action or actions that will either be allowed or denied to run based on the Effect entered. API actions are unique to each service (DeleteObject is the action defined here). Resource: Specifies the resources—for example, an Amazon S3 bucket or objects—that the policy applies to in Amazon Resource Name (ARN) format ( example-bucket/* is the resource defined here). This policy provides the necessary delete permissions on the resources of the Amazon S3 bucket to the group. Incorrect options: ** { \"Action\": [ \"s3:*Object\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket/*\" ], \"Effect\": \"Allow\" } ** - This policy is incorrect as the action value is invalid ** { \"Action\": [ \"s3:*\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket/*\" ], \"Effect\": \"Allow\" } ** - This policy is incorrect since it allows all actions on the resource, which violates the principle of least privilege, as required by the given use case. ** { \"Action\": [ \"s3:DeleteObject\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket*\" ], \"Effect\": \"Allow\" } ** - This is incorrect, as the resource name is incorrect. It should have a /* after the bucket name. Reference: https://aws.amazon.com/blogs/security/techniques-for-writing-least-privilege-iam-policies/",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A gaming company is looking at improving the availability and performance of its global flagship application which utilizes User Datagram Protocol and needs to support fast regional failover in case an AWS Region goes down. The company wants to continue using its own custom Domain Name System (DNS) service. Which of the following AWS services represents the best solution for this use-case?",
      "options": [
        "A. AWS Elastic Load Balancing (ELB)",
        "B. AWS Global Accelerator",
        "C. Amazon CloudFront",
        "D. Amazon Route 53"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: AWS Global Accelerator AWS Global Accelerator utilizes the Amazon global network, allowing you to improve the performance of your applications by lowering first-byte latency (the round trip time for a packet to go from a client to your endpoint and back again) and jitter (the variation of latency), and increasing throughput (the amount of time it takes to transfer data) as compared to the public internet. AWS Global Accelerator improves performance for a wide range of applications over TCP or UDP by proxying packets at the edge to applications running in one or more AWS Regions. Global Accelerator is a good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or Voice over IP, as well as for HTTP use cases that specifically require static IP addresses or deterministic, fast regional failover. Incorrect options: Amazon CloudFront - Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment. AWS Global Accelerator and Amazon CloudFront are separate services that use the AWS global network and its edge locations around the world. CloudFront improves performance for both cacheable content (such as images and videos) and dynamic content (such as API acceleration and dynamic site delivery), while Global Accelerator improves performance for a wide range of applications over TCP or UDP. AWS Elastic Load Balancing (ELB) - Both of the services, ELB and Global Accelerator solve the challenge of routing user requests to healthy application endpoints. AWS Global Accelerator relies on ELB to provide the traditional load balancing features such as support for internal and non-AWS endpoints, pre-warming, and Layer 7 routing. However, while ELB provides load balancing within one Region, AWS Global Accelerator provides traffic management across multiple Regions. A regional ELB load balancer is an ideal target for AWS Global Accelerator. By using a regional ELB load balancer, you can precisely distribute incoming application traffic across backends, such as Amazon EC2 instances or Amazon ECS tasks, within an AWS Region. If you have workloads that cater to a global client base, AWS recommends that you use AWS Global Accelerator. If you have workloads hosted in a single AWS Region and used by clients in and around the same Region, you can use an Application Load Balancer or Network Load Balancer to manage your resources. Amazon Route 53 - Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. It is designed to give developers and businesses an extremely reliable and cost-effective way to route end users to Internet applications by translating names like www.example.com into the numeric IP addresses like 192.0.2.1 that computers use to connect to each other. Route 53 is ruled out as the company wants to continue using its own custom DNS service. Reference: https://aws.amazon.com/global-accelerator/faqs/",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A gaming company is developing a mobile game that streams score updates to a backend processor and then publishes results on a leaderboard. The company has hired you as an AWS Certified Solutions Architect Associate to design a solution that can handle major traffic spikes, process the mobile game updates in the order of receipt, and store the processed updates in a highly available database. The company wants to minimize the management overhead required to maintain the solution. Which of the following will you recommend to meet these requirements?",
      "options": [
        "A. Push score updates to an Amazon Simple Queue Service (Amazon SQS) queue which uses a fleet of Amazon EC2 instances (with Auto Scaling) to process these updates in the Amazon SQS queue and then store these processed updates in an Amazon RDS MySQL database",
        "B. Push score updates to Amazon Kinesis Data Streams which uses an AWS Lambda function to process these updates and then store these processed updates in Amazon DynamoDB",
        "C. Push score updates to Amazon Kinesis Data Streams which uses a fleet of Amazon EC2 instances (with Auto Scaling) to process the updates in Amazon Kinesis Data Streams and then store these processed updates in Amazon DynamoDB",
        "D. Push score updates to an Amazon Simple Notification Service (Amazon SNS) topic, subscribe an AWS Lambda function to this Amazon SNS topic to process the updates and then store these processed updates in a SQL database running on Amazon EC2 instance"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Push score updates to Amazon Kinesis Data Streams which uses an AWS Lambda function to process these updates and then store these processed updates in Amazon DynamoDB To help ingest real-time data or streaming data at large scales, you can use Amazon Kinesis Data Streams (KDS). KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources. The data collected is available in milliseconds, enabling real-time analytics. KDS provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications. AWS Lambda integrates natively with Kinesis Data Streams. The polling, checkpointing, and error handling complexities are abstracted when you use this native integration. The processed data can then be configured to be saved in Amazon DynamoDB. Incorrect options: Push score updates to an Amazon Simple Queue Service (Amazon SQS) queue which uses a fleet of Amazon EC2 instances (with Auto Scaling) to process these updates in the Amazon SQS queue and then store these processed updates in an Amazon RDS MySQL database Push score updates to Amazon Kinesis Data Streams which uses a fleet of Amazon EC2 instances (with Auto Scaling) to process the updates in Amazon Kinesis Data Streams and then store these processed updates in Amazon DynamoDB Push score updates to an Amazon Simple Notification Service (Amazon SNS) topic, subscribe an AWS Lambda function to this Amazon SNS topic to process the updates and then store these processed updates in a SQL database running on Amazon EC2 instance These three options use Amazon EC2 instances as part of the solution architecture. The use-case seeks to minimize the management overhead required to maintain the solution. However, Amazon EC2 instances involve several maintenance activities such as managing the guest operating system and software deployed to the guest operating system, including updates and security patches, etc. Hence these options are incorrect. Reference: https://aws.amazon.com/blogs/big-data/best-practices-for-consuming-amazon-kinesis-data-streams-using-aws-lambda/",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "While consolidating logs for the weekly reporting, a development team at an e-commerce company noticed that an unusually large number of illegal AWS application programming interface (API) queries were made sometime during the week. Due to the off-season, there was no visible impact on the systems. However, this event led the management team to seek an automated solution that can trigger near-real-time warnings in case such an event recurs. Which of the following represents the best solution for the given scenario?",
      "options": [
        "A. Configure AWS CloudTrail to stream event data to Amazon Kinesis. Use Amazon Kinesis stream-level metrics in the Amazon CloudWatch to trigger an AWS Lambda function that will trigger an error workflow",
        "B. Run Amazon Athena SQL queries against AWS CloudTrail log files stored in Amazon S3 buckets. Use Amazon QuickSight to generate reports for managerial dashboards",
        "C. AWS Trusted Advisor publishes metrics about check results to Amazon CloudWatch. Create an alarm to track status changes for checks in the Service Limits category for the APIs. The alarm will then notify when the service quota is reached or exceeded",
        "D. Create an Amazon CloudWatch metric filter that processes AWS CloudTrail logs having API call details and looks at any errors by factoring in all the error codes that need to be tracked. Create an alarm based on this metric's rate to send an Amazon SNS notification to the required team"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: Create an Amazon CloudWatch metric filter that processes AWS CloudTrail logs having API call details and looks at any errors by factoring in all the error codes that need to be tracked. Create an alarm based on this metric's rate to send an Amazon SNS notification to the required team AWS CloudTrail log data can be ingested into Amazon CloudWatch to monitor and identify your AWS account activity against security threats, and create a governance framework for security best practices. You can analyze log trail event data in CloudWatch using features such as Logs Insight, Contributor Insights, Metric filters, and CloudWatch Alarms. AWS CloudTrail integrates with the Amazon CloudWatch service to publish the API calls being made to resources or services in the AWS account. The published event has invaluable information that can be used for compliance, auditing, and governance of your AWS accounts. Below we introduce several features available in CloudWatch to monitor API activity, analyze the logs at scale, and take action when malicious activity is discovered, without provisioning your infrastructure. For the AWS Cloudtrail logs available in Amazon CloudWatch Logs, you can begin searching and filtering the log data by creating one or more metric filters. Use these metric filters to turn log data into numerical CloudWatch metrics that you can graph or set a CloudWatch Alarm on. Note: AWS CloudTrail Insights helps AWS users identify and respond to unusual activity associated with write API calls by continuously analyzing CloudTrail management events. Insights events are logged when AWS CloudTrail detects unusual write management API activity in your account. If you have AWS CloudTrail Insights enabled and CloudTrail detects unusual activity, Insights events are delivered to the destination Amazon S3 bucket for your trail. You can also see the type of insight and the incident time when you view Insights events on the CloudTrail console. Unlike other types of events captured in a CloudTrail trail, Insights events are logged only when CloudTrail detects changes in your account's API usage that differ significantly from the account's typical usage patterns. Incorrect options: Configure AWS CloudTrail to stream event data to Amazon Kinesis. Use Amazon Kinesis stream-level metrics in the Amazon CloudWatch to trigger an AWS Lambda function that will trigger an error workflow - AWS CloudTrail cannot stream data to Amazon Kinesis. Amazon S3 buckets and Amazon CloudWatch logs are the only destinations possible. Run Amazon Athena SQL queries against AWS CloudTrail log files stored in Amazon S3 buckets. Use Amazon QuickSight to generate reports for managerial dashboards - Generating reports and visualizations help in understanding and analyzing patterns but is not useful as a near-real-time automatic solution for the given problem. AWS Trusted Advisor publishes metrics about check results to Amazon CloudWatch. Create an alarm to track status changes for checks in the Service Limits category for the APIs. The alarm will then notify when the service quota is reached or exceeded - When AWS Trusted Advisor refreshes your checks, Trusted Advisor publishes metrics about your check results to Amazon CloudWatch. You can view the metrics in CloudWatch. You can also create alarms to detect status changes to Trusted Advisor checks and status changes for resources, and service quota usage (formerly referred to as limits). The alarm will then notify you when you reach or exceed a service quota for your AWS account. However, the alarm is triggered only when the service limit is reached. We need a solution that raises an alarm when the number of API calls randomly increases or an abnormal pattern is detected. Hence, this option is not the right fit for the given use case. References: https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudwatch-alarms-for-cloudtrail.html#cloudwatch-alarms-for-cloudtrail-authorization-failures https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-insights-events-with-cloudtrail.html https://docs.aws.amazon.com/awssupport/latest/user/cloudwatch-metrics-ta.html",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A retail company's dynamic website is hosted using on-premises servers in its data center in the United States. The company is launching its website in Asia, and it wants to optimize the website loading times for new users in Asia. The website's backend must remain in the United States. The website is being launched in a few days, and an immediate solution is needed. What would you recommend?",
      "options": [
        "A. Use Amazon CloudFront with a custom origin pointing to the DNS record of the website on Amazon Route 53",
        "B. Use Amazon CloudFront with a custom origin pointing to the on-premises servers",
        "C. Migrate the website to Amazon S3. Use S3 cross-region replication (S3 CRR) between AWS Regions in the US and Asia",
        "D. Leverage a Amazon Route 53 geo-proximity routing policy pointing to on-premises servers"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Use Amazon CloudFront with a custom origin pointing to the on-premises servers Amazon CloudFront is a web service that gives businesses and web application developers an easy and cost-effective way to distribute content with low latency and high data transfer speeds. Amazon CloudFront uses standard cache control headers you set on your files to identify static and dynamic content. You can use different origins for different types of content on a single site – e.g. Amazon S3 for static objects, Amazon EC2 for dynamic content, and custom origins for third-party content. Amazon CloudFront: via - https://aws.amazon.com/cloudfront/ An origin server stores the original, definitive version of your objects. If you're serving content over HTTP, your origin server is either an Amazon S3 bucket or an HTTP server, such as a web server. Your HTTP server can run on an Amazon Elastic Compute Cloud (Amazon EC2) instance or on a server that you manage; these servers are also known as custom origins. via - https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html Amazon CloudFront employs a global network of edge locations and regional edge caches that cache copies of your content close to your viewers. Amazon CloudFront ensures that end-user requests are served by the closest edge location. As a result, viewer requests travel a short distance, improving performance for your viewers. Therefore for the given use case, the users in Asia will enjoy a low latency experience while using the website even though the on-premises servers continue to be in the US. Incorrect options: Use Amazon CloudFront with a custom origin pointing to the DNS record of the website on Amazon Route 53 - This option has been added as a distractor. CloudFront cannot have a custom origin pointing to the DNS record of the website on Route 53. Migrate the website to Amazon S3. Use S3 cross-region replication (S3 CRR) between AWS Regions in the US and Asia - The use case states that the company operates a dynamic website. You can use Amazon S3 to host a static website. On a static website, individual web pages include static content. They might also contain client-side scripts. By contrast, a dynamic website relies on server-side processing, including server-side scripts, such as PHP, JSP, or ASP.NET. Amazon S3 does not support server-side scripting, but AWS has other resources for hosting dynamic websites. So this option is incorrect. Leverage a Amazon Route 53 geo-proximity routing policy pointing to on-premises servers - Since the on-premises servers continue to be in the US, so even using a Route 53 geo-proximity routing policy that directs the users in Asia to the on-premises servers in the US would not reduce the latency for the users in Asia. So this option is incorrect. References: https://aws.amazon.com/cloudfront/ https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A company is in the process of migrating its on-premises SMB file shares to AWS so the company can get out of the business of managing multiple file servers across dozens of offices. The company has 200 terabytes of data in its file servers. The existing on-premises applications and native Windows workloads should continue to have low latency access to this data which needs to be stored on a file system service without any disruptions after the migration. The company also wants any new applications deployed on AWS to have access to this migrated data. Which of the following is the best solution to meet this requirement?",
      "options": [
        "A. Use Amazon FSx File Gateway to provide low-latency, on-premises access to fully managed file shares in Amazon FSx for Windows File Server. The applications deployed on AWS can access this data directly from Amazon FSx in AWS",
        "B. Use Amazon Storage Gateway’s File Gateway to provide low-latency, on-premises access to fully managed file shares in Amazon FSx for Windows File Server. The applications deployed on AWS can access this data directly from Amazon FSx in AWS",
        "C. Use AWS Storage Gateway’s File Gateway to provide low-latency, on-premises access to fully managed file shares in Amazon S3. The applications deployed on AWS can access this data directly from Amazon S3",
        "D. Use Amazon FSx File Gateway to provide low-latency, on-premises access to fully managed file shares in Amazon EFS. The applications deployed on AWS can access this data directly from Amazon EFS"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Use Amazon FSx File Gateway to provide low-latency, on-premises access to fully managed file shares in Amazon FSx for Windows File Server. The applications deployed on AWS can access this data directly from Amazon FSx in AWS For user or team file shares, and file-based application migrations, Amazon FSx File Gateway provides low-latency, on-premises access to fully managed file shares in Amazon FSx for Windows File Server. For applications deployed on AWS, you may access your file shares directly from Amazon FSx in AWS. For your native Windows workloads and users, or your SMB clients, Amazon FSx for Windows File Server provides all of the benefits of a native Windows SMB environment that is fully managed and secured and scaled like any other AWS service. You get detailed reporting, replication, backup, failover, and support for native Windows tools like DFS and Active Directory. Amazon FSx File Gateway: via - https://aws.amazon.com/storagegateway/file/ Incorrect options: Use Amazon Storage Gateway’s File Gateway to provide low-latency, on-premises access to fully managed file shares in Amazon FSx for Windows File Server. The applications deployed on AWS can access this data directly from Amazon FSx in AWS - When you need to access S3 using a file system protocol, you should use File Gateway. You get a local cache in the gateway that provides high throughput and low latency over SMB. AWS Storage Gateway’s File Gateway does not support file shares in Amazon FSx for Windows File Server, so this option is incorrect. AWS Storage Gateway’s File Gateway: Use AWS Storage Gateway’s File Gateway to provide low-latency, on-premises access to fully managed file shares in Amazon S3. The applications deployed on AWS can access this data directly from Amazon S3 - When you need to access S3 using a file system protocol, you should use File Gateway. You get a local cache in the gateway that provides high throughput and low latency over SMB. The given use case requires low latency access to data which needs to be stored on a file system service after migration. Since S3 is an object storage service, so this option is incorrect. Use Amazon FSx File Gateway to provide low-latency, on-premises access to fully managed file shares in Amazon EFS. The applications deployed on AWS can access this data directly from Amazon EFS - Amazon FSx File Gateway provides access to fully managed file shares in Amazon FSx for Windows File Server and it does not support EFS. You should also note that EFS uses the Network File System version 4 (NFS v4) protocol and it does not support SMB protocol. Therefore this option is incorrect for the given use case. References: https://aws.amazon.com/storagegateway/file/fsx/ https://aws.amazon.com/storagegateway/faqs/ https://aws.amazon.com/blogs/storage/aws-reinvent-recap-choosing-storage-for-on-premises-file-based-workloads/",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A data analytics company measures what the consumers watch and what advertising they’re exposed to. This real-time data is ingested into its on-premises data center and subsequently, the daily data feed is compressed into a single file and uploaded on Amazon S3 for backup. The typical compressed file size is around 2 gigabytes. Which of the following is the fastest way to upload the daily compressed file into Amazon S3?",
      "options": [
        "A. Upload the compressed file in a single operation",
        "B. Upload the compressed file using multipart upload",
        "C. FTP the compressed file into an Amazon EC2 instance that runs in the same region as the Amazon S3 bucket. Then transfer the file from the Amazon EC2 instance into the Amazon S3 bucket",
        "D. Upload the compressed file using multipart upload with Amazon S3 Transfer Acceleration (Amazon S3TA)"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: Upload the compressed file using multipart upload with Amazon S3 Transfer Acceleration (Amazon S3TA) Amazon S3 Transfer Acceleration (Amazon S3TA) enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket. Transfer Acceleration takes advantage of Amazon CloudFront’s globally distributed edge locations. As the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path. Multipart upload allows you to upload a single object as a set of parts. Each part is a contiguous portion of the object's data. You can upload these object parts independently and in any order. If transmission of any part fails, you can retransmit that part without affecting other parts. After all parts of your object are uploaded, Amazon S3 assembles these parts and creates the object. If you're uploading large objects over a stable high-bandwidth network, use multipart uploading to maximize the use of your available bandwidth by uploading object parts in parallel for multi-threaded performance. If you're uploading over a spotty network, use multipart uploading to increase resiliency to network errors by avoiding upload restarts. Incorrect options: Upload the compressed file in a single operation - In general, when your object size reaches 100 megabytes, you should consider using multipart uploads instead of uploading the object in a single operation. Multipart upload provides improved throughput - you can upload parts in parallel to improve throughput. Therefore, this option is not correct. Upload the compressed file using multipart upload - Although using multipart upload would certainly speed up the process, combining with Amazon S3 Transfer Acceleration (Amazon S3TA) would further improve the transfer speed. Therefore just using multipart upload is not the correct option. FTP the compressed file into an Amazon EC2 instance that runs in the same region as the Amazon S3 bucket. Then transfer the file from the Amazon EC2 instance into the Amazon S3 bucket - This is a roundabout process of getting the file into Amazon S3 and added as a distractor. Although it is technically feasible to follow this process, it would involve a lot of scripting and certainly would not be the fastest way to get the file into Amazon S3. References: https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A large financial institution operates an on-premises data center with hundreds of petabytes of data managed on Microsoft’s Distributed File System (DFS). The CTO wants the organization to transition into a hybrid cloud environment and run data-intensive analytics workloads that support DFS. Which of the following AWS services can facilitate the migration of these workloads?",
      "options": [
        "A. Amazon FSx for Windows File Server",
        "B. Microsoft SQL Server on AWS",
        "C. Amazon FSx for Lustre",
        "D. AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD)"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Amazon FSx for Windows File Server Amazon FSx for Windows File Server provides fully managed, highly reliable file storage that is accessible over the industry-standard Service Message Block (SMB) protocol. It is built on Windows Server, delivering a wide range of administrative features such as user quotas, end-user file restore, and Microsoft Active Directory (AD) integration. Amazon FSx supports the use of Microsoft’s Distributed File System (DFS) to organize shares into a single folder structure up to hundreds of PB in size. So this option is correct. How Amazon FSx for Windows File Server Works: via - https://aws.amazon.com/fsx/windows/ Incorrect options: Amazon FSx for Lustre Amazon FSx for Lustre makes it easy and cost-effective to launch and run the world’s most popular high-performance file system. It is used for workloads such as machine learning, high-performance computing (HPC), video processing, and financial modeling. Amazon FSx enables you to use Lustre file systems for any workload where storage speed matters. FSx for Lustre does not support Microsoft’s Distributed File System (DFS), so this option is incorrect. AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD) AWS Directory Service for Microsoft Active Directory, also known as AWS Managed Microsoft AD, enables your directory-aware workloads and AWS resources to use managed Active Directory in the AWS Cloud. AWS Managed Microsoft AD is built on the actual Microsoft Active Directory and does not require you to synchronize or replicate data from your existing Active Directory to the cloud. AWS Managed Microsoft AD does not support Microsoft’s Distributed File System (DFS), so this option is incorrect. Microsoft SQL Server on AWS Microsoft SQL Server on AWS offers you the flexibility to run Microsoft SQL Server database on AWS Cloud. Microsoft SQL Server on AWS does not support Microsoft’s Distributed File System (DFS), so this option is incorrect. Reference: https://aws.amazon.com/fsx/windows/",
      "reference": "Source: Practice Test #1 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A cybersecurity company uses a fleet of Amazon EC2 instances to run a proprietary application. The infrastructure maintenance group at the company wants to be notified via an email whenever the CPU utilization for any of the Amazon EC2 instances breaches a certain threshold. Which of the following services would you use for building a solution with the LEAST amount of development effort? (Select two)",
      "options": [
        "A. Amazon Simple Notification Service (Amazon SNS)",
        "B. Amazon Simple Queue Service (Amazon SQS)",
        "C. Amazon CloudWatch",
        "D. AWS Lambda",
        "E. AWS Step Functions"
      ],
      "correct_answer": "A",
      "explanation": "Correct options: Amazon Simple Notification Service (Amazon SNS) Amazon Simple Notification Service (Amazon SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. Amazon SNS provides topics for high-throughput, push-based, many-to-many messaging. Amazon CloudWatch Amazon CloudWatch is a monitoring and observability service built for DevOps engineers, developers, site reliability engineers (SREs), and IT managers. Amazon CloudWatch provides you with data and actionable insights to monitor your applications, respond to system-wide performance changes, optimize resource utilization, and get a unified view of operational health. Amazon CloudWatch allows you to monitor AWS cloud resources and the applications you run on AWS. You can use Amazon CloudWatch Alarms to send an email via Amazon SNS whenever any of the Amazon EC2 instances breaches a certain threshold. Hence both these options are correct. Incorrect options: AWS Lambda - With AWS Lambda, you can run code without provisioning or managing servers. You pay only for the compute time that you consume—there’s no charge when your code isn’t running. You can run code for virtually any type of application or backend service—all with zero administration. You cannot use AWS Lambda to monitor CPU utilization of Amazon EC2 instances or send notification emails, hence this option is incorrect. Amazon Simple Queue Service (Amazon SQS) - Amazon SQS Standard offers a reliable, highly scalable hosted queue for storing messages as they travel between computers. Amazon SQS lets you easily move data between distributed application components and helps you build applications in which messages are processed independently (with message-level ack/fail semantics), such as automated workflows. You cannot use Amazon SQS to monitor CPU utilization of Amazon EC2 instances or send notification emails, hence this option is incorrect. AWS Step Functions - AWS Step Functions lets you coordinate multiple AWS services into serverless workflows so you can build and update apps quickly. Using Step Functions, you can design and run workflows that stitch together services, such as AWS Lambda, AWS Fargate, and Amazon SageMaker, into feature-rich applications. You cannot use Step Functions to monitor CPU utilization of Amazon EC2 instances or send notification emails, hence this option is incorrect. References: https://aws.amazon.com/cloudwatch/faqs/ https://aws.amazon.com/sns/",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A Hollywood studio is planning a series of promotional events leading up to the launch of the trailer of its next sci-fi thriller. The executives at the studio want to create a static website with lots of animations in line with the theme of the movie. The studio has hired you as a solutions architect to build a scalable serverless solution. Which of the following represents the MOST cost-optimal and high-performance solution?",
      "options": [
        "A. Build the website as a static website hosted on Amazon S3. Create an Amazon CloudFront distribution with Amazon S3 as the origin. Use Amazon Route 53 to create an alias record that points to your Amazon CloudFront distribution",
        "B. Host the website on an Amazon EC2 instance. Create a Amazon CloudFront distribution with the Amazon EC2 instance as the custom origin",
        "C. Host the website on an instance in the studio's on-premises data center. Create an Amazon CloudFront distribution with this instance as the custom origin",
        "D. Host the website on AWS Lambda. Create an Amazon CloudFront distribution with Lambda as the origin"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Build the website as a static website hosted on Amazon S3. Create an Amazon CloudFront distribution with Amazon S3 as the origin. Use Amazon Route 53 to create an alias record that points to your Amazon CloudFront distribution You can use Amazon S3 to host a static website. On a static website, individual web pages include static content. They might also contain client-side scripts. To host a static website on Amazon S3, you configure an Amazon S3 bucket for website hosting and then upload your website content to the bucket. Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment. You can use Amazon CloudFront to improve the performance of your website. CloudFront makes your website files (such as HTML, images, and video) available from data centers around the world (called edge locations). When a visitor requests a file from your website, CloudFront automatically redirects the request to a copy of the file at the nearest edge location. This results in faster download times than if the visitor had requested the content from a data center that is located farther away. Therefore, this option is correct. Hosting a static website on Amazon S3: via - https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html Incorrect options: Host the website on AWS Lambda. Create an Amazon CloudFront distribution with Lambda as the origin With AWS Lambda, you can run code without provisioning or managing servers. You can't host a website on Lambda. Also, you can't have CloudFront in front of Lambda. So this option is incorrect. Host the website on an Amazon EC2 instance. Create a Amazon CloudFront distribution with the Amazon EC2 instance as the custom origin Host the website on an instance in the studio's on-premises data center. Create an Amazon CloudFront distribution with this instance as the custom origin Hosting the website on an Amazon EC2 instance or a data-center specific instance is ruled out as the studio wants a serverless solution. So both these options are incorrect. References: https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html https://docs.aws.amazon.com/AmazonS3/latest/dev/website-hosting-custom-domain-walkthrough.html https://docs.aws.amazon.com/AmazonS3/latest/dev/website-hosting-cloudfront-walkthrough.html",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A social photo-sharing web application is hosted on Amazon Elastic Compute Cloud (Amazon EC2) instances behind an Elastic Load Balancer. The app gives the users the ability to upload their photos and also shows a leaderboard on the homepage of the app. The uploaded photos are stored in Amazon Simple Storage Service (Amazon S3) and the leaderboard data is maintained in Amazon DynamoDB. The Amazon EC2 instances need to access both Amazon S3 and Amazon DynamoDB for these features. As a solutions architect, which of the following solutions would you recommend as the MOST secure option?",
      "options": [
        "A. Attach the appropriate IAM role to the Amazon EC2 instance profile so that the instance can access Amazon S3 and Amazon DynamoDB",
        "B. Save the AWS credentials (access key Id and secret access token) in a configuration file within the application code on the Amazon EC2 instances. Amazon EC2 instances can use these credentials to access Amazon S3 and Amazon DynamoDB",
        "C. Configure AWS CLI on the Amazon EC2 instances using a valid IAM user's credentials. The application code can then invoke shell scripts to access Amazon S3 and Amazon DynamoDB via AWS CLI",
        "D. Encrypt the AWS credentials via a custom encryption library and save it in a secret directory on the Amazon EC2 instances. The application code can then safely decrypt the AWS credentials to make the API calls to Amazon S3 and Amazon DynamoDB"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Attach the appropriate IAM role to the Amazon EC2 instance profile so that the instance can access Amazon S3 and Amazon DynamoDB Applications that run on an Amazon EC2 instance must include AWS credentials in their AWS API requests. You could have your developers store AWS credentials directly within the Amazon EC2 instance and allow applications in that instance to use those credentials. But developers would then have to manage the credentials and ensure that they securely pass the credentials to each instance and update each Amazon EC2 instance when it's time to rotate the credentials. Instead, you should use an IAM role to manage temporary credentials for applications that run on an Amazon EC2 instance. When you use a role, you don't have to distribute long-term credentials (such as a username and password or access keys) to an Amazon EC2 instance. The role supplies temporary permissions that applications can use when they make calls to other AWS resources. When you launch an Amazon EC2 instance, you specify an IAM role to associate with the instance. Applications that run on the instance can then use the role-supplied temporary credentials to sign API requests. Therefore, this option is correct. via - https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html Incorrect options: Save the AWS credentials (access key Id and secret access token) in a configuration file within the application code on the Amazon EC2 instances. Amazon EC2 instances can use these credentials to access Amazon S3 and Amazon DynamoDB Configure AWS CLI on the Amazon EC2 instances using a valid IAM user's credentials. The application code can then invoke shell scripts to access Amazon S3 and Amazon DynamoDB via AWS CLI Encrypt the AWS credentials via a custom encryption library and save it in a secret directory on the Amazon EC2 instances. The application code can then safely decrypt the AWS credentials to make the API calls to Amazon S3 and Amazon DynamoDB Keeping the AWS credentials (encrypted or plain text) on the Amazon EC2 instance is a bad security practice, therefore these three options using the AWS credentials are incorrect. Reference: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A weather forecast agency collects key weather metrics across multiple cities in the US and sends this data in the form of key-value pairs to AWS Cloud at a one-minute frequency. As a solutions architect, which of the following AWS services would you use to build a solution for processing and then reliably storing this data with high availability? (Select two)",
      "options": [
        "A. AWS Lambda",
        "B. Amazon ElastiCache",
        "C. Amazon DynamoDB",
        "D. Amazon RDS",
        "E. Amazon Redshift"
      ],
      "correct_answer": "A",
      "explanation": "Correct options: AWS Lambda With AWS Lambda, you can run code without provisioning or managing servers. You pay only for the compute time that you consume—there’s no charge when your code isn’t running. You can run code for virtually any type of application or backend service—all with zero administration. Amazon DynamoDB Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications. Amazon DynamoDB is a NoSQL database and it's best suited to store data in key-value pairs. AWS Lambda can be combined with DynamoDB to process and capture the key-value data from the IoT sources described in the use-case. So both these options are correct. Incorrect options: Amazon Redshift - Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large scale data set storage and analysis. You cannot use Redshift to capture data in key-value pairs from the IoT sources, so this option is not correct. Amazon ElastiCache - Amazon ElastiCache allows you to seamlessly set up, run, and scale popular open-Source compatible in-memory data stores in the cloud. Build data-intensive apps or boost the performance of your existing databases by retrieving data from high throughput and low latency in-memory data stores. Amazon ElastiCache is a popular choice for real-time use cases like Caching, Session Stores, Gaming, Geospatial Services, Real-Time Analytics, and Queuing. Elasticache is used as a caching layer in front of relational databases. It is not a good fit to store data in key-value pairs from the IoT sources, so this option is not correct. Amazon RDS - Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching, and backups. Relational databases are not a good fit to store data in key-value pairs, so this option is not correct. References: https://aws.amazon.com/dynamodb/ https://aws.amazon.com/lambda/faqs/",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An IT company has an Access Control Management (ACM) application that uses Amazon RDS for MySQL but is running into performance issues despite using Read Replicas. The company has hired you as a solutions architect to address these performance-related challenges without moving away from the underlying relational database schema. The company has branch offices across the world, and it needs the solution to work on a global scale. Which of the following will you recommend as the MOST cost-effective and high-performance solution?",
      "options": [
        "A. Use Amazon Aurora Global Database to enable fast local reads with low latency in each region",
        "B. Spin up Amazon EC2 instances in each AWS region, install MySQL databases and migrate the existing data into these new databases",
        "C. Use Amazon DynamoDB Global Tables to provide fast, local, read and write performance in each region",
        "D. Spin up a Amazon Redshift cluster in each AWS region. Migrate the existing data into Redshift clusters"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Use Amazon Aurora Global Database to enable fast local reads with low latency in each region Amazon Aurora is a MySQL and PostgreSQL-compatible relational database built for the cloud, that combines the performance and availability of traditional enterprise databases with the simplicity and cost-effectiveness of open source databases. Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 64TB per database instance. Aurora is not an in-memory database. Amazon Aurora Global Database is designed for globally distributed applications, allowing a single Amazon Aurora database to span multiple AWS regions. It replicates your data with no impact on database performance, enables fast local reads with low latency in each region, and provides disaster recovery from region-wide outages. Amazon Aurora Global Database is the correct choice for the given use-case. Amazon Aurora Global Database Features: via - https://aws.amazon.com/rds/aurora/global-database/ Incorrect options: Use Amazon DynamoDB Global Tables to provide fast, local, read and write performance in each region - Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications. Global Tables builds upon DynamoDB’s global footprint to provide you with a fully managed, multi-region, and multi-master database that provides fast, local, read, and write performance for massively scaled, global applications. Global Tables replicates your Amazon DynamoDB tables automatically across your choice of AWS regions. Given that the use-case wants you to continue with the underlying schema of the relational database, DynamoDB is not the right choice as it's a NoSQL database. Amazon DynamoDB Global Tables Overview: via - https://aws.amazon.com/dynamodb/global-tables/ Spin up a Amazon Redshift cluster in each AWS region. Migrate the existing data into Redshift clusters - Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large scale data set storage and analysis. Amazon Redshift is not suited to be used as a transactional relational database, so this option is not correct. Spin up Amazon EC2 instances in each AWS region, install MySQL databases and migrate the existing data into these new databases - Setting up Amazon EC2 instances in multiple regions with manually managed MySQL databases represents a maintenance nightmare and is not the correct choice for this use-case. References: https://aws.amazon.com/rds/aurora/global-database/ https://aws.amazon.com/dynamodb/global-tables/",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A silicon valley based startup has a content management application with the web-tier running on Amazon EC2 instances and the database tier running on Amazon Aurora. Currently, the entire infrastructure is located in us-east-1 region. The startup has 90% of its customers in the US and Europe. The engineering team is getting reports of deteriorated application performance from customers in Europe with high application load time. As a solutions architect, which of the following would you recommend addressing these performance issues? (Select two)",
      "options": [
        "A. Setup another fleet of Amazon EC2 instances for the web tier in the eu-west-1 region. Enable latency routing policy in Amazon Route 53",
        "B. Setup another fleet of Amazon EC2 instances for the web tier in the eu-west-1 region. Enable geolocation routing policy in Amazon Route 53",
        "C. Create Amazon Aurora read replicas in the eu-west-1 region",
        "D. Create Amazon Aurora Multi-AZ standby instance in the eu-west-1 region",
        "E. Setup another fleet of Amazon EC2 instances for the web tier in the eu-west-1 region. Enable failover routing policy in Amazon Route 53"
      ],
      "correct_answer": "A",
      "explanation": "Correct options: Setup another fleet of Amazon EC2 instances for the web tier in the eu-west-1 region. Enable latency routing policy in Amazon Route 53 Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. Use latency based routing when you have resources in multiple AWS Regions and you want to route traffic to the region that provides the lowest latency. To use latency-based routing, you create latency records for your resources in multiple AWS Regions. When Amazon Route 53 receives a DNS query for your domain or subdomain (example.com or acme.example.com), it determines which AWS Regions you've created latency records for, determines which region gives the user the lowest latency, and then selects a latency record for that region. Route 53 responds with the value from the selected record, such as the IP address for a web server. As customers in Europe are facing performance issues with high application load time, you can use latency based routing to reduce the latency. Hence this is the correct option. Amazon Route 53 Routing Policy Overview: via - https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html Create Amazon Aurora read replicas in the eu-west-1 region Amazon Aurora is a MySQL and PostgreSQL-compatible relational database built for the cloud, that combines the performance and availability of traditional enterprise databases with the simplicity and cost-effectiveness of open source databases. Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 64TB per database instance. Amazon Aurora read replicas can be used to scale out reads across regions. This will improve the application performance for users in Europe. Therefore, this is also a correct option for the given use-case. Incorrect options: Setup another fleet of Amazon EC2 instances for the web tier in the eu-west-1 region. Enable geolocation routing policy in Amazon Route 53 - Geolocation routing lets you choose the resources that serve your traffic based on the geographic location of your users, meaning the location that DNS queries originate from. For example, you might want all queries from Europe to be routed to an ELB load balancer in the Frankfurt region. You can also use geolocation routing to restrict the distribution of content to only the locations in which you have distribution rights. You cannot use geolocation routing to reduce latency, hence this option is incorrect. Setup another fleet of Amazon EC2 instances for the web tier in the eu-west-1 region. Enable failover routing policy in Amazon Route 53 - Failover routing lets you route traffic to a resource when the resource is healthy or to a different resource when the first resource is unhealthy. The primary and secondary records can route traffic to anything from an Amazon S3 bucket that is configured as a website to a complex tree of records. You cannot use failover routing to reduce latency, hence this option is incorrect. Create Amazon Aurora Multi-AZ standby instance in the eu-west-1 region - Amazon Aurora Multi-AZ enhances the availability and durability for the database, it does not help in read scaling, so it is not a correct option for the given use-case. References: https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html https://aws.amazon.com/blogs/aws/new-cross-region-read-replicas-for-amazon-aurora/",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "The engineering team at an e-commerce company is working on cost optimizations for Amazon Elastic Compute Cloud (Amazon EC2) instances. The team wants to manage the workload using a mix of on-demand and spot instances across multiple instance types. They would like to create an Auto Scaling group with a mix of these instances. Which of the following options would allow the engineering team to provision the instances for this use-case?",
      "options": [
        "A. You can only use a launch template to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost",
        "B. You can only use a launch configuration to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost",
        "C. You can use a launch configuration or a launch template to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost",
        "D. You can neither use a launch configuration nor a launch template to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: You can only use a launch template to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost A launch template is similar to a launch configuration, in that it specifies instance configuration information such as the ID of the Amazon Machine Image (AMI), the instance type, a key pair, security groups, and the other parameters that you use to launch EC2 instances. Also, defining a launch template instead of a launch configuration allows you to have multiple versions of a template. With launch templates, you can provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost. Hence this is the correct option. Incorrect options: You can only use a launch configuration to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost You can use a launch configuration or a launch template to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost A launch configuration is an instance configuration template that an Auto Scaling group uses to launch EC2 instances. When you create a launch configuration, you specify information for the instances such as the ID of the Amazon Machine Image (AMI), the instance type, a key pair, one or more security groups, and a block device mapping. You cannot use a launch configuration to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances. Therefore both these options are incorrect. You can neither use a launch configuration nor a launch template to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost - You can use a launch template to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances. So this option is incorrect. References: https://docs.aws.amazon.com/autoscaling/ec2/userguide/LaunchTemplates.html https://docs.aws.amazon.com/autoscaling/ec2/userguide/LaunchConfiguration.html",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An IT company is working on a client project to build a Supply Chain Management application. The web-tier of the application runs on an Amazon EC2 instance and the database tier is on Amazon RDS MySQL. For beta testing, all the resources are currently deployed in a single Availability Zone (AZ). The development team wants to improve application availability before the go-live. Given that all end users of the web application would be located in the US, which of the following would be the MOST resource-efficient solution?",
      "options": [
        "A. Deploy the web-tier Amazon EC2 instances in two Availability Zones (AZs), behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in read replica configuration",
        "B. Deploy the web-tier Amazon EC2 instances in two Availability Zones (AZs), behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in Multi-AZ configuration",
        "C. Deploy the web-tier Amazon EC2 instances in two regions, behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in Multi-AZ configuration",
        "D. Deploy the web-tier Amazon EC2 instances in two regions, behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in read replica configuration"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Deploy the web-tier Amazon EC2 instances in two Availability Zones (AZs), behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in Multi-AZ configuration Elastic Load Balancing automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and Lambda functions. It can handle the varying load of your application traffic in a single Availability Zone or across multiple Availability Zones. Therefore, deploying the web-tier Amazon EC2 instances in two Availability Zones (AZs), behind an Elastic Load Balancer would improve the availability of the application. Amazon RDS Multi-AZ deployments provide enhanced availability and durability for RDS database (DB) instances, making them a natural fit for production database workloads. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). Each Availability Zone (AZ) runs on its own physically distinct, independent infrastructure, and is engineered to be highly reliable. Deploying the Amazon RDS MySQL database in Multi-AZ configuration would improve availability and hence this is the correct option. Incorrect options: Deploy the web-tier Amazon EC2 instances in two Availability Zones (AZs), behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in read replica configuration Deploy the web-tier Amazon EC2 instances in two regions, behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in read replica configuration Amazon RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. Read replicas are meant to address scalability issues. You cannot use read replicas for improving availability, so both these options are incorrect. Exam Alert: Please review this comparison vis-a-vis Multi-AZ vs Read Replica for Amazon RDS: via - https://aws.amazon.com/rds/features/multi-az/ Deploy the web-tier Amazon EC2 instances in two regions, behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in Multi-AZ configuration - As Elastic Load Balancing does not work across regions, so this option is incorrect. Reference: https://aws.amazon.com/rds/features/multi-az/",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A financial services company has deployed its flagship application on Amazon EC2 instances. Since the application handles sensitive customer data, the security team at the company wants to ensure that any third-party Secure Sockets Layer certificate (SSL certificate) SSL/Transport Layer Security (TLS) certificates configured on Amazon EC2 instances via the AWS Certificate Manager (ACM) are renewed before their expiry date. The company has hired you as an AWS Certified Solutions Architect Associate to build a solution that notifies the security team 30 days before the certificate expiration. The solution should require the least amount of scripting and maintenance effort. What will you recommend?",
      "options": [
        "A. Monitor the days to expiry Amazon CloudWatch metric for certificates created via ACM. Create a CloudWatch alarm to monitor such certificates based on the days to expiry metric and then trigger a custom action of notifying the security team",
        "B. Monitor the days to expiry Amazon CloudWatch metric for certificates imported into ACM. Create a CloudWatch alarm to monitor such certificates based on the days to expiry metric and then trigger a custom action of notifying the security team",
        "C. Leverage AWS Config managed rule to check if any SSL/TLS certificates created via ACM are marked for expiration within 30 days. Configure the rule to trigger an Amazon SNS notification to the security team if any certificate expires within 30 days",
        "D. Leverage AWS Config managed rule to check if any third-party SSL/TLS certificates imported into ACM are marked for expiration within 30 days. Configure the rule to trigger an Amazon SNS notification to the security team if any certificate expires within 30 days"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: Leverage AWS Config managed rule to check if any third-party SSL/TLS certificates imported into ACM are marked for expiration within 30 days. Configure the rule to trigger an Amazon SNS notification to the security team if any certificate expires within 30 days AWS Certificate Manager is a service that lets you easily provision, manage, and deploy public and private Secure Sockets Layer/Transport Layer Security (SSL/TLS) certificates for use with AWS services and your internal connected resources. SSL/TLS certificates are used to secure network communications and establish the identity of websites over the Internet as well as resources on private networks. AWS Config provides a detailed view of the configuration of AWS resources in your AWS account. This includes how the resources are related to one another and how they were configured in the past so that you can see how the configurations and relationships change over time. via - https://docs.aws.amazon.com/config/latest/developerguide/how-does-config-work.html AWS Config provides AWS-managed rules, which are predefined, customizable rules that AWS Config uses to evaluate whether your AWS resources comply with common best practices. You can leverage an AWS Config managed rule to check if any ACM certificates in your account are marked for expiration within the specified number of days. Certificates provided by ACM are automatically renewed. ACM does not automatically renew the certificates that you import. The rule is NON_COMPLIANT if your certificates are about to expire. via - https://docs.aws.amazon.com/config/latest/developerguide/how-does-config-work.html You can configure AWS Config to stream configuration changes and notifications to an Amazon SNS topic. For example, when a resource is updated, you can get a notification sent to your email, so that you can view the changes. You can also be notified when AWS Config evaluates your custom or managed rules against your resources. Incorrect options: Monitor the days to expiry Amazon CloudWatch metric for certificates imported into ACM. Create a CloudWatch alarm to monitor such certificates based on the days to expiry metric and then trigger a custom action of notifying the security team - AWS Certificate Manager (ACM) does not attempt to renew third-party certificates that are imported. Also, an administrator needs to reconfigure missing DNS records for certificates that use DNS validation if the record was removed for any reason after the certificate was issued. Metrics and events provide you visibility into such certificates that require intervention to continue the renewal process. Amazon CloudWatch metrics and Amazon EventBridge events are enabled for all certificates that are managed by ACM. Users can monitor days to expiry as a metric for ACM certificates through Amazon CloudWatch. An Amazon EventBridge expiry event is published for any certificate that is at least 45 days away from expiry by default. Users can build alarms to monitor certificates based on days to expiry and also trigger custom actions such as calling a Lambda function or paging an administrator. It is certainly possible to use the days to expiry CloudWatch metric to build a CloudWatch alarm to monitor the imported ACM certificates. The alarm will, in turn, trigger a notification to the security team. But this option needs more configuration effort than directly using the AWS Config managed rule that is available off-the-shelf. Leverage AWS Config managed rule to check if any SSL/TLS certificates created via ACM are marked for expiration within 30 days. Configure the rule to trigger an Amazon SNS notification to the security team if any certificate expires within 30 days Monitor the days to expiry Amazon CloudWatch metric for certificates created via ACM. Create a CloudWatch alarm to monitor such certificates based on the days to expiry metric and then trigger a custom action of notifying the security team Any SSL/TLS certificates created via ACM do not need any monitoring/intervention for expiration. ACM automatically renews such certificates. Hence both these options are incorrect. References: https://docs.aws.amazon.com/config/latest/developerguide/WhatIsConfig.html https://docs.aws.amazon.com/config/latest/developerguide/how-does-config-work.html https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config.html https://docs.aws.amazon.com/config/latest/developerguide/acm-certificate-expiration-check.html https://aws.amazon.com/blogs/security/how-to-monitor-expirations-of-imported-certificates-in-aws-certificate-manager-acm/",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A financial services company has developed its flagship application on AWS Cloud with data security requirements such that the encryption key must be stored in a custom application running on-premises. The company wants to offload the data storage as well as the encryption process to Amazon S3 but continue to use the existing encryption key. Which of the following Amazon S3 encryption options allows the company to leverage Amazon S3 for storing data with given constraints?",
      "options": [
        "A. Server-Side Encryption with Amazon S3 managed keys (SSE-S3)",
        "B. Server-Side Encryption with AWS Key Management Service (AWS KMS) keys (SSE-KMS)",
        "C. Server-Side Encryption with Customer-Provided Keys (SSE-C)",
        "D. Client-Side Encryption with data encryption is done on the client-side before sending it to Amazon S3"
      ],
      "correct_answer": "C",
      "explanation": "Correct option: Server-Side Encryption with Customer-Provided Keys (SSE-C) You have the following options for protecting data at rest in Amazon S3: Server-Side Encryption – Request Amazon S3 to encrypt your object before saving it on disks in its data centers and then decrypt it when you download the objects. Client-Side Encryption – Encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools. For the given use-case, the company wants to manage the encryption keys via its custom application and let Amazon S3 manage the encryption, therefore you must use Server-Side Encryption with Customer-Provided Keys (SSE-C). Please review these three options for Server Side Encryption on Amazon S3: via - https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html Incorrect options: Server-Side Encryption with Amazon S3 managed keys (SSE-S3) - When you use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key. As an additional safeguard, it encrypts the key itself with a master key that it regularly rotates. So this option is incorrect. Server-Side Encryption with AWS Key Management Service (AWS KMS) keys (SSE-KMS) - Unless you specify otherwise, buckets use SSE-S3 by default to encrypt objects. However, you can choose to configure buckets to use server-side encryption with AWS Key Management Service (AWS KMS) keys (SSE-KMS) instead. Amazon S3 uses server-side encryption with AWS KMS (SSE-KMS) to encrypt your S3 object data. Also, when SSE-KMS is requested for the object, the S3 checksum as part of the object's metadata, is stored in encrypted form. Client-Side Encryption with data encryption is done on the client-side before sending it to Amazon S3 - You can encrypt the data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools. Reference: https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A media company has created an AWS Direct Connect connection for migrating its flagship application to the AWS Cloud. The on-premises application writes hundreds of video files into a mounted NFS file system daily. Post-migration, the company will host the application on an Amazon EC2 instance with a mounted Amazon Elastic File System (Amazon EFS) file system. Before the migration cutover, the company must build a process that will replicate the newly created on-premises video files to the Amazon EFS file system. Which of the following represents the MOST operationally efficient way to meet this requirement?",
      "options": [
        "A. Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an AWS VPC peering endpoint for Amazon EFS by using a private VIF. Set up an AWS DataSync scheduled task to send the video files to the Amazon EFS file system every 24 hours",
        "B. Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an Amazon S3 bucket by using public VIF. Set up an AWS Lambda function to process event notifications from Amazon S3 and copy the video files from Amazon S3 to the Amazon EFS file system",
        "C. Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an AWS PrivateLink interface VPC endpoint for Amazon EFS by using a private VIF. Set up an AWS DataSync scheduled task to send the video files to the Amazon EFS file system every 24 hours",
        "D. Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an Amazon S3 bucket by using a VPC gateway endpoint for Amazon S3. Set up an AWS Lambda function to process event notifications from Amazon S3 and copy the video files from Amazon S3 to the Amazon EFS file system"
      ],
      "correct_answer": "C",
      "explanation": "Correct option: Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an AWS PrivateLink interface VPC endpoint for Amazon EFS by using a private VIF. Set up an AWS DataSync scheduled task to send the video files to the Amazon EFS file system every 24 hours AWS DataSync is an online data transfer service that simplifies, automates, and accelerates copying large amounts of data between on-premises storage systems and AWS Storage services, as well as between AWS Storage services. You can use AWS DataSync to migrate data located on-premises, at the edge, or in other clouds to Amazon S3, Amazon EFS, Amazon FSx for Windows File Server, Amazon FSx for Lustre, Amazon FSx for OpenZFS, and Amazon FSx for NetApp ONTAP. AWS DataSync: via - https://aws.amazon.com/datasync/ To establish a private connection between your virtual private cloud (VPC) and the Amazon EFS API, you can create an interface VPC endpoint. You can also access the interface VPC endpoint from on-premises environments or other VPCs using AWS VPN, AWS Direct Connect, or VPC peering. AWS Direct Connect provides three types of virtual interfaces: public, private, and transit. AWS Direct Connect VIFs: via - https://aws.amazon.com/premiumsupport/knowledge-center/public-private-interface-dx/ For the given use case, you can send data over the Direct Connect connection to an AWS PrivateLink interface VPC endpoint for Amazon EFS by using a private VIF. Using task scheduling in AWS DataSync, you can periodically execute a transfer task from your source storage system to the destination. You can use the DataSync scheduled task to send the video files to the Amazon EFS file system every 24 hours. Incorrect options: Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an AWS VPC peering endpoint for Amazon EFS by using a private VIF. Set up an AWS DataSync scheduled task to send the video files to the Amazon EFS file system every 24 hours - A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them privately. You cannot use VPC peering to transfer data over the Direct Connect connection from the on-premises systems to AWS. So this option is incorrect. Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an Amazon S3 bucket by using public VIF. Set up an AWS Lambda function to process event notifications from Amazon S3 and copy the video files from Amazon S3 to the Amazon EFS file system - You can use a public virtual interface to connect to AWS resources that are reachable by a public IP address such as an Amazon Simple Storage Service (Amazon S3) bucket or AWS public endpoints. Although it is theoretically possible to set up this solution, however, it is not the most operationally efficient solution, since it involves sending data via AWS DataSync to Amazon S3 and then in turn using an AWS Lambda function to finally send data to Amazon EFS. Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an Amazon S3 bucket by using a VPC gateway endpoint for Amazon S3. Set up an AWS Lambda function to process event notifications from Amazon S3 and copy the video files from Amazon S3 to the Amazon EFS file system - You can access Amazon S3 from your VPC using gateway VPC endpoints. You cannot use the Amazon S3 gateway endpoint to transfer data over the AWS Direct Connect connection from the on-premises systems to Amazon S3. So this option is incorrect. References: https://aws.amazon.com/datasync/ https://aws.amazon.com/blogs/storage/transferring-files-from-on-premises-to-aws-and-back-without-leaving-your-vpc-using-aws-datasync/ https://docs.aws.amazon.com/efs/latest/ug/efs-vpc-endpoints.html https://aws.amazon.com/datasync/faqs/ https://aws.amazon.com/premiumsupport/knowledge-center/public-private-interface-dx/ https://docs.aws.amazon.com/datasync/latest/userguide/task-scheduling.html",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A big-data consulting firm is working on a client engagement where the extract, transform, and load (ETL) workloads are currently handled via a Hadoop cluster deployed in the on-premises data center. The client wants to migrate their ETL workloads to AWS Cloud. The AWS Cloud solution needs to be highly available with about 50 Amazon Elastic Compute Cloud (Amazon EC2) instances per Availability Zone (AZ). As a solutions architect, which of the following Amazon EC2 placement groups would you recommend for handling the distributed ETL workload?",
      "options": [
        "A. Cluster placement group",
        "B. Spread placement group",
        "C. Partition placement group",
        "D. Both Spread placement group and Partition placement group"
      ],
      "correct_answer": "C",
      "explanation": "Correct option: Partition placement group You can use placement groups to influence the placement of a group of interdependent instances to meet the needs of your workload. Depending on the type of workload, you can create a placement group using one of the following placement strategies: Partition placement group – spreads your instances across logical partitions such that groups of instances in one partition do not share the underlying hardware with groups of instances in different partitions. This strategy is typically used by large distributed and replicated workloads, such as Hadoop, Cassandra, and Kafka. Therefore, this is the correct option for the given use-case. via - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html Incorrect options: Cluster placement group Cluster Placement Group – packs instances close together inside an Availability Zone. This strategy enables workloads to achieve the low-latency network performance necessary for tightly-coupled node-to-node communication that is typical of HPC applications. This is not suited for distributed and replicated workloads such as Hadoop. via - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html Spread placement group Spread Placement Group – strictly places a small group of instances across distinct underlying hardware to reduce correlated failures. This is not suited for distributed and replicated workloads such as Hadoop. via - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html Both Spread placement group and Partition placement group - As mentioned earlier, the spread placement group is not suited for distributed and replicated workloads such as Hadoop. So this option is also incorrect. Reference: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An e-commerce application uses an Amazon Aurora Multi-AZ deployment for its database. While analyzing the performance metrics, the engineering team has found that the database reads are causing high input/output (I/O) and adding latency to the write requests against the database. As an AWS Certified Solutions Architect Associate, what would you recommend to separate the read requests from the write requests?",
      "options": [
        "A. Provision another Amazon Aurora database and link it to the primary database as a read replica",
        "B. Set up a read replica and modify the application to use the appropriate endpoint",
        "C. Configure the application to read from the Multi-AZ standby instance",
        "D. Activate read-through caching on the Amazon Aurora database"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Set up a read replica and modify the application to use the appropriate endpoint An Amazon Aurora DB cluster consists of one or more DB instances and a cluster volume that manages the data for those DB instances. An Aurora cluster volume is a virtual database storage volume that spans multiple Availability Zones (AZs), with each Availability Zone (AZ) having a copy of the DB cluster data. Two types of DB instances make up an Aurora DB cluster: Primary DB instance – Supports read and write operations, and performs all of the data modifications to the cluster volume. Each Aurora DB cluster has one primary DB instance. Aurora Replica – Connects to the same storage volume as the primary DB instance and supports only read operations. Each Aurora DB cluster can have up to 15 Aurora Replicas in addition to the primary DB instance. Aurora automatically fails over to an Aurora Replica in case the primary DB instance becomes unavailable. You can specify the failover priority for Aurora Replicas. Aurora Replicas can also offload read workloads from the primary DB instance. via - https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.html Aurora Replicas have two main purposes. You can issue queries to them to scale the read operations for your application. You typically do so by connecting to the reader endpoint of the cluster. That way, Aurora can spread the load for read-only connections across as many Aurora Replicas as you have in the cluster. Aurora Replicas also help to increase availability. If the writer instance in a cluster becomes unavailable, Aurora automatically promotes one of the reader instances to take its place as the new writer. While setting up a Multi-AZ deployment for Aurora, you create an Aurora replica or reader node in a different Availability Zone (AZ). Multi-AZ for Aurora: You use the reader endpoint for read-only connections for your Aurora cluster. This endpoint uses a load-balancing mechanism to help your cluster handle a query-intensive workload. The reader endpoint is the endpoint that you supply to applications that do reporting or other read-only operations on the cluster. The reader endpoint load-balances connections to available Aurora Replicas in an Aurora DB cluster. via - https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html Incorrect options: Provision another Amazon Aurora database and link it to the primary database as a read replica - You cannot provision another Aurora database and then link it as a read-replica for the primary database. This option is ruled out. Configure the application to read from the Multi-AZ standby instance - This option has been added as a distractor as Aurora does not have any entity called standby instance. You create a standby instance while setting up a Multi-AZ deployment for Amazon RDS and NOT for Aurora. Multi-AZ for Amazon RDS: Activate read-through caching on the Amazon Aurora database - Amazon Aurora does not have built-in support for read-through caching, so this option just serves as a distractor. To implement caching, you will need to integrate something like Amazon ElastiCache and that would need code changes for the application. References: https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.html https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.AuroraHighAvailability.html https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An IT company provides Amazon Simple Storage Service (Amazon S3) bucket access to specific users within the same account for completing project specific work. With changing business requirements, cross-account S3 access requests are also growing every month. The company is looking for a solution that can offer user level as well as account-level access permissions for the data stored in Amazon S3 buckets. As a Solutions Architect, which of the following would you suggest as the MOST optimized way of controlling access for this use-case?",
      "options": [
        "A. Use Amazon S3 Bucket Policies",
        "B. Use Identity and Access Management (IAM) policies",
        "C. Use Access Control Lists (ACLs)",
        "D. Use Security Groups"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Use Amazon S3 Bucket Policies Bucket policies in Amazon S3 can be used to add or deny permissions across some or all of the objects within a single bucket. Policies can be attached to users, groups, or Amazon S3 buckets, enabling centralized management of permissions. With bucket policies, you can grant users within your AWS Account or other AWS Accounts access to your Amazon S3 resources. You can further restrict access to specific resources based on certain conditions. For example, you can restrict access based on request time (Date Condition), whether the request was sent using SSL (Boolean Conditions), a requester’s IP address (IP Address Condition), or based on the requester's client application (String Conditions). To identify these conditions, you use policy keys. Types of access control in Amazon S3: via - https://d1.awsstatic.com/whitepapers/aws-security-whitepaper.pdf Incorrect options: Use Identity and Access Management (IAM) policies - AWS IAM enables organizations with many employees to create and manage multiple users under a single AWS account. IAM policies are attached to the users, enabling centralized control of permissions for users under your AWS Account to access buckets or objects. With IAM policies, you can only grant users within your own AWS account permission to access your Amazon S3 resources. So, this is not the right choice for the current requirement. Use Access Control Lists (ACLs) - Within Amazon S3, you can use ACLs to give read or write access on buckets or objects to groups of users. With ACLs, you can only grant other AWS accounts (not specific users) access to your Amazon S3 resources. So, this is not the right choice for the current requirement. Use Security Groups - A security group acts as a virtual firewall for Amazon EC2 instances to control incoming and outgoing traffic. Amazon S3 does not support Security Groups, this option just acts as a distractor. Reference: https://d1.awsstatic.com/whitepapers/aws-security-whitepaper.pdf",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An IT company has built a solution wherein an Amazon Redshift cluster writes data to an Amazon S3 bucket belonging to a different AWS account. However, it is found that the files created in the Amazon S3 bucket using the UNLOAD command from the Amazon Redshift cluster are not even accessible to the Amazon S3 bucket owner. What could be the reason for this denial of permission for the bucket owner?",
      "options": [
        "A. When objects are uploaded to Amazon S3 bucket from a different AWS account, the S3 bucket owner will get implicit permissions to access these objects. This issue seems to be due to an upload error that can be fixed by providing manual access from AWS console",
        "B. The owner of an Amazon S3 bucket has implicit access to all objects in his bucket. Permissions are set on objects after they are completely copied to the target location. Since the owner is unable to access the uploaded files, the write operation may be still in progress",
        "C. When two different AWS accounts are accessing an Amazon S3 bucket, both the accounts must share the bucket policies. An erroneous policy can lead to such permission failures",
        "D. By default, an Amazon S3 object is owned by the AWS account that uploaded it. So the Amazon S3 bucket owner will not implicitly have access to the objects written by the Amazon Redshift cluster"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: By default, an Amazon S3 object is owned by the AWS account that uploaded it. So the Amazon S3 bucket owner will not implicitly have access to the objects written by the Amazon Redshift cluster - By default, an Amazon S3 object is owned by the AWS account that uploaded it. This is true even when the bucket is owned by another account. Because the Amazon Redshift data files from the UNLOAD command were put into your bucket by another account, you (the bucket owner) don't have default permission to access those files. To get access to the data files, an AWS Identity and Access Management (IAM) role with cross-account permissions must run the UNLOAD command again. Follow these steps to set up the Amazon Redshift cluster with cross-account permissions to the bucket: From the account of the Amazon S3 bucket, create an IAM role (Bucket Role) with permissions to the bucket. From the account of the Amazon Redshift cluster, create another IAM role (Cluster Role) with permissions to assume the Bucket Role. Update the Bucket Role to grant bucket access and create a trust relationship with the Cluster Role. From the Amazon Redshift cluster, run the UNLOAD command using the Cluster Role and Bucket Role. This solution doesn't apply to Amazon Redshift clusters or Amazon S3 buckets that use server-side encryption with AWS Key Management Service (AWS KMS). Incorrect options: When objects are uploaded to Amazon S3 bucket from a different AWS account, the S3 bucket owner will get implicit permissions to access these objects. This issue seems to be due to an upload error that can be fixed by providing manual access from AWS console - By default, an Amazon S3 object is owned by the AWS account that uploaded it. So, the bucket owner will not have any default permissions on the objects. Therefore, this option is incorrect. The owner of an Amazon S3 bucket has implicit access to all objects in his bucket. Permissions are set on objects after they are completely copied to the target location. Since the owner is unable to access the uploaded files, the write operation may be still in progress - This is an incorrect statement, given only as a distractor. When two different AWS accounts are accessing an Amazon S3 bucket, both the accounts must share the bucket policies. An erroneous policy can lead to such permission failures - This is an incorrect statement, given only as a distractor. Reference: https://aws.amazon.com/premiumsupport/knowledge-center/s3-access-denied-redshift-unload/",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A developer needs to implement an AWS Lambda function in AWS account A that accesses an Amazon Simple Storage Service (Amazon S3) bucket in AWS account B. As a Solutions Architect, which of the following will you recommend to meet this requirement?",
      "options": [
        "A. Create an IAM role for the AWS Lambda function that grants access to the Amazon S3 bucket. Set the IAM role as the AWS Lambda function's execution role. Make sure that the bucket policy also grants access to the AWS Lambda function's execution role",
        "B. AWS Lambda cannot access resources across AWS accounts. Use Identity federation to work around this limitation of Lambda",
        "C. Create an IAM role for the AWS Lambda function that grants access to the Amazon S3 bucket. Set the IAM role as the Lambda function's execution role and that would give the AWS Lambda function cross-account access to the Amazon S3 bucket",
        "D. The Amazon S3 bucket owner should make the bucket public so that it can be accessed by the AWS Lambda function in the other AWS account"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Create an IAM role for the AWS Lambda function that grants access to the Amazon S3 bucket. Set the IAM role as the AWS Lambda function's execution role. Make sure that the bucket policy also grants access to the AWS Lambda function's execution role If the IAM role that you create for the Lambda function is in the same AWS account as the bucket, then you don't need to grant Amazon S3 permissions on both the IAM role and the bucket policy. Instead, you can grant the permissions on the IAM role and then verify that the bucket policy doesn't explicitly deny access to the Lambda function role. If the IAM role and the bucket are in different accounts, then you need to grant Amazon S3 permissions on both the IAM role and the bucket policy. Therefore, this is the right way of giving access to AWS Lambda for the given use-case. Complete list of steps to be followed: via - https://aws.amazon.com/premiumsupport/knowledge-center/lambda-execution-role-s3-bucket/ Incorrect options: AWS Lambda cannot access resources across AWS accounts. Use Identity federation to work around this limitation of Lambda - This is an incorrect statement, used only as a distractor. Create an IAM role for the AWS Lambda function that grants access to the Amazon S3 bucket. Set the IAM role as the Lambda function's execution role and that would give the AWS Lambda function cross-account access to the Amazon S3 bucket - When the execution role of AWS Lambda and Amazon S3 bucket to be accessed are from different accounts, then you need to grant Amazon S3 bucket access permissions to the IAM role and also ensure that the bucket policy grants access to the AWS Lambda function's execution role. The Amazon S3 bucket owner should make the bucket public so that it can be accessed by the AWS Lambda function in the other AWS account - Making the Amazon S3 bucket public for the given use-case will be considered as a security bad practice. It's usually done for very few use-cases such as hosting a website on Amazon S3. Therefore this option is incorrect. Reference: https://aws.amazon.com/premiumsupport/knowledge-center/lambda-execution-role-s3-bucket/",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A social media application is hosted on an Amazon EC2 fleet running behind an Application Load Balancer. The application traffic is fronted by an Amazon CloudFront distribution. The engineering team wants to decouple the user authentication process for the application, so that the application servers can just focus on the business logic. As a Solutions Architect, which of the following solutions would you recommend to the development team so that it requires minimal development effort?",
      "options": [
        "A. Use Amazon Cognito Authentication via Cognito User Pools for your Application Load Balancer",
        "B. Use Amazon Cognito Authentication via Cognito Identity Pools for your Application Load Balancer",
        "C. Use Amazon Cognito Authentication via Cognito User Pools for your Amazon CloudFront distribution",
        "D. Use Amazon Cognito Authentication via Cognito Identity Pools for your Amazon CloudFront distribution"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Use Amazon Cognito Authentication via Cognito User Pools for your Application Load Balancer Application Load Balancer can be used to securely authenticate users for accessing your applications. This enables you to offload the work of authenticating users to your load balancer so that your applications can focus on their business logic. You can use Cognito User Pools to authenticate users through well-known social IdPs, such as Amazon, Facebook, or Google, through the user pools supported by Amazon Cognito or through corporate identities, using SAML, LDAP, or Microsoft AD, through the user pools supported by Amazon Cognito. You configure user authentication by creating an authenticate action for one or more listener rules. via - https://docs.aws.amazon.com/elasticloadbalancing/latest/application/listener-authenticate-users.html Exam Alert: Please review the following note to understand the differences between Amazon Cognito User Pools and Amazon Cognito Identity Pools: via - https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html Incorrect options: Use Amazon Cognito Authentication via Cognito Identity Pools for your Application Load Balancer - There is no such thing as using Amazon Cognito Authentication via Cognito Identity Pools for managing user authentication for the application. Application-specific user authentication can be provided via Cognito User Pools. Amazon Cognito identity pools provide temporary AWS credentials for users who are guests (unauthenticated) and for users who have been authenticated and received a token. Use Amazon Cognito Authentication via Cognito User Pools for your Amazon CloudFront distribution - You cannot directly integrate Cognito User Pools with CloudFront distribution as you have to create a separate AWS Lambda@Edge function to accomplish the authentication via Cognito User Pools. This involves additional development effort, so this option is not the best fit for the given use-case. Use Amazon Cognito Authentication via Cognito Identity Pools for your Amazon CloudFront distribution - You cannot use Cognito Identity Pools for managing user authentication, so this option is not correct. References: https://docs.aws.amazon.com/elasticloadbalancing/latest/application/listener-authenticate-users.html https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html https://aws.amazon.com/blogs/networking-and-content-delivery/authorizationedge-using-cookies-protect-your-amazon-cloudfront-content-from-being-downloaded-by-unauthenticated-users/",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "The engineering team at a logistics company has noticed that the Auto Scaling group (ASG) is not terminating an unhealthy Amazon EC2 instance. As a Solutions Architect, which of the following options would you suggest to troubleshoot the issue? (Select three)",
      "options": [
        "A. A user might have updated the configuration of the Auto Scaling group (ASG) and increased the minimum number of instances forcing ASG to keep all instances alive",
        "B. The Amazon EC2 instance could be a spot instance type, which cannot be terminated by the Auto Scaling group (ASG)",
        "C. The health check grace period for the instance has not expired",
        "D. The instance maybe in Impaired status",
        "E. The instance has failed the Elastic Load Balancing (ELB) health check status",
        "F. A custom health check might have failed. The Auto Scaling group (ASG) does not terminate instances that are set unhealthy by custom checks"
      ],
      "correct_answer": "C",
      "explanation": "Correct options: The health check grace period for the instance has not expired Amazon EC2 Auto Scaling doesn't terminate an instance that came into service based on Amazon EC2 status checks and Elastic Load Balancing (ELB) health checks until the health check grace period expires. More on Health check grace period: via - https://docs.aws.amazon.com/autoscaling/ec2/userguide/healthcheck.html#health-check-grace-period The instance maybe in Impaired status Amazon EC2 Auto Scaling does not immediately terminate instances with an Impaired status. Instead, Amazon EC2 Auto Scaling waits a few minutes for the instance to recover. Amazon EC2 Auto Scaling might also delay or not terminate instances that fail to report data for status checks. This usually happens when there is insufficient data for the status check metrics in Amazon CloudWatch. The instance has failed the Elastic Load Balancing (ELB) health check status By default, Amazon EC2 Auto Scaling doesn't use the results of ELB health checks to determine an instance's health status when the group's health check configuration is set to EC2. As a result, Amazon EC2 Auto Scaling doesn't terminate instances that fail ELB health checks. If an instance's status is OutofService on the ELB console, but the instance's status is Healthy on the Amazon EC2 Auto Scaling console, confirm that the health check type is set to ELB. Incorrect options: The Amazon EC2 instance could be a spot instance type, which cannot be terminated by the Auto Scaling group (ASG) - This is an incorrect statement. Amazon EC2 Auto Scaling terminates Spot instances when capacity is no longer available or the Spot price exceeds your maximum price. A user might have updated the configuration of the Auto Scaling group (ASG) and increased the minimum number of instances forcing ASG to keep all instances alive - This statement is incorrect. If the configuration is updated and ASG needs more number of instances, ASG will launch new, healthy instances and does not keep unhealthy ones alive. A custom health check might have failed. The Auto Scaling group (ASG) does not terminate instances that are set unhealthy by custom checks - This statement is incorrect. You can define custom health checks in Amazon EC2 Auto Scaling. When a custom health check determines that an instance is unhealthy, the check manually triggers SetInstanceHealth and then sets the instance's state to Unhealthy. Amazon EC2 Auto Scaling then terminates the unhealthy instance. References: https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-terminate-instance/ https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-instance-how-terminated/",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An engineering team wants to examine the feasibility of the user data feature of Amazon EC2 for an upcoming project. Which of the following are true about the Amazon EC2 user data configuration? (Select two)",
      "options": [
        "A. By default, user data is executed every time an Amazon EC2 instance is re-started",
        "B. When an instance is running, you can update user data by using root user credentials",
        "C. By default, scripts entered as user data are executed with root user privileges",
        "D. By default, user data runs only during the boot cycle when you first launch an instance",
        "E. By default, scripts entered as user data do not have root user privileges for executing"
      ],
      "correct_answer": "C",
      "explanation": "Correct options: User Data is generally used to perform common automated configuration tasks and even run scripts after the instance starts. When you launch an instance in Amazon EC2, you can pass two types of user data - shell scripts and cloud-init directives. You can also pass this data into the launch wizard as plain text or as a file. By default, scripts entered as user data are executed with root user privileges Scripts entered as user data are executed as the root user, hence do not need the sudo command in the script. Any files you create will be owned by root; if you need non-root users to have file access, you should modify the permissions accordingly in the script. By default, user data runs only during the boot cycle when you first launch an instance By default, user data scripts and cloud-init directives run only during the boot cycle when you first launch an instance. You can update your configuration to ensure that your user data scripts and cloud-init directives run every time you restart your instance. Incorrect options: By default, user data is executed every time an Amazon EC2 instance is re-started - As discussed above, this is not a default configuration of the system. But, can be achieved by explicitly configuring the instance. When an instance is running, you can update user data by using root user credentials - You can't change the user data if the instance is running (even by using root user credentials), but you can view it. By default, scripts entered as user data do not have root user privileges for executing - Scripts entered as user data are executed as the root user, hence do not need the sudo command in the script. Reference: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A systems administrator has created a private hosted zone and associated it with a Virtual Private Cloud (VPC). However, the Domain Name System (DNS) queries for the private hosted zone remain unresolved. As a Solutions Architect, can you identify the Amazon Virtual Private Cloud (Amazon VPC) options to be configured in order to get the private hosted zone to work?",
      "options": [
        "A. Remove any overlapping namespaces for the private and public hosted zones",
        "B. Fix the Name server (NS) record and Start Of Authority (SOA) records that may have been created with wrong configurations",
        "C. Fix conflicts between your private hosted zone and any Resolver rule that routes traffic to your network for the same domain name, as it results in ambiguity over the route to be taken",
        "D. Enable DNS hostnames and DNS resolution for private hosted zones"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: Enable DNS hostnames and DNS resolution for private hosted zones DNS hostnames and DNS resolution are required settings for private hosted zones. DNS queries for private hosted zones can be resolved by the Amazon-provided VPC DNS server only. As a result, these options must be enabled for your private hosted zone to work. DNS hostnames: For non-default virtual private clouds that aren't created using the Amazon VPC wizard, this option is disabled by default. If you create a private hosted zone for a domain and create records in the zone without enabling DNS hostnames, private hosted zones aren't enabled. To use a private hosted zone, this option must be enabled. DNS resolution: Private hosted zones accept DNS queries only from a VPC DNS server. The IP address of the VPC DNS server is the reserved IP address at the base of the VPC IPv4 network range plus two. Enabling DNS resolution allows you to use the VPC DNS server as a Resolver for performing DNS resolution. Keep this option disabled if you're using a custom DNS server in the DHCP Options set, and you're not using a private hosted zone. Incorrect options: Remove any overlapping namespaces for the private and public hosted zones - If you have private and public hosted zones that have overlapping namespaces, such as example.com and accounting.example.com, then the Resolver routes traffic based on the most specific match. It won't result in unresolved queries, hence this option is wrong. Fix the Name server (NS) record and Start Of Authority (SOA) records that may have been created with wrong configurations - When you create a hosted zone, Amazon Route 53 automatically creates a name server (NS) record and a start of authority (SOA) record for the zone for public hosted zone. However, this issue is about the private hosted zone, hence this is an incorrect option. Fix conflicts between your private hosted zone and any Resolver rule that routes traffic to your network for the same domain name, as it results in ambiguity over the route to be taken - If you have a private hosted zone (example.com) and a Resolver rule that routes traffic to your network for the same domain name, the Resolver rule takes precedence. It won't result in unresolved queries. References: https://aws.amazon.com/premiumsupport/knowledge-center/vpc-enable-private-hosted-zone/ https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-private-considerations.html https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-public-considerations.html",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An e-commerce company operates multiple AWS accounts and has interconnected these accounts in a hub-and-spoke style using the AWS Transit Gateway. Amazon Virtual Private Cloud (Amazon VPCs) have been provisioned across these AWS accounts to facilitate network isolation. Which of the following solutions would reduce both the administrative overhead and the costs while providing shared access to services required by workloads in each of the VPCs?",
      "options": [
        "A. Build a shared services Amazon Virtual Private Cloud (Amazon VPC)",
        "B. Use Transit VPC to reduce cost and share the resources across Amazon Virtual Private Cloud (Amazon VPCs)",
        "C. Use Fully meshed VPC Peering connection",
        "D. Use VPCs connected with AWS Direct Connect"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Build a shared services Amazon Virtual Private Cloud (Amazon VPC) Consider an organization that has built a hub-and-spoke network with AWS Transit Gateway. VPCs have been provisioned into multiple AWS accounts, perhaps to facilitate network isolation or to enable delegated network administration. When deploying distributed architectures such as this, a popular approach is to build a \"shared services VPC, which provides access to services required by workloads in each of the VPCs. This might include directory services or VPC endpoints. Sharing resources from a central location instead of building them in each VPC may reduce administrative overhead and cost. Centralized VPC Endpoints (multiple VPCs): via - https://aws.amazon.com/blogs/architecture/reduce-cost-and-increase-security-with-amazon-vpc-endpoints/ A VPC endpoint allows you to privately connect your VPC to supported AWS services without requiring an Internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Endpoints are virtual devices that are horizontally scaled, redundant, and highly available VPC components. They allow communication between instances in your VPC and services without imposing availability risks or bandwidth constraints on your network traffic. VPC endpoints enable you to reduce data transfer charges resulting from network communication between private VPC resources (such as Amazon Elastic Cloud Compute—or EC2—instances) and AWS Services (such as Amazon Quantum Ledger Database, or QLDB). Without VPC endpoints configured, communications that originate from within a VPC destined for public AWS services must egress AWS to the public Internet in order to access AWS services. This network path incurs outbound data transfer charges. Data transfer charges for traffic egressing from Amazon EC2 to the Internet vary based on volume. With VPC endpoints configured, communication between your VPC and the associated AWS service does not leave the Amazon network. If your workload requires you to transfer significant volumes of data between your VPC and AWS, you can reduce costs by leveraging VPC endpoints. Incorrect options: Use Transit VPC to reduce cost and share the resources across Amazon Virtual Private Cloud (Amazon VPCs) - Transit VPC uses customer-managed Amazon Elastic Compute Cloud (Amazon EC2) VPN instances in a dedicated transit VPC with an Internet gateway. This design requires the customer to deploy, configure, and manage EC2-based VPN appliances, which will result in additional EC2, and potentially third-party product and licensing charges. Note that this design will generate additional data transfer charges for traffic traversing the transit VPC: data is charged when it is sent from a spoke VPC to the transit VPC, and again from the transit VPC to the on-premises network or a different AWS Region. Transit VPC is not the right choice here. More on Transit VPC: via - https://d0.awsstatic.com/aws-answers/AWS_Single_Region_Multi_VPC_Connectivity.pdf Use Fully meshed VPC Peering connection - This approach creates multiple peering connections to facilitate the sharing of information between resources in different VPCs. This design connects multiple VPCs in a fully meshed configuration, with peering connections between each pair of VPCs. With this configuration, each VPC has access to the resources in all other VPCs. Each peering connection requires modifications to all the other VPCs’ route tables and, as the number of VPCs grows, this can be difficult to maintain. And keep in mind that AWS recommends a maximum of 125 peering connections per VPC. It's complex to manage and isn't a right fit for the current scenario. More on Fully meshed VPC Peers: via - https://d0.awsstatic.com/aws-answers/AWS_Single_Region_Multi_VPC_Connectivity.pdf Use VPCs connected with AWS Direct Connect - This approach is a good alternative for customers who need to connect a high number of VPCs to a central VPC or on-premises resources, or who already have an AWS Direct Connect connection in place. This design also offers customers the ability to incorporate transitive routing into their network design. For example, if VPC A and VPC B are both connected to an on-premises network using AWS Direct Connect connections, then the two VPCs can be connected to each other via AWS Direct Connect. AWS Direct Connect requires physical cables and takes about a month for setting up, this is not an ideal solution for the given scenario. References: https://aws.amazon.com/blogs/architecture/reduce-cost-and-increase-security-with-amazon-vpc-endpoints/ https://d0.awsstatic.com/aws-answers/AWS_Single_Region_Multi_VPC_Connectivity.pdf",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A retail company wants to rollout and test a blue-green deployment for its global application in the next 48 hours. Most of the customers use mobile phones which are prone to Domain Name System (DNS) caching. The company has only two days left for the annual Thanksgiving sale to commence. As a Solutions Architect, which of the following options would you recommend to test the deployment on as many users as possible in the given time frame?",
      "options": [
        "A. Use Amazon Route 53 weighted routing to spread traffic across different deployments",
        "B. Use AWS Global Accelerator to distribute a portion of traffic to a particular deployment",
        "C. Use Elastic Load Balancing (ELB) to distribute traffic across deployments",
        "D. Use AWS CodeDeploy deployment options to choose the right deployment"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Blue/green deployment is a technique for releasing applications by shifting traffic between two identical environments running different versions of the application: \"Blue\" is the currently running version and \"green\" the new version. This type of deployment allows you to test features in the green environment without impacting the currently running version of your application. When you’re satisfied that the green version is working properly, you can gradually reroute the traffic from the old blue environment to the new green environment. Blue/green deployments can mitigate common risks associated with deploying software, such as downtime and rollback capability. Use AWS Global Accelerator to distribute a portion of traffic to a particular deployment AWS Global Accelerator is a network layer service that directs traffic to optimal endpoints over the AWS global network, this improves the availability and performance of your internet applications. It provides two static anycast IP addresses that act as a fixed entry point to your application endpoints in a single or multiple AWS Regions, such as your Application Load Balancers, Network Load Balancers, Elastic IP addresses or Amazon EC2 instances, in a single or in multiple AWS regions. AWS Global Accelerator uses endpoint weights to determine the proportion of traffic that is directed to endpoints in an endpoint group, and traffic dials to control the percentage of traffic that is directed to an endpoint group (an AWS region where your application is deployed). While relying on the DNS service is a great option for blue/green deployments, it may not fit use-cases that require a fast and controlled transition of the traffic. Some client devices and internet resolvers cache DNS answers for long periods; this DNS feature improves the efficiency of the DNS service as it reduces the DNS traffic across the Internet, and serves as a resiliency technique by preventing authoritative name-server overloads. The downside of this in blue/green deployments is that you don’t know how long it will take before all of your users receive updated IP addresses when you update a record, change your routing preference or when there is an application failure. With AWS Global Accelerator, you can shift traffic gradually or all at once between the blue and the green environment and vice-versa without being subject to DNS caching on client devices and internet resolvers, traffic dials and endpoint weights changes are effective within seconds. Incorrect options: Use Amazon Route 53 weighted routing to spread traffic across different deployments - Weighted routing lets you associate multiple resources with a single domain name (example.com) or subdomain name (acme.example.com) and choose how much traffic is routed to each resource. This can be useful for a variety of purposes, including load balancing and testing new versions of the software. As discussed earlier, DNS caching is a negative behavior for this use case and hence Amazon Route 53 is not a good option. Use Elastic Load Balancing (ELB) to distribute traffic across deployments - Elastic Load Balancing (ELB) can distribute traffic across healthy instances. You can also use the Application Load Balancers weighted target groups feature for blue/green deployments as it does not rely on the DNS service. In addition you don’t need to create new ALBs for the green environment. As the use-case refers to a global application, so this option cannot be used for a multi-Region solution which is needed for the given requirement. Use AWS CodeDeploy deployment options to choose the right deployment - In AWS CodeDeploy, a deployment is the process, and the components involved in the process, of installing content on one or more instances. This content can consist of code, web and configuration files, executables, packages, scripts, and so on. AWS CodeDeploy deploys content that is stored in a source repository, according to the configuration rules you specify. Blue/Green deployment is one of the deployment types that CodeDeploy supports. CodeDeploy is not meant to distribute traffic across instances, so this option is incorrect. References: https://aws.amazon.com/blogs/networking-and-content-delivery/using-aws-global-accelerator-to-achieve-blue-green-deployments https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-weighted",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A health-care solutions company wants to run their applications on single-tenant hardware to meet regulatory guidelines. Which of the following is the MOST cost-effective way of isolating their Amazon Elastic Compute Cloud (Amazon EC2)instances to a single tenant?",
      "options": [
        "A. Dedicated Instances",
        "B. Spot Instances",
        "C. Dedicated Hosts",
        "D. On-Demand Instances"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Dedicated Instances Dedicated Instances are Amazon EC2 instances that run in a virtual private cloud (VPC) on hardware that's dedicated to a single customer. Dedicated Instances that belong to different AWS accounts are physically isolated at a hardware level, even if those accounts are linked to a single-payer account. However, Dedicated Instances may share hardware with other instances from the same AWS account that are not Dedicated Instances. A Dedicated Host is also a physical server that's dedicated for your use. With a Dedicated Host, you have visibility and control over how instances are placed on the server. Differences between Dedicated Hosts and Dedicated Instances: via - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-hosts-overview.html#dedicated-hosts-dedicated-instances Incorrect options: Spot Instances - A Spot Instance is an unused Amazon EC2 instance that is available for less than the On-Demand price. Your Spot Instance runs whenever capacity is available and the maximum price per hour for your request exceeds the Spot price. Any instance present with unused capacity will be allocated. Even though this is cost-effective, it does not fulfill the single-tenant hardware requirement of the client and hence is not the correct option. Dedicated Hosts - An Amazon EC2 Dedicated Host is a physical server with EC2 instance capacity fully dedicated to your use. Dedicated Hosts allow you to use your existing software licenses on EC2 instances. With a Dedicated Host, you have visibility and control over how instances are placed on the server. This option is costlier than the Dedicated Instance and hence is not the right choice for the current requirement. On-Demand Instances - With On-Demand Instances, you pay for compute capacity by the second with no long-term commitments. You have full control over its lifecycle—you decide when to launch, stop, hibernate, start, reboot, or terminate it. Hardware isolation is not possible and on-demand has one of the costliest instance charges and hence is not the correct answer for current requirements. High Level Overview of Amazon EC2 Instance Purchase Options: via - https://aws.amazon.com/ec2/pricing/ References: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-instance.html https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-purchasing-options.html",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "You have been hired as a Solutions Architect to advise a company on the various authentication/authorization mechanisms that AWS offers to authorize an API call within the Amazon API Gateway. The company would prefer a solution that offers built-in user management. Which of the following solutions would you suggest as the best fit for the given use-case?",
      "options": [
        "A. Use AWS_IAM authorization",
        "B. Use Amazon Cognito User Pools",
        "C. Use AWS Lambda authorizer for Amazon API Gateway",
        "D. Use Amazon Cognito Identity Pools"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Use Amazon Cognito User Pools A user pool is a user directory in Amazon Cognito. You can leverage Amazon Cognito User Pools to either provide built-in user management or integrate with external identity providers, such as Facebook, Twitter, Google+, and Amazon. Whether your users sign-in directly or through a third party, all members of the user pool have a directory profile that you can access through a Software Development Kit (SDK). User pools provide: 1. Sign-up and sign-in services. 2. A built-in, customizable web UI to sign in users. 3. Social sign-in with Facebook, Google, Login with Amazon, and Sign in with Apple, as well as sign-in with SAML identity providers from your user pool. 4. User directory management and user profiles. 5. Security features such as multi-factor authentication (MFA), checks for compromised credentials, account takeover protection, and phone and email verification. 6. Customized workflows and user migration through AWS Lambda triggers. After creating an Amazon Cognito user pool, in API Gateway, you must then create a COGNITO_USER_POOLS authorizer that uses the user pool. Amazon Cognito User Pools: via - https://docs.aws.amazon.com/wellarchitected/latest/serverless-applications-lens/identity-and-access-management.html Incorrect options: Use AWS_IAM authorization - For consumers who currently are located within your AWS environment or have the means to retrieve AWS Identity and Access Management (IAM) temporary credentials to access your environment, you can use AWS_IAM authorization and add least-privileged permissions to the respective IAM role to securely invoke your API. API Gateway API Keys is not a security mechanism and should not be used for authorization unless it’s a public API. It should be used primarily to track a consumer’s usage across your API. Use AWS Lambda authorizer for Amazon API Gateway - If you have an existing Identity Provider (IdP), you can use an AWS Lambda authorizer for Amazon API Gateway to invoke a Lambda function to authenticate/validate a given user against your Identity Provider. You can use a Lambda authorizer for custom validation logic based on identity metadata. A Lambda authorizer can send additional information derived from a bearer token or request context values to your backend service. For example, the authorizer can return a map containing user IDs, user names, and scope. By using Lambda authorizers, your backend does not need to map authorization tokens to user-centric data, allowing you to limit the exposure of such information to just the authorization function. When using Lambda authorizers, AWS strictly advises against passing credentials or any sort of sensitive data via query string parameters or headers, so this is not as secure as using Amazon Cognito User Pools. In addition, both these options do not offer built-in user management. Use Amazon Cognito Identity Pools - The two main components of Amazon Cognito are user pools and identity pools. Identity pools provide AWS credentials to grant your users access to other AWS services. To enable users in your user pool to access AWS resources, you can configure an identity pool to exchange user pool tokens for AWS credentials. So, identity pools aren't an authentication mechanism in themselves and hence aren't a choice for this use case. References: https://docs.aws.amazon.com/wellarchitected/latest/serverless-applications-lens/identity-and-access-management.html https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-enable-cognito-user-pool.html https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A developer has configured inbound traffic for the relevant ports in both the Security Group of the Amazon EC2 instance as well as the network access control list (network ACL) of the subnet for the Amazon EC2 instance. The developer is, however, unable to connect to the service running on the Amazon EC2 instance. As a solutions architect, how will you fix this issue?",
      "options": [
        "A. Network access control list (network ACL) are stateful, so allowing inbound traffic to the necessary ports enables the connection. Security Groups are stateless, so you must allow both inbound and outbound traffic",
        "B. IAM Role defined in the Security Group is different from the IAM Role that is given access in the network access control list (network ACL)",
        "C. Security Groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network access control list (network ACL) are stateless, so you must allow both inbound and outbound traffic",
        "D. Rules associated with network access control list (network ACL) should never be modified from command line. An attempt to modify rules from command line blocks the rule and results in an erratic behavior"
      ],
      "correct_answer": "C",
      "explanation": "Correct option: Security Groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network access control list (network ACL) are stateless, so you must allow both inbound and outbound traffic Security groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network ACLs are stateless, so you must allow both inbound and outbound traffic. To enable the connection to a service running on an instance, the associated network ACL must allow both inbound traffic on the port that the service is listening on as well as allow outbound traffic from ephemeral ports. When a client connects to a server, a random port from the ephemeral port range (1024-65535) becomes the client's source port. The designated ephemeral port then becomes the destination port for return traffic from the service, so outbound traffic from the ephemeral port must be allowed in the network ACL. By default, network ACLs allow all inbound and outbound traffic. If your network ACL is more restrictive, then you need to explicitly allow traffic from the ephemeral port range. If you accept traffic from the internet, then you also must establish a route through an internet gateway. If you accept traffic over VPN or AWS Direct Connect, then you must establish a route through a virtual private gateway. Incorrect options: Network access control list (network ACL) are stateful, so allowing inbound traffic to the necessary ports enables the connection. Security Groups are stateless, so you must allow both inbound and outbound traffic - This is incorrect as already discussed. IAM Role defined in the Security Group is different from the IAM Role that is given access in the network access control list (network ACL) - This is a made-up option and just added as a distractor. Rules associated with network access control list (network ACL) should never be modified from command line. An attempt to modify rules from command line blocks the rule and results in an erratic behavior - This option is a distractor. AWS does not support modifying rules of Network ACLs from the command line tool. Reference: https://aws.amazon.com/premiumsupport/knowledge-center/resolve-connection-sg-acl-inbound/",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An IT company is working on client engagement to build a real-time data analytics tool for the Internet of Things (IoT) data. The IoT data is funneled into Amazon Kinesis Data Streams which further acts as the source of a delivery stream for Amazon Kinesis Firehose. The engineering team has now configured a Kinesis Agent to send IoT data from another set of devices to the same Amazon Kinesis Firehose delivery stream. They noticed that data is not reaching Kinesis Firehose as expected. As a solutions architect, which of the following options would you attribute as the MOST plausible root cause behind this issue?",
      "options": [
        "A. Kinesis Agent can only write to Amazon Kinesis Data Streams, not to Amazon Kinesis Firehose",
        "B. Amazon Kinesis Firehose delivery stream has reached its limit and needs to be scaled manually",
        "C. The data sent by Kinesis Agent is lost because of a configuration error",
        "D. Kinesis Agent cannot write to Amazon Kinesis Firehose for which the delivery stream source is already set as Amazon Kinesis Data Streams"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: Kinesis Agent cannot write to Amazon Kinesis Firehose for which the delivery stream source is already set as Amazon Kinesis Data Streams Amazon Kinesis Data Firehose is the easiest way to reliably load streaming data into data lakes, data stores, and analytics tools. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, transform, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security. When an Amazon Kinesis Data Stream is configured as the source of a Kinesis Firehose delivery stream, Firehose’s PutRecord and PutRecordBatch operations are disabled and Kinesis Agent cannot write to Kinesis Firehose Delivery Stream directly. Data needs to be added to the Amazon Kinesis Data Stream through the Kinesis Data Streams PutRecord and PutRecords operations instead. Therefore, this option is correct. Incorrect options: Kinesis Agent can only write to Amazon Kinesis Data Streams, not to Amazon Kinesis Firehose - Kinesis Agent is a stand-alone Java software application that offers an easy way to collect and send data to Amazon Kinesis Data Streams or Amazon Kinesis Firehose. So this option is incorrect. Amazon Kinesis Firehose delivery stream has reached its limit and needs to be scaled manually - Amazon Kinesis Firehose is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. Therefore this option is not correct. How Amazon Kinesis Firehose works: via - https://aws.amazon.com/kinesis/data-firehose/ The data sent by Kinesis Agent is lost because of a configuration error - This is a made-up option and has been added as a distractor. References: https://aws.amazon.com/kinesis/data-firehose/ https://docs.aws.amazon.com/streams/latest/dev/writing-with-agents.html https://docs.aws.amazon.com/firehose/latest/dev/writing-with-agents.html",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A financial services company wants a single log processing model for all the log files (consisting of system logs, application logs, database logs, etc) that can be processed in a serverless fashion and then durably stored for downstream analytics. The company wants to use an AWS managed service that automatically scales to match the throughput of the log data and requires no ongoing administration. As a solutions architect, which of the following AWS services would you recommend solving this problem?",
      "options": [
        "A. Amazon Kinesis Data Streams",
        "B. Amazon Kinesis Data Firehose",
        "C. Amazon EMR",
        "D. AWS Lambda"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Amazon Kinesis Data Firehose Amazon Kinesis Data Firehose is the easiest way to reliably load streaming data into data lakes, data stores, and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards you’re already using today. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. Therefore, this is the correct option. Please see this overview of how Kinesis Firehose works: via - https://aws.amazon.com/kinesis/data-firehose/ Incorrect options: Amazon Kinesis Data Streams - Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. The throughput of an Amazon Kinesis data stream is designed to scale without limits via increasing the number of shards within a data stream. With Amazon Kinesis Data Streams, you can scale up to a sufficient number of shards (note, however, that you'll need to provision enough shards ahead of time). As it requires manual administration of shards, it's not the correct choice for the given use-case. Amazon EMR - Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. With EMR you can run Petabyte-scale analysis at less than half of the cost of traditional on-premises solutions and over 3x faster than standard Apache Spark. Amazon EMR uses Hadoop, an open-source framework, to distribute your data and processing across a resizable cluster of Amazon EC2 instances. Using an Amazon EMR cluster would imply managing the underlying infrastructure so it’s ruled out. AWS Lambda - AWS Lambda lets you run code without provisioning or managing servers. It cannot be used for production-grade serverless log analytics. Reference: https://aws.amazon.com/kinesis/data-firehose/",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A silicon valley based startup has a two-tier architecture using Amazon EC2 instances for its flagship application. The web servers (listening on port 443), which have been assigned security group A, are in public subnets across two Availability Zones (AZs) and the MSSQL based database instances (listening on port 1433), which have been assigned security group B, are in two private subnets across two Availability Zones (AZs). The DevOps team wants to review the security configurations of the application architecture. As a solutions architect, which of the following options would you select as the MOST secure configuration? (Select two)",
      "options": [
        "A. For security group A: Add an inbound rule that allows traffic from all sources on port 443. Add an outbound rule with the destination as security group B on port 443",
        "B. For security group B: Add an inbound rule that allows traffic only from all sources on port 1433",
        "C. For security group B: Add an inbound rule that allows traffic only from security group A on port 443",
        "D. For security group A: Add an inbound rule that allows traffic from all sources on port 443. Add an outbound rule with the destination as security group B on port 1433",
        "E. For security group B: Add an inbound rule that allows traffic only from security group A on port 1433"
      ],
      "correct_answer": "D",
      "explanation": "Correct options: For security group A: Add an inbound rule that allows traffic from all sources on port 443. Add an outbound rule with the destination as security group B on port 1433 For security group B: Add an inbound rule that allows traffic only from security group A on port 1433 A security group acts as a virtual firewall that controls the traffic for one or more instances. When you launch an instance, you can specify one or more security groups; otherwise, we use the default security group. You can add rules to each security group that allows traffic to or from its associated instances. You can modify the rules for a security group at any time; the new rules are automatically applied to all instances that are associated with the security group. When we decide whether to allow traffic to reach an instance, we evaluate all the rules from all the security groups that are associated with the instance. The following are the characteristics of security group rules: By default, security groups allow all outbound traffic. Security group rules are always permissive; you can't create rules that deny access. Security groups are stateful The MOST secure configuration for the given use case is: For security group A: Add an inbound rule that allows traffic from all sources on port 443. Add an outbound rule with the destination as security group B on port 1433 The above rules make sure that web servers are listening for traffic on all sources on the HTTPS protocol on port 443. The web servers only allow outbound traffic to MSSQL servers in Security Group B on port 1433. For security group B: Add an inbound rule that allows traffic only from security group A on port 1433. The above rule makes sure that the MSSQL servers only accept traffic from web servers in security group A on port 1433. Therefore, both of these options are correct. Incorrect options: For security group A: Add an inbound rule that allows traffic from all sources on port 443. Add an outbound rule with the destination as security group B on port 443 - As the MSSQL based database instances are listening on port 1433, therefore for security group A, the outbound rule should be added on port 443 with the destination as security group B. For security group B: Add an inbound rule that allows traffic only from all sources on port 1433 - The inbound rule should allow traffic only from security group A on port 1433. Allowing traffic from all sources will compromise security. For security group B: Add an inbound rule that allows traffic only from security group A on port 443 - The inbound rule should allow traffic only from security group A on port 1433 because the MSSQL based database instances are listening on port 1433. Reference: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A startup has just developed a video backup service hosted on a fleet of Amazon EC2 instances. The Amazon EC2 instances are behind an Application Load Balancer and the instances are using Amazon Elastic Block Store (Amazon EBS) Volumes for storage. The service provides authenticated users the ability to upload videos that are then saved on the EBS volume attached to a given instance. On the first day of the beta launch, users start complaining that they can see only some of the videos in their uploaded videos backup. Every time the users log into the website, they claim to see a different subset of their uploaded videos. Which of the following is the MOST optimal solution to make sure that users can view all the uploaded videos? (Select two)",
      "options": [
        "A. Write a one time job to copy the videos from all Amazon EBS volumes to Amazon S3 Glacier Deep Archive and then modify the application to use Amazon S3 Glacier Deep Archive for storing the videos",
        "B. Write a one time job to copy the videos from all Amazon EBS volumes to Amazon S3 and then modify the application to use Amazon S3 standard for storing the videos",
        "C. Write a one time job to copy the videos from all Amazon EBS volumes to Amazon RDS and then modify the application to use Amazon RDS for storing the videos",
        "D. Mount Amazon Elastic File System (Amazon EFS) on all Amazon EC2 instances. Write a one time job to copy the videos from all Amazon EBS volumes to Amazon EFS. Modify the application to use Amazon EFS for storing the videos",
        "E. Write a one time job to copy the videos from all Amazon EBS volumes to Amazon DynamoDB and then modify the application to use Amazon DynamoDB for storing the videos"
      ],
      "correct_answer": "B",
      "explanation": "Correct options: Write a one time job to copy the videos from all Amazon EBS volumes to Amazon S3 and then modify the application to use Amazon S3 standard for storing the videos Mount Amazon Elastic File System (Amazon EFS) on all Amazon EC2 instances. Write a one time job to copy the videos from all Amazon EBS volumes to Amazon EFS. Modify the application to use Amazon EFS for storing the videos Amazon Elastic Block Store (EBS) is an easy to use, high-performance block storage service designed for use with Amazon Elastic Compute Cloud (EC2) for both throughput and transaction-intensive workloads at any scale. Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. It is built to scale on-demand to petabytes without disrupting applications, growing and shrinking automatically as you add and remove files, eliminating the need to provision and manage capacity to accommodate growth. Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. As Amazon EBS volumes are attached locally to the Amazon EC2 instances, therefore the uploaded videos are tied to specific Amazon EC2 instances. Every time the user logs in, they are directed to a different instance and therefore their videos get dispersed across multiple EBS volumes. The correct solution is to use either Amazon S3 or Amazon EFS to store the user videos. Incorrect options: Write a one time job to copy the videos from all Amazon EBS volumes to Amazon S3 Glacier Deep Archive and then modify the application to use Amazon S3 Glacier Deep Archive for storing the videos - Amazon S3 Glacier Deep Archive is meant to be used for long term data archival. It cannot be used to serve static content such as videos or images via a web application. So this option is incorrect. Write a one time job to copy the videos from all Amazon EBS volumes to Amazon RDS and then modify the application to use Amazon RDS for storing the videos - Amazon RDS is a relational database and not the right candidate for storing videos. Write a one time job to copy the videos from all Amazon EBS volumes to Amazon DynamoDB and then modify the application to use Amazon DynamoDB for storing the videos - Amazon DynamoDB is a NoSQL database and not the right candidate for storing videos. Reference: https://aws.amazon.com/ebs/",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "To improve the performance and security of the application, the engineering team at a company has created an Amazon CloudFront distribution with an Application Load Balancer as the custom origin. The team has also set up an AWS Web Application Firewall (AWS WAF) with Amazon CloudFront distribution. The security team at the company has noticed a surge in malicious attacks from a specific IP address to steal sensitive data stored on the Amazon EC2 instances. As a solutions architect, which of the following actions would you recommend to stop the attacks?",
      "options": [
        "A. Create a deny rule for the malicious IP in the network access control list (network ACL) associated with each of the instances",
        "B. Create a deny rule for the malicious IP in the Security Groups associated with each of the instances",
        "C. Create an IP match condition in the AWS WAF to block the malicious IP address",
        "D. Create a ticket with AWS support to take action against the malicious IP"
      ],
      "correct_answer": "C",
      "explanation": "Correct option: Create an IP match condition in the AWS WAF to block the malicious IP address AWS WAF is a web application firewall that helps protect your web applications or APIs against common web exploits that may affect availability, compromise security, or consume excessive resources. AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that block common attack patterns, such as SQL injection or cross-site scripting, and rules that filter out specific traffic patterns you define. How AWS WAF Works: via - https://aws.amazon.com/waf/ If you want to allow or block web requests based on the IP addresses that the requests originate from, create one or more IP match conditions. An IP match condition lists up to 10,000 IP addresses or IP address ranges that your requests originate from. So, this option is correct. Incorrect options: Create a deny rule for the malicious IP in the network access control list (network ACL) associated with each of the instances - Network access control list (network ACL) are not associated with instances. So this option is also ruled out. Create a deny rule for the malicious IP in the Security Groups associated with each of the instances - You cannot deny rules in Security Groups. So this option is ruled out. Create a ticket with AWS support to take action against the malicious IP - Managing the security of your application is your responsibility, not that of AWS, so you cannot raise a ticket for this issue. Reference: https://docs.aws.amazon.com/waf/latest/developerguide/classic-web-acl-ip-conditions.html",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An application is currently hosted on four Amazon EC2 instances (behind Application Load Balancer) deployed in a single Availability Zone (AZ). To maintain an acceptable level of end-user experience, the application needs at least 4 instances to be always available. As a solutions architect, which of the following would you recommend so that the application achieves high availability with MINIMUM cost?",
      "options": [
        "A. Deploy the instances in three Availability Zones (AZs). Launch two instances in each Availability Zone (AZ)",
        "B. Deploy the instances in two Availability Zones (AZs). Launch two instances in each Availability Zone (AZ)",
        "C. Deploy the instances in two Availability Zones (AZs). Launch four instances in each Availability Zone (AZ)",
        "D. Deploy the instances in one Availability Zones. Launch two instances in the Availability Zone (AZ)"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Deploy the instances in three Availability Zones (AZs). Launch two instances in each Availability Zone (AZ) The correct option is to deploy the instances in three Availability Zones (AZs) and launch two instances in each Availability Zone (AZ). Even if one of the AZs goes out of service, still we shall have 4 instances available and the application can maintain an acceptable level of end-user experience. Therefore, we can achieve high availability with just 6 instances in this case. Incorrect options: Deploy the instances in two Availability Zones (AZs). Launch two instances in each Availability Zone (AZ) - When we launch two instances in two AZs, we run the risk of falling below the minimum acceptable threshold of 4 instances if one of the AZs fails. So this option is ruled out. Deploy the instances in two Availability Zones (AZs). Launch four instances in each Availability Zone (AZ) - When we launch four instances in two AZs, we have to bear costs for 8 instances which is NOT cost-optimal. So this option is ruled out. Deploy the instances in one Availability Zones. Launch two instances in the Availability Zone (AZ) - We can't have just two instances in a single AZ as that is below the minimum acceptable threshold of 4 instances.",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A junior DevOps engineer wants to change the default configuration for Amazon EBS volume termination. By default, the root volume of an Amazon EC2 instance for an EBS-backed AMI is deleted when the instance terminates. Which option below helps change this default behavior to ensure that the volume persists even after the instance terminates?",
      "options": [
        "A. Set the TerminateOnDelete attribute to true",
        "B. Set the DeleteOnTermination attribute to false",
        "C. Set the TerminateOnDelete attribute to false",
        "D. Set the DeleteOnTermination attribute to true"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Set the DeleteOnTermination attribute to false An Amazon EC2 instance can be launched from either an instance store-backed AMI or an Amazon EBS-backed AMI. Instances that use Amazon EBS for the root device automatically have an Amazon EBS volume attached. By default, the root volume for an AMI backed by Amazon EBS is deleted when the instance terminates. The default behavior can be changed to ensure that the volume persists after the instance terminates. To change the default behavior, set the DeleteOnTermination attribute to false using a block device mapping. Incorrect options: Set the TerminateOnDelete attribute to true Set the TerminateOnDelete attribute to false Both these options are incorrect as there is no such attribute as TerminateOnDelete. These options have been added as distractors. Set the DeleteOnTermination attribute to true - If you set the DeleteOnTermination attribute to true, then the root volume for an AMI backed by Amazon EBS would be deleted when the instance terminates. Therefore, this option is incorrect. Reference: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/RootDeviceStorage.html",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An application runs big data workloads on Amazon Elastic Compute Cloud (Amazon EC2) instances. The application runs 24x7 all round the year and needs at least 20 instances to maintain a minimum acceptable performance threshold and the application needs 300 instances to handle spikes in the workload. Based on historical workloads processed by the application, it needs 80 instances 80% of the time. As a solutions architect, which of the following would you recommend as the MOST cost-optimal solution so that it can meet the workload demand in a steady state?",
      "options": [
        "A. Purchase 80 reserved instances (RIs). Provision additional on-demand and spot instances per the workload demand (Use Auto Scaling Group with launch template to provision the mix of on-demand and spot instances)",
        "B. Purchase 20 on-demand instances. Use Auto Scaling Group to provision the remaining instances as spot instances per the workload demand",
        "C. Purchase 80 spot instances. Use Auto Scaling Group to provision the remaining instances as on-demand instances per the workload demand",
        "D. Purchase 80 on-demand instances. Provision additional on-demand and spot instances per the workload demand (Use Auto Scaling Group with launch template to provision the mix of on-demand and spot instances)"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Purchase 80 reserved instances (RIs). Provision additional on-demand and spot instances per the workload demand (Use Auto Scaling Group with launch template to provision the mix of on-demand and spot instances) As the steady-state workload demand is 80 instances, we can save on costs by purchasing 80 reserved instances. Based on additional workload demand, we can specify a mix of on-demand and spot instances using Application Load Balancer with a launch template to provision the mix of on-demand and spot instances. Please see this detailed overview of various types of Amazon EC2 instances from a pricing perspective: via - https://aws.amazon.com/ec2/pricing/ Incorrect options: Purchase 20 on-demand instances. Use Auto Scaling Group to provision the remaining instances as spot instances per the workload demand - Provisioning 20 on-demand instances implies that there would be a shortfall of 60 instances 80% of the time. Provisioning all of these 60 instances as spot instances is highly risky as there is no guarantee regarding the availability of the spot instances, which means we may not even meet the steady-state requirement for the workload, so this option is incorrect. Purchase 80 on-demand instances. Provision additional on-demand and spot instances per the workload demand (Use Auto Scaling Group with launch template to provision the mix of on-demand and spot instances) - Provisioning 80 on-demand instances would end up costlier than the option where we provision 80 reserved instances. So this option is ruled out. Purchase 80 spot instances. Use Auto Scaling Group to provision the remaining instances as on-demand instances per the workload demand - The option to purchase 80 spot instances is incorrect, as there is no guarantee regarding the availability of the spot instances, which means we may not even meet the steady-state workload. Reference: https://aws.amazon.com/ec2/pricing/",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A big data consulting firm needs to set up a data lake on Amazon S3 for a Health-Care client. The data lake is split in raw and refined zones. For compliance reasons, the source data needs to be kept for a minimum of 5 years. The source data arrives in the raw zone and is then processed via an AWS Glue based extract, transform, and load (ETL) job into the refined zone. The business analysts run ad-hoc queries only on the data in the refined zone using Amazon Athena. The team is concerned about the cost of data storage in both the raw and refined zones as the data is increasing at a rate of 1 terabyte daily in each zone. As a solutions architect, which of the following would you recommend as the MOST cost-optimal solution? (Select two)",
      "options": [
        "A. Setup a lifecycle policy to transition the raw zone data into Amazon S3 Glacier Deep Archive after 1 day of object creation",
        "B. Create an AWS Lambda function based job to delete the raw zone data after 1 day",
        "C. Setup a lifecycle policy to transition the refined zone data into Amazon S3 Glacier Deep Archive after 1 day of object creation",
        "D. Use AWS Glue ETL job to write the transformed data in the refined zone using CSV format",
        "E. Use AWS Glue ETL job to write the transformed data in the refined zone using a compressed file format"
      ],
      "correct_answer": "A",
      "explanation": "Correct options: Setup a lifecycle policy to transition the raw zone data into Amazon S3 Glacier Deep Archive after 1 day of object creation You can manage your objects so that they are stored cost-effectively throughout their lifecycle by configuring their Amazon S3 Lifecycle. An S3 Lifecycle configuration is a set of rules that define actions that Amazon S3 applies to a group of objects. For example, you might choose to transition objects to the Amazon S3 Standard-IA storage class 30 days after you created them, or archive objects to the Amazon S3 Glacier storage class one year after creating them. For the given use-case, the raw zone consists of the source data, so it cannot be deleted due to compliance reasons. Therefore, you should use a lifecycle policy to transition the raw zone data into Amazon S3 Glacier Deep Archive after 1 day of object creation. Please read more about Amazon S3 Object Lifecycle Management: via - https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html Use AWS Glue ETL job to write the transformed data in the refined zone using a compressed file format AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. You cannot transition the refined zone data into Amazon S3 Glacier Deep Archive because it is used by the business analysts for ad-hoc querying. Therefore, the best optimization is to have the refined zone data stored in a compressed format via the Glue job. The compressed data would reduce the storage cost incurred on the data in the refined zone. Please see this example for a AWS Glue ETL Pipeline: via - https://aws.amazon.com/glue/ Incorrect options: Create an AWS Lambda function based job to delete the raw zone data after 1 day - As mentioned in the use-case, the source data needs to be kept for a minimum of 5 years for compliance reasons. Therefore the data in the raw zone cannot be deleted after 1 day. Setup a lifecycle policy to transition the refined zone data into Amazon S3 Glacier Deep Archive after 1 day of object creation - You cannot transition the refined zone data into Amazon S3 Glacier Deep Archive because it is used by the business analysts for ad-hoc querying. Hence this option is incorrect. Use AWS Glue ETL job to write the transformed data in the refined zone using CSV format - It is cost-optimal to write the data in the refined zone using a compressed format instead of CSV format. The compressed data would reduce the storage cost incurred on the data in the refined zone. So, this option is incorrect. References: https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html https://aws.amazon.com/glue/",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An IT company wants to optimize the costs incurred on its fleet of 100 Amazon EC2 instances for the next year. Based on historical analyses, the engineering team observed that 70 of these instances handle the compute services of its flagship application and need to be always available. The other 30 instances are used to handle batch jobs that can afford a delay in processing. As a solutions architect, which of the following would you recommend as the MOST cost-optimal solution?",
      "options": [
        "A. Purchase 70 reserved instances (RIs) and 30 spot instances",
        "B. Purchase 70 on-demand instances and 30 spot instances",
        "C. Purchase 70 reserved instances and 30 on-demand instances",
        "D. Purchase 70 on-demand instances and 30 reserved instances"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Purchase 70 reserved instances (RIs) and 30 spot instances As 70 instances need to be always available, these can be purchased as reserved instances for a one-year duration. The other 30 instances responsible for the batch job can be purchased as spot instances. Even if some of the spot instances are interrupted, other spot instances can continue with the job. Please see this detailed overview of various types of Amazon EC2 instances from a pricing perspective: via - https://aws.amazon.com/ec2/pricing/ Incorrect options: Purchase 70 on-demand instances and 30 spot instances Purchase 70 on-demand instances and 30 reserved instances Purchasing 70 on-demand instances would be costlier than 70 reserved instances, so these two options are ruled out. Purchase 70 reserved instances and 30 on-demand instances - Purchasing 30 instances as on-demand instances to handle the batch jobs would not be cost-optimal as these instances don't need to be always available. Spot instances are better at handling such batch jobs. So this option is not correct. Reference: https://aws.amazon.com/ec2/pricing/",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "You have multiple AWS accounts within a single AWS Region managed by AWS Organizations and you would like to ensure all Amazon EC2 instances in all these accounts can communicate privately. Which of the following solutions provides the capability at the CHEAPEST cost?",
      "options": [
        "A. Create a virtual private cloud (VPC) in an account and share one or more of its subnets with the other accounts using Resource Access Manager",
        "B. Create an AWS Transit Gateway and link all the virtual private cloud (VPCs) in all the accounts together",
        "C. Create a VPC peering connection between all virtual private cloud (VPCs)",
        "D. Create a Private Link between all the Amazon EC2 instances"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Create a virtual private cloud (VPC) in an account and share one or more of its subnets with the other accounts using Resource Access Manager AWS Resource Access Manager (RAM) is a service that enables you to easily and securely share AWS resources with any AWS account or within your AWS Organization. You can share AWS Transit Gateways, Subnets, AWS License Manager configurations, and Amazon Route 53 Resolver rules resources with RAM. RAM eliminates the need to create duplicate resources in multiple accounts, reducing the operational overhead of managing those resources in every single account you own. You can create resources centrally in a multi-account environment, and use RAM to share those resources across accounts in three simple steps: create a Resource Share, specify resources, and specify accounts. RAM is available to you at no additional charge. The correct solution is to share the subnet(s) within a VPC using RAM. This will allow all Amazon EC2 instances to be deployed in the same VPC (although from different accounts) and easily communicate with one another. How AWS Resource Access Manager (AWS RAM) Works: via - https://aws.amazon.com/ram/ Incorrect options: Create a Private Link between all the Amazon EC2 instances - AWS PrivateLink simplifies the security of data shared with cloud-based applications by eliminating the exposure of data to the public Internet. AWS PrivateLink provides private connectivity between VPCs, AWS services, and on-premises applications, securely on the Amazon network. Private Link is a distractor in this question. Private Link is leveraged to create a private connection between an application that is fronted by an NLB in an account, and an Elastic Network Interface (ENI) in another account, without the need of VPC peering and allowing the connections between the two to remain within the AWS network. Create a VPC peering connection between all virtual private cloud (VPCs) - A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses. Instances in either VPC can communicate with each other as if they are within the same network. You can create a VPC peering connection between your VPCs, or with a VPC in another AWS account. The VPCs can be in different regions (also known as an inter-region VPC peering connection). VPC peering connections will work, but won't efficiently scale if you add more accounts (you'll have to create many connections). Create an AWS Transit Gateway and link all the virtual private cloud (VPCs) in all the accounts together - AWS Transit Gateway is a service that enables customers to connect their Amazon Virtual Private Clouds (VPCs) and their on-premises networks to a single gateway. A Transit Gateway will work but will be an expensive solution. Here we want to minimize cost. References: https://aws.amazon.com/ram/ https://aws.amazon.com/privatelink/ https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html https://aws.amazon.com/transit-gateway/",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "What does this IAM policy do? { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"Mystery Policy\", \"Action\": [ \"ec2:RunInstances\" ], \"Effect\": \"Allow\", \"Resource\": \"*\", \"Condition\": { \"IpAddress\": { \"aws:SourceIp\": \"34.50.31.0/24\" } } } ] }",
      "options": [
        "A. It allows starting an Amazon EC2 instance only when the IP where the call originates is within the 34.50.31.0/24 CIDR block",
        "B. It allows starting an Amazon EC2 instance only when they have a Public IP within the 34.50.31.0/24 CIDR block",
        "C. It allows starting an Amazon EC2 instance only when they have an Elastic IP within the 34.50.31.0/24 CIDR block",
        "D. It allows starting an Amazon EC2 instance only when they have a Private IP within the 34.50.31.0/24 CIDR block"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: It allows starting an Amazon EC2 instance only when the IP where the call originates is within the 34.50.31.0/24 CIDR block You manage access in AWS by creating policies and attaching them to IAM identities (users, groups of users, or roles) or AWS resources. A policy is an object in AWS that, when associated with an identity or resource, defines their permissions. AWS evaluates these policies when an IAM principal (user or role) makes a request. Permissions in the policies determine whether the request is allowed or denied. Most policies are stored in AWS as JSON documents. AWS supports six types of policies: identity-based policies, resource-based policies, permissions boundaries, Organizations service control policy (SCPs), access control lists (ACLs), and session policies. Consider the following snippet from the given policy document: \"Condition\": { \"IpAddress\": { \"aws:SourceIp\": \"34.50.31.0/24\" } } The aws:SourceIP in this condition always represents the IP of the caller of the API. That is very helpful if you want to restrict access to certain AWS API for example from the public IP of your on-premises infrastructure. Please see this overview of Elastic vs Public vs Private IP addresses: elastic IP address (EIP) - An elastic IP address (EIP) is a static IPv4 address designed for dynamic cloud computing. An Elastic IP address is associated with your AWS account. With an Elastic IP address, you can mask the failure of an instance or software by rapidly remapping the address to another instance in your account. Private IP address - A private IPv4 address is an IP address that's not reachable over the Internet. You can use private IPv4 addresses for communication between instances in the same VPC. Public IP address - A public IP address is an IPv4 address that's reachable from the Internet. You can use public addresses for communication between your instances and the Internet. Please note 34.50.31.0/24 is a public IP range, not a private IP range. Private IP ranges are: 192.168.0.0 - 192.168.255.255 (65,536 IP addresses) 172.16.0.0 - 172.31.255.255 (1,048,576 IP addresses) 10.0.0.0 - 10.255.255.255 (16,777,216 IP addresses) Incorrect options: It allows starting an Amazon EC2 instance only when they have a Public IP within the 34.50.31.0/24 CIDR block It allows starting an Amazon EC2 instance only when they have an Elastic IP within the 34.50.31.0/24 CIDR block It allows starting an Amazon EC2 instance only when they have a Private IP within the 34.50.31.0/24 CIDR block Each of these three options suggests that the IP addresses of the Amazon EC2 instances must belong to the 34.50.31.0/24 CIDR block for the EC2 instances to start. Actually, the policy states that the AMazon EC2 instance should start only when the IP where the call originates is within the 34.50.31.0/24 CIDR block. Hence these options are incorrect. References: https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html https://aws.amazon.com/premiumsupport/knowledge-center/iam-restrict-calls-ip-addresses/ https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-instance-addressing.html",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "What does this IAM policy do? { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"Mystery Policy\", \"Action\": [ \"ec2:RunInstances\" ], \"Effect\": \"Allow\", \"Resource\": \"*\", \"Condition\": { \"StringEquals\": { \"aws:RequestedRegion\": \"eu-west-1\" } } } ] }",
      "options": [
        "A. It allows running Amazon EC2 instances only in the eu-west-1 region, and the API call can be made from anywhere in the world",
        "B. It allows running Amazon EC2 instances anywhere but in the eu-west-1 region",
        "C. It allows running Amazon EC2 instances in any region when the API call is originating from the eu-west-1 region",
        "D. It allows running Amazon EC2 instances in the eu-west-1 region, when the API call is made from the eu-west-1 region"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: It allows running Amazon EC2 instances only in the eu-west-1 region, and the API call can be made from anywhere in the world You manage access in AWS by creating policies and attaching them to IAM identities (users, groups of users, or roles) or AWS resources. A policy is an object in AWS that, when associated with an identity or resource, defines their permissions. AWS evaluates these policies when an IAM principal (user or role) makes a request. Permissions in the policies determine whether the request is allowed or denied. Most policies are stored in AWS as JSON documents. AWS supports six types of policies: identity-based policies, resource-based policies, permissions boundaries, Organizations service control policy (SCPs), access control lists (ACLs), and session policies. You can use the aws:RequestedRegion key to compare the AWS Region that was called in the request with the Region that you specify in the policy. You can use this global condition key to control which Regions can be requested. aws:RequestedRegion represents the target of the API call. So in this example, we can only launch an Amazon EC2 instance in eu-west-1, and we can do this API call from anywhere. Incorrect options: It allows running Amazon EC2 instances anywhere but in the eu-west-1 region It allows running Amazon EC2 instances in any region when the API call is originating from the eu-west-1 region It allows running Amazon EC2 instances in the eu-west-1 region, when the API call is made from the eu-west-1 region These three options contradict the earlier details provided in the explanation. To summarize, aws:RequestedRegion represents the target of the API call. So, we can only launch an Amazon EC2 instance in eu-west-1 region and we can do this API call from anywhere. Hence these options are incorrect. References: https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "You have a team of developers in your company, and you would like to ensure they can quickly experiment with AWS Managed Policies by attaching them to their accounts, but you would like to prevent them from doing an escalation of privileges, by granting themselves the AdministratorAccess managed policy. How should you proceed?",
      "options": [
        "A. For each developer, define an IAM permission boundary that will restrict the managed policies they can attach to themselves",
        "B. Put the developers into an IAM group, and then define an IAM permission boundary on the group that will restrict the managed policies they can attach to themselves",
        "C. Create a Service Control Policy (SCP) on your AWS account that restricts developers from attaching themselves the AdministratorAccess policy",
        "D. Attach an IAM policy to your developers, that prevents them from attaching the AdministratorAccess policy"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: For each developer, define an IAM permission boundary that will restrict the managed policies they can attach to themselves AWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature for using a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. An entity's permissions boundary allows it to perform only the actions that are allowed by both its identity-based policies and its permissions boundaries. Here we have to use an IAM permission boundary. They can only be applied to roles or users, not IAM groups. Permissions boundaries for IAM entities: via - https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html Incorrect options: Create a Service Control Policy (SCP) on your AWS account that restricts developers from attaching themselves the AdministratorAccess policy - Service control policy (SCP) is one type of policy that you can use to manage your organization. SCPs offer central control over the maximum available permissions for all accounts in your organization, allowing you to ensure your accounts stay within your organization’s access control guidelines. SCPs are available only in an organization that has all features enabled. SCPs aren't available if your organization has enabled only the consolidated billing features. Attaching an SCP to an AWS Organizations entity (root, OU, or account) defines a guardrail for what actions the principals can perform. If you consider this option, since AWS Organizations is not mentioned in this question, so we can't apply an SCP. Attach an IAM policy to your developers, that prevents them from attaching the AdministratorAccess policy - This option is incorrect as the developers can remove this policy from themselves and escalate their privileges. Put the developers into an IAM group, and then define an IAM permission boundary on the group that will restrict the managed policies they can attach to themselves - IAM permission boundary can only be applied to roles or users, not IAM groups. Hence this option is incorrect. References: https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "You would like to migrate an AWS account from an AWS Organization A to an AWS Organization B. What are the steps do to it?",
      "options": [
        "A. Remove the member account from the old organization. Send an invite to the member account from the new Organization. Accept the invite to the new organization from the member account",
        "B. Send an invite to the new organization. Accept the invite to the new organization from the member account. Remove the member account from the old organization",
        "C. Send an invite to the new organization. Remove the member account from the old organization. Accept the invite to the new organization from the member account",
        "D. Open an AWS Support ticket to ask them to migrate the account"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Remove the member account from the old organization. Send an invite to the member account from the new Organization. Accept the invite to the new organization from the member account AWS Organizations helps you centrally govern your environment as you grow and scale your workloads on AWS. Using AWS Organizations, you can automate account creation, create groups of accounts to reflect your business needs, and apply policies for these groups for governance. You can also simplify billing by setting up a single payment method for all of your AWS accounts. Through integrations with other AWS services, you can use Organizations to define central configurations and resource sharing across accounts in your organization. To migrate accounts from one organization to another, you must have root or IAM access to both the member and master accounts. Here are the steps to follow: 1. Remove the member account from the old organization 2. Send an invite to the member account from the new Organization 3. Accept the invite to the new organization from the member account Incorrect options: Send an invite to the new organization. Accept the invite to the new organization from the member account. Remove the member account from the old organization Send an invite to the new organization. Remove the member account from the old organization. Accept the invite to the new organization from the member account These two options contradict the steps described earlier for account migration from one organization to another. Open an AWS Support ticket to ask them to migrate the account - You don't need to contact AWS support for account migration. References: https://aws.amazon.com/organizations/ https://aws.amazon.com/premiumsupport/knowledge-center/organizations-move-accounts/",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "You would like to use AWS Snowball to move on-premises backups into a long term archival tier on AWS. Which solution provides the MOST cost savings?",
      "options": [
        "A. Create an AWS Snowball job and target an Amazon S3 bucket. Create a lifecycle policy to transition this data to Amazon S3 Glacier Deep Archive on the same day",
        "B. Create an AWS Snowball job and target a Amazon S3 Glacier Vault",
        "C. Create an AWS Snowball job and target an Amazon S3 bucket. Create a lifecycle policy to transition this data to Amazon S3 Glacier on the same day",
        "D. Create a AWS Snowball job and target an Amazon S3 Glacier Deep Archive Vault"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Create an AWS Snowball job and target an Amazon S3 bucket. Create a lifecycle policy to transition this data to Amazon S3 Glacier Deep Archive on the same day AWS Snowball, a part of the AWS Snow Family, is a data migration and edge computing device that comes in two options. Snowball Edge Storage Optimized devices provide both block storage and Amazon S3-compatible object storage, and 40 vCPUs. They are well suited for local storage and large scale data transfer. AWS Snowball Edge Compute Optimized devices provide 52 vCPUs, block and object storage, and an optional GPU for use cases like advanced machine learning and full-motion video analysis in disconnected environments. AWS Snowball Edge Storage Optimized is the optimal choice if you need to securely and quickly transfer dozens of terabytes to petabytes of data to AWS. It provides up to 80 terabytes of usable HDD storage, 40 vCPUs, 1 terabyte of SATA SSD storage, and up to 40 gigabytes network connectivity to address large scale data transfer and pre-processing use cases. The original AWS Snowball devices were transitioned out of service and AWS Snowball Edge Storage Optimized are now the primary devices used for data transfer. You may see the AWS Snowball device on the exam, just remember that the original AWS Snowball device had 80 terabytes of storage space. For this scenario, you will want to minimize the time spent in Amazon S3 Standard for all files to avoid unintended Amazon S3 Standard storage charges. To do this, AWS recommends using a zero-day lifecycle policy. From a cost perspective, when using a zero-day lifecycle policy, you are only charged Amazon S3 Glacier Deep Archive rates. When billed, the lifecycle policy is accounted for first, and if the destination is Amazon S3 Glacier Deep Archive, you are charged Amazon S3 Glacier Deep Archive rates for the transferred files. You can't move data directly from AWS Snowball into Amazon S3 Glacier, you need to go through Amazon S3 first, and then use a lifecycle policy. So this option is correct. Incorrect options: Create an AWS Snowball job and target a Amazon S3 Glacier Vault Create a AWS Snowball job and target an Amazon S3 Glacier Deep Archive Vault Amazon S3 Glacier and S3 Glacier Deep Archive are a secure, durable, and extremely low-cost Amazon S3 cloud storage classes for data archiving and long-term backup. They are designed to deliver 99.999999999% durability and provide comprehensive security and compliance capabilities that can help meet even the most stringent regulatory requirements. Finally, Amazon S3 Glacier Deep Archive provides more cost savings than Amazon S3 Glacier. Both these options are incorrect as you can't move data directly from AWS Snowball into a Amazon S3 Glacier Vault or a Glacier Deep Archive Vault. You need to go through Amazon S3 first and then use a lifecycle policy. Create an AWS Snowball job and target an Amazon S3 bucket. Create a lifecycle policy to transition this data to Amazon S3 Glacier on the same day - As Amazon S3 Glacier Deep Archive provides more cost savings than Amazon S3 Glacier, you should use Amazon S3 Glacier Deep Archive for long term archival for this use-case. References: https://aws.amazon.com/snowball/features/ https://aws.amazon.com/glacier/",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "You are establishing a monitoring solution for desktop systems, that will be sending telemetry data into AWS every 1 minute. Data for each system must be processed in order, independently, and you would like to scale the number of consumers to be possibly equal to the number of desktop systems that are being monitored. What do you recommend?",
      "options": [
        "A. Use an Amazon Simple Queue Service (Amazon SQS) FIFO (First-In-First-Out) queue, and make sure the telemetry data is sent with a Group ID attribute representing the value of the Desktop ID",
        "B. Use an Amazon Simple Queue Service (Amazon SQS) FIFO (First-In-First-Out) queue, and send the telemetry data as is",
        "C. Use an Amazon Simple Queue Service (Amazon SQS) standard queue, and send the telemetry data as is",
        "D. Use an Amazon Kinesis Data Stream, and send the telemetry data with a Partition ID that uses the value of the Desktop ID"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Use an Amazon Simple Queue Service (Amazon SQS) FIFO (First-In-First-Out) queue, and make sure the telemetry data is sent with a Group ID attribute representing the value of the Desktop ID Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent. We, therefore, need to use an SQS FIFO queue. If we don't specify a GroupID, then all the messages are in absolute order, but we can only have 1 consumer at most. To allow for multiple consumers to read data for each Desktop application, and to scale the number of consumers, we should use the \"Group ID\" attribute. So this is the correct option. Incorrect options: Use an Amazon Simple Queue Service (Amazon SQS) FIFO (First-In-First-Out) queue, and send the telemetry data as is - This is incorrect because if we send the telemetry data as is then we will not be able to scale the number of consumers to be equal to the number of desktop systems. In this case, each message will have its consumer. So we should use the \"Group ID\" attribute so that multiple consumers can read data for each Desktop application. Use an Amazon Simple Queue Service (Amazon SQS) standard queue, and send the telemetry data as is - An Amazon SQS standard queue has no ordering capability so that's ruled out. Use an Amazon Kinesis Data Stream, and send the telemetry data with a Partition ID that uses the value of the Desktop ID - Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. A Kinesis Data Stream would work and would give us the data for each desktop application within shards, but we can only have as many consumers as shards in Kinesis (which is in practice, much less than the number of producers). References: https://aws.amazon.com/blogs/compute/solving-complex-ordering-challenges-with-amazon-sqs-fifo-queues/ https://aws.amazon.com/sqs/faqs/ https://aws.amazon.com/kinesis/data-streams/faqs/",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A company has many Amazon Virtual Private Cloud (Amazon VPC) in various accounts, that need to be connected in a star network with one another and connected with on-premises networks through AWS Direct Connect. What do you recommend?",
      "options": [
        "A. VPC Peering Connection",
        "B. Virtual private gateway (VGW)",
        "C. AWS Transit Gateway",
        "D. AWS PrivateLink"
      ],
      "correct_answer": "C",
      "explanation": "Correct option: AWS Transit Gateway AWS Transit Gateway is a service that enables customers to connect their Amazon Virtual Private Clouds (VPCs) and their on-premises networks to a single gateway. With AWS Transit Gateway, you only have to create and manage a single connection from the central gateway into each Amazon VPC, on-premises data center, or remote office across your network. Transit Gateway acts as a hub that controls how traffic is routed among all the connected networks which act like spokes. So, this is a perfect use-case for the Transit Gateway. Without AWS Transit Gateway via - https://aws.amazon.com/transit-gateway/ With AWS Transit Gateway via - https://aws.amazon.com/transit-gateway/ Incorrect options: VPC Peering Connection - A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses. Instances in either VPC can communicate with each other as if they are within the same network. You can create a VPC peering connection between your VPCs, or with a VPC in another AWS account. The VPCs can be in different regions (also known as an inter-region VPC peering connection). VPC Peering helps connect two VPCs and is not transitive. It would require to create many peering connections between all the VPCs to have them connect. This alone wouldn't work, because we would need to also connect the on-premises data center through Direct Connect and Direct Connect Gateway, but that's not mentioned in this answer. Virtual private gateway (VGW) - A virtual private gateway (VGW), also known as a VPN Gateway, is the endpoint on the VPC side of your VPN connection. You can create a virtual private gateway before creating the VPC itself. VPN Gateway is a distractor here because we haven't mentioned a VPN. AWS PrivateLink - AWS PrivateLink simplifies the security of data shared with cloud-based applications by eliminating the exposure of data to the public Internet. AWS PrivateLink provides private connectivity between VPCs, AWS services, and on-premises applications, securely on the Amazon network. Private Link is utilized to create a private connection between an application that is fronted by an NLB in an account, and an Elastic Network Interface (ENI) in another account, without the need of VPC peering, and allowing the connections between the two to remain within the AWS network. References: https://aws.amazon.com/transit-gateway/ https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_CreateVpnGateway.html",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "What is true about Amazon RDS Read Replicas encryption?",
      "options": [
        "A. If the master database is encrypted, the read replicas are encrypted",
        "B. If the master database is encrypted, the read replicas can be either encrypted or unencrypted",
        "C. If the master database is unencrypted, the read replicas can be either encrypted or unencrypted",
        "D. If the master database is unencrypted, the read replicas are encrypted"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: If the master database is encrypted, the read replicas are encrypted Amazon RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. For the MySQL, MariaDB, PostgreSQL, Oracle, and SQL Server database engines, Amazon RDS creates a second DB instance using a snapshot of the source DB instance. It then uses the engines' native asynchronous replication to update the read replica whenever there is a change to the source DB instance. read replicas can be within an Availability Zone, Cross-AZ, or Cross-Region. On a database instance running with Amazon RDS encryption, data stored at rest in the underlying storage is encrypted, as are its automated backups, read replicas, and snapshots. Therefore, this option is correct. Amazon RDS Read Replica Overview: via - https://aws.amazon.com/rds/features/read-replicas/ Incorrect options: If the master database is encrypted, the read replicas can be either encrypted or unencrypted - If the master database is encrypted, the read replicas are necessarily encrypted, so this option is incorrect. If the master database is unencrypted, the read replicas can be either encrypted or unencrypted If the master database is unencrypted, the read replicas are encrypted If the master database is not encrypted, the read replicas cannot be encrypted, so both these options are incorrect. References: https://aws.amazon.com/rds/features/read-replicas/",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "Upon a security review of your AWS account, an AWS consultant has found that a few Amazon RDS databases are unencrypted. As a Solutions Architect, what steps must be taken to encrypt the Amazon RDS databases?",
      "options": [
        "A. Take a snapshot of the database, copy it as an encrypted snapshot, and restore a database from the encrypted snapshot. Terminate the previous database",
        "B. Create a Read Replica of the database, and encrypt the read replica. Promote the read replica as a standalone database, and terminate the previous database",
        "C. Enable Multi-AZ for the database, and make sure the standby instance is encrypted. Stop the main database to that the standby database kicks in, then disable Multi-AZ",
        "D. Enable encryption on the Amazon RDS database using the AWS Console"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Take a snapshot of the database, copy it as an encrypted snapshot, and restore a database from the encrypted snapshot. Terminate the previous database Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching and backups. You can encrypt your Amazon RDS DB instances and snapshots at rest by enabling the encryption option for your Amazon RDS DB instances. Data that is encrypted at rest includes the underlying storage for DB instances, its automated backups, read replicas, and snapshots. You can only enable encryption for an Amazon RDS DB instance when you create it, not after the DB instance is created. However, because you can encrypt a copy of an unencrypted DB snapshot, you can effectively add encryption to an unencrypted DB instance. That is, you can create a snapshot of your DB instance, and then create an encrypted copy of that snapshot. So this is the correct option. Incorrect options: Create a Read Replica of the database, and encrypt the read replica. Promote the read replica as a standalone database, and terminate the previous database - If the master is not encrypted, the read replicas cannot be encrypted. So this option is incorrect. Enable Multi-AZ for the database, and make sure the standby instance is encrypted. Stop the main database to that the standby database kicks in, then disable Multi-AZ - Multi-AZ is to help with High Availability, not encryption. So this option is incorrect. Enable encryption on the Amazon RDS database using the AWS Console - There is no direct option to encrypt an Amazon RDS database using the AWS Console. Steps to encrypt an un-encrypted RDS database: 1. Create a snapshot of the un-encrypted database 2. Copy the snapshot and enable encryption for the snapshot 3. Restore the database from the encrypted snapshot 4. Migrate applications to the new database, and delete the old database Reference: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A company has historically operated only in the us-east-1 region and stores encrypted data in Amazon S3 using SSE-KMS. As part of enhancing its security posture as well as improving the backup and recovery architecture, the company wants to store the encrypted data in Amazon S3 that is replicated into the us-west-1 AWS region. The security policies mandate that the data must be encrypted and decrypted using the same key in both AWS regions. Which of the following represents the best solution to address these requirements?",
      "options": [
        "A. Create an Amazon CloudWatch scheduled rule to invoke an AWS Lambda function to copy the daily data from the source bucket in us-east-1 region to the destination bucket in us-west-1 region. Provide AWS KMS key access to the AWS Lambda function for encryption and decryption operations on the data in the source and destination Amazon S3 buckets",
        "B. Create a new Amazon S3 bucket in the us-east-1 region with replication enabled from this new bucket into another bucket in us-west-1 region. Enable SSE-KMS encryption on the new bucket in us-east-1 region by using an AWS KMS multi-region key. Copy the existing data from the current Amazon S3 bucket in us-east-1 region into this new Amazon S3 bucket in us-east-1 region",
        "C. Change the AWS KMS single region key used for the current Amazon S3 bucket into an AWS KMS multi-region key. Enable Amazon S3 batch replication for the existing data in the current bucket in us-east-1 region into another bucket in us-west-1 region",
        "D. Enable replication for the current bucket in us-east-1 region into another bucket in us-west-1 region. Share the existing AWS KMS key from us-east-1 region to us-west-1 region"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Create a new Amazon S3 bucket in the us-east-1 region with replication enabled from this new bucket into another bucket in us-west-1 region. Enable SSE-KMS encryption on the new bucket in us-east-1 region by using an AWS KMS multi-region key. Copy the existing data from the current Amazon S3 bucket in us-east-1 region into this new Amazon S3 bucket in us-east-1 region AWS KMS supports multi-region keys, which are AWS KMS keys in different AWS regions that can be used interchangeably – as though you had the same key in multiple regions. Each set of related multi-region keys has the same key material and key ID, so you can encrypt data in one AWS region and decrypt it in a different AWS region without re-encrypting or making a cross-region call to AWS KMS. You can use multi-region AWS KMS keys in Amazon S3. However, Amazon S3 currently treats multi-region keys as though they were single-region keys, and does not use the multi-region features of the key. Multi-region AWS KMS keys: via - https://docs.aws.amazon.com/kms/latest/developerguide/multi-region-keys-overview.html For the given use case, you must create a new bucket in the us-east-1 region with replication enabled from this new bucket into another bucket in us-west-1 region. This would ensure that the data is available in another region for backup and recovery purposes. You should also enable SSE-KMS encryption on the new bucket in us-east-1 region by using an AWS KMS multi-region key so that the data can be encrypted and decrypted using the same key in both AWS regions. Since the existing data in the current bucket was encrypted using the AWS KMS key restricted to the us-east-1 region, so data must be copied to the new bucket in us-east-1 region for replication as well as multi-region KMS key based encryption to kick-in. To require server-side encryption of all objects in a particular Amazon S3 bucket, you can use a policy. For example, the following bucket policy denies the upload object (s3:PutObject) permission to everyone if the request does not include the x-amz-server-side-encryption header requesting server-side encryption with SSE-KMS. { \"Version\":\"2012-10-17\", \"Id\":\"PutObjectPolicy\", \"Statement\":[{ \"Sid\":\"DenyUnEncryptedObjectUploads\", \"Effect\":\"Deny\", \"Principal\":\"*\", \"Action\":\"s3:PutObject\", \"Resource\":\"arn:aws:s3:::DOC-EXAMPLE-BUCKET1/*\", \"Condition\":{ \"StringNotEquals\":{ \"s3:x-amz-server-side-encryption\":\"aws:kms\" } } } ] } The following example IAM policies show statements for using AWS KMS server-side encryption with replication. In this example, the encryption context is the object ARN. If you use SSE-KMS with an Amazon S3 Bucket Key enabled, you must use the bucket ARN as the encryption context. { \"Version\": \"2012-10-17\", \"Statement\": [{ \"Action\": [\"kms:Decrypt\"], \"Effect\": \"Allow\", \"Resource\": \"List of AWS KMS key ARNs used to encrypt source objects.\", \"Condition\": { \"StringLike\": { \"kms:ViaService\": \"s3.source-bucket-region.amazonaws.com\", \"kms:EncryptionContext:aws:s3:arn\": \"arn:aws:s3:::source-bucket-name/key-prefix1/*\" } } }, { \"Action\": [\"kms:Encrypt\"], \"Effect\": \"Allow\", \"Resource\": \"AWS KMS key ARNs (for the AWS Region of the destination bucket 1). Used to encrypt object replicas created in destination bucket 1.\", \"Condition\": { \"StringLike\": { \"kms:ViaService\": \"s3.destination-bucket-1-region.amazonaws.com\", \"kms:EncryptionContext:aws:s3:arn\": \"arn:aws:s3:::destination-bucket-name-1/key-prefix1/*\" } } }, { \"Action\": [\"kms:Encrypt\"], \"Effect\": \"Allow\", \"Resource\": \"AWS KMS key ARNs (for the AWS Region of destination bucket 2). Used to encrypt object replicas created in destination bucket 2.\", \"Condition\": { \"StringLike\": { \"kms:ViaService\": \"s3.destination-bucket-2-region.amazonaws.com\", \"kms:EncryptionContext:aws:s3:arn\": \"arn:aws:s3:::destination-bucket-2-name/key-prefix1*\" } } } ] } Incorrect options: Change the AWS KMS single region key used for the current Amazon S3 bucket into an AWS KMS multi-region key. Enable Amazon S3 batch replication for the existing data in the current bucket in us-east-1 region into another bucket in us-west-1 region - Amazon S3 batch replication can certainly be used to replicate the existing data in the current bucket in us-east-1 region into another bucket in us-west-1 region. However, you cannot convert an existing single-Region key to a multi-Region key. This design ensures that all data protected with existing single-Region keys maintain the same data residency and data sovereignty properties. So this option is incorrect. Enable replication for the current bucket in us-east-1 region into another bucket in us-west-1 region. Share the existing AWS KMS key from us-east-1 region to us-west-1 region - You cannot share an AWS KMS key to another region, so this option is incorrect. Create an Amazon CloudWatch scheduled rule to invoke an AWS Lambda function to copy the daily data from the source bucket in us-east-1 region to the destination bucket in us-west-1 region. Provide AWS KMS key access to the AWS Lambda function for encryption and decryption operations on the data in the source and destination Amazon S3 buckets - This option is a distractor as the daily frequency of data replication would result in significant data loss in case of a disaster. In addition, this option involves significant development effort to create the functionality to reliably replicate the data from source to destination buckets. So this option is not the best fit for the given use case. References: https://docs.aws.amazon.com/kms/latest/developerguide/multi-region-keys-overview.html https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-config-for-kms-objects.html",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "You would like to mount a network file system on Linux instances, where files will be stored and accessed frequently at first, and then infrequently. What solution is the MOST cost-effective?",
      "options": [
        "A. Amazon EFS Infrequent Access",
        "B. Amazon S3 Glacier Deep Archive",
        "C. Amazon S3 Intelligent Tiering",
        "D. Amazon FSx for Lustre"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Amazon EFS Infrequent Access Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. Amazon EFS is a regional service storing data within and across multiple Availability Zones (AZs) for high availability and durability. Amazon EFS Infrequent Access (EFS IA) is a storage class that provides price/performance that is cost-optimized for files, not accessed every day, with storage prices up to 92% lower compared to Amazon EFS Standard. Therefore, this is the correct option. How Amazon EFS works: via - https://aws.amazon.com/efs/ Incorrect options: Amazon S3 Intelligent Tiering - Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. The Amazon S3 Intelligent-Tiering storage class is designed to optimize costs by automatically moving data to the most cost-effective access tier, without performance impact or operational overhead. It works by storing objects in two access tiers: one tier that is optimized for frequent access and another lower-cost tier that is optimized for infrequent access. You can't mount a network file system on Amazon S3 Intelligent Tiering as it's an object storage service, so this option is incorrect. Amazon S3 Glacier Deep Archive - Amazon S3 Glacier and S3 Glacier Deep Archive are a secure, durable, and extremely low-cost Amazon S3 cloud storage classes for data archiving and long-term backup. They are designed to deliver 99.999999999% durability, and provide comprehensive security and compliance capabilities that can help meet even the most stringent regulatory requirements. You can't mount a network file system on Amazon S3 Glacier or S3 Glacier Deep Archive. These are data archiving solutions and hence this option is incorrect. Amazon FSx for Lustre - Amazon FSx for Lustre makes it easy and cost-effective to launch and run the world’s most popular high-performance file system. It is used for workloads such as machine learning, high-performance computing (HPC), video processing, and financial modeling. Amazon FSx enables you to use Lustre file systems for any workload where storage speed matters. Amazon FSx for Lustre is a file system better suited for distributed computing for HPC (high-performance computing) and is very expensive References: https://aws.amazon.com/efs/ https://aws.amazon.com/efs/features/infrequent-access/",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "Consider the following policy associated with an IAM group containing several users: { \"Version\":\"2012-10-17\", \"Id\":\"EC2TerminationPolicy\", \"Statement\":[ { \"Effect\":\"Deny\", \"Action\":\"ec2:*\", \"Resource\":\"*\", \"Condition\":{ \"StringNotEquals\":{ \"ec2:Region\":\"us-west-1\" } } }, { \"Effect\":\"Allow\", \"Action\":\"ec2:TerminateInstances\", \"Resource\":\"*\", \"Condition\":{ \"IpAddress\":{ \"aws:SourceIp\":\"10.200.200.0/24\" } } } ] } Which of the following options is correct?",
      "options": [
        "A. Users belonging to the IAM user group can terminate an Amazon EC2 instance in the us-west-1 region when the user's source IP is 10.200.200.200",
        "B. Users belonging to the IAM user group cannot terminate an Amazon EC2 instance in the us-west-1 region when the user's source IP is 10.200.200.200",
        "C. Users belonging to the IAM user group can terminate an Amazon EC2 instance in the us-west-1 region when the EC2 instance's IP address is 10.200.200.200",
        "D. Users belonging to the IAM user group can terminate an Amazon EC2 instance belonging to any region except the us-west-1 region when the user's source IP is 10.200.200.200"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Users belonging to the IAM user group can terminate an Amazon EC2 instance in the us-west-1 region when the user's source IP is 10.200.200.200 The given policy denies all EC2 specification actions on all resources when the region of the underlying resource is not us-west-1. The policy allows the terminate EC2 action on all resources when the source IP address is in the CIDR range 10.200.200.0/24, therefore it would allow the user with the source IP 10.200.200.200 to terminate the Amazon EC2 instance. Incorrect options: Users belonging to the IAM user group cannot terminate an Amazon EC2 instance in the us-west-1 region when the user's source IP is 10.200.200.200 Users belonging to the IAM user group can terminate an Amazon EC2 instance in the us-west-1 region when the EC2 instance's IP address is 10.200.200.200 Users belonging to the IAM user group can terminate an Amazon EC2 instance belonging to any region except the us-west-1 region when the user's source IP is 10.200.200.200 These three options contradict the explanation provided above, so these options are incorrect. Reference: https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "Your company has a monthly big data workload, running for about 2 hours, which can be efficiently distributed across multiple servers of various sizes, with a variable number of CPUs. The solution for the workload should be able to withstand server failures. Which is the MOST cost-optimal solution for this workload?",
      "options": [
        "A. Run the workload on a Spot Fleet",
        "B. Run the workload on Spot Instances",
        "C. Run the workload on Reserved Instances (RI)",
        "D. Run the workload on Dedicated Hosts"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Run the workload on a Spot Fleet The Spot Fleet selects the Spot Instance pools that meet your needs and launches Spot Instances to meet the target capacity for the fleet. By default, Spot Fleets are set to maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated. A Spot Instance is an unused Amazon EC2 instance that is available for less than the On-Demand price. Spot Instances provide great cost efficiency, but we need to select an instance type in advance. In this case, we want to use the most cost-optimal option and leave the selection of the cheapest spot instance to a Spot Fleet request, which can be optimized with the lowestPrice strategy. So this is the correct option. Key Spot Instance Concepts: via - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html Incorrect options: Run the workload on Spot Instances - A Spot Instance is an unused Amazon EC2 instance that is available for less than the On-Demand price. Because Spot Instances enable you to request unused Amazon EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly. The hourly price for a Spot Instance is called a Spot price. Only spot fleets can maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated, so spot instances, by themselves, are not the right fit for this use-case. Run the workload on Reserved Instances (RI) - Reserved Instances are less cost-optimized than Spot Instances, and most efficient when used continuously. Here the workload is once a month, so this is not efficient. Run the workload on Dedicated Hosts - Amazon EC2 Dedicated Hosts allow you to use your eligible software licenses from vendors such as Microsoft and Oracle on Amazon EC2 so that you get the flexibility and cost-effectiveness of using your licenses, but with the resiliency, simplicity, and elasticity of AWS. An Amazon EC2 Dedicated Host is a physical server fully dedicated for your use, so you can help address corporate compliance requirement. They're not particularly cost-efficient. So this option is not correct. References: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-fleet.html#spot-fleet-allocation-strategy",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A financial services company wants to store confidential data in Amazon S3 and it needs to meet the following data security and compliance norms: Encryption key usage must be logged for auditing purposes Encryption Keys must be rotated every year The data must be encrypted at rest Which is the MOST operationally efficient solution?",
      "options": [
        "A. Server-side encryption (SSE-S3) with automatic key rotation",
        "B. Server-side encryption with AWS Key Management Service (AWS KMS) keys (SSE-KMS) with automatic key rotation",
        "C. Server-side encryption with AWS Key Management Service (AWS KMS) keys (SSE-KMS) with manual key rotation",
        "D. Server-side encryption with customer-provided keys (SSE-C) with automatic key rotation"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Server-side encryption with AWS Key Management Service (AWS KMS) keys (SSE-KMS) with automatic key rotation Server-side encryption is the encryption of data at its destination by the application or service that receives it. Amazon S3 encrypts your data at the object level as it writes it to disks in its data centers and decrypts it for you when you access it. Amazon S3 now applies server-side encryption with Amazon S3 managed keys (SSE-S3) as the base level of encryption for every bucket in Amazon S3. Starting January 5, 2023, all new object uploads to Amazon S3 are automatically encrypted at no additional cost and with no impact on performance. Amazon S3 server-side encryption via - https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html AWS KMS is a service that combines secure, highly available hardware and software to provide a key management system scaled for the cloud. Amazon S3 uses server-side encryption with AWS KMS (SSE-KMS) to encrypt your S3 object data. Also, when SSE-KMS is requested for the object, the S3 checksum as part of the object's metadata, is stored in encrypted form. If you use KMS keys, you can use AWS KMS through the AWS Management Console or the AWS KMS API to do the following: Centrally create, view, edit, monitor, enable or disable, rotate, and schedule deletion of KMS keys. Define the policies that control how and by whom KMS keys can be used. Audit their usage to prove that they are being used correctly. Auditing is supported by the AWS KMS API, but not by the AWS KMSAWS Management Console. When you enable automatic key rotation for a KMS key, AWS KMS generates new cryptographic material for the KMS key every year. AWS KMS keys: via - https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html For the given use case, you can set up server-side encryption with AWS KMS Keys (SSE-KMS) with automatic key rotation. Incorrect options: Server-side encryption with AWS Key Management Service (AWS KMS) keys (SSE-KMS) with manual key rotation - Although it is possible to manually rotate the AWS KMS key, it is not the best fit solution as it is not operationally efficient. Server-side encryption (SSE-S3) with automatic key rotation - When you use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key. As an additional safeguard, it encrypts the key itself with a root key that it regularly rotates. However, with SSE-S3, you cannot log the usage of the encryption key for auditing purposes. So this option is incorrect. Server-side encryption with customer-provided keys (SSE-C) with automatic key rotation - It is possible to automatically rotate the customer-provided keys but you will need to develop the underlying solution to automate the key rotation. Therefore, this option is not operationally efficient. References: https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#master_keys https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A Machine Learning research group uses a proprietary computer vision application hosted on an Amazon EC2 instance. Every time the instance needs to be stopped and started again, the application takes about 3 minutes to start as some auxiliary software programs need to be executed so that the application can function. The research group would like to minimize the application boostrap time whenever the system needs to be stopped and then started at a later point in time. As a solutions architect, which of the following solutions would you recommend for this use-case?",
      "options": [
        "A. Use Amazon EC2 Instance Hibernate",
        "B. Use Amazon EC2 User-Data",
        "C. Use Amazon EC2 Meta-Data",
        "D. Create an Amazon Machine Image (AMI) and launch your Amazon EC2 instances from that"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Use Amazon EC2 Instance Hibernate When you hibernate an instance, AWS signals the operating system to perform hibernation (suspend-to-disk). Hibernation saves the contents from the instance memory (RAM) to your Amazon EBS root volume. AWS then persists the instance's Amazon EBS root volume and any attached Amazon EBS data volumes. When you start your instance: The Amazon EBS root volume is restored to its previous state The RAM contents are reloaded The processes that were previously running on the instance are resumed Previously attached data volumes are reattached and the instance retains its instance ID Overview of Amazon EC2 hibernation: via - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Hibernate.html By using Amazon EC2 hibernate, we have the capability to resume it at any point of time, with the application already launched, thus helping us cut the 3 minutes start time. Incorrect options: Use Amazon EC2 User-Data - Amazon EC2 instance user data is the data that you specified in the form of a configuration script while launching your instance. Here, the problem is that the application takes 3 minutes to launch, no matter what. EC2 user data won't help us because it's just here to help us execute a list of commands, not speed them up. Use Amazon EC2 Meta-Data - Amazon EC2 instance metadata is data about your instance that you can use to configure or manage the running instance. Instance metadata is divided into categories, for example, host name, events, and security groups. The EC2 meta-data is a distractor and can only help us determine some metadata attributes on our EC2 instances. Create an Amazon Machine Image (AMI) and launch your Amazon EC2 instances from that - An Amazon Machine Image (AMI) provides the information required to launch an instance. You must specify an AMI when you launch an instance. You can launch multiple instances from a single AMI when you need multiple instances with the same configuration. You can use different AMIs to launch instances when you need instances with different configurations. Creating an AMI may help with all the system dependencies, but it won't help us with speeding up the application start time. References: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Hibernate.html https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A media company is migrating its flagship application from its on-premises data center to AWS for improving the application's read-scaling capability as well as its availability. The existing architecture leverages a Microsoft SQL Server database that sees a heavy read load. The engineering team does a full copy of the production database at the start of the business day to populate a dev database. During this period, application users face high latency leading to a bad user experience. The company is looking at alternate database options and migrating database engines if required. What would you suggest?",
      "options": [
        "A. Leverage Amazon RDS for MySQL with a Multi-AZ deployment and use the standby instance as the dev database",
        "B. Leverage Amazon RDS for SQL server with a Multi-AZ deployment and read replicas. Use the read replica as the dev database",
        "C. Leverage Amazon Aurora MySQL with Multi-AZ Aurora Replicas and create the dev database by restoring from the automated backups of Amazon Aurora",
        "D. Leverage Amazon Aurora MySQL with Multi-AZ Aurora Replicas and restore the dev database via mysqldump"
      ],
      "correct_answer": "C",
      "explanation": "Correct option: Leverage Amazon Aurora MySQL with Multi-AZ Aurora Replicas and create the dev database by restoring from the automated backups of Amazon Aurora Amazon Aurora (Aurora) is a fully managed relational database engine that's compatible with MySQL and PostgreSQL. An Amazon Aurora DB cluster consists of one or more DB instances and a cluster volume that manages the data for those DB instances. An Aurora cluster volume is a virtual database storage volume that spans multiple Availability Zones (AZs), with each Availability Zone (AZ) having a copy of the Amazon Aurora DB cluster data. Aurora supports Multi-AZ Aurora Replicas that improve the application's read-scaling and availability. Amazon Aurora Overview: via - https://aws.amazon.com/rds/aurora/ Aurora backs up your cluster volume automatically and retains restore data for the length of the backup retention period. Aurora backups are continuous and incremental so you can quickly restore to any point within the backup retention period. No performance impact or interruption of database service occurs as backup data is being written. via - https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Backups.html Automated backups occur daily during the preferred backup window. If the backup requires more time than allotted to the backup window, the backup continues after the window ends, until it finishes. The backup window can't overlap with the weekly maintenance window for the DB cluster. Aurora backups are continuous and incremental, but the backup window is used to create a daily system backup that is preserved within the backup retention period. The latest restorable time for a DB cluster is the most recent point at which you can restore your DB cluster, typically within 5 minutes of the current time. For the given use case, you can create the dev database by restoring from the automated backups of Amazon Aurora. Incorrect options: Leverage Amazon Aurora MySQL with Multi-AZ Aurora Replicas and restore the dev database via mysqldump - Restoring the dev database via mysqldump would still result in a significant load on the primary DB, so this option fails to address the given requirement. Leverage Amazon RDS for MySQL with a Multi-AZ deployment and use the standby instance as the dev database - The standby is there just for handling failover in a Multi-AZ deployment. You cannot access the standby instance and use it as a dev database. Hence this option is incorrect. Leverage Amazon RDS for SQL server with a Multi-AZ deployment and read replicas. Use the read replica as the dev database - Amazon RDS supports Multi-AZ deployments for Microsoft SQL Server by using either SQL Server Database Mirroring (DBM) or Always On Availability Groups (AGs). Amazon RDS monitors and maintains the health of your Multi-AZ deployment. Multi-AZ deployments provide increased availability, data durability, and fault tolerance for DB instances. In the event of planned database maintenance or unplanned service disruption, Amazon RDS automatically fails over to the up-to-date secondary DB instance. For SQL Server, I/O activity is suspended briefly during backup for Multi-AZ deployments. A read replica is only meant to serve read traffic. The primary purpose of the read replica is to replicate the data in the primary DB instance. A read replica cannot be used as a dev database because it does not allow any database write operations. References: https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Backups.html https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/SQLServer.ReadReplicas.html",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "You would like to store a database password in a secure place, and enable automatic rotation of that password every 90 days. What do you recommend?",
      "options": [
        "A. AWS Secrets Manager",
        "B. AWS Systems Manager Parameter Store",
        "C. AWS Key Management Service (AWS KMS)",
        "D. AWS CloudHSM"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: AWS Secrets Manager AWS Secrets Manager helps you protect secrets needed to access your applications, services, and IT resources. The service enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. Users and applications retrieve secrets with a call to Secrets Manager APIs, eliminating the need to hardcode sensitive information in plain text. Secrets Manager offers secret rotation with built-in integration for Amazon RDS, Amazon Redshift, and Amazon DocumentDB. The correct answer here is Secrets Manager Incorrect options: AWS Key Management Service (AWS KMS) - AWS Key Management Service (AWS KMS) is a service that combines secure, highly available hardware and software to provide a key management system scaled for the cloud. When you use server-side encryption with AWS KMS (SSE-KMS), you can specify a customer-managed CMK that you have already created. SSE-KMS provides you with an audit trail that shows when your CMK was used and by whom. KMS is an encryption service, it's not a secrets store. So this option is incorrect. AWS CloudHSM - AWS CloudHSM is a cloud-based hardware security module (HSM) that enables you to easily generate and use your encryption keys on the AWS Cloud. With AWS CloudHSM, you can manage your encryption keys using FIPS 140-2 Level 3 validated HSMs. AWS CloudHSM is standards-compliant and enables you to export all of your keys to most other commercially-available HSMs, subject to your configurations. It is a fully-managed service that automates time-consuming administrative tasks for you, such as hardware provisioning, software patching, high-availability, and backups. AWS CloudHSM is also an encryption service, not a secrets store. So this option is incorrect. AWS Systems Manager Parameter Store - AWS Systems Manager Parameter Store (aka SSM Parameter Store) provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, Amazon EC2 instance IDs, Amazon Machine Image (AMI) IDs, and license codes as parameter values. You can store values as plain text or encrypted data. You can reference Systems Manager parameters in your scripts, commands, SSM documents, and configuration and automation workflows by using the unique name that you specified when you created the parameter. AWS Systems Manager Parameter Store can serve as a secrets store, but you must rotate the secrets yourself, it doesn't have an automatic capability for this. So this option is incorrect. References: https://aws.amazon.com/secrets-manager/ https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html https://aws.amazon.com/cloudhsm/ https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html https://aws.amazon.com/blogs/mt/the-right-way-to-store-secrets-using-parameter-store/ https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "Which of the following IAM policies provides read-only access to the Amazon S3 bucket mybucket and its content?",
      "options": [
        "A. { \"Version\":\"2012-10-17\", \"Statement\":[ { \"Effect\":\"Allow\", \"Action\":[ \"s3:ListBucket\" ], \"Resource\":\"arn:aws:s3:::mybucket\" }, { \"Effect\":\"Allow\", \"Action\":[ \"s3:GetObject\" ], \"Resource\":\"arn:aws:s3:::mybucket/*\" } ] }",
        "B. { \"Version\":\"2012-10-17\", \"Statement\":[ { \"Effect\":\"Allow\", \"Action\":[ \"s3:ListBucket\", \"s3:GetObject\" ], \"Resource\":\"arn:aws:s3:::mybucket\" } ] }",
        "C. { \"Version\":\"2012-10-17\", \"Statement\":[ { \"Effect\":\"Allow\", \"Action\":[ \"s3:ListBucket\", \"s3:GetObject\" ], \"Resource\":\"arn:aws:s3:::mybucket/*\" } ] }",
        "D. { \"Version\":\"2012-10-17\", \"Statement\":[ { \"Effect\":\"Allow\", \"Action\":[ \"s3:ListBucket\" ], \"Resource\":\"arn:aws:s3:::mybucket/*\" }, { \"Effect\":\"Allow\", \"Action\":[ \"s3:GetObject\" ], \"Resource\":\"arn:aws:s3:::mybucket\" } ] }"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: { \"Version\":\"2012-10-17\", \"Statement\":[ { \"Effect\":\"Allow\", \"Action\":[ \"s3:ListBucket\" ], \"Resource\":\"arn:aws:s3:::mybucket\" }, { \"Effect\":\"Allow\", \"Action\":[ \"s3:GetObject\" ], \"Resource\":\"arn:aws:s3:::mybucket/*\" } ] } You manage access in AWS by creating policies and attaching them to IAM identities (users, groups of users, or roles) or AWS resources. A policy is an object in AWS that, when associated with an identity or resource, defines their permissions. AWS evaluates these policies when an IAM principal (user or role) makes a request. Permissions in the policies determine whether the request is allowed or denied. Most policies are stored in AWS as JSON documents. AWS supports six types of policies: identity-based policies, resource-based policies, permissions boundaries, service control policy (SCP)of AWS Organizations, access control list (ACL), and session policies. s3:ListBucket is applied to buckets, so the ARN is in the form \"Resource\":\"arn:aws:s3:::mybucket\", without a trailing / s3:GetObject is applied to objects within the bucket, so the ARN is in the form \"Resource\":\"arn:aws:s3:::mybucket/*\", with a trailing /* to indicate all objects within the bucket mybucket Therefore, this is the correct option. Incorrect options: { \"Version\":\"2012-10-17\", \"Statement\":[ { \"Effect\":\"Allow\", \"Action\":[ \"s3:ListBucket\", \"s3:GetObject\" ], \"Resource\":\"arn:aws:s3:::mybucket\" } ] } This option is incorrect as it provides read-only access only to the bucket, not its contents. { \"Version\":\"2012-10-17\", \"Statement\":[ { \"Effect\":\"Allow\", \"Action\":[ \"s3:ListBucket\", \"s3:GetObject\" ], \"Resource\":\"arn:aws:s3:::mybucket/*\" } ] } This option is incorrect as it provides read-only access only to the objects within the bucket and it does not provide listBucket permissions to the bucket itself. { \"Version\":\"2012-10-17\", \"Statement\":[ { \"Effect\":\"Allow\", \"Action\":[ \"s3:ListBucket\" ], \"Resource\":\"arn:aws:s3:::mybucket/*\" }, { \"Effect\":\"Allow\", \"Action\":[ \"s3:GetObject\" ], \"Resource\":\"arn:aws:s3:::mybucket\" } ] } This option is incorrect as it provides listing access only to the bucket contents. References: https://aws.amazon.com/blogs/security/writing-iam-policies-how-to-grant-access-to-an-amazon-s3-bucket/ https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An HTTP application is deployed on an Auto Scaling Group, is accessible from an Application Load Balancer (ALB) that provides HTTPS termination, and accesses a PostgreSQL database managed by Amazon RDS. How should you configure the security groups? (Select three)",
      "options": [
        "A. The security group of Amazon RDS should have an inbound rule from the security group of the Amazon EC2 instances in the Auto Scaling group on port 5432",
        "B. The security group of the Amazon EC2 instances should have an inbound rule from the security group of the Application Load Balancer on port 80",
        "C. The security group of the Application Load Balancer should have an inbound rule from anywhere on port 443",
        "D. The security group of the Application Load Balancer should have an inbound rule from anywhere on port 80",
        "E. The security group of the Amazon EC2 instances should have an inbound rule from the security group of the Amazon RDS database on port 5432",
        "F. The security group of Amazon RDS should have an inbound rule from the security group of the Amazon EC2 instances in the Auto Scaling group on port 80"
      ],
      "correct_answer": "A",
      "explanation": "Correct options: The security group of Amazon RDS should have an inbound rule from the security group of the Amazon EC2 instances in the Auto Scaling group on port 5432 The security group of the Amazon EC2 instances should have an inbound rule from the security group of the Application Load Balancer on port 80 The security group of the Application Load Balancer should have an inbound rule from anywhere on port 443 A security group acts as a virtual firewall that controls the traffic for one or more instances. When you launch an instance, you can specify one or more security groups; otherwise, we use the default security group. You can add rules to each security group that allows traffic to or from its associated instances. You can modify the rules for a security group at any time; the new rules are automatically applied to all instances that are associated with the security group. When we decide whether to allow traffic to reach an instance, we evaluate all the rules from all the security groups that are associated with the instance. The following are the characteristics of security group rules: 1. By default, security groups allow all outbound traffic. 2. Security group rules are always permissive; you can't create rules that deny access. 3. Security groups are stateful PostgreSQL port = 5432 HTTP port = 80 HTTPS port = 443 The traffic goes like this : The client sends an HTTPS request to ALB on port 443. This is handled by the rule - \"The security group of the Application Load Balancer should have an inbound rule from anywhere on port 443\" The Application Load Balancer then forwards the request to one of the Amazon EC2 instances. This is handled by the rule - \"The security group of the Amazon EC2 instances should have an inbound rule from the security group of the Application Load Balancer on port 80\" The Amazon EC2 instance further accesses the PostgreSQL database managed by Amazon RDS on port 5432. This is handled by the rule - \"The security group of Amazon RDS should have an inbound rule from the security group of the Amazon EC2 instances in the Auto Scaling group on port 5432\" Incorrect options: The security group of the Application Load Balancer should have an inbound rule from anywhere on port 80 - The client sends an HTTPS request to ALB on port 443 and not on port 80, so this is incorrect. The security group of the Amazon EC2 instances should have an inbound rule from the security group of the Amazon RDS database on port 5432 - The security group of the Amazon EC2 instances should have an inbound rule from the security group of the Application Load Balancer and not from the security group of the Amazon RDS database, so this option is incorrect. The security group of Amazon RDS should have an inbound rule from the security group of the Amazon EC2 instances in the Auto Scaling group on port 80 - The Amazon EC2 instance further accesses the PostgreSQL database managed by Amazon RDS on port 5432 and not on port 80, so this option is incorrect. Reference: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "Your company has an on-premises Distributed File System Replication (DFSR) service to keep files synchronized on multiple Windows servers, and would like to migrate to AWS cloud. What do you recommend as a replacement for the DFSR?",
      "options": [
        "A. Amazon FSx for Windows File Server",
        "B. Amazon FSx for Lustre",
        "C. Amazon Elastic File System (Amazon EFS)",
        "D. Amazon Simple Storage Service (Amazon S3)"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Amazon FSx for Windows File Server Amazon FSx for Windows File Server provides fully managed, highly reliable file storage that is accessible over the industry-standard Service Message Block (SMB) protocol. It is built on Windows Server, delivering a wide range of administrative features such as user quotas, end-user file restore, and Microsoft Active Directory (AD) integration. The Distributed File System Replication (DFSR) service is a new multi-master replication engine that is used to keep folders synchronized on multiple servers. Amazon FSx supports the use of Microsoft’s Distributed File System (DFS) to organize shares into a single folder structure up to hundreds of PB in size. Amazon FSx for Windows is a perfect distributed file system, with replication capability, and can be mounted on Windows. How Amazon FSx for Windows Works: via - https://aws.amazon.com/fsx/windows/ Incorrect options: Amazon FSx for Lustre - Amazon FSx for Lustre makes it easy and cost-effective to launch and run the world’s most popular high-performance file system. It is used for workloads such as machine learning, high-performance computing (HPC), video processing, and financial modeling. The open-source Lustre file system is designed for applications that require fast storage – where you want your storage to keep up with your compute. Amazon FSx enables you to use Lustre file systems for any workload where storage speed matters. FSx for Lustre integrates with Amazon S3, making it easy to process data sets with the Lustre file system. Amazon FSx for Lustre is for Linux only, so this option is incorrect. Amazon Elastic File System (Amazon EFS) - Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. It is built to scale on-demand to petabytes without disrupting applications, growing and shrinking automatically as you add and remove files, eliminating the need to provision and manage capacity to accommodate growth. Amazon EFS is a network file system but for Linux only, so this option is incorrect. Amazon Simple Storage Service (Amazon S3) - Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. Amazon S3 cannot be mounted as a file system on Windows, so this option is incorrect. References: https://docs.microsoft.com/en-us/previous-versions/windows/desktop/dfsr/dfsr-overview https://aws.amazon.com/fsx/windows/ https://aws.amazon.com/fsx/lustre/",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A retail company wants to share sensitive accounting data that is stored in an Amazon RDS database instance with an external auditor. The auditor has its own AWS account and needs its own copy of the database. Which of the following would you recommend to securely share the database with the auditor?",
      "options": [
        "A. Create an encrypted snapshot of the database, share the snapshot, and allow access to the AWS Key Management Service (AWS KMS) encryption key",
        "B. Create a snapshot of the database in Amazon S3 and assign an IAM role to the auditor to grant access to the object in that bucket",
        "C. Export the database contents to text files, store the files in Amazon S3, and create a new IAM user for the auditor with access to that bucket",
        "D. Set up a read replica of the database and configure IAM standard database authentication to grant the auditor access"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Create an encrypted snapshot of the database, share the snapshot, and allow access to the AWS Key Management Service (AWS KMS) encryption key You can share the AWS Key Management Service (AWS KMS) key that was used to encrypt the snapshot with any accounts that you want to be able to access the snapshot. You can share AWS KMS Key with another AWS account by adding the other account to the AWS KMS key policy. Making an encrypted snapshot of the database will give the auditor a copy of the database, as required for the given use case. Incorrect options: Create a snapshot of the database in Amazon S3 and assign an IAM role to the auditor to grant access to the object in that bucket - Amazon RDS stores the DB snapshots in the Amazon S3 bucket belonging to the same AWS region where the Amazon RDS instance is located. Amazon RDS stores these on your behalf and you do not have direct access to these snapshots in Amazon S3, so it's not possible to grant access to the snapshot objects in Amazon S3. Export the database contents to text files, store the files in Amazon S3, and create a new IAM user for the auditor with access to that bucket - This solution is feasible though not optimal. It requires a lot of unnecessary work and is difficult to audit when such bulk data is exported into text files. Set up a read replica of the database and configure IAM standard database authentication to grant the auditor access - Read Replicas make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. Creating Read Replicas for audit purposes is overkill. Also, the question mentions that the auditor needs to have their own copy of the database, which is not possible with replicas. Reference: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ShareSnapshot.html",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "Your company has deployed an application that will perform a lot of overwrites and deletes on data and require the latest information to be available anytime data is read via queries on database tables. As a Solutions Architect, which database technology will you recommend?",
      "options": [
        "A. Amazon Relational Database Service (Amazon RDS)",
        "B. Amazon ElastiCache",
        "C. Amazon Neptune",
        "D. Amazon Simple Storage Service (Amazon S3)"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Amazon Relational Database Service (Amazon RDS) Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching, and backups. RDS allows you to create, read, update, and delete records without any item lock or ambiguity. All RDS transactions must be ACID compliant or be Atomic, Consistent, Isolated, and Durable to ensure data integrity. Atomicity requires that either transaction as a whole is successfully executed or if a part of the transaction fails, then the entire transaction be invalidated. Consistency mandates the data written to the database as part of the transaction must adhere to all defined rules, and restrictions including constraints, cascades, and triggers. Isolation is critical to achieving concurrency control and makes sure each transaction is independent unto itself. Durability requires that all of the changes made to the database be permanent once a transaction is completed. Hence, the best fit is Amazon RDS. Incorrect options: Amazon ElastiCache - Amazon ElastiCache allows you to seamlessly set up, run, and scale popular open-source compatible in-memory data stores in the cloud. Build data-intensive apps or boost the performance of your existing databases by retrieving data from high throughput and low latency in-memory data stores. Amazon ElastiCache is a popular choice for real-time use cases like Caching, Session Stores, Gaming, Geospatial Services, Real-Time Analytics, and Queuing. ElastiCache could work but it's a better fit as a caching technology to enhance reads. Amazon Simple Storage Service (Amazon S3) - This option is incorrect as Amazon S3 is not a database technology that supports queries on database tables out of the box. It is an object storage service that offers industry-leading scalability, data availability, security, and performance. Your applications can easily achieve thousands of transactions per second in request performance when uploading and retrieving storage from Amazon S3. After a successful write of a new object or an overwrite of an existing object, any subsequent read request immediately receives the latest version of the object. Amazon S3 also provides strong consistency for list operations, so after a write, you can immediately perform a listing of the objects in a bucket with any changes reflected. Strong read-after-write consistency helps when you need to immediately read an object after a write. For example, strong read-after-write consistency when you often read and list immediately after writing objects. Amazon Neptune - Amazon Neptune is a fast, reliable, fully-managed graph database service that makes it easy to build and run applications that work with highly connected datasets. The core of Amazon Neptune is a purpose-built, high-performance graph database engine optimized for storing billions of relationships and querying the graph with milliseconds latency. Amazon Neptune is highly available, with read replicas, point-in-time recovery, continuous backup to Amazon S3, and replication across Availability Zones. Neptune is secure with support for HTTPS encrypted client connections and encryption at rest. Amazon Neptune is fully managed, so you no longer need to worry about database management tasks such as hardware provisioning, software patching, setup, configuration, or backups. Amazon Neptune is a graph database so it's not a good fit. References: https://aws.amazon.com/relational-database/ https://aws.amazon.com/rds/ https://aws.amazon.com/neptune/ https://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html#ConsistencyModel",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A manufacturing company receives unreliable service from its data center provider because the company is located in an area prone to natural disasters. The company is not ready to fully migrate to the AWS Cloud, but it wants a failover environment on AWS in case the on-premises data center fails. The company runs web servers that connect to external vendors. The data available on AWS and on-premises must be uniform. Which of the following solutions would have the LEAST amount of downtime?",
      "options": [
        "A. Set up a Amazon Route 53 failover record. Execute an AWS CloudFormation template from a script to provision Amazon EC2 instances behind an Application Load Balancer. Set up AWS Storage Gateway with stored volumes to back up data to Amazon S3",
        "B. Set up a Amazon Route 53 failover record. Run application servers on Amazon EC2 instances behind an Application Load Balancer in an Auto Scaling group. Set up AWS Storage Gateway with stored volumes to back up data to Amazon S3",
        "C. Set up a Amazon Route 53 failover record. Set up an AWS Direct Connect connection between a VPC and the data center. Run application servers on Amazon EC2 in an Auto Scaling group. Run an AWS Lambda function to execute an AWS CloudFormation template to create an Application Load Balancer",
        "D. Set up a Amazon Route 53 failover record. Run an AWS Lambda function to execute an AWS CloudFormation template to launch two Amazon EC2 instances. Set up AWS Storage Gateway with stored volumes to back up data to Amazon S3. Set up an AWS Direct Connect connection between a VPC and the data center"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Set up a Amazon Route 53 failover record. Run application servers on Amazon EC2 instances behind an Application Load Balancer in an Auto Scaling group. Set up AWS Storage Gateway with stored volumes to back up data to Amazon S3 If you have multiple resources that perform the same function, you can configure DNS failover so that Route 53 will route your traffic from an unhealthy resource to a healthy resource. Elastic Load Balancing is used to automatically distribute your incoming application traffic across all the Amazon EC2 instances that you are running. You can use Elastic Load Balancing to manage incoming requests by optimally routing traffic so that no one instance is overwhelmed. Your load balancer acts as a single point of contact for all incoming web traffic to your Auto Scaling group. AWS Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage. It provides low-latency performance by caching frequently accessed data on-premises while storing data securely and durably in Amazon cloud storage services. Storage Gateway optimizes data transfer to AWS by sending only changed data and compressing data. Storage Gateway also integrates natively with Amazon S3 cloud storage which makes your data available for in-cloud processing. Incorrect options: Set up a Amazon Route 53 failover record. Execute an AWS CloudFormation template from a script to provision Amazon EC2 instances behind an Application Load Balancer. Set up AWS Storage Gateway with stored volumes to back up data to Amazon S3 Set up a Amazon Route 53 failover record. Run an AWS Lambda function to execute an AWS CloudFormation template to launch two Amazon EC2 instances. Set up AWS Storage Gateway with stored volumes to back up data to Amazon S3. Set up an AWS Direct Connect connection between a VPC and the data center Set up a Amazon Route 53 failover record. Set up an AWS Direct Connect connection between a VPC and the data center. Run application servers on Amazon EC2 in an Auto Scaling group. Run an AWS Lambda function to execute an AWS CloudFormation template to create an Application Load Balancer AWS CloudFormation is a convenient provisioning mechanism for a broad range of AWS and third-party resources. It supports the infrastructure needs of many different types of applications such as existing enterprise applications, legacy applications, applications built using a variety of AWS resources, and container-based solutions. These three options involve AWS CloudFormation as part of the solution. Now, AWS CloudFormation takes time to provision the resources and hence is not the right solution when LEAST amount of downtime is mandated for the given use case. Therefore, these options are not the right fit for the given requirement. References: https://aws.amazon.com/route53/ https://aws.amazon.com/storagegateway/",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A company is looking at storing their less frequently accessed files on AWS that can be concurrently accessed by hundreds of Amazon EC2 instances. The company needs the most cost-effective file storage service that provides immediate access to data whenever needed. Which of the following options represents the best solution for the given requirements?",
      "options": [
        "A. Amazon Elastic File System (EFS) Standard–IA storage class",
        "B. Amazon S3 Standard-Infrequent Access (S3 Standard-IA) storage class",
        "C. Amazon Elastic File System (EFS) Standard storage class",
        "D. Amazon Elastic Block Store (EBS)"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Amazon Elastic File System (EFS) Standard–IA storage class - Amazon EFS is a file storage service for use with Amazon compute (EC2, containers, serverless) and on-premises servers. Amazon EFS provides a file system interface, file system access semantics (such as strong consistency and file locking), and concurrently accessible storage for up to thousands of Amazon EC2 instances. The Amazon S3 Standard–IA storage class reduces storage costs for files that are not accessed every day. It does this without sacrificing the high availability, high durability, elasticity, and POSIX file system access that Amazon EFS provides. AWS recommends Standard-IA storage if you need your full dataset to be readily accessible and want to automatically save on storage costs for files that are less frequently accessed. Incorrect options: Amazon S3 Standard-Infrequent Access (S3 Standard-IA) storage class - Amazon S3 is an object storage service. Amazon S3 makes data available through an Internet API that can be accessed anywhere. It is not a file storage service, as is needed in the use case. Amazon Elastic File System (EFS) Standard storage class - Amazon EFS Standard storage classes are ideal for workloads that require the highest levels of durability and availability. The Amazon EFS Standard storage class is used for frequently accessed files. It is the storage class to which customer data is initially written for Standard storage classes. The company is also looking at cutting costs by optimally storing the infrequently accessed data. Hence, Amazon EFS standard storage class is not the right solution for the given use case. Amazon Elastic Block Store (EBS) - Amazon EBS is a block-level storage service for use with Amazon EC2. Amazon EBS can deliver performance for workloads that require the lowest latency access to data from a single Amazon EC2 instance. Amazon EBS volume cannot be accessed by hundreds of Amazon EC2 instances concurrently. It is not a file storage service, as is needed in the use case. Reference: https://docs.aws.amazon.com/efs/latest/ug/storage-classes.html",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A company is developing a global healthcare application that requires the least possible latency for database read/write operations from users in several geographies across the world. The company has hired you as an AWS Certified Solutions Architect Associate to build a solution using Amazon Aurora that offers an effective recovery point objective (RPO) of seconds and a recovery time objective (RTO) of a minute. Which of the following options would you recommend?",
      "options": [
        "A. Set up an Amazon Aurora serverless Database cluster",
        "B. Set up an Amazon Aurora provisioned Database cluster",
        "C. Set up an Amazon Aurora Global Database cluster",
        "D. Set up an Amazon Aurora multi-master Database cluster"
      ],
      "correct_answer": "C",
      "explanation": "Correct option: Set up an Amazon Aurora Global Database cluster Amazon Aurora Global Database is designed for globally distributed applications, allowing a single Amazon Aurora database to span multiple AWS Regions. It replicates your data with no impact on database performance, enables fast local reads with low latency in each Region, and provides disaster recovery from Region-wide outages. If your primary Region suffers a performance degradation or outage, you can promote one of the secondary Regions to take read/write responsibilities. An Aurora cluster can recover in less than 1 minute, even in the event of a complete Regional outage. This provides your application with an effective recovery point objective (RPO) of 1 second and a recovery time objective (RTO) of less than 1 minute, providing a strong foundation for a global business continuity plan. via - https://aws.amazon.com/rds/aurora/global-database/ Incorrect options: Set up an Amazon Aurora serverless Database cluster Set up an Amazon Aurora provisioned Database cluster Both these options work in a single AWS Region, so these options are incorrect. Set up an Amazon Aurora multi-master Database cluster - AWS does not offer the multi-master feature in a Aurora database cluster, so this option acts as a distractor. Reference: https://aws.amazon.com/rds/aurora/global-database/",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "The engineering manager for a content management application wants to set up Amazon RDS read replicas to provide enhanced performance and read scalability. The manager wants to understand the data transfer charges while setting up Amazon RDS read replicas. Which of the following would you identify as correct regarding the data transfer charges for Amazon RDS read replicas?",
      "options": [
        "A. There are data transfer charges for replicating data within the same Availability Zone (AZ)",
        "B. There are data transfer charges for replicating data within the same AWS Region",
        "C. There are data transfer charges for replicating data across AWS Regions",
        "D. There are no data transfer charges for replicating data across AWS Regions"
      ],
      "correct_answer": "C",
      "explanation": "Correct option: There are data transfer charges for replicating data across AWS Regions Amazon RDS Read Replicas provide enhanced performance and durability for Amazon RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. A read replica is billed as a standard DB Instance and at the same rates. You are not charged for the data transfer incurred in replicating data between your source DB instance and read replica within the same AWS Region. via - https://aws.amazon.com/rds/faqs/ Incorrect options: There are data transfer charges for replicating data within the same Availability Zone (AZ) There are data transfer charges for replicating data within the same AWS Region There are no data transfer charges for replicating data across AWS Regions These three options contradict the explanation provided above, so these options are incorrect. Reference: https://aws.amazon.com/rds/faqs/",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A company has recently launched a new mobile gaming application that the users are adopting rapidly. The company uses Amazon RDS MySQL as the database. The engineering team wants an urgent solution to this issue where the rapidly increasing workload might exceed the available database storage. As a solutions architect, which of the following solutions would you recommend so that it requires minimum development and systems administration effort to address this requirement?",
      "options": [
        "A. Enable storage auto-scaling for Amazon RDS MySQL",
        "B. Migrate RDS MySQL database to Amazon Aurora which offers storage auto-scaling",
        "C. Migrate Amazon RDS MySQL database to Amazon DynamoDB which automatically allocates storage space when required",
        "D. Create read replica for Amazon RDS MySQL"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Enable storage auto-scaling for Amazon RDS MySQL If your workload is unpredictable, you can enable storage autoscaling for an Amazon RDS DB instance. With storage autoscaling enabled, when Amazon RDS detects that you are running out of free database space it automatically scales up your storage. Amazon RDS starts a storage modification for an autoscaling-enabled DB instance when these factors apply: Free available space is less than 10 percent of the allocated storage. The low-storage condition lasts at least five minutes. At least six hours have passed since the last storage modification. The maximum storage threshold is the limit that you set for autoscaling the DB instance. You can't set the maximum storage threshold for autoscaling-enabled instances to a value greater than the maximum allocated storage. Incorrect options: Migrate RDS MySQL database to Amazon Aurora which offers storage auto-scaling - Although Aurora offers automatic storage scaling, this option is ruled out since it involves significant systems administration effort to migrate from Amazon RDS MySQL to Aurora. It is much easier to just enable storage auto-scaling for Amazon RDS MySQL. Migrate Amazon RDS MySQL database to Amazon DynamoDB which automatically allocates storage space when required - This option is ruled out since Amazon DynamoDB is a NoSQL database which implies significant development effort to change the application logic to connect and query data from the underlying database. It is much easier to just enable storage auto-scaling for Amazon RDS MySQL. Create read replica for Amazon RDS MySQL - Read replicas make it easy to take advantage of supported engines' built-in replication functionality to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. You can create multiple read replicas for a given source DB Instance and distribute your application’s read traffic amongst them. This option acts as a distractor as read replicas cannot help to automatically scale storage for the primary database. Reference: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIOPS.StorageTypes.html",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An analytics company wants to improve the performance of its big data processing workflows running on Amazon Elastic File System (Amazon EFS). Which of the following performance modes should be used for Amazon EFS to address this requirement?",
      "options": [
        "A. Provisioned Throughput",
        "B. Bursting Throughput",
        "C. General Purpose",
        "D. Max I/O"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: Max I/O How Amazon EFS Works: via - https://aws.amazon.com/efs/ Max I/O performance mode is used to scale to higher levels of aggregate throughput and operations per second. This scaling is done with a tradeoff of slightly higher latencies for file metadata operations. Highly parallelized applications and workloads, such as big data analysis, media processing, and genomic analysis, can benefit from this mode. via - https://docs.aws.amazon.com/efs/latest/ug/performance.html Incorrect options: Provisioned Throughput Bursting Throughput These two options have been added as distractors as these refer to the throughput mode of Amazon EFS and not the performance mode. There are two throughput modes to choose from for your file system, Bursting Throughput and Provisioned Throughput. With Bursting Throughput mode, throughput on Amazon EFS scales as the size of your file system in the standard storage class grows. With Provisioned Throughput mode, you can instantly provision the throughput of your file system (in MiB/s) independent of the amount of data stored. General Purpose - General Purpose performance mode is ideal for latency-sensitive use cases, like web serving environments, content management systems, home directories, and general file serving. If you don't choose a performance mode when you create your file system, Amazon EFS selects the General Purpose mode for you by default. References: https://docs.aws.amazon.com/efs/latest/ug/performance.html https://aws.amazon.com/efs/",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "Amazon EC2 Auto Scaling needs to terminate an instance from Availability Zone (AZ) us-east-1a as it has the most number of instances amongst the Availability Zone (AZs) being used currently. There are 4 instances in the Availability Zone (AZ) us-east-1a like so: Instance A has the oldest launch template, Instance B has the oldest launch configuration, Instance C has the newest launch configuration and Instance D is closest to the next billing hour. Which of the following instances would be terminated per the default termination policy?",
      "options": [
        "A. Instance A",
        "B. Instance B",
        "C. Instance C",
        "D. Instance D"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Instance B Per the default termination policy, the first priority is given to any allocation strategy for On-Demand vs Spot instances. As no such information has been provided for the given use-case, so this criterion can be ignored. The next priority is to consider any instance with the oldest launch template unless there is an instance that uses a launch configuration. So this rules out Instance A. Next, you need to consider any instance which has the oldest launch configuration. This implies Instance B will be selected for termination and Instance C will also be ruled out as it has the newest launch configuration. Instance D, which is closest to the next billing hour, is not selected as this criterion is last in the order of priority. Please see this note for a deep-dive on the default termination policy: via - https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html Incorrect options: Instance A Instance C Instance D These three options contradict the explanation provided above, so these options are incorrect. Reference: https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html",
      "reference": "Source: Practice Test #2 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A financial services company wants to identify any sensitive data stored on its Amazon S3 buckets. The company also wants to monitor and protect all data stored on Amazon S3 against any malicious activity. As a solutions architect, which of the following solutions would you recommend to help address the given requirements?",
      "options": [
        "A. Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use Amazon Macie to identify any sensitive data stored on Amazon S3",
        "B. Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3 as well as to identify any sensitive data stored on Amazon S3",
        "C. Use Amazon Macie to monitor any malicious activity on data stored in Amazon S3 as well as to identify any sensitive data stored on Amazon S3",
        "D. Use Amazon Macie to monitor any malicious activity on data stored in Amazon S3. Use Amazon GuardDuty to identify any sensitive data stored on Amazon S3"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use Amazon Macie to identify any sensitive data stored on Amazon S3 Amazon GuardDuty offers threat detection that enables you to continuously monitor and protect your AWS accounts, workloads, and data stored in Amazon S3. GuardDuty analyzes continuous streams of meta-data generated from your account and network activity found in AWS CloudTrail Events, Amazon VPC Flow Logs, and DNS Logs. It also uses integrated threat intelligence such as known malicious IP addresses, anomaly detection, and machine learning to identify threats more accurately. How Amazon GuardDuty works: via - https://aws.amazon.com/guardduty/ Amazon Macie is a fully managed data security and data privacy service that uses machine learning and pattern matching to discover and protect your sensitive data on Amazon S3. Macie automatically detects a large and growing list of sensitive data types, including personally identifiable information (PII) such as names, addresses, and credit card numbers. It also gives you constant visibility of the data security and data privacy of your data stored in Amazon S3. How Amazon Macie works: via - https://aws.amazon.com/macie/ Incorrect options: Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3 as well as to identify any sensitive data stored on Amazon S3 Use Amazon Macie to monitor any malicious activity on data stored in Amazon S3 as well as to identify any sensitive data stored on Amazon S3 Use Amazon Macie to monitor any malicious activity on data stored in Amazon S3. Use Amazon GuardDuty to identify any sensitive data stored on Amazon S3 These three options contradict the explanation provided above, so these options are incorrect. References: https://aws.amazon.com/guardduty/ https://aws.amazon.com/macie/",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "The engineering team at an e-commerce company wants to migrate from Amazon Simple Queue Service (Amazon SQS) Standard queues to FIFO (First-In-First-Out) queues with batching. As a solutions architect, which of the following steps would you have in the migration checklist? (Select three)",
      "options": [
        "A. Delete the existing standard queue and recreate it as a FIFO (First-In-First-Out) queue",
        "B. Convert the existing standard queue into a FIFO (First-In-First-Out) queue",
        "C. Make sure that the name of the FIFO (First-In-First-Out) queue ends with the .fifo suffix",
        "D. Make sure that the name of the FIFO (First-In-First-Out) queue is the same as the standard queue",
        "E. Make sure that the throughput for the target FIFO (First-In-First-Out) queue does not exceed 3,000 messages per second",
        "F. Make sure that the throughput for the target FIFO (First-In-First-Out) queue does not exceed 300 messages per second"
      ],
      "correct_answer": "A",
      "explanation": "Correct options: Delete the existing standard queue and recreate it as a FIFO (First-In-First-Out) queue Make sure that the name of the FIFO (First-In-First-Out) queue ends with the .fifo suffix Make sure that the throughput for the target FIFO (First-In-First-Out) queue does not exceed 3,000 messages per second Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. Amazon SQS eliminates the complexity and overhead associated with managing and operating message oriented middleware, and empowers developers to focus on differentiating work. Using Amazon SQS, you can send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be available. Amazon SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent. By default, FIFO queues support up to 3,000 messages per second with batching, or up to 300 messages per second (300 send, receive, or delete operations per second) without batching. Therefore, using batching you can meet a throughput requirement of upto 3,000 messages per second. The name of a FIFO queue must end with the .fifo suffix. The suffix counts towards the 80-character queue name limit. To determine whether a queue is FIFO, you can check whether the queue name ends with the suffix. If you have an existing application that uses standard queues and you want to take advantage of the ordering or exactly-once processing features of FIFO queues, you need to configure the queue and your application correctly. You can't convert an existing standard queue into a FIFO queue. To make the move, you must either create a new FIFO queue for your application or delete your existing standard queue and recreate it as a FIFO queue. Incorrect options: Convert the existing standard queue into a FIFO (First-In-First-Out) queue Make sure that the name of the FIFO (First-In-First-Out) queue is the same as the standard queue - The name of a FIFO queue must end with the .fifo suffix. Make sure that the throughput for the target FIFO (First-In-First-Out) queue does not exceed 300 messages per second - By default, FIFO queues support up to 3,000 messages per second with batching. References: https://aws.amazon.com/sqs/faqs/ https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A global pharmaceutical company wants to move most of the on-premises data into Amazon S3, Amazon Elastic File System (Amazon EFS), and Amazon FSx for Windows File Server easily, quickly, and cost-effectively. As a solutions architect, which of the following solutions would you recommend as the BEST fit to automate and accelerate online data transfers to these AWS storage services?",
      "options": [
        "A. Use AWS DataSync to automate and accelerate online data transfers to the given AWS storage services",
        "B. Use AWS Snowball Edge Storage Optimized device to automate and accelerate online data transfers to the given AWS storage services",
        "C. Use AWS Transfer Family to automate and accelerate online data transfers to the given AWS storage services",
        "D. Use File Gateway to automate and accelerate online data transfers to the given AWS storage services"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Use AWS DataSync to automate and accelerate online data transfers to the given AWS storage services AWS DataSync is an online data transfer service that simplifies, automates, and accelerates copying large amounts of data to and from AWS storage services over the internet or AWS Direct Connect. AWS DataSync fully automates and accelerates moving large active datasets to AWS, up to 10 times faster than command-line tools. It is natively integrated with Amazon S3, Amazon EFS, Amazon FSx for Windows File Server, Amazon CloudWatch, and AWS CloudTrail, which provides seamless and secure access to your storage services, as well as detailed monitoring of the transfer. AWS DataSync uses a purpose-built network protocol and scale out architecture to transfer data. A single DataSync agent is capable of saturating a 10 Gbps network link. AWS DataSync fully automates the data transfer. It comes with retry and network resiliency mechanisms, network optimizations, built-in task scheduling, monitoring via the DataSync API and Console, and Amazon CloudWatch metrics, events, and logs that provide granular visibility into the transfer process. AWS DataSync performs data integrity verification both during the transfer and at the end of the transfer. How AWS DataSync Works: via - https://aws.amazon.com/datasync/ Incorrect options: Use AWS Snowball Edge Storage Optimized device to automate and accelerate online data transfers to the given AWS storage services - AWS Snowball Edge Storage Optimized is the optimal choice if you need to securely and quickly transfer dozens of terabytes to petabytes of data to AWS. It provides up to 80 TB of usable HDD storage, 40 vCPUs, 1 TB of SATA SSD storage, and up to 40 Gb network connectivity to address large scale data transfer and pre-processing use cases. As each Snowball Edge Storage Optimized device can handle 80TB of data, you can order 10 such devices to take care of the data transfer for all applications. The original Snowball devices were transitioned out of service and Snowball Edge Storage Optimized are now the primary devices used for data transfer. You may see the Snowball device on the exam, just remember that the original Snowball device had 80TB of storage space. AWS Snowball Edge is suitable for offline data transfers, for customers who are bandwidth constrained or transferring data from remote, disconnected, or austere environments. Therefore, it cannot support automated and accelerated online data transfers. Use AWS Transfer Family to automate and accelerate online data transfers to the given AWS storage services - The AWS Transfer Family provides fully managed support for file transfers directly into and out of Amazon S3 and Amazon EFS. Therefore, it cannot support migration into the other AWS storage services mentioned in the given use-case (Amazon FSx for Windows File Server). Use File Gateway to automate and accelerate online data transfers to the given AWS storage services - AWS Storage Gateway's file interface, or file gateway, offers you a seamless way to connect to the cloud to store application data files and backup images as durable objects on Amazon S3 cloud storage. File gateway offers SMB or NFS-based access to data in Amazon S3 with local caching. It can be used for on-premises applications, and for Amazon EC2-based applications that need file protocol access to S3 object storage. Therefore, it cannot support migration into the other AWS storage services mentioned in the given use-case (such as EFS and Amazon FSx for Windows File Server). References: https://aws.amazon.com/datasync/faqs/ https://aws.amazon.com/storagegateway/file/ https://aws.amazon.com/aws-transfer-family/",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A company has a license-based, expensive, legacy commercial database solution deployed at its on-premises data center. The company wants to migrate this database to a more efficient, open-source, and cost-effective option on AWS Cloud. The CTO at the company wants a solution that can handle complex database configurations such as secondary indexes, foreign keys, and stored procedures. As a solutions architect, which of the following AWS services should be combined to handle this use-case? (Select two)",
      "options": [
        "A. AWS Snowball Edge",
        "B. AWS Schema Conversion Tool (AWS SCT)",
        "C. AWS Database Migration Service (AWS DMS)",
        "D. AWS Glue",
        "E. Basic Schema Copy"
      ],
      "correct_answer": "B",
      "explanation": "Correct options: AWS Schema Conversion Tool (AWS SCT) AWS Database Migration Service (AWS DMS) AWS Database Migration Service helps you migrate databases to AWS quickly and securely. The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database. AWS Database Migration Service supports homogeneous migrations such as Oracle to Oracle, as well as heterogeneous migrations between different database platforms, such as Oracle or Microsoft SQL Server to Amazon Aurora. Given the use-case where the CTO at the company wants to move away from license-based, expensive, legacy commercial database solutions deployed at the on-premises data center to more efficient, open-source, and cost-effective options on AWS Cloud, this is an example of heterogeneous database migrations. For such a scenario, the source and target databases engines are different, like in the case of Oracle to Amazon Aurora, Oracle to PostgreSQL, or Microsoft SQL Server to MySQL migrations. In this case, the schema structure, data types, and database code of source and target databases can be quite different, requiring a schema and code transformation before the data migration starts. That makes heterogeneous migrations a two-step process. First use the AWS Schema Conversion Tool to convert the source schema and code to match that of the target database, and then use the AWS Database Migration Service to migrate data from the source database to the target database. All the required data type conversions will automatically be done by the AWS Database Migration Service during the migration. The source database can be located on your on-premises environment outside of AWS, running on an Amazon EC2 instance, or it can be an Amazon RDS database. The target can be a database in Amazon EC2 or Amazon RDS. Heterogeneous Database Migrations: via - https://aws.amazon.com/dms/ Incorrect options: AWS Snowball Edge - AWS Snowball Edge Storage Optimized is the optimal choice if you need to securely and quickly transfer dozens of terabytes to petabytes of data to AWS. It provides up to 80 TB of usable HDD storage, 40 vCPUs, 1 TB of SATA SSD storage, and up to 40 Gb network connectivity to address large scale data transfer and pre-processing use cases. As each Snowball Edge Storage Optimized device can handle 80TB of data, you can order 10 such devices to take care of the data transfer for all applications. The original Snowball devices were transitioned out of service and AWS Snowball Edge Storage Optimized are now the primary devices used for data transfer. You may see the Snowball device on the exam, just remember that the original Snowball device had 80TB of storage space. AWS Snowball Edge cannot be used for database migrations. AWS Glue - AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. AWS Glue job is meant to be used for batch ETL data processing. Therefore, it cannot be used for database migrations. Basic Schema Copy - To quickly migrate a database schema to your target instance you can rely on the Basic Schema Copy feature of AWS Database Migration Service. Basic Schema Copy will automatically create tables and primary keys in the target instance if the target does not already contain tables with the same names. Basic Schema Copy is great for doing a test migration, or when you are migrating databases heterogeneously e.g. Oracle to MySQL or SQL Server to Oracle. Basic Schema Copy will not migrate secondary indexes, foreign keys or stored procedures. When you need to use a more customizable schema migration process (e.g. when you are migrating your production database and need to move your stored procedures and secondary database objects), you must use the AWS Schema Conversion Tool. References: https://aws.amazon.com/dms/ https://aws.amazon.com/dms/faqs/ https://aws.amazon.com/dms/schema-conversion-tool/",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "The business analytics team at a company has been running ad-hoc queries on Oracle and PostgreSQL services on Amazon RDS to prepare daily reports for senior management. To facilitate the business analytics reporting, the engineering team now wants to continuously replicate this data and consolidate these databases into a petabyte-scale data warehouse by streaming data to Amazon Redshift. As a solutions architect, which of the following would you recommend as the MOST resource-efficient solution that requires the LEAST amount of development time without the need to manage the underlying infrastructure?",
      "options": [
        "A. Use AWS Database Migration Service (AWS DMS) to replicate the data from the databases into Amazon Redshift",
        "B. Use AWS Glue to replicate the data from the databases into Amazon Redshift",
        "C. Use AWS EMR to replicate the data from the databases into Amazon Redshift",
        "D. Use Amazon Kinesis Data Streams to replicate the data from the databases into Amazon Redshift"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Use AWS Database Migration Service (AWS DMS) to replicate the data from the databases into Amazon Redshift AWS Database Migration Service helps you migrate databases to AWS quickly and securely. The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database. With AWS Database Migration Service, you can continuously replicate your data with high availability and consolidate databases into a petabyte-scale data warehouse by streaming data to Amazon Redshift and Amazon S3. Continuous Data Replication via - https://aws.amazon.com/dms/ You can migrate data to Amazon Redshift databases using AWS Database Migration Service. Amazon Redshift is a fully managed, petabyte-scale data warehouse service in the cloud. With an Amazon Redshift database as a target, you can migrate data from all of the other supported source databases. The Amazon Redshift cluster must be in the same AWS account and the same AWS Region as the replication instance. During a database migration to Amazon Redshift, AWS DMS first moves data to an Amazon S3 bucket. When the files reside in an Amazon S3 bucket, AWS DMS then transfers them to the proper tables in the Amazon Redshift data warehouse. AWS DMS creates the S3 bucket in the same AWS Region as the Amazon Redshift database. The AWS DMS replication instance must be located in that same region. Incorrect options: Use AWS Glue to replicate the data from the databases into Amazon Redshift - AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. AWS Glue job is meant to be used for batch ETL data processing. Using AWS Glue involves significant development efforts to write custom migration scripts to copy the database data into Redshift. Use AWS EMR to replicate the data from the databases into Amazon Redshift - Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. With EMR you can run Petabyte-scale analysis at less than half of the cost of traditional on-premises solutions and over 3x faster than standard Apache Spark. For short-running jobs, you can spin up and spin down clusters and pay per second for the instances used. For long-running workloads, you can create highly available clusters that automatically scale to meet demand. Amazon EMR uses Hadoop, an open-source framework, to distribute your data and processing across a resizable cluster of Amazon EC2 instances. Using EMR involves significant infrastructure management efforts to set up and maintain the EMR cluster. Additionally this option involves a major development effort to write custom migration jobs to copy the database data into Redshift. Use Amazon Kinesis Data Streams to replicate the data from the databases into Amazon Redshift - Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. However, the user is expected to manually provision an appropriate number of shards to process the expected volume of the incoming data stream. The throughput of an Amazon Kinesis data stream is designed to scale without limits via increasing the number of shards within a data stream. Therefore Kinesis Data Streams is not the right fit for this use-case. References: https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Redshift.html https://aws.amazon.com/dms/",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "Which of the following AWS services provides a highly available and fault-tolerant solution to capture the clickstream events from the source and then provide a concurrent feed of the data stream to the downstream applications?",
      "options": [
        "A. Amazon Kinesis Data Streams",
        "B. Amazon Kinesis Data Firehose",
        "C. Amazon Kinesis Data Analytics",
        "D. Amazon Simple Queue Service (Amazon SQS)"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Amazon Kinesis Data Streams Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more. Amazon Kinesis Data Streams enables real-time processing of streaming big data. It provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications. The Amazon Kinesis Client Library (KCL) delivers all records for a given partition key to the same record processor, making it easier to build multiple applications reading from the same Amazon Kinesis data stream (for example, to perform counting, aggregation, and filtering). Amazon Kinesis Data Streams is recommended when you need the ability for multiple applications to consume the same stream concurrently. For example, you have one application that updates a real-time dashboard and another application that archives data to Amazon Redshift. You want both applications to consume data from the same stream concurrently and independently. KDS provides the ability for multiple applications to consume the same stream concurrently via - https://aws.amazon.com/kinesis/data-streams/faqs/ Incorrect options: Amazon Kinesis Data Firehose - Amazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards you’re already using today. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security. As Kinesis Data Firehose is used to load streaming data into data stores, therefore this option is incorrect. Amazon Kinesis Data Analytics - Amazon Kinesis Data Analytics is the easiest way to analyze streaming data in real-time. You can quickly build SQL queries and sophisticated Java applications using built-in templates and operators for common processing functions to organize, transform, aggregate, and analyze data at any scale. Kinesis Data Analytics enables you to easily and quickly build queries and sophisticated streaming applications in three simple steps: setup your streaming data sources, write your queries or streaming applications and set up your destination for processed data. As Kinesis Data Analytics is used to build SQL queries and sophisticated Java applications, therefore this option is incorrect. Amazon Simple Queue Service (Amazon SQS) - Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. Amazon SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent. For SQS, you cannot have the same message being consumed by multiple consumers at the same time, therefore this option is incorrect. Exam alert: Please remember that Amazon Kinesis Data Firehose is used to load streaming data into data stores (Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk) whereas Kinesis Data Streams provides support for real-time processing of streaming data. It provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple downstream Amazon Kinesis Applications. References: https://aws.amazon.com/kinesis/data-streams/faqs/ https://aws.amazon.com/kinesis/data-firehose/faqs/ https://aws.amazon.com/kinesis/data-analytics/faqs/",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "The engineering team at a company wants to use Amazon Simple Queue Service (Amazon SQS) to decouple components of the underlying application architecture. However, the team is concerned about the VPC-bound components accessing Amazon Simple Queue Service (Amazon SQS) over the public internet. As a solutions architect, which of the following solutions would you recommend to address this use-case?",
      "options": [
        "A. Use VPC endpoint to access Amazon SQS",
        "B. Use Internet Gateway to access Amazon SQS",
        "C. Use Network Address Translation (NAT) instance to access Amazon SQS",
        "D. Use VPN connection to access Amazon SQS"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Use VPC endpoint to access Amazon SQS AWS customers can access Amazon Simple Queue Service (Amazon SQS) from their Amazon Virtual Private Cloud (Amazon VPC) using VPC endpoints, without using public IPs, and without needing to traverse the public internet. VPC endpoints for Amazon SQS are powered by AWS PrivateLink, a highly available, scalable technology that enables you to privately connect your VPC to supported AWS services. Amazon VPC endpoints are easy to configure. They also provide reliable connectivity to Amazon SQS without requiring an internet gateway, Network Address Translation (NAT) instance, VPN connection, or AWS Direct Connect connection. With VPC endpoints, the data between your Amazon VPC and Amazon SQS queue is transferred within the Amazon network, helping protect your instances from internet traffic. AWS PrivateLink simplifies the security of data shared with cloud-based applications by eliminating the exposure of data to the public Internet. AWS PrivateLink provides private connectivity between VPCs, AWS services, and on-premises applications, securely on the Amazon network. AWS PrivateLink makes it easy to connect services across different accounts and VPCs to significantly simplify the network architecture. Incorrect options: Use Internet Gateway to access Amazon SQS - An internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between instances in your VPC and the internet. It, therefore, imposes no availability risks or bandwidth constraints on your network traffic. This option is ruled out as the team does not want to use the public internet to access Amazon SQS. Use VPN connection to access Amazon SQS - AWS Site-to-Site VPN (aka VPN Connection) enables you to securely connect your on-premises network or branch office site to your Amazon Virtual Private Cloud (Amazon VPC). You can securely extend your data center or branch office network to the cloud with an AWS Site-to-Site VPN connection. A VPC VPN Connection utilizes IPSec to establish encrypted network connectivity between your intranet and Amazon VPC over the Internet. VPN Connections can be configured in minutes and are a good solution if you have an immediate need, have low to modest bandwidth requirements, and can tolerate the inherent variability in Internet-based connectivity. As the existing infrastructure is within AWS Cloud, therefore a VPN connection is not required. Use Network Address Translation (NAT) instance to access Amazon SQS - You can use a network address translation (NAT) instance in a public subnet in your VPC to enable instances in the private subnet to initiate outbound IPv4 traffic to the Internet or other AWS services, but prevent the instances from receiving inbound traffic initiated by someone on the Internet. Amazon provides Amazon Linux AMIs that are configured to run as NAT instances. These AMIs include the string amzn-ami-vpc-nat in their names, so you can search for them in the Amazon EC2 console. This option is ruled out because NAT instances are used to provide internet access to any instances in a private subnet. References: https://aws.amazon.com/privatelink/ https://aws.amazon.com/about-aws/whats-new/2018/12/amazon-sqs-vpc-endpoints-aws-privatelink/",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A media company wants a low-latency way to distribute live sports results which are delivered via a proprietary application using UDP protocol. As a solutions architect, which of the following solutions would you recommend such that it offers the BEST performance for this use case?",
      "options": [
        "A. Use Elastic Load Balancing (ELB) to provide a low latency way to distribute live sports results",
        "B. Use Amazon CloudFront to provide a low latency way to distribute live sports results",
        "C. Use AWS Global Accelerator to provide a low latency way to distribute live sports results",
        "D. Use Auto Scaling group to provide a low latency way to distribute live sports results"
      ],
      "correct_answer": "C",
      "explanation": "Correct option: Use AWS Global Accelerator to provide a low latency way to distribute live sports results AWS Global Accelerator is a networking service that helps you improve the availability and performance of the applications that you offer to your global users. AWS Global Accelerator is easy to set up, configure, and manage. It provides static IP addresses that provide a fixed entry point to your applications and eliminate the complexity of managing specific IP addresses for different AWS Regions and Availability Zones (AZs). AWS Global Accelerator always routes user traffic to the optimal endpoint based on performance, reacting instantly to changes in application health, your user’s location, and policies that you configure. Global Accelerator is a good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or Voice over IP. Therefore, this option is correct. How AWS Global Accelerator Works: via - https://aws.amazon.com/global-accelerator/ Incorrect options: Use Amazon CloudFront to provide a low latency way to distribute live sports results - Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment. Amazon CloudFront points of presence (POPs) (edge locations) make sure that popular content can be served quickly to your viewers. Amazon CloudFront also has regional edge caches that bring more of your content closer to your viewers, even when the content is not popular enough to stay at a POP, to help improve performance for that content. Regional edge caches help with all types of content, particularly content that tends to become less popular over time. Examples include user-generated content, such as video, photos, or artwork; e-commerce assets such as product photos and videos; and news and event-related content that might suddenly find new popularity. CloudFront supports HTTP/RTMP protocol based requests, therefore this option is incorrect. Use Elastic Load Balancing (ELB) to provide a low latency way to distribute live sports results - Elastic Load Balancing (ELB) automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and AWS Lambda functions. It can handle the varying load of your application traffic in a single Availability Zone or across multiple Availability Zones. Elastic Load Balancer cannot help with decreasing latency of incoming traffic from the source. Use Auto Scaling group to provide a low latency way to distribute live sports results - Amazon EC2 Auto Scaling helps you ensure that you have the correct number of Amazon EC2 instances available to handle the load for your application. You create collections of Amazon EC2 instances, called Auto Scaling groups. You can specify the minimum number of instances in each Auto Scaling group, and Amazon EC2 Auto Scaling ensures that your group never goes below this size. Auto Scaling group cannot help with decreasing latency of incoming traffic from the source. Exam Alert: Please note the differences between the capabilities of AWS Global Accelerator and Amazon CloudFront - AWS Global Accelerator and Amazon CloudFront are separate services that use the AWS global network and its edge locations around the world. Amazon CloudFront improves performance for both cacheable content (such as images and videos) and dynamic content (such as API acceleration and dynamic site delivery). AWS Global Accelerator improves performance for a wide range of applications over TCP or UDP by proxying packets at the edge to applications running in one or more AWS Regions. AWS Global Accelerator is a good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or Voice over IP, as well as for HTTP use cases that specifically require static IP addresses or deterministic, fast regional failover. Both services integrate with AWS Shield for DDoS protection. References: https://aws.amazon.com/global-accelerator/ https://aws.amazon.com/cloudfront/faqs/",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "The DevOps team at an IT company has created a custom VPC (V1) and attached an Internet Gateway (I1) to the VPC. The team has also created a subnet (S1) in this custom VPC and added a route to this subnet's route table (R1) that directs internet-bound traffic to the Internet Gateway. Now the team launches an Amazon EC2 instance (E1) in the subnet S1 and assigns a public IPv4 address to this instance. Next the team also launches a Network Address Translation (NAT) instance (N1) in the subnet S1. Under the given infrastructure setup, which of the following entities is doing the Network Address Translation for the Amazon EC2 instance E1?",
      "options": [
        "A. Network Address Translation (NAT) instance (N1)",
        "B. Internet Gateway (I1)",
        "C. Subnet (S1)",
        "D. Route Table (R1)"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Internet Gateway (I1) An Internet Gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between your VPC and the internet. An Internet Gateway serves two purposes: to provide a target in your VPC route tables for internet-routable traffic and to perform network address translation (NAT) for instances that have been assigned public IPv4 addresses. Therefore, for instance E1, the Network Address Translation is done by Internet Gateway I1. Additionally, an Internet Gateway supports IPv4 and IPv6 traffic. It does not cause availability risks or bandwidth constraints on your network traffic. To enable access to or from the internet for instances in a subnet in a VPC, you must do the following: Attach an Internet gateway to your VPC. Add a route to your subnet's route table that directs internet-bound traffic to the internet gateway. If a subnet is associated with a route table that has a route to an internet gateway, it's known as a public subnet. If a subnet is associated with a route table that does not have a route to an internet gateway, it's known as a private subnet. Ensure that instances in your subnet have a globally unique IP address (public IPv4 address, Elastic IP address, or IPv6 address). Ensure that your network access control lists and security group rules allow the relevant traffic to flow to and from your instance. Internet Gateway Overview: via - https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html Incorrect options: Network Address Translation (NAT) instance (N1) - You can use a network address translation (NAT) instance in a public subnet in your VPC to enable instances in the private subnet to initiate outbound IPv4 traffic to the Internet or other AWS services, but prevent the instances from receiving inbound traffic initiated by someone on the Internet. As the instance E1 is in a public subnet, therefore this option is not correct. Subnet (S1) Route Table (R1) A virtual private cloud (VPC) is a virtual network dedicated to your AWS account. A subnet is a range of IP addresses in your VPC. A route table contains a set of rules, called routes, that are used to determine where network traffic is directed. Therefore neither Subnet nor Route Table can be used for Network Address Translation. References: https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html https://docs.aws.amazon.com/vpc/latest/userguide/VPC_NAT_Instance.html",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A company has hired you as an AWS Certified Solutions Architect – Associate to help with redesigning a real-time data processor. The company wants to build custom applications that process and analyze the streaming data for its specialized needs. Which solution will you recommend to address this use-case?",
      "options": [
        "A. Use Amazon Simple Queue Service (Amazon SQS) to process the data streams as well as decouple the producers and consumers for the real-time data processor",
        "B. Use Amazon Simple Notification Service (Amazon SNS) to process the data streams as well as decouple the producers and consumers for the real-time data processor",
        "C. Use Amazon Kinesis Data Streams to process the data streams as well as decouple the producers and consumers for the real-time data processor",
        "D. Use Amazon Kinesis Data Firehose to process the data streams as well as decouple the producers and consumers for the real-time data processor"
      ],
      "correct_answer": "C",
      "explanation": "Correct option: Use Amazon Kinesis Data Streams to process the data streams as well as decouple the producers and consumers for the real-time data processor Amazon Kinesis Data Streams is useful for rapidly moving data off data producers and then continuously processing the data, be it to transform the data before emitting to a data store, run real-time metrics and analytics, or derive more complex data streams for further processing. Kinesis data streams can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. Kinesis Data Streams Overview: via - https://aws.amazon.com/kinesis/data-streams/ Incorrect options: Use Amazon Simple Notification Service (Amazon SNS) to process the data streams as well as decouple the producers and consumers for the real-time data processor - Amazon Simple Notification Service (Amazon SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. SNS cannot be used to decouple the producers and consumers for the real-time data processor as described in the given use-case. Use Amazon Simple Queue Service (Amazon SQS) to process the data streams as well as decouple the producers and consumers for the real-time data processor - Amazon Simple Queue Service (Amazon SQS) offers a secure, durable, and available hosted queue that lets you integrate and decouple distributed software systems and components. SQS cannot be used to decouple the producers and consumers for the real-time data processor as described in the given use-case. Use Amazon Kinesis Data Firehose to process the data streams as well as decouple the producers and consumers for the real-time data processor - Amazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. Kinesis Firehose cannot be used to process and analyze the streaming data in custom applications. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics. Amazon Kinesis Data Firehose Overview via - https://aws.amazon.com/kinesis/data-firehose/ References: https://aws.amazon.com/kinesis/data-streams/ https://aws.amazon.com/kinesis/data-firehose/",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A financial services company is looking to move its on-premises IT infrastructure to AWS Cloud. The company has multiple long-term server bound licenses across the application stack and the CTO wants to continue to utilize those licenses while moving to AWS. As a solutions architect, which of the following would you recommend as the MOST cost-effective solution?",
      "options": [
        "A. Use Amazon EC2 on-demand instances",
        "B. Use Amazon EC2 reserved instances (RI)",
        "C. Use Amazon EC2 dedicated instances",
        "D. Use Amazon EC2 dedicated hosts"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: Use Amazon EC2 dedicated hosts You can use Dedicated Hosts to launch Amazon EC2 instances on physical servers that are dedicated for your use. Dedicated Hosts give you additional visibility and control over how instances are placed on a physical server, and you can reliably use the same physical server over time. As a result, Dedicated Hosts enable you to use your existing server-bound software licenses like Windows Server and address corporate compliance and regulatory requirements. Incorrect options: Use Amazon EC2 dedicated instances - Dedicated instances are Amazon EC2 instances that run in a VPC on hardware that's dedicated to a single customer. Your dedicated instances are physically isolated at the host hardware level from instances that belong to other AWS accounts. Dedicated instances may share hardware with other instances from the same AWS account that are not dedicated instances. Dedicated instances cannot be used for existing server-bound software licenses. Use Amazon EC2 on-demand instances Use Amazon EC2 reserved instances (RI) Amazon EC2 presents a virtual computing environment, allowing you to use web service interfaces to launch instances with a variety of operating systems, load them with your custom application environment, manage your network’s access permissions, and run your image using as many or few systems as you desire. Amazon EC2 provides the following purchasing options to enable you to optimize your costs based on your needs: On-Demand Instances – Pay, by the second, for the instances that you launch. Reserved Instances (RI) – Reduce your Amazon EC2 costs by making a commitment to a consistent instance configuration, including instance type and Region, for a term of 1 or 3 years. Neither on-demand instances nor reserved instances can be used for existing server-bound software licenses. References: https://aws.amazon.com/ec2/dedicated-hosts/ https://aws.amazon.com/ec2/dedicated-hosts/faqs/ https://aws.amazon.com/ec2/pricing/dedicated-instances/",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "The DevOps team at an IT company is provisioning a two-tier application in a VPC with a public subnet and a private subnet. The team wants to use either a Network Address Translation (NAT) instance or a Network Address Translation (NAT) gateway in the public subnet to enable instances in the private subnet to initiate outbound IPv4 traffic to the internet but needs some technical assistance in terms of the configuration options available for the Network Address Translation (NAT) instance and the Network Address Translation (NAT) gateway. As a solutions architect, which of the following options would you identify as CORRECT? (Select three)",
      "options": [
        "A. NAT gateway supports port forwarding",
        "B. Security Groups can be associated with a NAT gateway",
        "C. NAT gateway can be used as a bastion server",
        "D. NAT instance can be used as a bastion server",
        "E. Security Groups can be associated with a NAT instance",
        "F. NAT instance supports port forwarding"
      ],
      "correct_answer": "D",
      "explanation": "Correct options: NAT instance can be used as a bastion server Security Groups can be associated with a NAT instance NAT instance supports port forwarding A NAT instance or a NAT Gateway can be used in a public subnet in your VPC to enable instances in the private subnet to initiate outbound IPv4 traffic to the Internet. How NAT Gateway works: via - https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html How NAT Instance works: via - https://docs.aws.amazon.com/vpc/latest/userguide/VPC_NAT_Instance.html Please see this high-level summary of the differences between NAT instances and NAT gateways relevant to the options described in the question: via - https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html Incorrect options: NAT gateway supports port forwarding Security Groups can be associated with a NAT gateway NAT gateway can be used as a bastion server These three options contradict the details provided in the explanation above, so these options are incorrect. Reference: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A leading online gaming company is migrating its flagship application to AWS Cloud for delivering its online games to users across the world. The company would like to use a Network Load Balancer to handle millions of requests per second. The engineering team has provisioned multiple instances in a public subnet and specified these instance IDs as the targets for the NLB. As a solutions architect, can you help the engineering team understand the correct routing mechanism for these target instances?",
      "options": [
        "A. Traffic is routed to instances using the primary private IP address specified in the primary network interface for the instance",
        "B. Traffic is routed to instances using the primary public IP address specified in the primary network interface for the instance",
        "C. Traffic is routed to instances using the primary elastic IP address specified in the primary network interface for the instance",
        "D. Traffic is routed to instances using the instance ID specified in the primary network interface for the instance"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Traffic is routed to instances using the primary private IP address specified in the primary network interface for the instance A Network Load Balancer functions at the fourth layer of the Open Systems Interconnection (OSI) model. It can handle millions of requests per second. After the load balancer receives a connection request, it selects a target from the target group for the default rule. It attempts to open a TCP connection to the selected target on the port specified in the listener configuration. Request Routing and IP Addresses - If you specify targets using an instance ID, traffic is routed to instances using the primary private IP address specified in the primary network interface for the instance. The load balancer rewrites the destination IP address from the data packet before forwarding it to the target instance. If you specify targets using IP addresses, you can route traffic to an instance using any private IP address from one or more network interfaces. This enables multiple applications on an instance to use the same port. Note that each network interface can have its security group. The load balancer rewrites the destination IP address before forwarding it to the target. Incorrect options: Traffic is routed to instances using the primary public IP address specified in the primary network interface for the instance - If you specify targets using an instance ID, traffic is routed to instances using the primary private IP address specified in the primary network interface for the instance. So public IP address cannot be used to route the traffic to the instance. Traffic is routed to instances using the primary elastic IP address specified in the primary network interface for the instance - If you specify targets using an instance ID, traffic is routed to instances using the primary private IP address specified in the primary network interface for the instance. So elastic IP address cannot be used to route the traffic to the instance. Traffic is routed to instances using the instance ID specified in the primary network interface for the instance - You cannot use instance ID to route traffic to the instance. This option is just added as a distractor. References: https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-target-groups.html",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An e-commerce company is using Elastic Load Balancing (ELB) for its fleet of Amazon EC2 instances spread across two Availability Zones (AZs), with one instance as a target in Availability Zone A and four instances as targets in Availability Zone B. The company is doing benchmarking for server performance when cross-zone load balancing is enabled compared to the case when cross-zone load balancing is disabled. As a solutions architect, which of the following traffic distribution outcomes would you identify as correct?",
      "options": [
        "A. With cross-zone load balancing enabled, one instance in Availability Zone A receives 20% traffic and four instances in Availability Zone B receive 20% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives 50% traffic and four instances in Availability Zone B receive 12.5% traffic each",
        "B. With cross-zone load balancing enabled, one instance in Availability Zone A receives 50% traffic and four instances in Availability Zone B receive 12.5% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives 20% traffic and four instances in Availability Zone B receive 20% traffic each",
        "C. With cross-zone load balancing enabled, one instance in Availability Zone A receives no traffic and four instances in Availability Zone B receive 25% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives 50% traffic and four instances in Availability Zone B receive 12.5% traffic each",
        "D. With cross-zone load balancing enabled, one instance in Availability Zone A receives 20% traffic and four instances in Availability Zone B receive 20% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives no traffic and four instances in Availability Zone B receive 25% traffic each"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: With cross-zone load balancing enabled, one instance in Availability Zone A receives 20% traffic and four instances in Availability Zone B receive 20% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives 50% traffic and four instances in Availability Zone B receive 12.5% traffic each The nodes for your load balancer distribute requests from clients to registered targets. When cross-zone load balancing is enabled, each load balancer node distributes traffic across the registered targets in all enabled Availability Zones. Therefore, one instance in Availability Zone A receives 20% traffic and four instances in Availability Zone B receive 20% traffic each. When cross-zone load balancing is disabled, each load balancer node distributes traffic only across the registered targets in its Availability Zone. Therefore, one instance in Availability Zone A receives 50% traffic and four instances in Availability Zone B receive 12.5% traffic each. Consider the following diagrams (the scenario illustrated in the diagrams involves 10 target instances split across 2 AZs) to understand the effect of cross-zone load balancing. If cross-zone load balancing is enabled, each of the 10 targets receives 10% of the traffic. This is because each load balancer node can route its 50% of the client traffic to all 10 targets. via - https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html If cross-zone load balancing is disabled: Each of the two targets in Availability Zone A receives 25% of the traffic. Each of the eight targets in Availability Zone B receives 6.25% of the traffic. This is because each load balancer node can route its 50% of the client traffic only to targets in its Availability Zone via - https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html Incorrect options: With cross-zone load balancing enabled, one instance in Availability Zone A receives 50% traffic and four instances in Availability Zone B receive 12.5% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives 20% traffic and four instances in Availability Zone B receive 20% traffic each With cross-zone load balancing enabled, one instance in Availability Zone A receives no traffic and four instances in Availability Zone B receive 25% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives 50% traffic and four instances in Availability Zone B receive 12.5% traffic each With cross-zone load balancing enabled, one instance in Availability Zone A receives 20% traffic and four instances in Availability Zone B receive 20% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives no traffic and four instances in Availability Zone B receive 25% traffic each These three options contradict the details provided in the explanation above, so these options are incorrect. Reference: https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A healthcare company has deployed its web application on Amazon Elastic Container Service (Amazon ECS) container instances running behind an Application Load Balancer. The website slows down when the traffic spikes and the website availability is also reduced. The development team has configured Amazon CloudWatch alarms to receive notifications whenever there is an availability constraint so the team can scale out resources. The company wants an automated solution to respond to such events. Which of the following addresses the given use case?",
      "options": [
        "A. Configure AWS Auto Scaling to scale out the Amazon ECS cluster when the ECS service's CPU utilization rises above a threshold",
        "B. Configure AWS Auto Scaling to scale out the Amazon ECS cluster when the Application Load Balancer's target group's CPU utilization rises above a threshold",
        "C. Configure AWS Auto Scaling to scale out the Amazon ECS cluster when the Application Load Balancer's CPU utilization rises above a threshold",
        "D. Configure AWS Auto Scaling to scale out the Amazon ECS cluster when the CloudWatch alarm's CPU utilization rises above a threshold"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Configure AWS Auto Scaling to scale out the Amazon ECS cluster when the ECS service's CPU utilization rises above a threshold You use the Amazon ECS first-run wizard to create a cluster and a service that runs behind an Elastic Load Balancing load balancer. Then you can configure a target tracking scaling policy that scales your service automatically based on the current application load as measured by the service's CPU utilization (from the ECS, ClusterName, and ServiceName category in CloudWatch). When the average CPU utilization of your service rises above 75% (meaning that more than 75% of the CPU that is reserved for the service is being used), a scale out alarm triggers Service Auto Scaling to add another task to your service to help out with the increased load. Conversely, when the average CPU utilization of your service drops below the target utilization for a sustained period, a scale-in alarm triggers a decrease in the service's desired count to free up those cluster resources for other tasks and services. via - https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-configure-auto-scaling.html Incorrect options: Configure AWS Auto Scaling to scale out the Amazon ECS cluster when the Application Load Balancer's target group's CPU utilization rises above a threshold Configure AWS Auto Scaling to scale out the Amazon ECS cluster when the Application Load Balancer's CPU utilization rises above a threshold Configure AWS Auto Scaling to scale out the Amazon ECS cluster when the CloudWatch alarm's CPU utilization rises above a threshold These three options contradict the explanation provided above, so these options are incorrect. References: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-autoscaling-targettracking.html https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-configure-auto-scaling.html",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An IT company is looking to move its on-premises infrastructure to AWS Cloud. The company has a portfolio of applications with a few of them using server bound licenses that are valid for the next year. To utilize the licenses, the CTO wants to use dedicated hosts for a one year term and then migrate the given instances to default tenancy thereafter. As a solutions architect, which of the following options would you identify as CORRECT for changing the tenancy of an instance after you have launched it? (Select two)",
      "options": [
        "A. You can change the tenancy of an instance from dedicated to host",
        "B. You can change the tenancy of an instance from host to dedicated",
        "C. You can change the tenancy of an instance from default to dedicated",
        "D. You can change the tenancy of an instance from dedicated to default",
        "E. You can change the tenancy of an instance from default to host"
      ],
      "correct_answer": "A",
      "explanation": "Correct options: You can change the tenancy of an instance from dedicated to host You can change the tenancy of an instance from host to dedicated Each Amazon EC2 instance that you launch into a VPC has a tenancy attribute. This attribute has the following values. via - https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-monitoring.html By default, Amazon EC2 instances run on a shared-tenancy basis. Dedicated Instances are Amazon EC2 instances that run in a virtual private cloud (VPC) on hardware that's dedicated to a single customer. Dedicated Instances that belong to different AWS accounts are physically isolated at the hardware level. However, Dedicated Instances may share hardware with other instances from the same AWS account that is not Dedicated Instances. A Dedicated Host is also a physical server that's dedicated to your use. With a Dedicated Host, you have visibility and control over how instances are placed on the server. Incorrect options: You can change the tenancy of an instance from default to dedicated - You can only change the tenancy of an instance from dedicated to host, or from host to dedicated after you've launched it. Therefore, this option is incorrect. You can change the tenancy of an instance from dedicated to default - You can only change the tenancy of an instance from dedicated to host, or from host to dedicated after you've launched it. Therefore, this option is incorrect. You can change the tenancy of an instance from default to host - You can only change the tenancy of an instance from dedicated to host, or from host to dedicated after you've launched it. Therefore, this option is incorrect. Reference: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-instance.html",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A retail company uses AWS Cloud to manage its IT infrastructure. The company has set up AWS Organizations to manage several departments running their AWS accounts and using resources such as Amazon EC2 instances and Amazon RDS databases. The company wants to provide shared and centrally-managed VPCs to all departments using applications that need a high degree of interconnectivity. As a solutions architect, which of the following options would you choose to facilitate this use-case?",
      "options": [
        "A. Use VPC sharing to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations",
        "B. Use VPC sharing to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations",
        "C. Use VPC peering to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations",
        "D. Use VPC peering to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Use VPC sharing to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations VPC sharing (part of Resource Access Manager) allows multiple AWS accounts to create their application resources such as Amazon EC2 instances, Amazon RDS databases, Amazon Redshift clusters, and AWS Lambda functions, into shared and centrally-managed Amazon Virtual Private Clouds (VPCs). To set this up, the account that owns the VPC (owner) shares one or more subnets with other accounts (participants) that belong to the same organization from AWS Organizations. After a subnet is shared, the participants can view, create, modify, and delete their application resources in the subnets shared with them. Participants cannot view, modify, or delete resources that belong to other participants or the VPC owner. You can share Amazon VPCs to leverage the implicit routing within a VPC for applications that require a high degree of interconnectivity and are within the same trust boundaries. This reduces the number of VPCs that you create and manage while using separate accounts for billing and access control. Incorrect options: Use VPC sharing to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations - Using VPC sharing, an account that owns the VPC (owner) shares one or more subnets with other accounts (participants) that belong to the same organization from AWS Organizations. The owner account cannot share the VPC itself. Therefore this option is incorrect. Use VPC peering to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations - A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses. Instances in either VPC can communicate with each other as if they are within the same network. VPC peering does not facilitate centrally managed VPCs. Therefore this option is incorrect. Use VPC peering to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations - A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses. Instances in either VPC can communicate with each other as if they are within the same network. VPC peering does not facilitate centrally managed VPCs. Moreover, an AWS owner account cannot share the VPC itself with another AWS account. Therefore this option is incorrect. References: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-sharing.html https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A retail organization is moving some of its on-premises data to AWS Cloud. The DevOps team at the organization has set up an AWS Managed IPSec VPN Connection between their remote on-premises network and their Amazon VPC over the internet. Which of the following represents the correct configuration for the IPSec VPN Connection?",
      "options": [
        "A. Create a virtual private gateway (VGW) on the on-premises side of the VPN and a Customer Gateway on the AWS side of the VPN",
        "B. Create a Customer Gateway on both the AWS side of the VPN as well as the on-premises side of the VPN",
        "C. Create a virtual private gateway (VGW) on the AWS side of the VPN and a Customer Gateway on the on-premises side of the VPN",
        "D. Create a virtual private gateway (VGW) on both the AWS side of the VPN as well as the on-premises side of the VPN"
      ],
      "correct_answer": "C",
      "explanation": "Correct option: Create a virtual private gateway (VGW) on the AWS side of the VPN and a Customer Gateway on the on-premises side of the VPN Amazon VPC provides the facility to create an IPsec VPN connection (also known as AWS site-to-site VPN) between remote customer networks and their Amazon VPC over the internet. The following are the key concepts for a site-to-site VPN: Virtual private gateway: A virtual private gateway (VGW), also known as a VPN Gateway is the endpoint on the AWS VPC side of your VPN connection. VPN connection: A secure connection between your on-premises equipment and your VPCs. VPN tunnel: An encrypted link where data can pass from the customer network to or from AWS. Customer Gateway: An AWS resource that provides information to AWS about your Customer Gateway device. Customer Gateway device: A physical device or software application on the customer side of the Site-to-Site VPN connection. AWS Managed IPSec VPN via - https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-managed-vpn-network-to-amazon.html Incorrect options: Create a virtual private gateway (VGW) on the on-premises side of the VPN and a Customer Gateway on the AWS side of the VPN - You need to create a virtual private gateway (VGW) on the AWS side of the VPN and a Customer Gateway on the on-premises side of the VPN. Therefore, this option is wrong. Create a Customer Gateway on both the AWS side of the VPN as well as the on-premises side of the VPN - You need to create a virtual private gateway (VGW) on the AWS side of the VPN and a Customer Gateway on the on-premises side of the VPN. Therefore, this option is wrong. Create a virtual private gateway (VGW) on both the AWS side of the VPN as well as the on-premises side of the VPN - You need to create a virtual private gateway (VGW) on the AWS side of the VPN and a Customer Gateway on the on-premises side of the VPN. Therefore, this option is wrong. References: https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-managed-vpn-network-to-amazon.html https://docs.aws.amazon.com/vpn/latest/s2svpn/VPC_VPN.html",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A startup has created a new web application for users to complete a risk assessment survey for COVID-19 symptoms via a self-administered questionnaire. The startup has purchased the domain covid19survey.com using Amazon Route 53. The web development team would like to create Amazon Route 53 record so that all traffic for covid19survey.com is routed to www.covid19survey.com. As a solutions architect, which of the following is the MOST cost-effective solution that you would recommend to the web development team?",
      "options": [
        "A. Create a CNAME record for covid19survey.com that routes traffic to www.covid19survey.com",
        "B. Create an alias record for covid19survey.com that routes traffic to www.covid19survey.com",
        "C. Create an MX record for covid19survey.com that routes traffic to www.covid19survey.com",
        "D. Create an NS record for covid19survey.com that routes traffic to www.covid19survey.com"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Create an alias record for covid19survey.com that routes traffic to www.covid19survey.com Alias records provide Amazon Route 53–specific extension to DNS functionality. Alias records let you route traffic to selected AWS resources, such as Amazon CloudFront distributions and Amazon S3 buckets. You can create an alias record at the top node of a DNS namespace, also known as the zone apex, however, you cannot create a CNAME record for the top node of the DNS namespace. So, if you register the DNS name covid19survey.com, the zone apex is covid19survey.com. You can't create a CNAME record for covid19survey.com, but you can create an alias record for covid19survey.com that routes traffic to www.covid19survey.com. Exam Alert: You should also note that Amazon Route 53 doesn't charge for alias queries to AWS resources but Route 53 does charge for CNAME queries. Additionally, an alias record can only redirect queries to selected AWS resources such as Amazon S3 buckets, Amazon CloudFront distributions, and another record in the same Amazon Route 53 hosted zone; however a CNAME record can redirect DNS queries to any DNS record. So, you can create a CNAME record that redirects queries from app.covid19survey.com to app.covid19survey.net. Incorrect options: Create a CNAME record for covid19survey.com that routes traffic to www.covid19survey.com - You cannot create a CNAME record for the top node of the DNS namespace, so this option is incorrect. Create an MX record for covid19survey.com that routes traffic to www.covid19survey.com - An MX record specifies the names of your mail servers and, if you have two or more mail servers, the priority order. It cannot be used to create Amazon Route 53 record to route traffic for the top node of the DNS namespace, so this option is incorrect. Create an NS record for covid19survey.com that routes traffic to www.covid19survey.com - An NS record identifies the name servers for the hosted zone. It cannot be used to create Amazon Route 53 record to route traffic for the top node of the DNS namespace, so this option is incorrect. References: https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/ResourceRecordTypes.html",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A leading news aggregation company offers hundreds of digital products and services for customers ranging from law firms to banks to consumers. The company bills its clients based on per unit of clickstream data provided to the clients. As the company operates in a regulated industry, it needs to have the same ordered clickstream data available for auditing within a window of 7 days. As a solutions architect, which of the following AWS services provides the ability to run the billing process and auditing process on the given clickstream data in the same order?",
      "options": [
        "A. Amazon Simple Queue Service (SQS)",
        "B. Amazon Kinesis Data Firehose",
        "C. Amazon Kinesis Data Analytics",
        "D. Amazon Kinesis Data Streams"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: Amazon Kinesis Data Streams Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more. Amazon Kinesis Data Streams enables real-time processing of streaming big data. It provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications. The Amazon Kinesis Client Library (KCL) delivers all records for a given partition key to the same record processor, making it easier to build multiple applications reading from the same Amazon Kinesis data stream (for example, to perform counting, aggregation, and filtering). Amazon Kinesis Data Streams is recommended when you need the ability to consume records in the same order a few hours later. For example, you have a billing application and an audit application that runs a few hours behind the billing application. Because Amazon Kinesis Data Streams stores data for a maximum of 365 days, you can easily run the audit application up to 7 days behind the billing application. KDS provides the ability to consume records in the same order a few hours later via - https://aws.amazon.com/kinesis/data-streams/faqs/ Incorrect options: Amazon Kinesis Data Firehose - Amazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards you’re already using today. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security. As Amazon Kinesis Data Firehose is used to load streaming data into data stores , therefore this option is incorrect. Amazon Kinesis Data Analytics - Amazon Kinesis Data Analytics is the easiest way to analyze streaming data in real-time. You can quickly build SQL queries and sophisticated Java applications using built-in templates and operators for common processing functions to organize, transform, aggregate, and analyze data at any scale. Kinesis Data Analytics enables you to easily and quickly build queries and sophisticated streaming applications in three simple steps: setup your streaming data sources, write your queries or streaming applications and set up your destination for processed data. As Amazon Kinesis Data Analytics is used to build SQL queries and sophisticated Java applications, therefore this option is incorrect. Amazon Simple Queue Service (SQS) - Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. Amazon SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. Amazon SQS FIFO (First-In-First-Out) queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent. For Amazon SQS, you cannot have the same message being consumed by multiple consumers in the same order a few hours later, therefore this option is incorrect. References: https://aws.amazon.com/kinesis/data-streams/faqs/ https://aws.amazon.com/kinesis/data-firehose/faqs/ https://aws.amazon.com/kinesis/data-analytics/faqs/",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A video conferencing application is hosted on a fleet of EC2 instances which are part of an Auto Scaling group. The Auto Scaling group uses a Launch Template (LT1) with \"dedicated\" instance tenancy but the VPC (V1) used by the Launch Template LT1 has the instance tenancy set to default. Later the DevOps team creates a new Launch Template (LT2) with shared (default) instance tenancy but the VPC (V2) used by the Launch Template LT2 has the instance tenancy set to dedicated. Which of the following is correct regarding the instances launched via Launch Template LT1 and Launch Template LT2?",
      "options": [
        "A. The instances launched by Launch Template LT1 will have dedicated instance tenancy while the instances launched by the Launch Template LT2 will have shared (default) instance tenancy",
        "B. The instances launched by Launch Template LT1 will have default instance tenancy while the instances launched by the Launch Template LT2 will have dedicated instance tenancy",
        "C. The instances launched by both Launch Template LT1 and Launch Template LT2 will have default instance tenancy",
        "D. The instances launched by both Launch Template LT1 and Launch Template LT2 will have dedicated instance tenancy"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: The instances launched by both Launch Template LT1 and Launch Template LT2 will have dedicated instance tenancy A launch template specifies instance configuration information. It includes the ID of the Amazon Machine Image (AMI), the instance type, a key pair, security groups, and other parameters used to launch EC2 instances. If you've launched an EC2 instance before, you specified the same information to launch the instance. When you create a Launch Template, the default value for the instance tenancy is shared and the instance tenancy is controlled by the tenancy attribute of the VPC. If you set the Launch Template Tenancy to shared (default) and the VPC Tenancy is set to dedicated, then the instances have dedicated tenancy. If you set the Launch Template Tenancy to dedicated and the VPC Tenancy is set to default, then again the instances have dedicated tenancy. Amazon EC2 provides three options for the tenancy of your EC2 instances: Shared (Shared) – Multiple AWS accounts may share the same physical hardware. This is the default tenancy option when launching an instance. Dedicated instances (Dedicated) – Your instance runs on single-tenant hardware. No other AWS customer shares the same physical server. Dedicated Hosts (Dedicated host) – The instance runs on a physical server that is dedicated to your use. Using Dedicated Hosts makes it easier to bring your own licenses (BYOL) that have dedicated hardware requirements to EC2 and meet compliance use cases. If you choose this option, you must provide a host resource group for Tenancy host resource group. Incorrect options: The instances launched by Launch Template LT1 will have dedicated instance tenancy while the instances launched by the Launch Template LT2 will have shared (default) instance tenancy - If either Launch Template Tenancy or VPC Tenancy is set to dedicated, then the instance tenancy is also dedicated. Therefore, this option is incorrect. The instances launched by Launch Template LT1 will have default instance tenancy while the instances launched by the Launch Template LT2 will have dedicated instance tenancy - If either Launch Template Tenancy or VPC Tenancy is set to dedicated, then the instance tenancy is also dedicated. Therefore, this option is incorrect. The instances launched by both Launch Template LT1 and Launch Template LT2 will have default instance tenancy - If either Launch Template Tenancy or VPC Tenancy is set to dedicated, then the instance tenancy is also dedicated. Therefore, this option is incorrect. Reference: https://docs.aws.amazon.com/autoscaling/ec2/userguide/advanced-settings-for-your-launch-template.html",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An IT consultant is helping a small business revamp their technology infrastructure on the AWS Cloud. The business has two AWS accounts and all resources are provisioned in the us-west-2 region. The IT consultant is trying to launch an Amazon EC2 instance in each of the two AWS accounts such that the instances are in the same Availability Zone (AZ) of the us-west-2 region. Even after selecting the same default subnet (us-west-2a) while launching the instances in each of the AWS accounts, the IT consultant notices that the Availability Zones (AZs) are still different. As a solutions architect, which of the following would you suggest resolving this issue?",
      "options": [
        "A. Reach out to AWS Support for creating the Amazon EC2 instances in the same Availability Zone (AZ) across the two AWS accounts",
        "B. Use Availability Zone (AZ) ID to uniquely identify the Availability Zones across the two AWS Accounts",
        "C. Use the default subnet to uniquely identify the Availability Zones across the two AWS Accounts",
        "D. Use the default VPC to uniquely identify the Availability Zones across the two AWS Accounts"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Use Availability Zone (AZ) ID to uniquely identify the Availability Zones across the two AWS Accounts An Availability Zone is represented by a region code followed by a letter identifier; for example, us-east-1a. To ensure that resources are distributed across the Availability Zones for a region, AWS maps Availability Zones to names for each AWS account. For example, the Availability Zone us-west-2a for one AWS account might not be the same location as us-west-2a for another AWS account. To coordinate Availability Zones across accounts, you must use the AZ ID, which is a unique and consistent identifier for an Availability Zone. For example, usw2-az2 is an AZ ID for the us-west-2 region and it has the same location in every AWS account. Viewing AZ IDs enables you to determine the location of resources in one account relative to the resources in another account. For example, if you share a subnet in the Availability Zone with the AZ ID usw2-az2 with another account, this subnet is available to that account in the Availability Zone whose AZ ID is also usw2-az2. You can view the AZ IDs by going to the service health section of the Amazon EC2 Dashboard via your AWS Management Console. Availability Zone (AZ) IDs for Availability Zones: Incorrect options: Use the default VPC to uniquely identify the Availability Zones across the two AWS Accounts - A virtual private cloud (VPC) is a virtual network dedicated to your AWS account. It is logically isolated from other virtual networks in the AWS Cloud. Since a VPC spans an AWS region, it cannot be used to uniquely identify an Availability Zone. Therefore, this option is incorrect. Use the default subnet to uniquely identify the Availability Zones across the two AWS Accounts - A subnet is a range of IP addresses in your VPC. A subnet spans an Availability Zone of an AWS region. The default subnet representing the Availability Zone us-west-2a for one AWS account might not be the same location as us-west-2a for another AWS account. Therefore, this option is incorrect. Reach out to AWS Support for creating the Amazon EC2 instances in the same Availability Zone (AZ) across the two AWS accounts - Since the AZ ID is a unique and consistent identifier for an Availability Zone, there is no need to contact AWS Support. Therefore, this option is incorrect. Reference: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A media company has its corporate headquarters in Los Angeles with an on-premises data center using an AWS Direct Connect connection to the AWS VPC. The branch offices in San Francisco and Miami use AWS Site-to-Site VPN connections to connect to the AWS VPC. The company is looking for a solution to have the branch offices send and receive data with each other as well as with their corporate headquarters. As a solutions architect, which of the following AWS services would you recommend addressing this use-case?",
      "options": [
        "A. VPC Peering connection",
        "B. AWS VPN CloudHub",
        "C. Software VPN",
        "D. VPC Endpoint"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: AWS VPN CloudHub If you have multiple AWS Site-to-Site VPN connections, you can provide secure communication between sites using the AWS VPN CloudHub. This enables your remote sites to communicate with each other, and not just with the VPC. Sites that use AWS Direct Connect connections to the virtual private gateway can also be part of the AWS VPN CloudHub. The VPN CloudHub operates on a simple hub-and-spoke model that you can use with or without a VPC. This design is suitable if you have multiple branch offices and existing internet connections and would like to implement a convenient, potentially low-cost hub-and-spoke model for primary or backup connectivity between these remote offices. Per the given use-case, the corporate headquarters has an AWS Direct Connect connection to the VPC and the branch offices have Site-to-Site VPN connections to the VPC. Therefore using the AWS VPN CloudHub, branch offices can send and receive data with each other as well as with their corporate headquarters. AWS VPN CloudHub: via - https://docs.aws.amazon.com/vpn/latest/s2svpn/VPN_CloudHub.html Incorrect options: VPC Endpoint - A VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by AWS PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Instances in your VPC do not require public IP addresses to communicate with resources in the service. AWS PrivateLink simplifies the security of data shared with cloud-based applications by eliminating the exposure of data to the public Internet. When you use VPC endpoint, the traffic between your VPC and the other AWS service does not leave the Amazon network, therefore this option cannot be used to send and receive data between the remote branch offices of the company. VPC Peering connection - A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses. Instances in either VPC can communicate with each other as if they are within the same network. VPC peering facilitates a connection between two VPCs within the AWS network, therefore this option cannot be used to send and receive data between the remote branch offices of the company. Software VPN - Amazon VPC offers you the flexibility to fully manage both sides of your Amazon VPC connectivity by creating a VPN connection between your remote network and a software VPN appliance running in your Amazon VPC network. Since Software VPN just handles connectivity between the remote network and Amazon VPC, therefore it cannot be used to send and receive data between the remote branch offices of the company. References: https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-vpn-cloudhub-network-to-amazon.html https://docs.aws.amazon.com/vpn/latest/s2svpn/VPN_CloudHub.html",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A biotechnology company has multiple High Performance Computing (HPC) workflows that quickly and accurately process and analyze genomes for hereditary diseases. The company is looking to migrate these workflows from their on-premises infrastructure to AWS Cloud. As a solutions architect, which of the following networking components would you recommend on the Amazon EC2 instances running these HPC workflows?",
      "options": [
        "A. Elastic Fabric Adapter (EFA)",
        "B. Elastic Network Interface (ENI)",
        "C. Elastic Network Adapter (ENA)",
        "D. Elastic IP Address (EIP)"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Elastic Fabric Adapter (EFA) An Elastic Fabric Adapter (EFA) is a network device that you can attach to your Amazon EC2 instance to accelerate High Performance Computing (HPC) and machine learning applications. It enhances the performance of inter-instance communication that is critical for scaling HPC and machine learning applications. EFA devices provide all Elastic Network Adapter (ENA) devices functionalities plus a new OS bypass hardware interface that allows user-space applications to communicate directly with the hardware-provided reliable transport functionality. How Elastic Fabric Adapter Works: via - https://aws.amazon.com/hpc/efa/ Incorrect options: Elastic Network Interface (ENI) - An Elastic Network Interface (ENI) is a logical networking component in a VPC that represents a virtual network card. You can create a network interface, attach it to an instance, detach it from an instance, and attach it to another instance. The ENI is the simplest networking component available on AWS and is insufficient for HPC workflows. Elastic Network Adapter (ENA) - Elastic Network Adapter (ENA) devices support enhanced networking via single root I/O virtualization (SR-IOV) to provide high-performance networking capabilities. Although enhanced networking provides higher bandwidth, higher packet per second (PPS) performance, and consistently lower inter-instance latencies, still EFA is a better fit for the given use-case because the EFA device provides all the functionality of an ENA device, plus hardware support for applications to communicate directly with the EFA device without involving the instance kernel (OS-bypass communication) using an extended programming interface. Elastic IP Address (EIP) - An Elastic IP address (EIP) is a static IPv4 address associated with your AWS account. An Elastic IP address is a public IPv4 address, which is reachable from the internet. It is not a networking device that can be used to facilitate HPC workflows. References: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa.html https://aws.amazon.com/hpc/efa/",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A developer has configured inbound traffic for the relevant ports in both the Security Group of the Amazon EC2 instance as well as the Network Access Control List (Network ACL) of the subnet for the Amazon EC2 instance. The developer is, however, unable to connect to the service running on the Amazon EC2 instance. As a solutions architect, how will you fix this issue?",
      "options": [
        "A. Network ACLs are stateful, so allowing inbound traffic to the necessary ports enables the connection. Security Groups are stateless, so you must allow both inbound and outbound traffic",
        "B. IAM Role defined in the Security Group is different from the IAM Role that is given access in the Network ACLs",
        "C. Security Groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network ACLs are stateless, so you must allow both inbound and outbound traffic",
        "D. Rules associated with Network ACLs should never be modified from command line. An attempt to modify rules from command line blocks the rule and results in an erratic behavior"
      ],
      "correct_answer": "C",
      "explanation": "Correct option: Security Groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network ACLs are stateless, so you must allow both inbound and outbound traffic Security groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network ACLs are stateless, so you must allow both inbound and outbound traffic. To enable the connection to a service running on an instance, the associated network ACL must allow both inbound traffic on the port that the service is listening on as well as allow outbound traffic from ephemeral ports. When a client connects to a server, a random port from the ephemeral port range (1024-65535) becomes the client's source port. The designated ephemeral port then becomes the destination port for return traffic from the service, so outbound traffic from the ephemeral port must be allowed in the network ACL. By default, network ACLs allow all inbound and outbound traffic. If your network ACL is more restrictive, then you need to explicitly allow traffic from the ephemeral port range. If you accept traffic from the internet, then you also must establish a route through an internet gateway. If you accept traffic over VPN or AWS Direct Connect, then you must establish a route through a virtual private gateway (VGW). Incorrect options: Network ACLs are stateful, so allowing inbound traffic to the necessary ports enables the connection. Security Groups are stateless, so you must allow both inbound and outbound traffic - This is incorrect as already discussed. IAM Role defined in the Security Group is different from the IAM Role that is given access in the Network ACLs - This is a made-up option and just added as a distractor. Rules associated with Network ACLs should never be modified from command line. An attempt to modify rules from command line blocks the rule and results in an erratic behavior - This option is a distractor. AWS does not support modifying rules of Network ACLs from the command line tool. Reference: https://aws.amazon.com/premiumsupport/knowledge-center/resolve-connection-sg-acl-inbound/",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An IT company is using Amazon Simple Queue Service (Amazon SQS) queues for decoupling the various components of application architecture. As the consuming components need additional time to process Amazon Simple Queue Service (Amazon SQS) messages, the company wants to postpone the delivery of new messages to the queue for a few seconds. As a solutions architect, which of the following solutions would you suggest to the company?",
      "options": [
        "A. Use Amazon SQS FIFO queues to postpone the delivery of new messages to the queue for a few seconds",
        "B. Use dead-letter queues to postpone the delivery of new messages to the queue for a few seconds",
        "C. Use visibility timeout to postpone the delivery of new messages to the queue for a few seconds",
        "D. Use delay queues to postpone the delivery of new messages to the queue for a few seconds"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: Use delay queues to postpone the delivery of new messages to the queue for a few seconds Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. Amazon SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. Amazon SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent. Delay queues let you postpone the delivery of new messages to a queue for several seconds, for example, when your consumer application needs additional time to process messages. If you create a delay queue, any messages that you send to the queue remain invisible to consumers for the duration of the delay period. The default (minimum) delay for a queue is 0 seconds. The maximum is 15 minutes. via - https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html Incorrect options: Use Amazon SQS FIFO queues to postpone the delivery of new messages to the queue for a few seconds - Amazon SQS FIFO (First-In-First-Out) queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent. You cannot use Amazon SQS FIFO queues to postpone the delivery of new messages to the queue for a few seconds. Use dead-letter queues to postpone the delivery of new messages to the queue for a few seconds - Dead-letter queues can be used by other queues (source queues) as a target for messages that can't be processed (consumed) successfully. Dead-letter queues are useful for debugging your application or messaging system because they let you isolate problematic messages to determine why their processing doesn't succeed. You cannot use dead-letter queues to postpone the delivery of new messages to the queue for a few seconds. Use visibility timeout to postpone the delivery of new messages to the queue for a few seconds - Visibility timeout is a period during which Amazon SQS prevents other consumers from receiving and processing a given message. The default visibility timeout for a message is 30 seconds. The minimum is 0 seconds. The maximum is 12 hours. You cannot use visibility timeout to postpone the delivery of new messages to the queue for a few seconds. Reference: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A big data analytics company is working on a real-time vehicle tracking solution. The data processing workflow involves both I/O intensive and throughput intensive database workloads. The development team needs to store this real-time data in a NoSQL database hosted on an Amazon EC2 instance and needs to support up to 25,000 IOPS per volume. As a solutions architect, which of the following Amazon Elastic Block Store (Amazon EBS) volume types would you recommend for this use-case?",
      "options": [
        "A. General Purpose SSD (gp2)",
        "B. Cold HDD (sc1)",
        "C. Provisioned IOPS SSD (io1)",
        "D. Throughput Optimized HDD (st1)"
      ],
      "correct_answer": "C",
      "explanation": "Correct option: Provisioned IOPS SSD (io1) Provisioned IOPS SSD (io1) is backed by solid-state drives (SSDs) and is a high-performance Amazon EBS storage option designed for critical, I/O intensive database and application workloads, as well as throughput-intensive database workloads. io1 is designed to deliver a consistent baseline performance of up to 50 IOPS/GB to a maximum of 64,000 IOPS and provide up to 1,000 MB/s of throughput per volume. Therefore, the io1 volume type would be able to meet the requirement of 25,000 IOPS per volume for the given use-case. Incorrect options: General Purpose SSD (gp2) - gp2 is backed by solid-state drives (SSDs) and is suitable for a broad range of transactional workloads, including dev/test environments, low-latency interactive applications, and boot volumes. It supports max IOPS/Volume of 16,000. Cold HDD (sc1) - sc1 is backed by hard disk drives (HDDs). It is ideal for less frequently accessed workloads with large, cold datasets. It supports max IOPS/Volume of 250. Throughput Optimized HDD (st1) - st1 is backed by hard disk drives (HDDs) and is ideal for frequently accessed, throughput-intensive workloads with large datasets and large I/O sizes, such as MapReduce, Kafka, log processing, data warehouse, and ETL workloads. It supports max IOPS/Volume of 500. Reference: https://aws.amazon.com/ebs/volume-types/",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An e-commerce company uses Microsoft Active Directory to provide users and groups with access to resources on the on-premises infrastructure. The company has extended its IT infrastructure to AWS in the form of a hybrid cloud. The engineering team at the company wants to run directory-aware workloads on AWS for a SQL Server-based application. The team also wants to configure a trust relationship to enable single sign-on (SSO) for its users to access resources in either domain. As a solutions architect, which of the following AWS services would you recommend for this use-case?",
      "options": [
        "A. Active Directory Connector",
        "B. AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD)",
        "C. Simple Active Directory (Simple AD)",
        "D. Amazon Cloud Directory"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD) AWS Directory Service provides multiple ways to use Amazon Cloud Directory and Microsoft Active Directory (AD) with other AWS services. AWS Directory Service for Microsoft Active Directory (aka AWS Managed Microsoft AD) is powered by an actual Microsoft Windows Server Active Directory (AD), managed by AWS. With AWS Managed Microsoft AD, you can run directory-aware workloads in the AWS Cloud such as SQL Server-based applications. You can also configure a trust relationship between AWS Managed Microsoft AD in the AWS Cloud and your existing on-premises Microsoft Active Directory, providing users and groups with access to resources in either domain, using single sign-on (SSO). Incorrect options: Active Directory Connector - Use AD Connector if you only need to allow your on-premises users to log in to AWS applications and services with their Active Directory credentials. AD Connector simply connects your existing on-premises Active Directory to AWS. You cannot use it to run directory-aware workloads on AWS, hence this option is not correct. Simple Active Directory (Simple AD) - Simple AD provides a subset of the features offered by AWS Managed Microsoft AD. Simple AD is a standalone managed directory that is powered by a Samba 4 Active Directory Compatible Server. Simple AD does not support features such as trust relationships with other domains. Therefore, this option is not correct. Amazon Cloud Directory - Amazon Cloud Directory is a cloud-native directory that can store hundreds of millions of application-specific objects with multiple relationships and schemas. Use Amazon Cloud Directory if you need a highly scalable directory store for your application’s hierarchical data. You cannot use it to establish trust relationships with other domains on the on-premises infrastructure. Therefore, this option is not correct. Exam Alert: You may see questions on choosing \"AWS Managed Microsoft AD\" vs \"AD Connector\" vs \"Simple AD\" on the exam. Just remember that you should use AD Connector if you only need to allow your on-premises users to log in to AWS applications with their Active Directory credentials. AWS Managed Microsoft AD would also allow you to run directory-aware workloads in the AWS Cloud. AWS Managed Microsoft AD is your best choice if you have more than 5,000 users and need a trust relationship set up between an AWS hosted directory and your on-premises directories. Simple AD is the least expensive option and your best choice if you have 5,000 or fewer users and don’t need the more advanced Microsoft Active Directory features such as trust relationships with other domains. Reference: https://docs.aws.amazon.com/directoryservice/latest/admin-guide/what_is.html",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A financial services company has recently migrated from on-premises infrastructure to AWS Cloud. The DevOps team wants to implement a solution that allows all resource configurations to be reviewed and make sure that they meet compliance guidelines. Also, the solution should be able to offer the capability to look into the resource configuration history across the application stack. As a solutions architect, which of the following solutions would you recommend to the team?",
      "options": [
        "A. Use AWS Config to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes",
        "B. Use Amazon CloudWatch to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes",
        "C. Use AWS CloudTrail to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes",
        "D. Use AWS Systems Manager to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Use AWS Config to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. With Config, you can review changes in configurations and relationships between AWS resources, dive into detailed resource configuration histories, and determine your overall compliance against the configurations specified in your internal guidelines. You can use Config to answer questions such as - “What did my AWS resource look like at xyz point in time?” How AWS Config Works: via - https://aws.amazon.com/config/ Incorrect options: Use Amazon CloudWatch to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes - AWS CloudWatch provides you with data and actionable insights to monitor your applications, respond to system-wide performance changes, optimize resource utilization, and get a unified view of operational health. You cannot use Amazon CloudWatch to maintain a history of resource configuration changes. Use AWS CloudTrail to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes - With AWS CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. You can use AWS CloudTrail to answer questions such as - “Who made an API call to modify this resource?”. AWS CloudTrail provides an event history of your AWS account activity thereby enabling governance, compliance, operational auditing, and risk auditing of your AWS account. You cannot use AWS CloudTrail to maintain a history of resource configuration changes. Use AWS Systems Manager to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes - Using AWS Systems Manager, you can group resources, like Amazon EC2 instances, Amazon S3 buckets, or Amazon RDS instances, by application, view operational data for monitoring and troubleshooting, and take action on your groups of resources. You cannot use AWS Systems Manager to maintain a history of resource configuration changes. Exam Alert: You may see scenario-based questions asking you to select one of Amazon CloudWatch vs AWS CloudTrail vs AWS Config. Just remember this thumb rule - Think resource performance monitoring, events, and alerts; think Amazon CloudWatch. Think account-specific activity and audit; think AWS CloudTrail. Think resource-specific history, audit, and compliance; think AWS Config. References: https://aws.amazon.com/config/ https://aws.amazon.com/cloudwatch/ https://aws.amazon.com/cloudtrail/ https://aws.amazon.com/systems-manager/",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A social media startup uses AWS Cloud to manage its IT infrastructure. The engineering team at the startup wants to perform weekly database rollovers for a MySQL database server using a serverless cron job that typically takes about 5 minutes to execute the database rollover script written in Python. The database rollover will archive the past week’s data from the production database to keep the database small while still keeping its data accessible. As a solutions architect, which of the following would you recommend as the MOST cost-efficient and reliable solution?",
      "options": [
        "A. Create a time-based schedule option within an AWS Glue job to invoke itself every week and run the database rollover script",
        "B. Schedule a weekly Amazon EventBridge event cron expression to invoke an AWS Lambda function that runs the database rollover job",
        "C. Provision an Amazon EC2 spot instance to run the database rollover script to be run via an OS-based weekly cron expression",
        "D. Provision an Amazon EC2 scheduled reserved instance to run the database rollover script to be run via an OS-based weekly cron expression"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Schedule a weekly Amazon EventBridge event cron expression to invoke an AWS Lambda function that runs the database rollover job AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume. AWS Lambda supports standard rate and cron expressions for frequencies of up to once per minute. Schedule expressions using rate or cron: Incorrect options: Create a time-based schedule option within an AWS Glue job to invoke itself every week and run the database rollover script - AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. AWS Glue job is meant to be used for batch ETL data processing and it's not the right fit for running a database rollover script. Although AWS Glue is also serverless, AWS Lambda is a more cost-effective option compared to AWS Glue. Provision an Amazon EC2 spot instance to run the database rollover script to be run via an OS-based weekly cron expression - A Spot Instance is an unused Amazon EC2 instance that is available for less than the On-Demand price. Because Spot Instances enable you to request unused Amazon EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly (up to 90% off the On-Demand price). As the Spot Instance runs whenever capacity is available, there is no guarantee that the weekly job will be executed during the defined time window. Additionally, the given use-case requires a serverless solution, therefore this option is incorrect. Provision an Amazon EC2 scheduled reserved instance to run the database rollover script to be run via an OS-based weekly cron expression - Scheduled Reserved Instances run on a part-time basis. Scheduled Reserved Instances option allows you to use reserve capacity on a recurring daily, weekly, and monthly schedules. Scheduled Reserved Instances are available for one-year terms at 5-10% below On-Demand rates. As the given use-case requires a serverless solution, therefore this option is incorrect. References: https://aws.amazon.com/lambda/ https://docs.aws.amazon.com/lambda/latest/dg/services-cloudwatchevents-expressions.html",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A company has a hybrid cloud structure for its on-premises data center and AWS Cloud infrastructure. The company wants to build a web log archival solution such that only the most frequently accessed logs are available as cached data locally while backing up all logs on Amazon S3. As a solutions architect, which of the following solutions would you recommend for this use-case?",
      "options": [
        "A. Use AWS Direct Connect to store the most frequently accessed logs locally for low-latency access while storing the full backup of logs in an Amazon S3 bucket",
        "B. Use AWS Volume Gateway - Stored Volume - to store the most frequently accessed logs locally for low-latency access while storing the full volume with all logs in its Amazon S3 service bucket",
        "C. Use AWS Snowball Edge Storage Optimized device to store the most frequently accessed logs locally for low-latency access while storing the full backup of logs in an Amazon S3 bucket",
        "D. Use AWS Volume Gateway - Cached Volume - to store the most frequently accessed logs locally for low-latency access while storing the full volume with all logs in its Amazon S3 service bucket"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: Use AWS Volume Gateway - Cached Volume - to store the most frequently accessed logs locally for low-latency access while storing the full volume with all logs in its Amazon S3 service bucket AWS Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage. The service provides three different types of gateways – Tape Gateway, File Gateway, and Volume Gateway – that seamlessly connect on-premises applications to cloud storage, caching data locally for low-latency access. With cached volumes, the AWS Volume Gateway stores the full volume in its Amazon S3 service bucket, and just the recently accessed data is retained in the gateway’s local cache for low-latency access. Incorrect options: Use AWS Direct Connect to store the most frequently accessed logs locally for low-latency access while storing the full backup of logs in an Amazon S3 bucket - AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations. AWS Direct connect cannot be used to store the most frequently accessed logs locally for low-latency access. Use AWS Volume Gateway - Stored Volume - to store the most frequently accessed logs locally for low-latency access while storing the full volume with all logs in its Amazon S3 service bucket - With stored volumes, your entire data volume is available locally in the gateway, for fast read access. Volume Gateway also maintains an asynchronous copy of your stored volume in the service’s Amazon S3 bucket. This does not fit the requirements per the given use-case, hence this option is not correct. Use AWS Snowball Edge Storage Optimized device to store the most frequently accessed logs locally for low-latency access while storing the full backup of logs in an Amazon S3 bucket - You can use Snowball Edge Storage Optimized device to securely and quickly transfer dozens of terabytes to petabytes of data to AWS. Snowball Edge Storage Optimized device cannot be used to store the most frequently accessed logs locally for low-latency access. Reference: https://aws.amazon.com/storagegateway/volume/",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A media streaming company is looking to migrate its on-premises infrastructure into the AWS Cloud. The engineering team is looking for a fully managed NoSQL persistent data store with in-memory caching to maintain low latency that is critical for real-time scenarios such as video streaming and interactive content. The team expects the number of concurrent users to touch up to a million so the database should be able to scale elastically. As a solutions architect, which of the following AWS services would you recommend for this use-case?",
      "options": [
        "A. Amazon DynamoDB",
        "B. Amazon DocumentDB",
        "C. Amazon ElastiCache",
        "D. Amazon RDS"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Amazon DynamoDB Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications. DAX is a DynamoDB-compatible caching service that enables you to benefit from fast in-memory performance for demanding applications. Companies use caching through DynamoDB Accelerator (DAX) when they have high read volumes or need submillisecond read latency. Incorrect options: Amazon DocumentDB - Amazon DocumentDB is a fast, scalable, highly available, and fully managed document database service that supports MongoDB workloads. As a document database, Amazon DocumentDB makes it easy to store, query, and index JSON data. Although DocumentDB is fully managed, it does not have an in-memory caching layer. Amazon ElastiCache - Amazon ElastiCache allows you to set up popular open-Source compatible in-memory data stores in the cloud. You can build data-intensive apps or boost the performance of your existing databases by retrieving data from high throughput and low latency in-memory data stores such as Redis and Memcached. Elasticache is used as a caching layer. It's not a fully managed NoSQL database. Amazon RDS - Amazon RDS makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching, and backups. It's not a NoSQL database. References: https://aws.amazon.com/dynamodb/",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A company wants to improve its gaming application by adding a leaderboard that uses a complex proprietary algorithm based on the participating user's performance metrics to identify the top users on a real-time basis. The technical requirements mandate high elasticity, low latency, and real-time processing to deliver customizable user data for the community of users. The leaderboard would be accessed by millions of users simultaneously. Which of the following options support the case for using Amazon ElastiCache to meet the given requirements? (Select two)",
      "options": [
        "A. Use Amazon ElastiCache to improve latency and throughput for read-heavy application workloads",
        "B. Use Amazon ElastiCache to improve latency and throughput for write-heavy application workloads",
        "C. Use Amazon ElastiCache to improve the performance of Extract-Transform-Load (ETL) workloads",
        "D. Use Amazon ElastiCache to improve the performance of compute-intensive workloads",
        "E. Use Amazon ElastiCache to run highly complex JOIN queries"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Use Amazon ElastiCache to improve latency and throughput for read-heavy application workloads Use Amazon ElastiCache to improve the performance of compute-intensive workloads Amazon ElastiCache allows you to run in-memory data stores in the AWS cloud. Amazon ElastiCache is a popular choice for real-time use cases like Caching, Session Stores, Gaming, Geospatial Services, Real-Time Analytics, and Queuing. via - https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/elasticache-use-cases.html Amazon ElastiCache can be used to significantly improve latency and throughput for many read-heavy application workloads (such as social networking, gaming, media sharing, leaderboard, and Q&A portals) or compute-intensive workloads (such as a recommendation engine) by allowing you to store the objects that are often read in the cache. Overview of Amazon ElastiCache features: via - https://aws.amazon.com/elasticache/features/ Incorrect options: Use Amazon ElastiCache to improve latency and throughput for write-heavy application workloads - As mentioned earlier in the explanation, Amazon ElastiCache can be used to significantly improve latency and throughput for many read-heavy application workloads. Caching is not a good fit for write-heavy applications as the cache goes stale at a very fast rate. Use Amazon ElastiCache to improve the performance of Extract-Transform-Load (ETL) workloads - ETL workloads involve reading and transforming high-volume data which is not a good fit for caching. You should use AWS Glue or Amazon EMR to facilitate ETL workloads. Use Amazon ElastiCache to run highly complex JOIN queries - Complex JSON queries can be run on relational databases such as Amazon RDS or Amazon Aurora. Amazon ElastiCache is not a good fit for this use case. References: https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/elasticache-use-cases.html https://aws.amazon.com/elasticache/features/",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A financial services company is migrating their messaging queues from self-managed message-oriented middleware systems to Amazon Simple Queue Service (Amazon SQS). The development team at the company wants to minimize the costs of using Amazon SQS. As a solutions architect, which of the following options would you recommend for the given use-case?",
      "options": [
        "A. Use SQS short polling to retrieve messages from your Amazon SQS queues",
        "B. Use SQS visibility timeout to retrieve messages from your Amazon SQS queues",
        "C. Use SQS long polling to retrieve messages from your Amazon SQS queues",
        "D. Use SQS message timer to retrieve messages from your Amazon SQS queues"
      ],
      "correct_answer": "C",
      "explanation": "Correct option: Use SQS long polling to retrieve messages from your Amazon SQS queues Amazon Simple Queue Service (Amazon SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. Amazon SQS provides short polling and long polling to receive messages from a queue. By default, queues use short polling. With short polling, Amazon SQS sends the response right away, even if the query found no messages. With long polling, Amazon SQS sends a response after it collects at least one available message, up to the maximum number of messages specified in the request. Amazon SQS sends an empty response only if the polling wait time expires. Long polling makes it inexpensive to retrieve messages from your Amazon SQS queue as soon as the messages are available. Using long polling can reduce the cost of using SQS because you can reduce the number of empty receives. Short Polling vs Long Polling: via - https://aws.amazon.com/sqs/faqs/ Incorrect options: Use SQS short polling to retrieve messages from your Amazon SQS queues - With short polling, Amazon SQS sends the response right away, even if the query found no messages. You end up paying more because of the increased number of empty receives. Use SQS visibility timeout to retrieve messages from your Amazon SQS queues - Visibility timeout is a period during which Amazon SQS prevents other consumers from receiving and processing a given message. The default visibility timeout for a message is 30 seconds. The minimum is 0 seconds. The maximum is 12 hours. You cannot use visibility timeout to retrieve messages from your Amazon SQS queues. This option has been added as a distractor. Use SQS message timer to retrieve messages from your Amazon SQS queues - You can use message timers to set an initial invisibility period for a message added to a queue. So, if you send a message with a 60-second timer, the message isn't visible to consumers for its first 60 seconds in the queue. The default (minimum) delay for a message is 0 seconds. The maximum is 15 minutes. You cannot use message timer to retrieve messages from your Amazon SQS queues. This option has been added as a distractor. References: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html https://aws.amazon.com/sqs/faqs/",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A retail company has connected its on-premises data center to the AWS Cloud via AWS Direct Connect. The company wants to be able to resolve Domain Name System (DNS) queries for any resources in the on-premises network from the AWS VPC and also resolve any DNS queries for resources in the AWS VPC from the on-premises network. As a solutions architect, which of the following solutions can be combined to address the given use case? (Select two)",
      "options": [
        "A. Create an outbound endpoint on Amazon Route 53 Resolver and then DNS resolvers on the on-premises network can forward DNS queries to Amazon Route 53 Resolver via this endpoint",
        "B. Create an inbound endpoint on Amazon Route 53 Resolver and then Amazon Route 53 Resolver can conditionally forward queries to resolvers on the on-premises network via this endpoint",
        "C. Create a universal endpoint on Amazon Route 53 Resolver and then Amazon Route 53 Resolver can receive and forward queries to resolvers on the on-premises network via this endpoint",
        "D. Create an inbound endpoint on Amazon Route 53 Resolver and then DNS resolvers on the on-premises network can forward DNS queries to Amazon Route 53 Resolver via this endpoint",
        "E. Create an outbound endpoint on Amazon Route 53 Resolver and then Amazon Route 53 Resolver can conditionally forward queries to resolvers on the on-premises network via this endpoint"
      ],
      "correct_answer": "D",
      "explanation": "Correct options: Create an inbound endpoint on Amazon Route 53 Resolver and then DNS resolvers on the on-premises network can forward DNS queries to Amazon Route 53 Resolver via this endpoint Create an outbound endpoint on Amazon Route 53 Resolver and then Amazon Route 53 Resolver can conditionally forward queries to resolvers on the on-premises network via this endpoint Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. Amazon Route 53 effectively connects user requests to infrastructure running in AWS – such as Amazon EC2 instances – and can also be used to route users to infrastructure outside of AWS. By default, Amazon Route 53 Resolver automatically answers DNS queries for local VPC domain names for Amazon EC2 instances. You can integrate DNS resolution between Resolver and DNS resolvers on your on-premises network by configuring forwarding rules. To resolve any DNS queries for resources in the AWS VPC from the on-premises network, you can create an inbound endpoint on Amazon Route 53 Resolver and then DNS resolvers on the on-premises network can forward DNS queries to Amazon Route 53 Resolver via this endpoint. Resolver Inbound Endpoint: via - https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resolver.html To resolve DNS queries for any resources in the on-premises network from the AWS VPC, you can create an outbound endpoint on Amazon Route 53 Resolver and then Amazon Route 53 Resolver can conditionally forward queries to resolvers on the on-premises network via this endpoint. To conditionally forward queries, you need to create Resolver rules that specify the domain names for the DNS queries that you want to forward (such as example.com) and the IP addresses of the DNS resolvers on the on-premises network that you want to forward the queries to. Resolver Outbound Endpoint: via - https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resolver.html Incorrect options: Create an outbound endpoint on Amazon Route 53 Resolver and then DNS resolvers on the on-premises network can forward DNS queries to Amazon Route 53 Resolver via this endpoint - DNS resolvers on the on-premises network can forward DNS queries to Amazon Route 53 Resolver via an inbound endpoint. Hence, this option is incorrect. Create an inbound endpoint on Amazon Route 53 Resolver and then Amazon Route 53 Resolver can conditionally forward queries to resolvers on the on-premises network via this endpoint - Amazon Route 53 Resolver can conditionally forward queries to resolvers on the on-premises network via an outbound endpoint. Hence, this option is incorrect. Create a universal endpoint on Amazon Route 53 Resolver and then Amazon Route 53 Resolver can receive and forward queries to resolvers on the on-premises network via this endpoint - There is no such thing as a universal endpoint on Amazon Route 53 Resolver. This option has been added as a distractor. References: https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resolver-getting-started.html https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resolver.html",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A company has set up AWS Organizations to manage several departments running their own AWS accounts. The departments operate from different countries and are spread across various AWS Regions. The company wants to set up a consistent resource provisioning process across departments so that each resource follows pre-defined configurations such as using a specific type of Amazon EC2 instances, specific IAM roles for AWS Lambda functions, etc. As a solutions architect, which of the following options would you recommend for this use-case?",
      "options": [
        "A. Use AWS CloudFormation StackSets to deploy the same template across AWS accounts and regions",
        "B. Use AWS CloudFormation templates to deploy the same template across AWS accounts and regions",
        "C. Use AWS CloudFormation stacks to deploy the same template across AWS accounts and regions",
        "D. Use AWS Resource Access Manager (AWS RAM) to deploy the same template across AWS accounts and regions"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Use AWS CloudFormation StackSets to deploy the same template across AWS accounts and regions AWS CloudFormation StackSet extends the functionality of stacks by enabling you to create, update, or delete stacks across multiple accounts and regions with a single operation. A stack set lets you create stacks in AWS accounts across regions by using a single AWS CloudFormation template. Using an administrator account of an \"AWS Organization\", you define and manage an AWS CloudFormation template, and use the template as the basis for provisioning stacks into selected target accounts of an \"AWS Organization\" across specified regions. AWS CloudFormation StackSets: via - https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-concepts.html Incorrect options: Use AWS CloudFormation templates to deploy the same template across AWS accounts and regions - AWS Cloudformation template is a JSON or YAML-format, text-based file that describes all the AWS resources you need to deploy to run your application. A template acts as a blueprint for a stack. AWS CloudFormation templates cannot be used to deploy the same template across AWS accounts and regions. Use AWS CloudFormation stacks to deploy the same template across AWS accounts and regions - AWS CloudFormation stack is a set of AWS resources that are created and managed as a single unit when AWS CloudFormation instantiates a template. A stack cannot be used to deploy the same template across AWS accounts and regions. Use AWS Resource Access Manager (AWS RAM) to deploy the same template across AWS accounts and regions - AWS Resource Access Manager (AWS RAM) is a service that enables you to easily and securely share AWS resources with any AWS account or within your AWS Organization. Resource Access Manager cannot be used to deploy the same template across AWS accounts and regions. References: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-concepts.html https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-whatis-howdoesitwork.html",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A media startup is looking at hosting their web application on AWS Cloud. The application will be accessed by users from different geographic regions of the world to upload and download video files that can reach a maximum size of 10 gigabytes. The startup wants the solution to be cost-effective and scalable with the lowest possible latency for a great user experience. As a Solutions Architect, which of the following will you suggest as an optimal solution to meet the given requirements?",
      "options": [
        "A. Use Amazon S3 for hosting the web application and use Amazon CloudFront for faster distribution of content to geographically dispersed users",
        "B. Use Amazon EC2 with AWS Global Accelerator for faster distribution of content, while using Amazon S3 as storage service",
        "C. Use Amazon EC2 with Amazon ElastiCache for faster distribution of content, while Amazon S3 can be used as a storage service",
        "D. Use Amazon S3 for hosting the web application and use Amazon S3 Transfer Acceleration (Amazon S3TA) to reduce the latency that geographically dispersed users might face"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: Use Amazon S3 for hosting the web application and use Amazon S3 Transfer Acceleration (Amazon S3TA) to reduce the latency that geographically dispersed users might face Amazon S3 Transfer Acceleration (S3TA) can speed up content transfers to and from Amazon S3 by as much as 50-500% for long-distance transfer of larger objects. Customers who have either web or mobile applications with widespread users or applications hosted far away from their S3 bucket can experience long and variable upload and download speeds over the Internet. S3 Transfer Acceleration (S3TA) reduces the variability in Internet routing, congestion, and speeds that can affect transfers, and logically shortens the distance to S3 for remote applications. S3TA improves transfer performance by routing traffic through Amazon CloudFront’s globally distributed Edge Locations and over AWS backbone networks, and by using network protocol optimizations. For applications interacting with your Amazon S3 buckets through the S3 API from outside of your bucket’s region, S3TA helps avoid the variability in Internet routing and congestion. It does this by routing your uploads and downloads over the AWS global network infrastructure, so you get the benefit of AWS network optimizations. Incorrect options: Use Amazon S3 for hosting the web application and use Amazon CloudFront for faster distribution of content to geographically dispersed users - Amazon S3 with Amazon CloudFront is a very powerful way of distributing static content to geographically dispersed users with low latency speeds. If you have objects that are smaller than 1GB or if the data set is less than 1GB in size, you should consider using Amazon CloudFront's PUT/POST commands for optimal performance. The given use case has data larger than 1GB and hence S3 Transfer Acceleration is a better option. via - https://aws.amazon.com/s3/faqs/ Use Amazon EC2 with AWS Global Accelerator for faster distribution of content, while using Amazon S3 as storage service- AWS Global Accelerator is a networking service that sends your user’s traffic through Amazon Web Service’s global network infrastructure, improving your internet user performance by up to 60%. With AWS Global Accelerator, you are provided two global static customer-facing IPs to simplify traffic management. On the back end, add or remove your AWS application origins, such as Network Load Balancers, Application Load Balancers, Elastic IPs, and Amazon EC2 Instances, without making user-facing changes. As discussed, AWS Global Accelerator is meant for a different use case and is not meant for increasing the speed of Amazon S3 uploads or downloads. Use Amazon EC2 with Amazon ElastiCache for faster distribution of content, while Amazon S3 can be used as a storage service - Amazon ElastiCache allows you to seamlessly set up, run, and scale popular open-Source compatible in-memory data stores in the cloud. Build data-intensive apps or boost the performance of your existing databases by retrieving data from high throughput and low latency in-memory data stores. Amazon ElastiCache is a popular choice for real-time use cases like Caching, Session Stores, Gaming, Geospatial Services, Real-Time Analytics, and Queuing. Amazon S3 Transfer Acceleration is a better performing option than opting for Amazon EC2 with Amazon ElastiCache, which is not meant to address the given use-case. Reference: [https://aws.amazon.com/s3/transfer-acceleration/](https://aws.amazon.com/s3/transfer-acceleration https://aws.amazon.com/s3/faqs/",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A startup has recently moved their monolithic web application to AWS Cloud. The application runs on a single Amazon EC2 instance. Currently, the user base is small and the startup does not want to spend effort on elaborate disaster recovery strategies or Auto Scaling Group. The application can afford a maximum downtime of 10 minutes. In case of a failure, which of these options would you suggest as a cost-effective and automatic recovery procedure for the instance?",
      "options": [
        "A. Configure Amazon EventBridge events that can trigger the recovery of the Amazon EC2 instance, in case the instance or the application fails",
        "B. Configure an Amazon CloudWatch alarm that triggers the recovery of the Amazon EC2 instance, in case the instance fails. The instance can be configured with Amazon Elastic Block Store (Amazon EBS) or with instance store volumes",
        "C. Configure an Amazon CloudWatch alarm that triggers the recovery of the Amazon EC2 instance, in case the instance fails. The instance, however, should only be configured with an Amazon EBS volume",
        "D. Configure AWS Trusted Advisor to monitor the health check of Amazon EC2 instance and provide a remedial action in case an unhealthy flag is detected"
      ],
      "correct_answer": "C",
      "explanation": "Correct option: Configure an Amazon CloudWatch alarm that triggers the recovery of the Amazon EC2 instance, in case the instance fails. The instance, however, should only be configured with an Amazon EBS volume If your instance fails a system status check, you can use Amazon CloudWatch alarm actions to automatically recover it. The recover option is available for over 90% of deployed customer Amazon EC2 instances. The Amazon CloudWatch recovery option works only for system check failures, not for instance status check failures. Also, if you terminate your instance, then it can't be recovered. You can create an Amazon CloudWatch alarm that monitors an Amazon EC2 instance and automatically recovers the instance if it becomes impaired due to an underlying hardware failure or a problem that requires AWS involvement to repair. Terminated instances cannot be recovered. A recovered instance is identical to the original instance, including the instance ID, private IP addresses, Elastic IP addresses, and all instance metadata. If the impaired instance is in a placement group, the recovered instance runs in the placement group. The automatic recovery process attempts to recover your instance for up to three separate failures per day. Your instance may subsequently be retired if automatic recovery fails and a hardware degradation is determined to be the root cause for the original system status check failure. Incorrect options: Configure Amazon EventBridge events that can trigger the recovery of the Amazon EC2 instance, in case the instance or the application fails - You cannot use Amazon EventBridge events to directly trigger the recovery of the Amazon EC2 instance. Configure an Amazon CloudWatch alarm that triggers the recovery of the Amazon EC2 instance, in case the instance fails. The instance can be configured with Amazon Elastic Block Store (Amazon EBS) or with instance store volumes - The recover action is supported only on instances that have Amazon EBS volumes configured on them, instance store volumes are not supported for automatic recovery by Amazon CloudWatch alarms. Configure AWS Trusted Advisor to monitor the health check of Amazon EC2 instance and provide a remedial action in case an unhealthy flag is detected - You can use Amazon EventBridge events to detect and react to changes in the status of AWS Trusted Advisor checks. This support is only available with AWS Business Support and AWS Enterprise Support. AWS Trusted Advisor by itself does not support health checks of Amazon EC2 instances or their recovery. Reference: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recover.html",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A legacy application is built using a tightly-coupled monolithic architecture. Due to a sharp increase in the number of users, the application performance has degraded. The company now wants to decouple the architecture and adopt AWS microservices architecture. Some of these microservices need to handle fast running processes whereas other microservices need to handle slower processes. Which of these options would you identify as the right way of connecting these microservices?",
      "options": [
        "A. Use Amazon Simple Notification Service (Amazon SNS) to decouple microservices running faster processes from the microservices running slower ones",
        "B. Configure Amazon Kinesis Data Streams to decouple microservices running faster processes from the microservices running slower ones",
        "C. Add Amazon EventBridge to decouple the complex architecture",
        "D. Configure Amazon Simple Queue Service (Amazon SQS) queue to decouple microservices running faster processes from the microservices running slower ones"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: Configure Amazon Simple Queue Service (Amazon SQS) queue to decouple microservices running faster processes from the microservices running slower ones Amazon Simple Queue Service (Amazon SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. Amazon SQS eliminates the complexity and overhead associated with managing and operating message-oriented middleware and empowers developers to focus on differentiating work. Using SQS, you can send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be available. Use Amazon SQS to transmit any volume of data, at any level of throughput, without losing messages or requiring other services to be available. Amazon SQS lets you decouple application components so that they run and fail independently, increasing the overall fault tolerance of the system. Multiple copies of every message are stored redundantly across multiple availability zones so that they are available whenever needed. Being able to store the messages and replay them is a very important feature in decoupling the system architecture, as is needed in the current use case. Incorrect options: Use Amazon Simple Notification Service (Amazon SNS) to decouple microservices running faster processes from the microservices running slower ones - Amazon SNS follows the \"publish-subscribe\" (pub-sub) messaging paradigm, with notifications being delivered to clients using a \"push\" mechanism. This is an important difference between Amazon SNS and Amazon SQS. Whereas Amazon SQS is a polling mechanism, that gives applications the chance to poll at their own comfort, the push mechanism assumes the other applications are present. For the current requirement, we need messages to be stored till they are processed by the downstream applications. Hence, Amazon SQS is the right choice. Configure Amazon Kinesis Data Streams to decouple microservices running faster processes from the microservices running slower ones - Amazon Kinesis Data Streams are used for streaming real-time high-volume data. Amazon Kinesis is a publish-subscribe model, used when publisher applications need to publish the same data to different consumers in parallel. Amazon SQS is the right fit for the current use case. Add Amazon EventBridge to decouple the complex architecture - This event-based service is extremely useful for connecting non-AWS SaaS (Software as a Service) services to AWS services. With Amazon Eventbridge, the downstream application would need to immediately process the events whenever they arrive, thereby making it a tightly coupled scenario. Hence, this option is not correct. References: https://aws.amazon.com/sqs/",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A small business has been running its IT systems on the on-premises infrastructure but the business now plans to migrate to AWS Cloud for operational efficiencies. As a Solutions Architect, can you suggest a cost-effective serverless solution for its flagship application that has both static and dynamic content?",
      "options": [
        "A. Host both the static and dynamic content of the web application on Amazon S3 and use Amazon CloudFront for distribution across diverse regions/countries",
        "B. Host the static content on Amazon S3 and use AWS Lambda with Amazon DynamoDB for the serverless web application that handles dynamic content. Amazon CloudFront will sit in front of AWS Lambda for distribution across diverse regions",
        "C. Host the static content on Amazon S3 and use Amazon EC2 with Amazon RDS for generating the dynamic content. Amazon CloudFront can be configured in front of Amazon EC2 instance, to make global distribution easy",
        "D. Host both the static and dynamic content of the web application on Amazon EC2 with Amazon RDS as database. Amazon CloudFront should be configured to distribute the content across geographically disperse regions"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Host the static content on Amazon S3 and use AWS Lambda with Amazon DynamoDB for the serverless web application that handles dynamic content. Amazon CloudFront will sit in front of AWS Lambda for distribution across diverse regions AWS Lambda with Amazon DynamoDB is the right answer for a serverless solution. Amazon CloudFront will help in enhancing user experience by delivering content, across different geographic locations with low latency. Amazon S3 is a cost-effective and faster way of distributing static content for web applications. Incorrect options: Host both the static and dynamic content of the web application on Amazon S3 and use Amazon CloudFront for distribution across diverse regions/countries - Amazon S3 is not the right fit for hosting Dynamic content, so this option is incorrect. Host the static content on Amazon S3 and use Amazon EC2 with Amazon RDS for generating the dynamic content. Amazon CloudFront can be configured in front of Amazon EC2 instance, to make global distribution easy - The company is looking for a serverless solution, and Amazon EC2 is not a serverless service as the Amazon EC2 instances have to be managed by AWS customers. Host both the static and dynamic content of the web application on Amazon EC2 with Amazon RDS as database. Amazon CloudFront should be configured to distribute the content across geographically disperse regions - This is a possible solution, but not a cost-effective or optimal one. Since static content can be cost-effectively managed on Amazon S3 and can be accessed and distributed faster when compared to fetching the content from the Amazon EC2 server. Reference: https://aws.amazon.com/blogs/networking-and-content-delivery/deliver-your-apps-dynamic-content-using-amazon-cloudfront-getting-started-template/",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "The application maintenance team at a company has noticed that the production application is very slow when the business reports are run on the Amazon RDS database. These reports fetch a large amount of data and have complex queries with multiple joins, spanning across multiple business-critical core tables. CPU, memory, and storage metrics are around 50% of the total capacity. Can you recommend an improved and cost-effective way of generating the business reports while keeping the production application unaffected?",
      "options": [
        "A. Increase the size of Amazon RDS instance",
        "B. Migrate from General Purpose SSD to magnetic storage to enhance IOPS",
        "C. Create a read replica and connect the report generation tool/application to it",
        "D. Configure the Amazon RDS instance to be Multi-AZ DB instance, and connect the report generation tool to the DB instance in a different AZ"
      ],
      "correct_answer": "C",
      "explanation": "Correct option: Create a read replica and connect the report generation tool/application to it Amazon RDS Read Replicas provide enhanced performance and durability for Amazon RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. You can create one or more replicas of a given source DB Instance and serve high-volume application read traffic from multiple copies of your data, thereby increasing aggregate read throughput. Read replicas can also be promoted when needed to become standalone DB instances. There are a variety of scenarios where deploying one or more read replicas for a given source DB instance may make sense. Common reasons for deploying a read replica include: Scaling beyond the compute or I/O capacity of a single DB instance for read-heavy database workloads. This excess read traffic can be directed to one or more read replicas. Serving read traffic while the source DB instance is unavailable. If your source DB Instance cannot take I/O requests (e.g. due to I/O suspension for backups or scheduled maintenance), you can direct read traffic to your read replica(s). For this use case, keep in mind that the data on the read replica may be “stale” since the source DB Instance is unavailable. Business reporting or data warehousing scenarios; you may want business reporting queries to run against a read replica, rather than your primary, production DB Instance. You may use a read replica for disaster recovery of the source DB instance, either in the same AWS Region or in another Region. Comparing Read Replicas with Multi-AZ and Multi-Region Amazon RDS deployments: via - https://aws.amazon.com/rds/features/read-replicas/ Incorrect options: Increase the size of Amazon RDS instance - This will not help as it's mentioned that the CPU, memory, and storage are running at only 50% of the total capacity. Migrate from General Purpose SSD to magnetic storage to enhance IOPS - This is incorrect. Amazon RDS supports magnetic storage for backward compatibility only. AWS recommends that you use General Purpose SSD or Provisioned IOPS for any storage needs. Configure the Amazon RDS instance to be Multi-AZ DB instance, and connect the report generation tool to the DB instance in a different AZ - Amazon RDS Multi-AZ deployments provide enhanced availability and durability for RDS database (DB) instances, making them a natural fit for production database workloads. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). Each AZ runs on its own physically distinct, independent infrastructure, and is engineered to be highly reliable. However, you cannot read from the standby database, making multi-AZ, an incorrect option for the given scenario. Reference: https://aws.amazon.com/rds/features/read-replicas/",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An online gaming application has a large chunk of its traffic coming from users who download static assets such as historic leaderboard reports and the game tactics for various games. The current infrastructure and design are unable to cope up with the traffic and application freezes on most of the pages. Which of the following is a cost-optimal solution that does not need provisioning of infrastructure?",
      "options": [
        "A. Configure AWS Lambda with an Amazon RDS database to provide a serverless architecture",
        "B. Use Amazon CloudFront with Amazon DynamoDB for greater speed and low latency access to static assets",
        "C. Use AWS Lambda with Amazon ElastiCache and Amazon RDS for serving static assets at high speed and low latency",
        "D. Use Amazon CloudFront with Amazon S3 as the storage solution for the static assets"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: Use Amazon CloudFront with Amazon S3 as the storage solution for the static assets When you put your content in an Amazon S3 bucket in the cloud, a lot of things become much easier. First, you don’t need to plan for and allocate a specific amount of storage space because Amazon S3 buckets scale automatically. As Amazon S3 is a serverless service, you don’t need to manage or patch servers that store files yourself; you just put and get your content. Finally, even if you require a server for your application (for example, because you have a dynamic application), the server can be smaller because it doesn’t have to handle requests for static content. Amazon CloudFront is a content delivery network (CDN) service that delivers static and dynamic web content, video streams, and APIs around the world, securely and at scale. By design, delivering data out of Amazon CloudFront can be more cost-effective than delivering it from Amazon S3 directly to your users. Amazon CloudFront serves content through a worldwide network of data centers called Edge Locations. Using edge servers to cache and serve content improves performance by providing content closer to where viewers are located. When a user requests content that you serve with Amazon CloudFront, their request is routed to a nearby Edge Location. If Amazon CloudFront has a cached copy of the requested file, CloudFront delivers it to the user, providing a fast (low-latency) response. If the file they’ve requested isn’t yet cached, CloudFront retrieves it from your origin – for example, the Amazon S3 bucket where you’ve stored your content. Then, for the next local request for the same content, it’s already cached nearby and can be served immediately. By caching your content in Edge Locations, Amazon CloudFront reduces the load on your Amazon S3 bucket and helps ensure a faster response for your users when they request content. Also, data transfer out for content by using Amazon CloudFront is often more cost-effective than serving files directly from Amazon S3, and there is no data transfer fee from Amazon S3 to Amazon CloudFront. You only pay for what is delivered to the internet from Amazon CloudFront, plus request fees. Incorrect options: Configure AWS Lambda with an Amazon RDS database to provide a serverless architecture - Amazon RDS is not the right choice for the current scenario because of the overhead of a database management system, as the given use-case can be addressed by using Amazon S3 storage solution. Use Amazon CloudFront with Amazon DynamoDB for greater speed and low latency access to static assets - Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. But, Amazon DynamoDB is overkill for the given use-case and will prove to be a very costly solution. Use AWS Lambda with Amazon ElastiCache and Amazon RDS for serving static assets at high speed and low latency - As discussed above, Amazon RDS is not needed for this use case where web application needs to display static pages and facilitate downloads of historic data. Amazon S3 is much better suited for this requirement. Reference: https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An IT company hosts windows based applications on its on-premises data center. The company is looking at moving the business to the AWS Cloud. The cloud solution should offer shared storage space that multiple applications can access without a need for replication. Also, the solution should integrate with the company's self-managed Active Directory domain. Which of the following solutions addresses these requirements with the minimal integration effort?",
      "options": [
        "A. Use Amazon FSx for Windows File Server as a shared storage solution",
        "B. Use File Gateway of AWS Storage Gateway to create a hybrid storage solution",
        "C. Use Amazon FSx for Lustre as a shared storage solution with millisecond latencies",
        "D. Use Amazon Elastic File System (Amazon EFS) as a shared storage solution"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Use Amazon FSx for Windows File Server as a shared storage solution Amazon FSx for Windows File Server provides fully managed, highly reliable, and scalable file storage that is accessible over the industry-standard Server Message Block (SMB) protocol. It is built on Windows Server, delivering a wide range of administrative features such as user quotas, end-user file restore, and Microsoft Active Directory (AD) integration. It offers single-AZ and multi-AZ deployment options, fully managed backups, and encryption of data at rest and in transit. You can optimize cost and performance for your workload needs with SSD and HDD storage options; and you can scale storage and change the throughput performance of your file system at any time. With Amazon FSx, you get highly available and durable file storage starting from $0.013 per GB-month. Data deduplication enables you to optimize costs even further by removing redundant data. You can increase your file system storage and scale throughput capacity at any time, making it easy to respond to changing business needs. There are no upfront costs or licensing fees. How Amazon FSx for Windows File Server works: via - https://aws.amazon.com/fsx/windows/ Incorrect options: Use File Gateway of AWS Storage Gateway to create a hybrid storage solution - AWS Storage Gateway connects an on-premises software appliance with cloud-based storage to provide seamless integration between your on-premises IT environment and the AWS storage infrastructure. Storage Gateway uses Amazon S3 to store data on AWS Cloud and from here the on-premises data can seamlessly integrate with Cloud services. It is not suited to be used as a shared storage space that multiple applications can access in parallel. Use Amazon FSx for Lustre as a shared storage solution with millisecond latencies - Amazon FSx for Lustre is a fully managed service that provides cost-effective, high-performance storage for compute workloads. Many workloads such as machine learning, high performance computing (HPC), video rendering, and financial simulations depend on compute instances accessing the same set of data through high-performance shared storage. Lustre is Linux based, hence it is not the right choice since the use case is about Windows-based applications. Use Amazon Elastic File System (Amazon EFS) as a shared storage solution - Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. Amazon EFS is a powerful, shared storage solution that would have been the right answer if the customer systems were Linux based. Amazon EFS is compatible with only Linux-based AMIs for Amazon EC2. Reference: https://aws.amazon.com/fsx/windows/",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A company has its application servers in the public subnet that connect to the Amazon RDS instances in the private subnet. For regular maintenance, the Amazon RDS instances need patch fixes that need to be downloaded from the internet. Considering the company uses only IPv4 addressing and is looking for a fully managed service, which of the following would you suggest as an optimal solution?",
      "options": [
        "A. Configure a Network Address Translation instance (NAT instance) in the public subnet of the VPC",
        "B. Configure a Network Address Translation gateway (NAT gateway) in the public subnet of the VPC",
        "C. Configure the Internet Gateway of the VPC to be accessible to the private subnet resources by changing the route tables",
        "D. Configure an Egress-only internet gateway for the resources in the private subnet of the VPC"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Configure a Network Address Translation gateway (NAT gateway) in the public subnet of the VPC You can use a Network Address Translation gateway (NAT gateway) to enable instances in a private subnet to connect to the internet or other AWS services, but prevent the internet from initiating a connection with those instances. To create a NAT gateway, you must specify the public subnet in which the NAT gateway should reside. You must also specify an Elastic IP address to associate with the NAT gateway when you create it. The Elastic IP address cannot be changed after you associate it with the NAT Gateway. After you've created a NAT gateway, you must update the route table associated with one or more of your private subnets to point internet-bound traffic to the NAT gateway. This enables instances in your private subnets to communicate with the internet. If you no longer need a NAT gateway, you can delete it. Deleting a NAT gateway disassociates its Elastic IP address, but does not release the address from your account. VPC architecture with NAT: via - https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html Incorrect options: Configure an Egress-only internet gateway for the resources in the private subnet of the VPC - An Egress-only internet gateway is an Internet Gateway that supports IPv6 traffic, so this option is not correct for the given use-case. Configure a Network Address Translation instance (NAT instance) in the public subnet of the VPC - You can use a network address translation (NAT) instance in a public subnet in your VPC to enable instances in the private subnet to initiate outbound IPv4 traffic to the internet or other AWS services, but prevent the instances from receiving inbound traffic initiated by someone on the internet. NAT instances are not a managed service, it has to be managed and maintained by the customer. Configure the Internet Gateway of the VPC to be accessible to the private subnet resources by changing the route tables - Internet Gateway cannot be used directly with a private subnet. It is not possible to set up this configuration, without a NAT instance or a NAT gateway in the public subnet. References: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html https://docs.aws.amazon.com/vpc/latest/userguide/VPC_NAT_Instance.html https://docs.aws.amazon.com/vpc/latest/userguide/egress-only-internet-gateway.html",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A gaming company uses Application Load Balancers in front of Amazon EC2 instances for different services and microservices. The architecture has now become complex with too many Application Load Balancers in multiple AWS Regions. Security updates, firewall configurations, and traffic routing logic have become complex with too many IP addresses and configurations. The company is looking at an easy and effective way to bring down the number of IP addresses allowed by the firewall and easily manage the entire network infrastructure. Which of these options represents an appropriate solution for this requirement?",
      "options": [
        "A. Configure Elastic IPs for each of the Application Load Balancers in each Region",
        "B. Set up a Network Load Balancer with elastic IP address. Register the private IPs of all the Application Load Balancers as targets of this Network Load Balancer",
        "C. Launch AWS Global Accelerator and create endpoints for all the Regions. Register the Application Load Balancers of each Region to the corresponding endpoints",
        "D. Assign an Elastic IP to an Auto Scaling Group (ASG), and set up multiple Amazon EC2 instances to run behind the Auto Scaling Groups, for each of the Regions"
      ],
      "correct_answer": "C",
      "explanation": "Correct option: Launch AWS Global Accelerator and create endpoints for all the Regions. Register the Application Load Balancers of each Region to the corresponding endpoints AWS Global Accelerator is a networking service that sends your user’s traffic through Amazon Web Service’s global network infrastructure, improving your internet user performance by up to 60%. When the internet is congested, Global Accelerator’s automatic routing optimizations will help keep your packet loss, jitter, and latency consistently low. With AWS Global Accelerator, you are provided two global static customer-facing IPs to simplify traffic management. On the back end, add or remove your AWS application origins, such as Network Load Balancers, Application Load Balancers, elastic IP address (EIP), and Amazon EC2 Instances, without making user-facing changes. To mitigate endpoint failure, AWS Global Accelerator automatically re-routes your traffic to your nearest healthy available endpoint. Simplified and resilient traffic routing for multi-Region applications: via - https://aws.amazon.com/global-accelerator/ Incorrect options: Configure Elastic IPs for each of the Application Load Balancers in each Region - An Application Load Balancer cannot be assigned an Elastic IP address (static IP address). Set up a Network Load Balancer with elastic IP address. Register the private IPs of all the Application Load Balancers as targets of this Network Load Balancer - A Network Load Balancer can be configured to take an Elastic IP address. However, with hundreds of Application Load Balancers and Network Load Balancers, the solution will be equally cumbersome to manage. Assign an Elastic IP to an Auto Scaling Group (ASG), and set up multiple Amazon EC2 instances to run behind the Auto Scaling Groups, for each of the Regions - You cannot assign an elastic IP address to an Auto Scaling Group (ASG), since ASG just manages a collection of Amazon EC2 instances. References: https://aws.amazon.com/global-accelerator/ https://aws.amazon.com/blogs/networking-and-content-delivery/using-static-ip-addresses-for-application-load-balancers/",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A health care application processes the real-time health data of the patients into an analytics workflow. With a sharp increase in the number of users, the system has become slow and sometimes even unresponsive as it does not have a retry mechanism. The startup is looking at a scalable solution that has minimal implementation overhead. Which of the following would you recommend as a scalable alternative to the current solution?",
      "options": [
        "A. Use Amazon Simple Notification Service (Amazon SNS) for data ingestion and configure AWS Lambda to trigger logic for downstream processing",
        "B. Use Amazon Simple Queue Service (Amazon SQS) for data ingestion and configure AWS Lambda to trigger logic for downstream processing",
        "C. Use Amazon API Gateway with the existing REST-based interface to create a high performing architecture",
        "D. Use Amazon Kinesis Data Streams to ingest the data, process it using AWS Lambda or run analytics using Amazon Kinesis Data Analytics"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: Use Amazon Kinesis Data Streams to ingest the data, process it using AWS Lambda or run analytics using Amazon Kinesis Data Analytics Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service with support for retry mechanism. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more. KDS makes sure your streaming data is available to multiple real-time analytics applications, to Amazon S3, or AWS Lambda within 70 milliseconds of the data being collected. Amazon Kinesis data streams scale from megabytes to terabytes per hour and scale from thousands to millions of PUT records per second. You can dynamically adjust the throughput of your stream at any time based on the volume of your input data. How Data Streams work: via - https://aws.amazon.com/kinesis/data-streams/?nc=sn&loc=2&dn=2 Incorrect options: Use Amazon Simple Notification Service (Amazon SNS) for data ingestion and configure AWS Lambda to trigger logic for downstream processing - Amazon Simple Notification Service (Amazon SNS) is a fully managed messaging service for both application-to-application (A2A) and application-to-person (A2P) communication. Amazon SNS is a push mechanism that does not support robust retry mechanisms, as is needed in the current use case. Use Amazon Simple Queue Service (Amazon SQS) for data ingestion and configure AWS Lambda to trigger logic for downstream processing - Amazon Simple Queue Service (Amazon SQS) is a messaging service that helps in decoupling systems and reducing the complexity of architecture. Amazon SQS can still work but Amazon Kinesis Data streams is custom made for streaming real-time data. Use Amazon API Gateway with the existing REST-based interface to create a high performing architecture - Amazon API Gateway is not meant for handling real-time streaming data. Reference: https://aws.amazon.com/kinesis/data-streams/?nc=sn&loc=2&dn=2",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An e-commerce company has deployed its application on several Amazon EC2 instances that are configured in a private subnet using IPv4. These Amazon EC2 instances read and write a huge volume of data to and from Amazon S3 in the same AWS region. The company has set up subnet routing to direct all the internet-bound traffic through a Network Address Translation gateway (NAT gateway). The company wants to build the most cost-optimal solution without impacting the application's ability to communicate with Amazon S3 or the internet. As an AWS Certified Solutions Architect Associate, which of the following would you recommend?",
      "options": [
        "A. Set up a VPC gateway endpoint for Amazon S3. Attach an endpoint policy to the endpoint. Update the route table to direct the S3-bound traffic to the VPC endpoint",
        "B. Provision an internet gateway. Update the route table in the private subnet to route traffic to the internet gateway. Update the network ACL (NACL) to allow the S3-bound traffic",
        "C. Set up an egress-only internet gateway in the public subnet. Update the route table in the private subnet to route traffic to the internet gateway. Update the network ACL to allow the S3-bound traffic",
        "D. Set up a Gateway Load Balancer (GWLB) endpoint for Amazon S3. Update the route table in the private subnet to direct the S3-bound traffic via the Gateway Load Balancer (GWLB) endpoint"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Set up a VPC gateway endpoint for Amazon S3. Attach an endpoint policy to the endpoint. Update the route table to direct the S3-bound traffic to the VPC endpoint Gateway endpoints provide reliable connectivity to Amazon S3 without requiring an internet gateway or a NAT device for your VPC. After you create the gateway endpoint, you can add it as a target in your route table for traffic destined from your VPC to Amazon S3. There is no additional charge for using gateway endpoints. The VPC endpoint policy for the gateway endpoint controls access to Amazon S3 from the VPC through the endpoint. The default policy allows full access. via - https://docs.aws.amazon.com/vpc/latest/privatelink/gateway-endpoints.html Using the VPC gateway endpoint allows the Amazon EC2 instances to reach Amazon S3 without using the public internet. Since the data transfer remains within the same AWS region, so there is no data transfer costs for ingress as well as egress traffic. Hence this is the most cost-optimal solution. Incorrect options: Provision an internet gateway. Update the route table in the private subnet to route traffic to the internet gateway. Update the network ACL (NACL) to allow the S3-bound traffic - If a subnet is associated with a route table that has a route to an internet gateway, it's known as a public subnet. If a subnet is associated with a route table that does not have a route to an internet gateway, it's known as a private subnet. This option has been added as a distractor as adding a route to the internet gateway in the route table associated with the private subnet would make the subnet public. This would also make the internet-bound routing to the NAT gateway redundant. This option has been added as a distractor. Set up an egress-only internet gateway in the public subnet. Update the route table in the private subnet to route traffic to the internet gateway. Update the network ACL to allow the S3-bound traffic - An egress-only internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows outbound communication over IPv6 from instances in your VPC to the internet, and prevents the internet from initiating an IPv6 connection with your instances. Since the use case talks about only IPv4 traffic, so this option is incorrect. Set up a Gateway Load Balancer (GWLB) endpoint for Amazon S3. Update the route table in the private subnet to direct the S3-bound traffic via the Gateway Load Balancer (GWLB) endpoint - Gateway Load Balancers use Gateway Load Balancer endpoints to securely exchange traffic across VPC boundaries. A Gateway Load Balancer endpoint is a VPC endpoint that provides private connectivity between virtual appliances in the service provider VPC and application servers in the service consumer VPC. You cannot set up a gateway load balancer endpoint to access Amazon S3. This option has been added as a distractor. References: https://docs.aws.amazon.com/vpc/latest/privatelink/gateway-endpoints.html https://aws.amazon.com/premiumsupport/knowledge-center/vpc-reduce-nat-gateway-transfer-costs/ https://docs.aws.amazon.com/vpc/latest/privatelink/vpce-gateway-load-balancer.html https://docs.aws.amazon.com/vpc/latest/userguide/egress-only-internet-gateway.html",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An IT training company hosted its website on Amazon S3 a couple of years ago. Due to COVID-19 related travel restrictions, the training website has suddenly gained traction. With an almost 300% increase in the requests served per day, the company's AWS costs have sky-rocketed for just the Amazon S3 outbound data costs. As a Solutions Architect, can you suggest an alternate method to reduce costs while keeping the latency low?",
      "options": [
        "A. To reduce Amazon S3 cost, the data can be saved on an Amazon EBS volume connected to an Amazon EC2 instance that can host the application",
        "B. Use Amazon EFS service, as it provides a shared, scalable, fully managed elastic NFS file system for storing AWS Cloud or on-premises data",
        "C. Configure Amazon CloudFront to distribute the data hosted on Amazon S3 cost-effectively",
        "D. Configure Amazon S3 Batch Operations to read data in bulk at one go, to reduce the number of calls made to Amazon S3 buckets"
      ],
      "correct_answer": "C",
      "explanation": "Correct option: Configure Amazon CloudFront to distribute the data hosted on Amazon S3 cost-effectively Storing content with Amazon S3 provides a lot of advantages. But to help optimize your application’s performance and security while effectively managing cost, AWS recommends that you also set up Amazon CloudFront to work with your Amazon S3 bucket to serve and protect the content. Amazon CloudFront is a content delivery network (CDN) service that delivers static and dynamic web content, video streams, and APIs around the world, securely and at scale. By design, delivering data out of Amazon CloudFront can be more cost-effective than delivering it from Amazon S3 directly to your users. Amazon CloudFront serves content through a worldwide network of data centers called Edge Locations. Using edge servers to cache and serve content improves performance by providing content closer to where viewers are located. Amazon CloudFront has edge servers in locations all around the world. When a user requests content that you serve with Amazon CloudFront, their request is routed to a nearby Edge Location. If CloudFront has a cached copy of the requested file, CloudFront delivers it to the user, providing a fast (low-latency) response. If the file they’ve requested isn’t yet cached, Amazon CloudFront retrieves it from your origin – for example, the S3 bucket where you’ve stored your content. Then, for the next local request for the same content, it’s already cached nearby and can be served immediately. By caching your content in Edge Locations, Amazon CloudFront reduces the load on your Amazon S3 bucket and helps ensure a faster response for your users when they request content. Also, data transfer out for content by using CloudFront is often more cost-effective than serving files directly from Amazon S3, and there is no data transfer fee from Amazon S3 to CloudFront. You only pay for what is delivered to the internet from Amazon CloudFront, plus request fees. Incorrect options: To reduce Amazon S3 cost, the data can be saved on an Amazon EBS volume connected to an Amazon EC2 instance that can host the application - Amazon EBS volumes are fast and are relatively cheap (though Amazon S3 is still a cheaper alternative). But, Amazon EBS volumes are accessible only through Amazon EC2 instances and are bound to a specific region. Use Amazon EFS service, as it provides a shared, scalable, fully managed elastic NFS file system for storing AWS Cloud or on-premises data - Amazon EFS is a shareable file system that can be mounted onto Amazon EC2 instances. Amazon EFS is costlier than Amazon EBS and not a solution if the company is looking at reducing costs. Configure Amazon S3 Batch Operations to read data in bulk at one go, to reduce the number of calls made to Amazon S3 buckets - This statement is incorrect and given only as a distractor. You can use Amazon S3 Batch Operations to perform large-scale batch operations on Amazon S3 objects, and it has nothing to do with content distribution. Reference: https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A data analytics company manages an application that stores user data in a Amazon DynamoDB table. The development team has observed that once in a while, the application writes corrupted data in the Amazon DynamoDB table. As soon as the issue is detected, the team needs to remove the corrupted data at the earliest. What do you recommend?",
      "options": [
        "A. Configure the Amazon DynamoDB table as a global table and point the application to use the table from another AWS region that has no corrupted data",
        "B. Use Amazon DynamoDB Streams to restore the table to the state just before corrupted data was written",
        "C. Use Amazon DynamoDB on-demand backup to restore the table to the state just before corrupted data was written",
        "D. Use Amazon DynamoDB point in time recovery to restore the table to the state just before corrupted data was written"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: Use Amazon DynamoDB point in time recovery to restore the table to the state just before corrupted data was written Amazon DynamoDB enables you to back up your table data continuously by using point-in-time recovery (PITR). When you enable PITR, DynamoDB backs up your table data automatically with per-second granularity so that you can restore to any given second in the preceding 35 days. PITR helps protect you against accidental writes and deletes. For example, if a test script writes accidentally to a production DynamoDB table or someone mistakenly issues a \"DeleteItem\" call, PITR has you covered. Incorrect options: Use Amazon DynamoDB on-demand backup to restore the table to the state just before corrupted data was written - The on-demand backup and restore process scales without degrading the performance or availability of your applications. It uses a new and unique distributed technology that lets you complete backups in seconds regardless of table size. You can create backups that are consistent within seconds across thousands of partitions without worrying about schedules or long-running backup processes. All on-demand backups are cataloged, discoverable, and retained until they are explicitly deleted. On-demand backup is created upon request. So this option is not correct since an on-demand backup cannot be created pre-emptively to handle data corruption issues that happen once in a while. Configure the Amazon DynamoDB table as a global table and point the application to use the table from another AWS region that has no corrupted data - Global tables build on the global Amazon DynamoDB footprint to provide you with a fully managed, multi-Region, and multi-active database that delivers fast, local, read and write performance for massively scaled, global applications. Global tables eliminate the difficult work of replicating data between Regions and resolving update conflicts, enabling you to focus on your application's business logic. In addition, global tables enable your applications to stay highly available even in the unlikely event of isolation or degradation of an entire Region. Any changes made to any item in any replica table are replicated to all the other replicas within the same global table. In a global table, a newly written item is usually propagated to all replica tables within a second. With a global table, each replica table stores the same set of data items. Amazon DynamoDB does not support partial replication of only some of the items. If applications update the same item in different Regions at about the same time, conflicts can arise. To help ensure eventual consistency, Amazon DynamoDB global tables use a last-writer-wins reconciliation between concurrent updates, in which DynamoDB makes its best effort to determine the last writer. With this conflict resolution mechanism, all replicas agree on the latest update and converge toward a state in which they all have identical data. Global tables replicate your Amazon DynamoDB tables automatically across your choice of AWS Regions. This option has been added as a distractor since you cannot point the application to use the table from another AWS region, since there is no \"other\" table in another region. It's just a single logical Global table. Use Amazon DynamoDB Streams to restore the table to the state just before corrupted data was written - Amazon DynamoDB Streams captures a time-ordered sequence of item-level modifications in any Amazon DynamoDB table and stores this information in a log for up to 24 hours. Applications can access this log and view the data items as they appeared before and after they were modified, in near-real time. A DynamoDB stream is an ordered flow of information about changes to items in a Amazon DynamoDB table. When you enable a stream on a table, DynamoDB captures information about every modification to data items in the table. Amazon DynamoDB Streams writes stream records in near-real time so that you can build applications that consume these streams and take action based on the contents. It will take considerable effort and custom coding to reliably rebuild table data to the state just before any corrupted data was written. So this option is not the best fit. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/BackupRestore.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/PointInTimeRecovery_Howitworks.html https://aws.amazon.com/dynamodb/global-tables/ https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "The database backend for a retail company's website is hosted on Amazon RDS for MySQL having a primary instance and three read replicas to support read scalability. The company has mandated that the read replicas should lag no more than 1 second behind the primary instance to provide the best possible user experience. The read replicas are falling further behind during periods of peak traffic spikes, resulting in a bad user experience as the searches produce inconsistent results. You have been hired as an AWS Certified Solutions Architect - Associate to reduce the replication lag as much as possible with minimal changes to the application code or the effort required to manage the underlying resources. Which of the following will you recommend?",
      "options": [
        "A. Host the MySQL primary database on a memory-optimized Amazon EC2 instance. Spin up additional compute-optimized Amazon EC2 instances to host the read replicas",
        "B. Set up an Amazon ElastiCache for Redis cluster in front of the MySQL database. Update the website to check the cache before querying the read replicas",
        "C. Set up database migration from Amazon RDS MySQL to Amazon DynamoDB. Provision a large number of read capacity units (RCUs) to support the required throughput and enable Auto-Scaling",
        "D. Set up database migration from Amazon RDS MySQL to Amazon Aurora MySQL. Swap out the MySQL read replicas with Aurora Replicas. Configure Aurora Auto Scaling"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: Set up database migration from Amazon RDS MySQL to Amazon Aurora MySQL. Swap out the MySQL read replicas with Aurora Replicas. Configure Aurora Auto Scaling Aurora features a distributed, fault-tolerant, and self-healing storage system that is decoupled from compute resources and auto-scales up to 128 TiB per database instance. It delivers high performance and availability with up to 15 low-latency read replicas, point-in-time recovery, continuous backup to Amazon Simple Storage Service (Amazon S3), and replication across three Availability Zones (AZs). Since Amazon Aurora Replicas share the same data volume as the primary instance in the same AWS Region, there is virtually no replication lag. The replica lag times are in the 10s of milliseconds (compared to the replication lag of seconds in the case of MySQL read replicas). Therefore, this is the right option to ensure that the read replicas lag no more than 1 second behind the primary instance. Aurora Replicas: via - https://aws.amazon.com/rds/aurora/faqs/ Incorrect options: Host the MySQL primary database on a memory-optimized Amazon EC2 instance. Spin up additional compute-optimized Amazon EC2 instances to host the read replicas - Hosting the MySQL primary database and the read replicas on the Amazon EC2 instances would result in significant overhead to manage the underlying resources such as OS patching, database patching, etc. So this option is incorrect. Set up an Amazon ElastiCache for Redis cluster in front of the MySQL database. Update the website to check the cache before querying the read replicas - Introducing a caching layer would result in significant changes to the application code, so this option is incorrect. Set up database migration from Amazon RDS MySQL to Amazon DynamoDB. Provision a large number of read capacity units (RCUs) to support the required throughput and enable Auto-Scaling - Introducing a NoSQL database, such as Amazon DynamoDB, would result in significant changes to the application code since the database queries would have to be re-written for Amazon DynamoDB. Therefore, this option is incorrect. Reference: https://aws.amazon.com/rds/aurora/faqs/",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "The development team at a retail company wants to optimize the cost of Amazon EC2 instances. The team wants to move certain nightly batch jobs to spot instances. The team has hired you as a solutions architect to provide the initial guidance. Which of the following would you identify as CORRECT regarding the capabilities of spot instances? (Select three)",
      "options": [
        "A. When you cancel an active spot request, it terminates the associated instance as well",
        "B. If a spot request is persistent, then it is opened again after your Spot Instance is interrupted",
        "C. If a spot request is persistent, then it is opened again after you stop the Spot Instance",
        "D. Spot Fleets can maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated",
        "E. When you cancel an active spot request, it does not terminate the associated instance",
        "F. Spot Fleets cannot maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated"
      ],
      "correct_answer": "B",
      "explanation": "Correct options: If a spot request is persistent, then it is opened again after your Spot Instance is interrupted A Spot Instance is an unused Amazon EC2 instance that is available for less than the On-Demand price. Because Spot Instances enable you to request unused Amazon EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly. The hourly price for a Spot Instance is called a Spot price. The Spot price of each instance type in each Availability Zone is set by Amazon EC2 and adjusted gradually based on the long-term supply of and demand for Spot Instances. A Spot Instance request is either one-time or persistent. If the spot request is persistent, the request is opened again after your Spot Instance is interrupted. If the request is persistent and you stop your Spot Instance, the request only opens after you start your Spot Instance. How Spot requests work: via - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-requests.html Spot Fleets can maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated A Spot Fleet is a set of Spot Instances and optionally On-Demand Instances that is launched based on criteria that you specify. The Spot Fleet selects the Spot capacity pools that meet your needs and launches Spot Instances to meet the target capacity for the fleet. By default, Spot Fleets are set to maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated. You can submit a Spot Fleet as a one-time request, which does not persist after the instances have been terminated. You can include On-Demand Instance requests in a Spot Fleet request. When you cancel an active spot request, it does not terminate the associated instance If your Spot Instance request is active and has an associated running Spot Instance, or your Spot Instance request is disabled and has an associated stopped Spot Instance, canceling the request does not terminate the instance; you must terminate the running Spot Instance manually. Moreover, to cancel a persistent Spot request and terminate its Spot Instances, you must cancel the Spot request first and then terminate the Spot Instances. Incorrect options: When you cancel an active spot request, it terminates the associated instance as well - If your Spot Instance request is active and has an associated running Spot Instance, then canceling the request does not terminate the instance; you must terminate the running Spot Instance manually. So, this option is incorrect. If a spot request is persistent, then it is opened again after you stop the Spot Instance - If the request is persistent and you stop your Spot Instance, the request only opens after you start your Spot Instance. So, this option is incorrect. Spot Fleets cannot maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated - As mentioned above, Spot Fleets can maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated. References: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-requests.html https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-fleet.html",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A DevOps engineer at an IT company just upgraded an Amazon EC2 instance type from t2.nano (0.5G of RAM, 1 vCPU) to u-12tb1.metal (12.3 TB of RAM, 448 vCPUs). How would you categorize this upgrade?",
      "options": [
        "A. This is a scale up example of vertical scalability",
        "B. This is a scale out example of vertical scalability",
        "C. This is a scale up example of horizontal scalability",
        "D. This is an example of high availability"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: This is a scale up example of vertical scalability Vertical scalability means increasing the size of the instance. For example, your application runs on a t2.micro. Scaling up that application vertically means running it on a larger instance such as t2.large. Scaling down that application vertically means running it on a smaller instance such as t2.nano. Scalability is very common for non-distributed systems, such as a database. There’s usually a limit to how much you can vertically scale (hardware limit). In this case, as the instance type was upgraded from t2.nano to u-12tb1.metal, this is a scale up example of vertical scalability. Incorrect options: This is a scale up example of horizontal scalability - Horizontal Scalability means increasing the number of instances/systems for your application. When you increase the number of instances, it's called scale out whereas if you decrease the number of instances, it's called scale-in. Scale up is used in conjunction with vertical scaling and not with horizontal scaling. Hence this is incorrect. This is a scale out example of vertical scalability - Scale out is used in conjunction with horizontal scaling and not with vertical scaling. Hence this is incorrect. This is an example of high availability - High availability means running your application/system in at least 2 data centers (== Availability Zones). The goal of high availability is to survive a data center loss. An example of High Availability is when you run instances for the same application across multi AZ. This option has been added as a distractor.",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A retail company has its flagship application running on a fleet of Amazon EC2 instances behind Elastic Load Balancing (ELB). The engineering team has been seeing recurrent issues wherein the in-flight requests from the ELB to the Amazon EC2 instances are getting dropped when an instance becomes unhealthy. Which of the following features can be used to address this issue?",
      "options": [
        "A. Cross Zone load balancing",
        "B. Connection Draining",
        "C. Sticky Sessions",
        "D. Idle Timeout"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Connection Draining To ensure that Elastic Load Balancing stops sending requests to instances that are de-registering or unhealthy while keeping the existing connections open, use connection draining. This enables the load balancer to complete in-flight requests made to instances that are de-registering or unhealthy. The maximum timeout value can be set between 1 and 3,600 seconds (the default is 300 seconds). When the maximum time limit is reached, the load balancer forcibly closes connections to the de-registering instance. Incorrect options: Cross Zone load balancing - The nodes for your load balancer distribute requests from clients to registered targets. When cross-zone load balancing is enabled, each load balancer node distributes traffic across the registered targets in all enabled Availability Zones (AZs). Cross Zone load balancing cannot be used to complete in-flight requests made to instances that are de-registering or unhealthy. Sticky Sessions - You can use the sticky session feature (also known as session affinity) to enable the load balancer to bind a user's session to a specific instance. This ensures that all requests from the user during the session are sent to the same instance. Sticky sessions cannot be used to complete in-flight requests made to instances that are de-registering or unhealthy. Idle Timeout - For each request that a client makes through Elastic Load Balancing, the load balancer maintains two connections. The front-end connection is between the client and the load balancer. The back-end connection is between the load balancer and a registered Amazon EC2 instance. The load balancer has a configured \"idle timeout\" period that applies to its connections. If no data has been sent or received by the time that the \"idle timeout\" period elapses, the load balancer closes the connection. \"Idle timeout\" can not be used to complete in-flight requests made to instances that are de-registering or unhealthy. Reference: https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/config-conn-drain.html",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An e-commerce company runs its web application on Amazon EC2 instances in an Auto Scaling group and it's configured to handle consumer orders in an Amazon Simple Queue Service (Amazon SQS) queue for downstream processing. The DevOps team has observed that the performance of the application goes down in case of a sudden spike in orders received. As a solutions architect, which of the following solutions would you recommend to address this use-case?",
      "options": [
        "A. Use a target tracking scaling policy based on a custom Amazon SQS queue metric",
        "B. Use a simple scaling policy based on a custom Amazon SQS queue metric",
        "C. Use a step scaling policy based on a custom Amazon SQS queue metric",
        "D. Use a scheduled scaling policy based on a custom Amazon SQS queue metric"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Use a target tracking scaling policy based on a custom Amazon SQS queue metric If you use a target tracking scaling policy based on a custom Amazon SQS queue metric, dynamic scaling can adjust to the demand curve of your application more effectively. You may use an existing CloudWatch Amazon SQS metric like ApproximateNumberOfMessagesVisible for target tracking but you could still face an issue so that the number of messages in the queue might not change proportionally to the size of the Auto Scaling group that processes messages from the queue. The solution is to use a backlog per instance metric with the target value being the acceptable backlog per instance to maintain. To calculate your backlog per instance, divide the ApproximateNumberOfMessages queue attribute by the number of instances in the InService state for the Auto Scaling group. Then set a target value for the Acceptable backlog per instance. To illustrate with an example, let's say that the current ApproximateNumberOfMessages is 1500 and the fleet's running capacity is 10. If the average processing time is 0.1 seconds for each message and the longest acceptable latency is 10 seconds, then the acceptable backlog per instance is 10 / 0.1, which equals 100. This means that 100 is the target value for your target tracking policy. If the backlog per instance is currently at 150 (1500 / 10), your fleet scales out, and it scales out by five instances to maintain proportion to the target value. Scaling Based on Amazon SQS: via - https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html Incorrect options: Use a simple scaling policy based on a custom Amazon SQS queue metric - With simple scaling, you choose scaling metrics and threshold values for the Amazon CloudWatch alarms that trigger the scaling process. The main issue with simple scaling is that after a scaling activity is started, the policy must wait for the scaling activity or health check replacement to complete and the cooldown period to expire before responding to additional alarms. This implies that the application would not be able to react quickly to sudden spikes in orders. Use a step scaling policy based on a custom Amazon SQS queue metric - With step scaling, you choose scaling metrics and threshold values for the Amazon CloudWatch alarms that trigger the scaling process. When step adjustments are applied, they increase or decrease the current capacity of your Auto Scaling group, and the adjustments vary based on the size of the alarm breach. For the given use-case, step scaling would try to approximate the correct number of instances by increasing/decreasing the steps as per the policy. This is not as efficient as the target tracking policy where you can calculate the exact number of instances required to handle the spike in orders. Use a scheduled scaling policy based on a custom Amazon SQS queue metric - Scheduled scaling allows you to set your scaling schedule. For example, let's say that every week the traffic to your web application starts to increase on Wednesday, remains high on Thursday, and starts to decrease on Friday. You can plan your scaling actions based on the predictable traffic patterns of your web application. Scaling actions are performed automatically as a function of time and date. You cannot use scheduled scaling policies to address the sudden spike in orders. Reference: https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "The DevOps team at a multi-national company is helping its subsidiaries standardize Amazon EC2 instances by using the same Amazon Machine Image (AMI). Some of these subsidiaries are in the same AWS region but use different AWS accounts whereas others are in different AWS regions but use the same AWS account as the parent company. The DevOps team has hired you as a solutions architect for this project. Which of the following would you identify as CORRECT regarding the capabilities of an Amazon Machine Image (AMI)? (Select three)",
      "options": [
        "A. You cannot copy an Amazon Machine Image (AMI) across AWS Regions",
        "B. You cannot share an Amazon Machine Image (AMI) with another AWS account",
        "C. Copying an Amazon Machine Image (AMI) backed by an encrypted snapshot results in an unencrypted target snapshot",
        "D. You can copy an Amazon Machine Image (AMI) across AWS Regions",
        "E. You can share an Amazon Machine Image (AMI) with another AWS account",
        "F. Copying an Amazon Machine Image (AMI) backed by an encrypted snapshot cannot result in an unencrypted target snapshot"
      ],
      "correct_answer": "D",
      "explanation": "Correct options: You can copy an Amazon Machine Image (AMI) across AWS Regions You can share an Amazon Machine Image (AMI) with another AWS account Copying an Amazon Machine Image (AMI) backed by an encrypted snapshot cannot result in an unencrypted target snapshot An Amazon Machine Image (AMI) provides the information required to launch an instance. An AMI includes the following: One or more Amazon EBS snapshots, or, for instance-store-backed AMIs, a template for the root volume of the instance. Launch permissions that control which AWS accounts can use the AMI to launch instances. A block device mapping that specifies the volumes to attach to the instance when it's launched. You can copy an AMI within or across AWS Regions using the AWS Management Console, the AWS Command Line Interface or SDKs, or the Amazon EC2 API, all of which support the CopyImage action. You can copy both Amazon EBS-backed AMIs and instance-store-backed AMIs. You can copy AMIs with encrypted snapshots and also change encryption status during the copy process. Therefore, the option - \"You can copy an AMI across AWS Regions\" - is correct. Copying AMIs across regions: via - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html The following table shows encryption support for various AMI-copying scenarios. While it is possible to copy an unencrypted snapshot to yield an encrypted snapshot, you cannot copy an encrypted snapshot to yield an unencrypted one. Therefore, the option - \"Copying an AMI backed by an encrypted snapshot cannot result in an unencrypted target snapshot\" is correct. via - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html You can share an AMI with another AWS account. To copy an AMI that was shared with you from another account, the owner of the source AMI must grant you read permissions for the storage that backs the AMI, either the associated Amazon EBS snapshot (for an Amazon EBS-backed AMI) or an associated S3 bucket (for an instance store-backed AMI). Therefore, the option - \"You can share an AMI with another AWS account\" - is correct. Incorrect options: You cannot copy an Amazon Machine Image (AMI) across AWS Regions You cannot share an Amazon Machine Image (AMI) with another AWS account Copying an Amazon Machine Image (AMI) backed by an encrypted snapshot results in an unencrypted target snapshot These three options contradict the details provided in the explanation above. Reference: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A financial services company wants to move the Windows file server clusters out of their datacenters. They are looking for cloud file storage offerings that provide full Windows compatibility. Can you identify the AWS storage services that provide highly reliable file storage that is accessible over the industry-standard Server Message Block (SMB) protocol compatible with Windows systems? (Select two)",
      "options": [
        "A. Amazon Elastic File System (Amazon EFS)",
        "B. Amazon Elastic Block Store (Amazon EBS)",
        "C. Amazon FSx for Windows File Server",
        "D. Amazon Simple Storage Service (Amazon S3)",
        "E. File Gateway Configuration of AWS Storage Gateway"
      ],
      "correct_answer": "C",
      "explanation": "Correct options: Amazon FSx for Windows File Server Amazon FSx for Windows File Server is a fully managed, highly reliable file storage that is accessible over the industry-standard Server Message Block (SMB) protocol. It is built on Windows Server, delivering a wide range of administrative features such as user quotas, end-user file restore, and Microsoft Active Directory (AD) integration. File Gateway Configuration of AWS Storage Gateway Depending on the use case, AWS Storage Gateway provides 3 types of storage interfaces for on-premises applications: File, Volume, and Tape. The File Gateway enables you to store and retrieve objects in Amazon S3 using file protocols such as Network File System (NFS) and Server Message Block (SMB). Incorrect options: Amazon Elastic File System (Amazon EFS) - Amazon EFS is a file storage service for use with Amazon EC2. Amazon EFS provides a file system interface, file system access semantics, and concurrently-accessible storage for up to thousands of Amazon EC2 instances. Amazon EFS uses the Network File System protocol. EFS does not support SMB protocol. Amazon Elastic Block Store (Amazon EBS) - Amazon EBS is a block-level storage service for use with Amazon EC2. Amazon EBS can deliver performance for workloads that require the lowest latency access to data from a single EC2 instance. EBS does not support SMB protocol. Amazon Simple Storage Service (Amazon S3) - Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. Amazon S3 provides a simple, standards-based REST web services interface that is designed to work with any Internet-development toolkit. S3 does not support SMB protocol. References: https://aws.amazon.com/fsx/windows/ https://aws.amazon.com/storagegateway/file/",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "The engineering team at a social media company wants to use Amazon CloudWatch alarms to automatically recover Amazon EC2 instances if they become impaired. The team has hired you as a solutions architect to provide subject matter expertise. As a solutions architect, which of the following statements would you identify as CORRECT regarding this automatic recovery process? (Select two)",
      "options": [
        "A. Terminated Amazon EC2 instances can be recovered if they are configured at the launch of instance",
        "B. A recovered instance is identical to the original instance, including the instance ID, private IP addresses, Elastic IP addresses, and all instance metadata",
        "C. If your instance has a public IPv4 address, it retains the public IPv4 address after recovery",
        "D. During instance recovery, the instance is migrated during an instance reboot, and any data that is in-memory is retained",
        "E. If your instance has a public IPv4 address, it does not retain the public IPv4 address after recovery"
      ],
      "correct_answer": "B",
      "explanation": "Correct options: A recovered instance is identical to the original instance, including the instance ID, private IP addresses, Elastic IP addresses, and all instance metadata If your instance has a public IPv4 address, it retains the public IPv4 address after recovery You can create an Amazon CloudWatch alarm to automatically recover the Amazon EC2 instance if it becomes impaired due to an underlying hardware failure or a problem that requires AWS involvement to repair. Terminated instances cannot be recovered. A recovered instance is identical to the original instance, including the instance ID, private IP addresses, Elastic IP addresses, and all instance metadata. If the impaired instance is in a placement group, the recovered instance runs in the placement group. If your instance has a public IPv4 address, it retains the public IPv4 address after recovery. During instance recovery, the instance is migrated during an instance reboot, and any data that is in-memory is lost. Incorrect options: Terminated Amazon EC2 instances can be recovered if they are configured at the launch of instance - This is incorrect as terminated instances cannot be recovered. During instance recovery, the instance is migrated during an instance reboot, and any data that is in-memory is retained - As mentioned above, during instance recovery, the instance is migrated during an instance reboot, and any data that is in-memory is lost. If your instance has a public IPv4 address, it does not retain the public IPv4 address after recovery - As mentioned above, if your instance has a public IPv4 address, it retains the public IPv4 address after recovery. Reference: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recover.html",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A leading bank has moved its IT infrastructure to AWS Cloud and they have been using Amazon EC2 Auto Scaling for their web servers. This has helped them deal with traffic spikes effectively. But, their MySQL relational database has now become a bottleneck and they urgently need a fully managed auto scaling solution for their relational database to address any unpredictable changes in the traffic. Can you identify the AWS service that is best suited for this use-case?",
      "options": [
        "A. Amazon DynamoDB",
        "B. Amazon ElastiCache",
        "C. Amazon Aurora Serverless",
        "D. Amazon Aurora"
      ],
      "correct_answer": "C",
      "explanation": "Correct options: Amazon Aurora Serverless Amazon Aurora Serverless is an on-demand, auto-scaling configuration for Amazon Aurora (MySQL-compatible and PostgreSQL-compatible editions), where the database will automatically start-up, shut down, and scale capacity up or down based on your application's needs. It enables you to run your database in the cloud without managing any database instances. It's a simple, cost-effective option for infrequent, intermittent, or unpredictable workloads. You pay on a per-second basis for the database capacity you use when the database is active and migrate between standard and serverless configurations with a few clicks in the Amazon RDS Management Console. Incorrect options: Amazon DynamoDB - Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications. DynamoDB can handle more than 10 trillion requests per day and can support peaks of more than 20 million requests per second. But, it is a NoSQL database service and hence not a fit for the given use-case. Amazon ElastiCache - Amazon ElastiCache allows you to set up popular open-Source compatible in-memory data stores in the cloud. You can build data-intensive apps or boost the performance of your existing databases by retrieving data from high throughput and low latency in-memory data stores such as Redis and Memcached. Elasticache is used as a caching layer. It's not a fully managed MySQL database. Amazon Aurora - Amazon Aurora is a MySQL and PostgreSQL-compatible relational database built for the cloud, that combines the performance and availability of traditional enterprise databases with the simplicity and cost-effectiveness of open source databases. Amazon Aurora is up to five times faster than standard MySQL databases and three times faster than standard PostgreSQL databases. Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 124 TB per database instance. But, its not a complete auto scaling solution and neither is it fully managed like Aurora serverless. Hence is not the right fit for the given use-case. Reference: https://aws.amazon.com/rds/aurora/serverless/",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An AWS Organization is using Service Control Policies (SCPs) for central control over the maximum available permissions for all accounts in their organization. This allows the organization to ensure that all accounts stay within the organization’s access control guidelines. Which of the given scenarios are correct regarding the permissions described below? (Select three)",
      "options": [
        "A. If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable service control policy (SCP), the user or role can't perform that action",
        "B. If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable service control policy (SCP), the user or role can still perform that action",
        "C. Service control policy (SCP) affects all users and roles in the member accounts, including root user of the member accounts",
        "D. Service control policy (SCP) affects all users and roles in the member accounts, excluding root user of the member accounts",
        "E. Service control policy (SCP) affects service-linked roles",
        "F. Service control policy (SCP) does not affect service-linked role"
      ],
      "correct_answer": "A",
      "explanation": "Correct options: If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable service control policy (SCP), the user or role can't perform that action Service control policy (SCP) affects all users and roles in the member accounts, including root user of the member accounts Service control policy (SCP) does not affect service-linked role Service control policy (SCP) are one type of policy that can be used to manage your organization. Service control policy (SCP) offers central control over the maximum available permissions for all accounts in your organization, allowing you to ensure your accounts stay within your organization’s access control guidelines. In service control policy (SCP), you can restrict which AWS services, resources, and individual API actions the users and roles in each member account can access. You can also define conditions for when to restrict access to AWS services, resources, and API actions. These restrictions even override the administrators of member accounts in the organization. Please note the following effects on permissions vis-a-vis the service control policy (SCP): If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable service control policy (SCP), the user or role can't perform that action. Service control policy (SCP) affects all users and roles in the member accounts, including root user of the member accounts. Service control policy (SCP) does not affect any service-linked role. Incorrect options: If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable service control policy (SCP), the user or role can still perform that action Service control policy (SCP) affects all users and roles in the member accounts, excluding root user of the member accounts Service control policy (SCP) affects service-linked roles These three options contradict the details provided in the explanation above. Reference: https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "The engineering team at a company is moving the static content from the company's logistics website hosted on Amazon EC2 instances to an Amazon S3 bucket. The team wants to use an Amazon CloudFront distribution to deliver the static content. The security group used by the Amazon EC2 instances allows the website to be accessed by a limited set of IP ranges from the company's suppliers. Post-migration to Amazon CloudFront, access to the static content should only be allowed from the aforementioned IP addresses. Which options would you combine to build a solution to meet these requirements? (Select two)",
      "options": [
        "A. Create an AWS Web Application Firewall (AWS WAF) ACL and use an IP match condition to allow traffic only from those IPs that are allowed in the Amazon EC2 security group. Associate this new AWS WAF ACL with the Amazon S3 bucket policy",
        "B. Create a new NACL that allows traffic from the same IPs as specified in the current Amazon EC2 security group. Associate this new NACL with the Amazon CloudFront distribution",
        "C. Create a new security group that allows traffic from the same IPs as specified in the current Amazon EC2 security group. Associate this new security group with the Amazon CloudFront distribution",
        "D. Configure an origin access identity (OAI) and associate it with the Amazon CloudFront distribution. Set up the permissions in the Amazon S3 bucket policy so that only the OAI can read the objects",
        "E. Create an AWS WAF ACL and use an IP match condition to allow traffic only from those IPs that are allowed in the Amazon EC2 security group. Associate this new AWS WAF ACL with the Amazon CloudFront distribution"
      ],
      "correct_answer": "D",
      "explanation": "Correct options: Configure an origin access identity (OAI) and associate it with the Amazon CloudFront distribution. Set up the permissions in the Amazon S3 bucket policy so that only the OAI can read the objects When you use Amazon CloudFront with an Amazon S3 bucket as the origin, you can configure Amazon CloudFront and Amazon S3 in a way that provides the following benefits: Restricts access to the Amazon S3 bucket so that it's not publicly accessible Makes sure that viewers (users) can access the content in the bucket only through the specified Amazon CloudFront distribution—that is, prevents them from accessing the content directly from the bucket, or through an unintended CloudFront distribution. To do this, configure Amazon CloudFront to send authenticated requests to Amazon S3, and configure Amazon S3 to only allow access to authenticated requests from Amazon CloudFront. Amazon CloudFront provides two ways to send authenticated requests to an Amazon S3 origin: origin access control (OAC) and origin access identity (OAI). Exam Alert: Please note that AWS recommends using OAC because it supports: All Amazon S3 buckets in all AWS Regions, including opt-in Regions launched after December 2022 Amazon S3 server-side encryption with AWS KMS (SSE-KMS) Dynamic requests (POST, PUT, etc.) to Amazon S3 OAI doesn't work for the scenarios in the preceding list, or it requires extra workarounds in those scenarios. However, you will continue to see answers enlisting OAI as the preferred option in the actual exam as it takes about 6 months/1 year for a new feature to appear in the exam. Create an AWS WAF ACL and use an IP match condition to allow traffic only from those IPs that are allowed in the Amazon EC2 security group. Associate this new AWS WAF ACL with the Amazon CloudFront distribution AWS WAF is a web application firewall that lets you monitor the HTTP and HTTPS requests that are forwarded to your protected web application resources. You can protect the following resource types: Amazon CloudFront distribution Amazon API Gateway REST API Application Load Balancer AWS AppSync GraphQL API Amazon Cognito user pool AWS WAF also lets you control access to your content. Based on conditions that you specify, such as the IP addresses that requests originate from or the values of query strings, your protected resource responds to requests either with the requested content, with an HTTP 403 status code (Forbidden), or with a custom response. If you want to allow or block web requests based on the IP addresses that the requests originate from, create one or more IP match conditions via your AWS WAF. An IP match condition lists up to 10,000 IP addresses or IP address ranges that your requests originate from. For the given use case, you should add those IP addresses that are allowed in the Amazon EC2 security group into the IP match condition. Incorrect options: Create an AWS Web Application Firewall (AWS WAF) ACL and use an IP match condition to allow traffic only from those IPs that are allowed in the Amazon EC2 security group. Associate this new AWS WAF ACL with the Amazon S3 bucket policy - You cannot associate an AWS WAF ACL with an Amazon S3 bucket policy. Create a new NACL that allows traffic from the same IPs as specified in the current Amazon EC2 security group. Associate this new NACL with the Amazon CloudFront distribution - NACL is associated with a subnet within a VPC. Amazon CloudFront delivers your content through a worldwide network of data centers called edge locations. So a NACL cannot be associated with a Amazon CloudFront distribution. Create a new security group that allows traffic from the same IPs as specified in the current Amazon EC2 security group. Associate this new security group with the Amazon CloudFront distribution - A security group acts as a virtual firewall for your Amazon EC2 instances to control incoming and outgoing traffic. Inbound rules control the incoming traffic to your instance, and outbound rules control the outgoing traffic from your instance. Amazon CloudFront delivers your content through a worldwide network of data centers called edge locations. So a security group cannot be associated with Amazon CloudFront distribution. References: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html https://docs.aws.amazon.com/waf/latest/developerguide/classic-web-acl-ip-conditions.html",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A global manufacturing company with facilities in the US, Europe, and Asia is designing a new distributed application to optimize its procurement workflow. The orders booked in one AWS Region should be visible to all AWS Regions in a second or less. The database should be able to facilitate failover with a short Recovery Time Objective (RTO). The uptime of the application is critical to ensure that the manufacturing processes are not impacted. As a solutions architect, which of the following will you recommend as the MOST cost-effective solution?",
      "options": [
        "A. Provision Amazon Aurora Global Database",
        "B. Provision Amazon RDS for MySQL with a cross-Region read replica",
        "C. Provision Amazon RDS for PostgreSQL with a cross-Region read replica",
        "D. Provision Amazon DynamoDB global tables"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Provision Amazon Aurora Global Database An Aurora global database provides more comprehensive failover capabilities than the failover provided by a default Aurora DB cluster. By using an Aurora global database, you can plan for and recover from disaster fairly quickly. Recovery from disaster is typically measured using values for RTO and RPO. Recovery time objective (RTO) – The time it takes a system to return to a working state after a disaster. In other words, RTO measures downtime. For an Aurora global database, RTO can be in the order of minutes. Recovery point objective (RPO) – The amount of data that can be lost (measured in time). For an Aurora global database, RPO is typically measured in seconds. With an Aurora global database, you can choose from two different approaches to failover: Managed planned failover – This feature is intended for controlled environments, such as disaster recovery (DR) testing scenarios, operational maintenance, and other planned operational procedures. Managed planned failover allows you to relocate the primary DB cluster of your Aurora global database to one of the secondary Regions. Because this feature synchronizes secondary DB clusters with the primary before making any other changes, RPO is 0 (no data loss). Unplanned failover (\"detach and promote\") – To recover from an unplanned outage, you can perform a cross-Region failover to one of the secondaries in your Aurora global database. The RTO for this manual process depends on how quickly you can perform the tasks listed in Recovering an Amazon Aurora global database from an unplanned outage. The RPO is typically measured in seconds, but this depends on the Aurora storage replication lag across the network at the time of the failure. Disaster Recovery in Aurora Global Databases: via - https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database-disaster-recovery.html Incorrect options: Provision Amazon RDS for MySQL with a cross-Region read replica Provision Amazon RDS for PostgreSQL with a cross-Region read replica Amazon RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. For a failover, read replicas have to be manually promoted to a standalone database instance since the process is not automatic. Hence, the RTO will be quite high, so both these options are not correct for this use case. Provision Amazon DynamoDB global tables - Aurora Global Database is good for applications that need to support cross-Region reads with low latency updates and the ability to quickly failover between regions. DynamoDB global tables provide cross-region active-active capabilities with high performance, but you lose some of the data access flexibility that comes with SQL-based databases. Due to the active-active configuration of DynamoDB global tables, there is no concept of failover because the application writes to the table in its region, and then the data is replicated to keep the other regions' table in sync. DynamoDB global tables is a much costlier solution than Aurora Global Database for the given requirement. References: https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database-disaster-recovery.html#aurora-global-database-failover https://aws.amazon.com/blogs/database/how-to-use-amazon-dynamodb-global-tables-to-power-multiregion-architectures/ https://aws.amazon.com/rds/features/read-replicas/ https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GlobalTables.html https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database.html",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An engineering lead is designing a VPC with public and private subnets. The VPC and subnets use IPv4 CIDR blocks. There is one public subnet and one private subnet in each of three Availability Zones (AZs) for high availability. An internet gateway is used to provide internet access for the public subnets. The private subnets require access to the internet to allow Amazon EC2 instances to download software updates. Which of the following options represents the correct solution to set up internet access for the private subnets?",
      "options": [
        "A. Set up three NAT gateways, one in each private subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the NAT gateway in its AZ",
        "B. Set up three Internet gateways, one in each private subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the Internet gateway in its AZ",
        "C. Set up three NAT gateways, one in each public subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the NAT gateway in its AZ",
        "D. Set up three egress-only internet gateways, one in each public subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the egress-only internet gateway in its AZ"
      ],
      "correct_answer": "C",
      "explanation": "Correct option: Set up three NAT gateways, one in each public subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the NAT gateway in its AZ You can use a network address translation (NAT) gateway to enable instances in a private subnet to connect to the internet or other AWS services, but prevent the internet from initiating a connection with those instances. To create a NAT gateway, you must specify the public subnet in which the NAT gateway should reside. You must also specify an Elastic IP address to associate with the NAT gateway when you create it. The Elastic IP address cannot be changed after you associate it with the NAT Gateway. After you've created a NAT gateway, you must update the route table associated with one or more of your private subnets to point internet-bound traffic to the NAT gateway. This enables instances in your private subnets to communicate with the internet. Each NAT gateway is created in a specific Availability Zone and implemented with redundancy in that zone. If you have resources in multiple Availability Zones and they share one NAT gateway, and if the NAT gateway’s Availability Zone is down, resources in the other Availability Zones lose internet access. To create an Availability Zone-independent architecture, create a NAT gateway in each Availability Zone and configure your routing to ensure that resources use the NAT gateway in the same Availability Zone. How NAT gateway works: via - https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html Incorrect options: Set up three NAT gateways, one in each private subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the NAT gateway in its AZ - NAT gateways need to be set up in public subnets, so this option is incorrect. Set up three Internet gateways, one in each private subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the Internet gateway in its AZ - Internet gateways cannot be provisioned in private subnets of a VPC. Set up three egress-only internet gateways, one in each public subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the egress-only internet gateway in its AZ - An Egress-only Internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows outbound communication over IPv6 from instances in your VPC to the internet, and prevents the internet from initiating an IPv6 connection with your instances. The given use-case is for IPv4 traffic, hence an Egress-only Internet gateway is not an option. Reference: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A company recently experienced a database outage in its on-premises data center. The company now wants to migrate to a reliable database solution on AWS that minimizes data loss and stores every transaction on at least two nodes. Which of the following solutions meets these requirements?",
      "options": [
        "A. Set up an Amazon RDS MySQL DB instance and then create a read replica in another Availability Zone that synchronously replicates the data",
        "B. Set up an Amazon RDS MySQL DB instance and then create a read replica in a separate AWS Region that synchronously replicates the data",
        "C. Set up an Amazon RDS MySQL DB instance with Multi-AZ functionality enabled to synchronously replicate the data",
        "D. Set up an Amazon EC2 instance with a MySQL DB engine installed that triggers an AWS Lambda function to synchronously replicate the data to an Amazon RDS MySQL DB instance"
      ],
      "correct_answer": "C",
      "explanation": "Correct option: Set up an Amazon RDS MySQL DB instance with Multi-AZ functionality enabled to synchronously replicate the data When you provision an RDS Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). Each AZ runs on its own physically distinct, independent infrastructure, and is engineered to be highly reliable. Running a DB instance with high availability can enhance availability during planned system maintenance, and help protect your databases against DB instance failure and Availability Zone disruption. In the event of a planned or unplanned outage of your DB instance, Amazon RDS automatically switches to a standby replica in another Availability Zone if you have enabled Multi-AZ. The time it takes for the failover to complete depends on the database activity and other conditions at the time the primary DB instance became unavailable. Failover times are typically 60–120 seconds. via - https://aws.amazon.com/rds/features/multi-az/ Incorrect options: Set up an Amazon RDS MySQL DB instance and then create a read replica in another Availability Zone that synchronously replicates the data Set up an Amazon RDS MySQL DB instance and then create a read replica in a separate AWS Region that synchronously replicates the data Amazon RDS uses the MariaDB, Microsoft SQL Server, MySQL, Oracle, and PostgreSQL DB engines' built-in replication functionality to create a special type of DB instance called a read replica from a source DB instance. The source DB instance becomes the primary DB instance. Updates made to the primary DB instance are asynchronously copied to the read replica. You can reduce the load on your primary DB instance by routing read queries from your applications to the read replica. Using read replicas, you can elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. Both these options talk about creating a read replica that synchronously replicates the data, but in reality, any updates made to the primary DB instance are asynchronously copied to the read replica. So both these options are incorrect. via - https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html Set up an Amazon EC2 instance with a MySQL DB engine installed that triggers an AWS Lambda function to synchronously replicate the data to an Amazon RDS MySQL DB instance - Setting up a database on an Amazon EC2 instance would not be reliable as you would have to monitor and manage the underlying Amazon EC2 instance for any issues or outages. In addition, using AWS Lambda to replicate the data from EC2 based MySQL DB to an Amazon RDS MySQL DB would make the solution really complex since the same functionality can be achieved out-of-the-box using RDS Multi-AZ configuration. References: https://aws.amazon.com/rds/features/multi-az/ https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "Your application is hosted by a provider on yourapp.provider.com. You would like to have your users access your application using www.your-domain.com, which you own and manage under Amazon Route 53. Which Amazon Route 53 record should you create?",
      "options": [
        "A. Create an A record",
        "B. Create a CNAME record",
        "C. Create a PTR record",
        "D. Create an Alias Record"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Create a CNAME record A CNAME record maps DNS queries for the name of the current record, such as acme.example.com, to another domain (example.com or example.net) or subdomain (acme.example.com or zenith.example.org). CNAME records can be used to map one domain name to another. Although you should keep in mind that the DNS protocol does not allow you to create a CNAME record for the top node of a DNS namespace, also known as the zone apex. For example, if you register the DNS name example.com, the zone apex is example.com. You cannot create a CNAME record for example.com, but you can create CNAME records for www.example.com, newproduct.example.com, and so on. Please review the major differences between CNAME and Alias Records: via - https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html Incorrect options: Create an A record - Used to point a domain or subdomain to an IP address. 'A record' cannot be used to map one domain name to another. Create a PTR record - A Pointer (PTR) record resolves an IP address to a fully-qualified domain name (FQDN) as an opposite to what A record does. PTR records are also called Reverse DNS records. 'PTR record' cannot be used to map one domain name to another. Create an Alias Record - Alias records let you route traffic to selected AWS resources, such as Amazon CloudFront distributions and Amazon S3 buckets. They also let you route traffic from one record in a hosted zone to another record. 3rd party websites do not qualify for these as we have no control over those. 'Alias record' cannot be used to map one domain name to another. Reference: https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "The DevOps team at an IT company has recently migrated to AWS and they are configuring security groups for their two-tier application with public web servers and private database servers. The team wants to understand the allowed configuration options for an inbound rule for a security group. As a solutions architect, which of the following would you identify as an INVALID option for setting up such a configuration?",
      "options": [
        "A. You can use a security group as the custom source for the inbound rule",
        "B. You can use a range of IP addresses in CIDR block notation as the custom source for the inbound rule",
        "C. You can use an IP address as the custom source for the inbound rule",
        "D. You can use an Internet Gateway ID as the custom source for the inbound rule"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: You can use an Internet Gateway ID as the custom source for the inbound rule A security group acts as a virtual firewall that controls the traffic for one or more instances. When you launch an instance, you can specify one or more security groups; otherwise, you can use the default security group. You can add rules to each security group that allows traffic to or from its associated instances. You can modify the rules for a security group at any time; the new rules are automatically applied to all instances that are associated with the security group. Please see this list of allowed source or destination for security group rules: via - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html Therefore, you cannot use an Internet Gateway ID as the custom source for the inbound rule. Incorrect options: You can use a security group as the custom source for the inbound rule You can use a range of IP addresses in CIDR block notation as the custom source for the inbound rule You can use an IP address as the custom source for the inbound rule As described in the list of allowed sources or destinations for security group rules, the above options are supported. Reference: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html",
      "reference": "Source: Practice Test #3 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "Your company is deploying a website running on AWS Elastic Beanstalk. The website takes over 45 minutes for the installation and contains both static as well as dynamic files that must be generated during the installation process. As a Solutions Architect, you would like to bring the time to create a new instance in your AWS Elastic Beanstalk deployment to be less than 2 minutes. Which of the following options should be combined to build a solution for this requirement? (Select two)",
      "options": [
        "A. Use Amazon EC2 user data to customize the dynamic installation parts at boot time",
        "B. Create a Golden Amazon Machine Image (AMI) with the static installation components already setup",
        "C. Store the installation files in Amazon S3 so they can be quickly retrieved",
        "D. Use Amazon EC2 user data to install the application at boot time",
        "E. Use AWS Elastic Beanstalk deployment caching feature"
      ],
      "correct_answer": "A",
      "explanation": "Correct options: AWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services developed with Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker on familiar servers such as Apache, Nginx, Passenger, and IIS. You can simply upload your code and Elastic Beanstalk automatically handles the deployment, from capacity provisioning, load balancing, auto-scaling to application health monitoring. At the same time, you retain full control over the AWS resources powering your application and can access the underlying resources at any time. When you create an AWS Elastic Beanstalk environment, you can specify an Amazon Machine Image (AMI) to use instead of the standard Elastic Beanstalk AMI included in your platform version. A custom AMI can improve provisioning times when instances are launched in your environment if you need to install a lot of software that isn't included in the standard AMIs. Create a Golden Amazon Machine Image (AMI) with the static installation components already setup A Golden AMI is an AMI that you standardize through configuration, consistent security patching, and hardening. It also contains agents you approve for logging, security, performance monitoring, etc. For the given use-case, you can have the static installation components already setup via the golden AMI. Use Amazon EC2 user data to customize the dynamic installation parts at boot time Amazon EC2 instance user data is the data that you specified in the form of a configuration script while launching your instance. You can use Amazon EC2 user data to customize the dynamic installation parts at boot time, rather than installing the application itself at boot time. Incorrect options: Store the installation files in Amazon S3 so they can be quickly retrieved - Amazon S3 bucket can be used as a storage location for your source code, logs, and other artifacts that are created when you use AWS Elastic Beanstalk. It cannot be used to run or generate dynamic files since Amazon S3 is not an environment but a storage service. Use Amazon EC2 user data to install the application at boot time - User data of an instance can be used to perform common automated configuration tasks or run scripts after the instance starts. User data, cannot, however, be used to install the application since it takes over 45 minutes for the installation which contains static as well as dynamic files that must be generated during the installation process. Use AWS Elastic Beanstalk deployment caching feature - AWS Elastic Beanstalk deployment caching is a made-up option. It is just added as a distractor. References: https://aws.amazon.com/elasticbeanstalk/ https://aws.amazon.com/blogs/awsmarketplace/announcing-the-golden-ami-pipeline/ https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/AWSHowTo.S3.html https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.customenv.html https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-add-user-data.html",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "As a Solutions Architect, you are tasked to design a distributed application that will run on various Amazon EC2 instances. This application needs to have the highest performance local disk to cache data. Also, data is copied through an Amazon EC2 to EC2 replication mechanism. It is acceptable if the instance loses its data when stopped or terminated. Which storage solution do you recommend?",
      "options": [
        "A. Amazon Elastic File System (Amazon EFS)",
        "B. Instance Store",
        "C. Amazon Elastic Block Store (EBS)",
        "D. Amazon Simple Storage Service (Amazon S3)"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Instance Store An instance store provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host computer. Instance store is ideal for the temporary storage of information that changes frequently, such as buffers, caches, scratch data, and other temporary content, or for data that is replicated across a fleet of instances, such as a load-balanced pool of web servers. Instance store volumes are included as part of the instance's usage cost. Some instance types use NVMe or SATA-based solid-state drives (SSD) to deliver high random I/O performance. This is a good option when you need storage with very low latency, but you don't need the data to persist when the instance terminates. Incorrect options: Amazon Elastic Block Store (EBS) - Amazon Elastic Block Store (EBS) is an easy to use, high-performance block storage service designed for use with Amazon Elastic Compute Cloud (EC2) for both throughput and transaction-intensive workloads at any scale. Amazon Elastic File System (Amazon EFS) - Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. It is built to scale on-demand to petabytes without disrupting applications, growing and shrinking automatically as you add and remove files, eliminating the need to provision and manage capacity to accommodate growth. Amazon Simple Storage Service (Amazon S3) - Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. Your applications can easily achieve thousands of transactions per second in request performance when uploading and retrieving storage from Amazon S3. Instance Store will have the highest disk performance but you would lose the storage if the instance is terminated, which is acceptable in this case. Amazon EBS volumes would provide good performance as far as disk goes, but not as good as Instance Store. Amazon EBS data survives instance termination or reboots. Amazon EFS is a network drive and it would not match the performance of the Instance Store. Finally, Amazon S3 cannot be mounted as a local disk (natively). Reference: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A company has migrated its application from a monolith architecture to a microservices based architecture. The development team has updated the Amazon Route 53 simple record to point \"myapp.mydomain.com\" from the old Load Balancer to the new one. The users are still not redirected to the new Load Balancer. What has gone wrong in the configuration?",
      "options": [
        "A. The Time To Live (TTL) is still in effect",
        "B. The health checks are failing",
        "C. The Alias Record is misconfigured",
        "D. The CNAME Record is misconfigured"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. Amazon Route 53 effectively connects user requests to infrastructure running in AWS – such as Amazon EC2 instances, Elastic Load Balancing load balancers, or Amazon S3 buckets – and can also be used to route users to infrastructure outside of AWS. You can use Amazon Route 53 to configure DNS health checks to route traffic to healthy endpoints or to independently monitor the health of your application and its endpoints. Amazon Route 53 Traffic Flow makes it easy for you to manage traffic globally through a variety of routing types, including Latency Based Routing, Geo DNS, Geoproximity, and Weighted Round Robin—all of which can be combined with DNS Failover to enable a variety of low-latency, fault-tolerant architectures. The Time To Live (TTL) is still in effect TTL (time to live), is the amount of time, in seconds, that you want DNS recursive resolvers to cache information about a record. If you specify a longer value (for example, 172800 seconds, or two days), you reduce the number of calls that DNS recursive resolvers must make to Amazon Route 53 to get the latest information for the record. This has the effect of reducing latency and reducing your bill for Route 53 service. However, if you specify a longer value for TTL, it takes longer for changes to the record (for example, a new IP address) to take effect because recursive resolvers use the values in their cache for longer periods before they ask Route 53 for the latest information. If you're changing settings for a domain or subdomain that's already in use, AWS recommends that you initially specify a shorter value, such as 300 seconds, and increase the value after you confirm that the new settings are correct. For this use-case, the most likely issue is that the TTL is still in effect so you have to wait until it expires for the new request to perform another DNS query and get the value for the new Load Balancer. Incorrect options: The CNAME Record is misconfigured - A CNAME record can redirect DNS queries to any DNS record. For example, you can create a CNAME record that redirects queries from acme.example.com to zenith.example.com or to acme.example.org. You don't need to use Amazon Route 53 as the DNS service for the domain that you're redirecting queries to. The Alias Record is misconfigured - Amazon Route 53 also offers alias records, which are an Amazon Route 53-specific extension to DNS. Alias records let you route traffic to selected AWS resources, such as Amazon CloudFront distributions and Amazon S3 buckets. They also let you route traffic from one record in a hosted zone to another record. Unlike a CNAME record, you can create an alias record at the top node of a DNS namespace, also known as the zone apex. For example, if you register the DNS name example.com, the zone apex is example.com. You can't create a CNAME record for example.com, but you can create an alias record for example.com that routes traffic to www.example.com. The health checks are failing - Simple Records do not have health checks, so this option is incorrect. References: https://aws.amazon.com/route53/ https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-values-basic.html https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An Internet of Things (IoT) company would like to have a streaming system that performs real-time analytics on the ingested IoT data. Once the analytics is done, the company would like to send notifications back to the mobile applications of the IoT device owners. As a solutions architect, which of the following AWS technologies would you recommend to send these notifications to the mobile applications?",
      "options": [
        "A. Amazon Kinesis with Amazon Simple Notification Service (Amazon SNS)",
        "B. Amazon Simple Queue Service (Amazon SQS) with Amazon Simple Notification Service (Amazon SNS)",
        "C. Amazon Kinesis with Amazon Simple Email Service (Amazon SES)",
        "D. Amazon Kinesis with Amazon Simple Queue Service (Amazon SQS)"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Amazon Kinesis with Amazon Simple Notification Service (Amazon SNS) Amazon Kinesis makes it easy to collect, process, and analyze real-time, streaming data so you can get timely insights and react quickly to new information. Amazon Kinesis offers key capabilities to cost-effectively process streaming data at any scale, along with the flexibility to choose the tools that best suit the requirements of your application. With Amazon Kinesis, you can ingest real-time data such as video, audio, application logs, website clickstreams, and IoT telemetry data for machine learning, analytics, and other applications. Amazon Kinesis enables you to process and analyze data as it arrives and respond instantly instead of having to wait until all your data is collected before the processing can begin. Amazon Kinesis will be great for event streaming from the IoT devices, but not for sending notifications as it doesn't have such a feature. Amazon Simple Notification Service (Amazon SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. Amazon SNS provides topics for high-throughput, push-based, many-to-many messaging. Amazon SNS is a notification service and will be perfect for this use case. Streaming data with Amazon Kinesis and using Amazon SNS to send the response notifications is the optimal solution for the current scenario. Incorrect options: Amazon Simple Queue Service (Amazon SQS) with Amazon Simple Notification Service (Amazon SNS) - Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. Amazon SQS eliminates the complexity and overhead associated with managing and operating message-oriented middleware and empowers developers to focus on differentiating work. Using SQS, you can send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be available. Kinesis is better for streaming data since queues aren't meant for real-time streaming of data. Amazon Kinesis with Amazon Simple Email Service (Amazon SES) - Amazon Simple Email Service (Amazon SES) is a cloud-based email sending service designed to help digital marketers and application developers send marketing, notification, and transactional emails. It is a reliable, cost-effective service for businesses of all sizes that use email to keep in contact with their customers. It is an email service and not a notification service as is the requirement in the current use case. Amazon Kinesis with Amazon Simple Queue Service (Amazon SQS) - As explained above, Amazon Kinesis works well for streaming real-time data. Amazon SQS is a queuing service that helps decouple system architecture by offering flexibility and ease of maintenance. It cannot send notifications. Amazon SQS is paired with SNS to provide this functionality. References: https://aws.amazon.com/sns/ https://aws.amazon.com/kinesis/ https://aws.amazon.com/ses/ https://aws.amazon.com/sqs/",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "The engineering team at a global e-commerce company is currently reviewing their disaster recovery strategy. The team has outlined that they need to be able to quickly recover their application stack with a Recovery Time Objective (RTO) of 5 minutes, in all of the AWS Regions that the application runs. The application stack currently takes over 45 minutes to install on a Linux system. As a Solutions architect, which of the following options would you recommend as the disaster recovery strategy?",
      "options": [
        "A. Store the installation files in Amazon S3 for quicker retrieval",
        "B. Use Amazon EC2 user data to speed up the installation process",
        "C. Create an Amazon Machine Image (AMI) after installing the software and use this AMI to run the recovery process in other Regions",
        "D. Create an Amazon Machine Image (AMI) after installing the software and copy the AMI across all Regions. Use this Region-specific AMI to run the recovery process in the respective Regions"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: Create an Amazon Machine Image (AMI) after installing the software and copy the AMI across all Regions. Use this Region-specific AMI to run the recovery process in the respective Regions An Amazon Machine Image (AMI) provides the information required to launch an instance. You must specify an AMI when you launch an instance. You can launch multiple instances from a single AMI when you need multiple instances with the same configuration. You can use different AMIs to launch instances when you need instances with different configurations. For the current use case, you need to create an AMI such that the application stack is already set up. But AMIs are bound to the Region they are created in. So, you need to copy the AMI across Regions for disaster recovery readiness. Copying a source AMI results in an identical but distinct target AMI with its own unique identifier. In the case of an Amazon EBS-backed AMI, each of its backing snapshots is, by default, copied to an identical but distinct target snapshot. (The sole exceptions are when you choose to encrypt or re-encrypt the snapshot.) You can change or deregister the source AMI with no effect on the target AMI. The reverse is also true. There are no charges for copying an AMI. However, standard storage and data transfer rates apply. If you copy an Amazon EBS-backed AMI, you will incur charges for the storage of any additional Amazon EBS snapshots. AWS does not copy launch permissions, user-defined tags, or Amazon S3 bucket permissions from the source AMI to the new AMI. After the copy operation is complete, you can apply launch permissions, user-defined tags, and Amazon S3 bucket permissions to the new AMI. AMIs Cross-Region copying: via - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html Incorrect options: Store the installation files in Amazon S3 for quicker retrieval - Amazon Simple Storage Service (Amazon S3) is an object storage service from AWS. It will not help with speeding up the installation since Amazon S3 is a storage service. You will still need an Amazon EC2 instance to have the necessary installation environment. Use Amazon EC2 user data to speed up the installation process - User data of an Amazon EC2 instance can be used to perform common automated configuration tasks or run scripts after the instance starts. User data, cannot, however, be used to install the application. Amazon EC2 user data would not help as it would run the same installation script for the same duration of 45 minutes. Create an Amazon Machine Image (AMI) after installing the software and use this AMI to run the recovery process in other Regions - As discussed above, AMIs are Region-specific and need to be copied to all Regions they are intended to be used in. References: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A company has grown from a small startup to an enterprise employing over 1000 people. As the team size has grown, the company has recently observed some strange behavior, with Amazon S3 buckets settings being changed regularly. How can you figure out what's happening without restricting the rights of the users?",
      "options": [
        "A. Implement an IAM policy to forbid users to change Amazon S3 bucket settings",
        "B. Use Amazon S3 access logs to analyze user access using Athena",
        "C. Implement a bucket policy requiring AWS Multi-Factor Authentication (AWS MFA) for all operations",
        "D. Use AWS CloudTrail to analyze API calls"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: Use AWS CloudTrail to analyze API calls AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. With AWS CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. AWS CloudTrail provides event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command-line tools, and other AWS services. In general, to analyze any API calls made within an AWS account, AWS CloudTrail is used. You can record the actions that are taken by users, roles, or AWS services on Amazon S3 resources and maintain log records for auditing and compliance purposes. To do this, you can use server access logging, AWS CloudTrail logging, or a combination of both. AWS recommends that you use AWS CloudTrail for logging bucket and object-level actions for your Amazon S3 resources. Incorrect options: Implement an IAM policy to forbid users to change Amazon S3 bucket settings - You manage access in AWS by creating policies and attaching them to IAM identities (users, groups of users, or roles) or AWS resources. A policy is an object in AWS that, when associated with an identity or resource, defines their permissions. AWS evaluates these policies when an IAM principal (user or role) makes a request. Permissions in the policies determine whether the request is allowed or denied. Most policies are stored in AWS as JSON documents. AWS supports six types of policies: identity-based policies, resource-based policies, permissions boundaries, AWS Organizations service control policy (SCP), access control list (ACL), and session policies. Implementing an IAM policy to forbid users would be disruptive and wouldn't go unnoticed. Use Amazon S3 access logs to analyze user access using Athena - Amazon S3 server access logging provides detailed records for the requests that are made to a bucket. Server access logs are useful for many applications. For example, access log information can be useful in security and access audits. It can also help you learn about your customer base and understand your Amazon S3 bill. AWS recommends that you use AWS CloudTrail for logging bucket and object-level actions for your Amazon S3 resources, as it provides more options to store, analyze and act on the log information. Implement a bucket policy requiring AWS Multi-Factor Authentication (AWS MFA) for all operations - Amazon S3 supports MFA-protected API access, a feature that can enforce multi-factor authentication (MFA) for access to your Amazon S3 resources. Multi-factor authentication provides an extra level of security that you can apply to your AWS environment. It is a security feature that requires users to prove the physical possession of an MFA device by providing a valid MFA code. Changing the bucket policy to require MFA would not go unnoticed. References: https://docs.aws.amazon.com/AmazonS3/latest/userguide/logging-with-S3.html https://aws.amazon.com/cloudtrail/ https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerLogs.html https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html#example-bucket-policies-use-case-7",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A mobile gaming company is experiencing heavy read traffic to its Amazon Relational Database Service (Amazon RDS) database that retrieves player’s scores and stats. The company is using an Amazon RDS database instance type that is not cost-effective for their budget. The company would like to implement a strategy to deal with the high volume of read traffic, reduce latency, and also downsize the instance size to cut costs. Which of the following solutions do you recommend?",
      "options": [
        "A. Setup Amazon RDS Read Replicas",
        "B. Setup Amazon ElastiCache in front of Amazon RDS",
        "C. Move to Amazon Redshift",
        "D. Switch application code to AWS Lambda for better performance"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Setup Amazon ElastiCache in front of Amazon RDS Amazon ElastiCache is an ideal front-end for data stores such as Amazon RDS, providing a high-performance middle tier for applications with extremely high request rates and/or low latency requirements. The best part of caching is that it’s minimally invasive to implement and by doing so, your application performance regarding both scale and speed is dramatically improved. Incorrect options: Setup Amazon RDS Read Replicas - Adding read replicas would further add to the database costs and will not help in reducing latency when compared to a caching solution. So this option is ruled out. Move to Amazon Redshift - Amazon Redshift is optimized for datasets ranging from a few hundred gigabytes to a petabyte or more. If the company is looking at cost-cutting, moving to Amazon Redshift from Amazon RDS is not an option. Switch application code to AWS Lambda for better performance - AWS Lambda can help in running data processing workflows. But, data still needs to be read from RDS and hence we need a solution to speed up the data reads and not before/after processing. Reference: https://aws.amazon.com/caching/database-caching/",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A junior developer has downloaded a sample Amazon S3 bucket policy to make changes to it based on new company-wide access policies. He has requested your help in understanding this bucket policy. As a Solutions Architect, which of the following would you identify as the correct description for the given policy? { \"Version\": \"2012-10-17\", \"Id\": \"S3PolicyId1\", \"Statement\": [ { \"Sid\": \"IPAllow\", \"Effect\": \"Allow\", \"Principal\": \"*\", \"Action\": \"s3:*\", \"Resource\": \"arn:aws:s3:::examplebucket/*\", \"Condition\": { \"IpAddress\": {\"aws:SourceIp\": \"54.240.143.0/24\"}, \"NotIpAddress\": {\"aws:SourceIp\": \"54.240.143.188/32\"} } } ] }",
      "options": [
        "A. It ensures the Amazon S3 bucket is exposing an external IP within the Classless Inter-Domain Routing (CIDR) range specified, except one IP",
        "B. It authorizes an entire Classless Inter-Domain Routing (CIDR) except one IP address to access the Amazon S3 bucket",
        "C. It ensures Amazon EC2 instances that have inherited a security group can access the bucket",
        "D. It authorizes an IP address and a Classless Inter-Domain Routing (CIDR) to access the S3 bucket"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: It authorizes an entire Classless Inter-Domain Routing (CIDR) except one IP address to access the Amazon S3 bucket You manage access in AWS by creating policies and attaching them to IAM identities (users, groups of users, or roles) or AWS resources. A policy is an object in AWS that, when associated with an identity or resource, defines their permissions. AWS evaluates these policies when an IAM principal (user or role) makes a request. Permissions in the policies determine whether the request is allowed or denied. Most policies are stored in AWS as JSON documents. AWS supports six types of policies: identity-based policies, resource-based policies, permissions boundaries, AWS Organizations SCPs, ACLs, and session policies. Let's analyze the bucket policy one step at a time: The snippet \"Effect\": \"Allow\" implies an allow effect. The snippet \"Principal\": \"*\" implies any Principal. The snippet \"Action\": \"s3:*\" implies any Amazon S3 API. The snippet \"Resource\": \"arn:aws:s3:::examplebucket/*\" implies that the resource can be the bucket examplebucket and its contents. Consider the last snippet of the given bucket policy: \"Condition\": { \"IpAddress\": {\"aws:SourceIp\": \"54.240.143.0/24\"}, \"NotIpAddress\": {\"aws:SourceIp\": \"54.240.143.188/32\"} } This snippet implies that if the source IP is in the CIDR block \"54.240.143.0/24\" (== 54.240.143.0 - 54.240.143.255), then it is allowed to access the examplebucket and its contents. However, the source IP cannot be in the CIDR \"54.240.143.188/32\" (== 1 IP, 54.240.143.188/32), which means one IP address is explicitly blocked from accessing the examplebucket and its contents. Example Bucket policies: via - https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html Incorrect options: It ensures the Amazon S3 bucket is exposing an external IP within the Classless Inter-Domain Routing (CIDR) range specified, except one IP It ensures Amazon EC2 instances that have inherited a security group can access the bucket It authorizes an IP address and a Classless Inter-Domain Routing (CIDR) to access the S3 bucket These three options contradict the explanation provided above, so these options are incorrect. Reference: https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "You are working for a software as a service (SaaS) company as a solutions architect and help design solutions for the company's customers. One of the customers is a bank and has a requirement to whitelist a public IP when the bank is accessing external services across the internet. Which architectural choice do you recommend to maintain high availability, support scaling-up to 10 instances and comply with the bank's requirements?",
      "options": [
        "A. Use a Network Load Balancer with an Auto Scaling Group",
        "B. Use a Classic Load Balancer with an Auto Scaling Group",
        "C. Use an Application Load Balancer with an Auto Scaling Group",
        "D. Use an Auto Scaling Group with Dynamic Elastic IPs attachment"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Use a Network Load Balancer with an Auto Scaling Group Network Load Balancer is best suited for use-cases involving low latency and high throughput workloads that involve scaling to millions of requests per second. Network Load Balancer operates at the connection level (Layer 4), routing connections to targets - Amazon EC2 instances, microservices, and containers – within Amazon Virtual Private Cloud (Amazon VPC) based on IP protocol data. A Network Load Balancer functions at the fourth layer of the Open Systems Interconnection (OSI) model. It can handle millions of requests per second. Network Load Balancers expose a fixed IP to the public web, therefore allowing your application to be predictably reached using this IP, while allowing you to scale your application behind the Network Load Balancer using an ASG. Incorrect options: Classic Load Balancers and Application Load Balancers use the private IP addresses associated with their Elastic network interfaces as the source IP address for requests forwarded to your web servers. These IP addresses can be used for various purposes, such as allowing the load balancer traffic on the web servers and for request processing. It's a best practice to use security group referencing on the web servers for whitelisting load balancer traffic from Classic Load Balancers or Application Load Balancers. However, because Network Load Balancers don't support security groups, based on the target group configurations, the IP addresses of the clients or the private IP addresses associated with the Network Load Balancers must be allowed on the web server's security group. Use a Classic Load Balancer with an Auto Scaling Group - Classic Load Balancer provides basic load balancing across multiple Amazon EC2 instances and operates at both the request level and connection level. Classic Load Balancer is intended for applications that were built within the Amazon EC2-Classic network. Use an Application Load Balancer with an Auto Scaling Group - Application Load Balancer operates at the request level (layer 7), routing traffic to targets – Amazon EC2 instances, containers, IP addresses and AWS Lambda functions based on the content of the request. Ideal for advanced load balancing of HTTP and HTTPS traffic, Application Load Balancer provides advanced request routing targeted at the delivery of modern application architectures, including microservices and container-based applications. Application and Classic Load Balancers expose a fixed DNS (=URL) rather than the IP address. So these are incorrect options for the given use-case. Use an Auto Scaling Group with Dynamic Elastic IPs attachment - The option \"Use an Auto Scaling Group (ASG) with Dynamic Elastic IPs attachment\" has been added as a distractor. ASG does not have a dynamic Elastic IPs attachment feature. References: https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html https://aws.amazon.com/premiumsupport/knowledge-center/elb-find-load-balancer-IP/ https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-internet-facing-load-balancers.html",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A ride-sharing company wants to improve the ride-tracking system that stores GPS coordinates for all rides. The engineering team at the company is looking for a NoSQL database that has single-digit millisecond latency, can scale horizontally, and is serverless, so that they can perform high-frequency lookups reliably. As a Solutions Architect, which database do you recommend for their requirements?",
      "options": [
        "A. Amazon Neptune",
        "B. Amazon Relational Database Service (Amazon RDS)",
        "C. Amazon ElastiCache",
        "D. Amazon DynamoDB"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: Amazon DynamoDB Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-Region, multi-master, durable NoSQL database with built-in security, backup and restore, and in-memory caching for internet-scale applications. DynamoDB can handle more than 10 trillion requests per day and can support peaks of more than 20 million requests per second. DynamoDB is serverless, has single-digit millisecond latency and scales horizontally. This is the correct choice for the given requirements. Sample Amazon DynamoDB solution for Real time applications: via - https://aws.amazon.com/dynamodb/ Incorrect options: Amazon ElastiCache - Amazon ElastiCache allows you to seamlessly set up, run, and scale popular open-Source compatible in-memory data stores, compatible with Redis or Memcached. Build data-intensive apps or boost the performance of your existing databases by retrieving data from high throughput and low latency in-memory data stores. Amazon ElastiCache is a popular choice for real-time use cases like Caching, Session Stores, Gaming, Geospatial Services, Real-Time Analytics, and Queuing. The primary use-case for ElastiCache is that of a caching service and it should not be used as the main database. How Amazon ElastiCache works: via - https://aws.amazon.com/elasticache/ Amazon Relational Database Service (Amazon RDS) - Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching and backups. Relational databases are not NoSQL databases and these cannot provide the millisecond latency that the current use case needs, hence it's an incorrect choice. Amazon Neptune - Amazon Neptune is a fast, reliable, fully-managed graph database service that makes it easy to build and run applications that work with highly connected datasets. The core of Amazon Neptune is a purpose-built, high-performance graph database engine optimized for storing billions of relationships and querying the graph with milliseconds latency. Neptune powers graph use cases such as recommendation engines, fraud detection, knowledge graphs, drug discovery, and network security. Example Use cases of Amazon Neptune: via - https://aws.amazon.com/neptune/ References: https://aws.amazon.com/dynamodb/ https://aws.amazon.com/elasticache/ https://aws.amazon.com/neptune/",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "The development team at a social media company wants to handle some complicated queries such as \"What are the number of likes on the videos that have been posted by friends of a user A?\". As a solutions architect, which of the following AWS database services would you suggest as the BEST fit to handle such use cases?",
      "options": [
        "A. Amazon Neptune",
        "B. Amazon Redshift",
        "C. Amazon OpenSearch Service",
        "D. Amazon Aurora"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Amazon Neptune Amazon Neptune is a fast, reliable, fully managed graph database service that makes it easy to build and run applications that work with highly connected datasets. The core of Amazon Neptune is a purpose-built, high-performance graph database engine optimized for storing billions of relationships and querying the graph with milliseconds latency. Neptune powers graph use cases such as recommendation engines, fraud detection, knowledge graphs, drug discovery, and network security. Amazon Neptune is highly available, with read replicas, point-in-time recovery, continuous backup to Amazon S3, and replication across Availability Zones. Neptune is secure with support for HTTPS encrypted client connections and encryption at rest. Neptune is fully managed, so you no longer need to worry about database management tasks such as hardware provisioning, software patching, setup, configuration, or backups. Amazon Neptune can quickly and easily process large sets of user-profiles and interactions to build social networking applications. Neptune enables highly interactive graph queries with high throughput to bring social features into your applications. For example, if you are building a social feed into your application, you can use Neptune to provide results that prioritize showing your users the latest updates from their family, from friends whose updates they ‘Like,’ and from friends who live close to them. Social Networking example with Amazon Neptune: via - https://aws.amazon.com/neptune/ Identity graphs example with Amazon Neptune: via - https://aws.amazon.com/neptune/ Incorrect options: Amazon OpenSearch Service - Amazon OpenSearch Service is a managed service that makes it easy for you to perform interactive log analytics, real-time application monitoring, website search, and more. OpenSearch is an open source, distributed search and analytics suite derived from Elasticsearch. Amazon OpenSearch Service offers the latest versions of OpenSearch, support for 19 versions of Elasticsearch (1.5 to 7.10 versions), as well as visualization capabilities powered by OpenSearch Dashboards and Kibana (1.5 to 7.10 versions). Amazon OpenSearch Service currently has tens of thousands of active customers with hundreds of thousands of clusters under management processing trillions of requests per month. Amazon Redshift - Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large scale data set storage and analysis. The given use-case is not about data warehousing, so this is not a correct option. Amazon Aurora - Amazon Aurora is a MySQL and PostgreSQL-compatible relational database built for the cloud, that combines the performance and availability of traditional enterprise databases with the simplicity and cost-effectiveness of open source databases. Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 64 terabytes per database instance. Aurora is not an in-memory database. Here, we need a graph database due to the highly connected datasets and queries, therefore Neptune is the best answer. Reference: https://aws.amazon.com/neptune/",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A company has built a serverless application using Amazon API Gateway and AWS Lambda. The backend is leveraging an Amazon Aurora MySQL database. The web application was initially launched in the Americas and the company would now like to expand it to Europe, where a read-only version will be available to improve latency. You plan on deploying the Amazon API Gateway and AWS Lambda using AWS CloudFormation, but would like to have a read-only copy of your data in Europe as well. As a Solutions Architect, what do you recommend?",
      "options": [
        "A. Create an AWS Lambda function to periodically back up and restore the Amazon Aurora database in another region",
        "B. Use Amazon Aurora Read Replicas",
        "C. Use Amazon DynamoDB Streams",
        "D. Use Amazon Aurora Multi-AZ"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Use Amazon Aurora Read Replicas Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 64TB per database instance. It delivers high performance and availability with up to 15 low-latency read replicas, point-in-time recovery, continuous backup to Amazon S3, and replication across three Availability Zones (AZs). Aurora Replicas are independent endpoints in an Aurora DB cluster, best used for scaling read operations. Up to 15 Aurora Replicas can be distributed across the Availability Zones (AZs) that a DB cluster spans within an AWS Region. The DB cluster volume is made up of multiple copies of the data for the DB cluster. However, the data in the cluster volume is represented as a single, logical volume to the primary instance and Aurora Replicas in the DB cluster. You can also set up two Aurora MySQL DB clusters in different AWS Regions, by creating an Aurora Read Replica of an Amazon Aurora MySQL DB cluster in a different AWS Region. In this way, Aurora Read Replicas can be deployed globally. Incorrect options: Use Amazon Aurora Multi-AZ - Aurora, stores copies of the data in a DB cluster across multiple Availability Zones in a single AWS Region, regardless of whether the instances in the DB cluster span multiple Availability Zones (AZs). When data is written to the primary DB instance, Aurora synchronously replicates the data across Availability Zones (AZs) to six storage nodes associated with your cluster volume. Doing so provides data redundancy, eliminates I/O freezes, and minimizes latency spikes during system backups. Always remember that the main purpose for multi-AZ is high availability whereas the main purpose of read replicas is read scalability. Use Amazon DynamoDB Streams - Amazon DynamoDB Streams is a powerful service that you can combine with other AWS services to solve many problems. When enabled, DynamoDB Streams captures a time-ordered sequence of item-level modifications in a DynamoDB table and durably stores the information for up to 24 hours. Applications can access a series of stream records, which contain an item change, from a DynamoDB stream in near real-time. Create an AWS Lambda function to periodically back up and restore the Amazon Aurora database in another region - AWS Lambda can be used to create backups for Amazon RDS. But, the process is not an optimized solution, especially when Amazon Aurora already offers the read replica feature. References: https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html https://aws.amazon.com/rds/features/multi-az/ https://aws.amazon.com/rds/features/read-replicas/",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A company has developed a popular photo-sharing website using a serverless pattern on the AWS Cloud using Amazon API Gateway and AWS Lambda. The backend uses an Amazon RDS PostgreSQL database. The website is experiencing high read traffic and the AWS Lambda functions are putting an increased read load on the Amazon RDS database. The architecture team is planning to increase the read throughput of the database, without changing the application's core logic. As a Solutions Architect, what do you recommend?",
      "options": [
        "A. Use Amazon RDS Multi-AZ feature",
        "B. Use Amazon RDS Read Replicas",
        "C. Use Amazon ElastiCache",
        "D. Use Amazon DynamoDB"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Use Amazon RDS Read Replicas Amazon RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. You can create one or more replicas of a given source DB Instance and serve high-volume application read traffic from multiple copies of your data, thereby increasing aggregate read throughput. Read replicas can also be promoted when needed to become standalone DB instances. More on Amazon RDS Read Replicas: via - https://aws.amazon.com/rds/features/read-replicas/ Incorrect options: Use Amazon RDS Multi-AZ feature - Amazon RDS Multi-AZ deployments provide enhanced availability and durability for RDS database (DB) instances, making them a natural fit for production database workloads. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). Each AZ runs on its own physically distinct, independent infrastructure, and is engineered to be highly reliable. In case of an infrastructure failure, Amazon RDS performs an automatic failover to the standby (or to a read replica in the case of Amazon Aurora), so that you can resume database operations as soon as the failover is complete. Use Amazon ElastiCache - Amazon ElastiCache allows you to seamlessly set up, run, and scale popular open-Source compatible in-memory data stores in the cloud. Build data-intensive apps or boost the performance of your existing databases by retrieving data from high throughput and low latency in-memory data stores. Amazon ElastiCache is a popular choice for real-time use cases like Caching, Session Stores, Gaming, Geospatial Services, Real-Time Analytics, and Queuing. Use Amazon DynamoDB - Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-Region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications. Amazon DynamoDB can handle more than 10 trillion requests per day and can support peaks of more than 20 million requests per second. Amazon RDS Multi-AZ helps with disaster recovery in case of an AZ failure. Amazon ElastiCache would definitely help with the read load, but would require a refactor of the application's core logic. Amazon DynamoDB with DAX would also probably help with the read load, but once again it would require a refactor of the application's core logic. Here, our only option to scale reads is to use Amazon RDS Read Replicas. References: https://aws.amazon.com/rds/features/multi-az/ https://aws.amazon.com/rds/features/read-replicas/",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A digital media company needs to manage uploads of around 1 terabyte each from an application being used by a partner company. As a Solutions Architect, how will you handle the upload of these files to Amazon S3?",
      "options": [
        "A. Use multi-part upload feature of Amazon S3",
        "B. Use Amazon S3 Versioning",
        "C. Use AWS Snowball",
        "D. Use AWS Direct Connect to provide extra bandwidth"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Use multi-part upload feature of Amazon S3 Multi-part upload allows you to upload a single object as a set of parts. Each part is a contiguous portion of the object's data. You can upload these object parts independently and in any order. If transmission of any part fails, you can retransmit that part without affecting other parts. After all parts of your object are uploaded, Amazon S3 assembles these parts and creates the object. AWS recommends that you use multi-part uploading in the following ways: 1. If you're uploading large objects over a stable high-bandwidth network, use multi-part uploading to maximize the use of your available bandwidth by uploading object parts in parallel for multi-threaded performance. 2. If you're uploading over a spotty network, use multi-part uploading to increase resiliency to network errors by avoiding upload restarts. When using multi-part uploading, you need to retry uploading only parts that are interrupted during the upload. You don't need to restart uploading your object from the beginning. In general, when your object size reaches 100 megabytes, you should consider using multipart uploads instead of uploading the object in a single operation. If the file is greater than 5 gigabytes in size, you must use multi-part upload to upload that file to Amazon S3. Incorrect options: Use Amazon S3 Versioning - Amazon S3 Versioning is a means of keeping multiple variants of an object in the same bucket. You can use versioning to preserve, retrieve, and restore every version of every object stored in your Amazon S3 bucket. With versioning, you can easily recover from both unintended user actions and application failures. When you enable versioning for a bucket, if Amazon S3 receives multiple write requests for the same object simultaneously, it stores all of the objects. If you overwrite an object, it results in a new object version in the bucket. You can always restore the previous version. Use AWS Direct Connect to provide extra bandwidth - AWS Direct Connect is a cloud service solution that makes it easy to establish a dedicated network connection from your premises to AWS. Using AWS Direct Connect, you can establish private connectivity between AWS and your datacenter, office, or colocation environment, which in many cases can reduce your network costs, increase bandwidth throughput, and provide a more consistent network experience than Internet-based connections. AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations. This dedicated connection can be partitioned into multiple virtual interfaces. This allows you to use the same connection to access public resources such as objects stored in Amazon S3 using public IP address space, and private resources such as Amazon EC2 instances running within an Amazon Virtual Private Cloud (VPC) using private IP space, while maintaining network separation between the public and private environments. Virtual interfaces can be reconfigured at any time to meet your changing needs. This is a physical connection that takes at least a month to set up. Use AWS Snowball - AWS Snowball Edge Storage Optimized is the optimal choice if you need to securely and quickly transfer dozens of terabytes to petabytes of data to AWS. It provides up to 80 terabytes of usable HDD storage, 40 vCPUs, 1 TB of SATA SSD storage, and up to 40 gigabytes network connectivity to address large scale data transfer and pre-processing use cases. (The original AWS Snowball devices were transitioned out of service and AWS Snowball Edge Storage Optimized are now the primary devices used for data transfer. You may see the Snowball device on the exam, just remember that the original AWS Snowball device had 80 terabytes of storage space). References: https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html https://docs.aws.amazon.com/AmazonS3/latest/dev/UploadingObjects.html",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A company has noticed that its Amazon EBS Elastic Volume (io1) accounts for 90% of the cost and the remaining 10% cost can be attributed to the Amazon EC2 instance. The Amazon CloudWatch metrics report that both the Amazon EC2 instance and the Amazon EBS volume are under-utilized. The Amazon CloudWatch metrics also show that the Amazon EBS volume has occasional I/O bursts. The entire infrastructure is managed by AWS CloudFormation. As a Solutions Architect, what do you propose to reduce the costs?",
      "options": [
        "A. Don't use a AWS CloudFormation template to create the database as the AWS CloudFormation service incurs greater service charges",
        "B. Keep the Amazon EBS volume to io1 and reduce the IOPS",
        "C. Convert the Amazon EC2 instance EBS volume to gp2",
        "D. Change the Amazon EC2 instance type to something much smaller"
      ],
      "correct_answer": "C",
      "explanation": "Correct option: Amazon EBS provides the various volume types, that differ in performance characteristics and price so that you can tailor your storage performance and cost to the needs of your applications. The volumes types fall into two categories: SSD-backed volumes optimized for transactional workloads involving frequent read/write operations with small I/O size, where the dominant performance attribute is IOPS. HDD-backed volumes optimized for large streaming workloads where throughput (measured in MiB/s) is a better performance measure than IOPS Provisioned IOPS SSD (io1) volumes are designed to meet the needs of I/O-intensive workloads, particularly database workloads, that are sensitive to storage performance and consistency. Unlike gp2, which uses a bucket and credit model to calculate performance, an io1 volume allows you to specify a consistent IOPS rate when you create the volume, and Amazon EBS delivers the provisioned performance 99.9 percent of the time. Convert the Amazon EC2 instance EBS volume to gp2 General Purpose SSD (gp2) volumes offer cost-effective storage that is ideal for a broad range of workloads. These volumes deliver single-digit millisecond latencies and the ability to burst to 3,000 IOPS for an extended duration. Between a minimum of 100 IOPS (at 33.33 GiB and below) and a maximum of 16,000 IOPS (at 5,334 GiB and above), baseline performance scales linearly at 3 IOPS per GiB of volume size. AWS designs gp2 volumes to deliver a provisioned performance of 99% uptime. A gp2 volume can range in size from 1 GiB to 16 TiB. Therefore, gp2 is the right choice as it is more cost-effective than io1, and it also allows a burst in performance when needed. Incorrect options: Keep the Amazon EBS volume to io1 and reduce the IOPS - Keeping the Amazon EBS volume to io1 and reducing the IOPS may interfere with the burst of performance we need, so this option is ruled out. Change the Amazon EC2 instance type to something much smaller - Changing the Amazon EC2 instance type to something much smaller won't affect 90% of the costs that are incurred, therefore this option is also incorrect. Don't use a AWS CloudFormation template to create the database as the AWS CloudFormation service incurs greater service charges - This statement is incorrect as AWS CloudFormation is a free service to use. The resources that are invoked by CloudFormation are charged as per their utilization rates, but using AWS CloudFormation will not cost anything. References: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html#EBSVolumeTypes_gp2 https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html#EBSVolumeTypes_piops",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "As a solutions architect, you have created a solution that utilizes an Application Load Balancer with stickiness and an Auto Scaling Group (ASG). The Auto Scaling Group spans across 2 Availability Zones (AZs). AZ-A has 3 Amazon EC2 instances and AZ-B has 4 Amazon EC2 instances. The Auto Scaling Group is about to go into a scale-in event due to the triggering of a Amazon CloudWatch alarm. What will happen under the default Auto Scaling Group configuration?",
      "options": [
        "A. The instance with the oldest launch template or launch configuration will be terminated in AZ-B",
        "B. A random instance in the AZ-A will be terminated",
        "C. An instance in the AZ-A will be created",
        "D. A random instance will be terminated in AZ-B"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: The instance with the oldest launch template or launch configuration will be terminated in AZ-B Amazon EC2 Auto Scaling helps you ensure that you have the correct number of Amazon EC2 instances available to handle the load for your application. You create collections of Amazon EC2 instances, called Auto Scaling groups. You can specify the minimum number of instances in each Auto Scaling group, and Amazon EC2 Auto Scaling ensures that your group never goes below this size. With each Auto Scaling group, you can control when it adds instances (referred to as scaling out) or removes instances (referred to as scaling in) from your network architecture. The default termination policy is designed to help ensure that your instances span Availability Zones evenly for high availability. The default policy is kept generic and flexible to cover a range of scenarios. The default termination policy behavior is as follows: 1. Determine which Availability Zones (Azs) have the most instances and at least one instance that is not protected from scale-in. 2. Determine which instances to terminate to align the remaining instances to the allocation strategy for the On-Demand or Spot Instance that is terminating. 3. Determine whether any of the instances use the oldest launch template or configuration: 3.a. Determine whether any of the instances use the oldest launch template unless there are instances that use a launch configuration. 3.b. Determine whether any of the instances use the oldest launch configuration. 4. After applying all of the above criteria, if there are multiple unprotected instances to terminate, determine which instances are closest to the next billing hour. Per the given use-case, AZs will be balanced first, then the instance with the oldest launch template or launch configuration within the applicable AZ (AZ-B) will be terminated. Default Termination policy: via - https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html Incorrect options: A random instance in the AZ-A will be terminated An instance in the AZ-A will be created A random instance will be terminated in AZ-B These three options contradict the details provided in the explanation above. Hence these are incorrect. Reference: https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "What does this AWS CloudFormation snippet do? (Select three) SecurityGroupIngress: - IpProtocol: tcp FromPort: 80 ToPort: 80 CidrIp: 0.0.0.0/0 - IpProtocol: tcp FromPort: 22 ToPort: 22 CidrIp: 192.168.1.1/32",
      "options": [
        "A. It configures the inbound rules of a network access control list (network ACL)",
        "B. It allows any IP to pass through on the HTTP port",
        "C. It only allows the IP 0.0.0.0 to reach HTTP",
        "D. It prevents traffic from reaching on HTTP unless from the IP 192.168.1.1",
        "E. It configures a security group's inbound rules",
        "F. It lets traffic flow from one IP on port 22",
        "G. It configures a security group's outbound rules"
      ],
      "correct_answer": "B",
      "explanation": "Correct options: It allows any IP to pass through on the HTTP port It configures a security group's inbound rules It lets traffic flow from one IP on port 22 A security group acts as a virtual firewall that controls the traffic for one or more instances. When you launch an instance, you can specify one or more security groups; otherwise, we use the default security group. You can add rules to each security group that allows traffic to or from its associated instances. You can modify the rules for a security group at any time; the new rules are automatically applied to all instances that are associated with the security group. When we decide whether to allow traffic to reach an instance, we evaluate all the rules from all the security groups that are associated with the instance. The following are the characteristics of security group rules: 1. By default, security groups allow all outbound traffic. 2. Security group rules are always permissive; you can't create rules that deny access. 3. Security groups are stateful AWS CloudFormation provides a common language for you to model and provision AWS and third-party application resources in your cloud environment. AWS CloudFormation allows you to use programming languages or a simple text file to model and provision, in an automated and secure manner, all the resources needed for your applications across all regions and accounts. This gives you a single source of truth for your AWS and third-party resources. Considering the given AWS CloudFormation snippet, 0.0.0.0/0 means any IP, not the IP 0.0.0.0. Ingress means traffic going into your instance, and Security Groups are different from NACL. Each \"-\" in our security group rule represents a different rule (YAML syntax) Therefore the AWS CloudFormation snippet creates two Security Group inbound rules that allow any IP to pass through on the HTTP port and lets traffic flow from one source IP (192.168.1.1) on port 22. Incorrect options: It configures the inbound rules of a network access control list (network ACL) - A Network Access Control List ( Network ACL) is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets. You might set up network ACLs with rules similar to your security groups to add an additional layer of security to your VPC. It only allows the IP 0.0.0.0 to reach HTTP It prevents traffic from reaching on HTTP unless from the IP 192.168.1.1 It configures a security group's outbound rules These three options contradict the description provided above. So these are incorrect. References: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html https://aws.amazon.com/cloudformation/",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A retail company is using AWS Site-to-Site VPN connections for secure connectivity to its AWS cloud resources from its on-premises data center. Due to a surge in traffic across the VPN connections to the AWS cloud, users are experiencing slower VPN connectivity. Which of the following options will maximize the VPN throughput?",
      "options": [
        "A. Use Transfer Acceleration for the VPN connection to maximize the throughput",
        "B. Use AWS Global Accelerator for the VPN connection to maximize the throughput",
        "C. Create a virtual private gateway with equal cost multipath routing and multiple channels",
        "D. Create an AWS Transit Gateway with equal cost multipath routing and add additional VPN tunnels"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: Create an AWS Transit Gateway with equal cost multipath routing and add additional VPN tunnels VPN connection is a secure connection between your on-premises equipment and your VPCs. Each VPN connection has two VPN tunnels which you can use for high availability. A VPN tunnel is an encrypted link where data can pass from the customer network to or from AWS. The following diagram shows the high-level connectivity with virtual private gateways. With AWS Transit Gateway, you can simplify the connectivity between multiple VPCs and also connect to any VPC attached to AWS Transit Gateway with a single VPN connection. AWS Transit Gateway also enables you to scale the IPsec VPN throughput with equal cost multi-path (ECMP) routing support over multiple VPN tunnels. A single VPN tunnel still has a maximum throughput of 1.25 Gbps. If you establish multiple VPN tunnels to an ECMP-enabled transit gateway, it can scale beyond the default maximum limit of 1.25 Gbps. You also must enable the dynamic routing option on your transit gateway to be able to take advantage of ECMP for scalability. via - https://aws.amazon.com/premiumsupport/knowledge-center/transit-gateway-ecmp-multiple-tunnels/ Incorrect options: Use Transfer Acceleration for the VPN connection to maximize the throughput - Transfer Acceleration is an Amazon S3 bucket-level feature that enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket. Transfer Acceleration is designed to optimize transfer speeds from across the world into S3 buckets. Transfer Acceleration takes advantage of the globally distributed edge locations in Amazon CloudFront. This option has been added as a distractor as it is not relevant to AWS VPN connections. Use AWS Global Accelerator for the VPN connection to maximize the throughput - AWS Global Accelerator is a networking service that improves the performance of your users’ traffic by up to 60% using the global network infrastructure of AWS. When the internet is congested, AWS Global Accelerator optimizes the path to your application to keep packet loss, jitter, and latency consistently low. With Global Accelerator, you are provided two global static public IPs that act as a fixed entry point to your application, improving availability. Global Accelerator automatically re-routes your traffic to your nearest healthy available endpoint to mitigate endpoint failure. AWS Global Accelerator can be used to optimize the network path, using the congestion-free AWS global network to route traffic to the endpoint that provides the best application performance . You can use an accelerated VPN connection to avoid network disruptions that might occur when traffic is routed over the public internet. AWS Global Accelerator will not maximize the VPN throughput, so it is not the best fit for the given use case. Create a virtual private gateway with equal cost multipath routing and multiple channels - A virtual private gateway is the VPN endpoint on the Amazon side of your Site-to-Site VPN connection that can be attached to a single VPC. A virtual private gateway does not support equal cost multi-path (ECMP) routing, so this option is incorrect. References: https://docs.aws.amazon.com/AmazonS3/latest/userguide/transfer-acceleration.html https://aws.amazon.com/premiumsupport/knowledge-center/transit-gateway-ecmp-multiple-tunnels/",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An e-commerce company wants to migrate its on-premises application to AWS. The application consists of application servers and a Microsoft SQL Server database. The solution should result in the maximum possible availability for the database layer while minimizing operational and management overhead. As a solutions architect, which of the following would you recommend to meet the given requirements?",
      "options": [
        "A. Migrate the data to Amazon EC2 instance hosted SQL Server database. Deploy the Amazon EC2 instances in a Multi-AZ configuration",
        "B. Migrate the data to Amazon RDS for SQL Server database in a cross-region read-replica configuration",
        "C. Migrate the data to Amazon RDS for SQL Server database in a cross-region Multi-AZ deployment",
        "D. Migrate the data to Amazon RDS for SQL Server database in a Multi-AZ deployment"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: Migrate the data to Amazon RDS for SQL Server database in a Multi-AZ deployment Amazon RDS supports Multi-AZ deployments for Microsoft SQL Server by using either SQL Server Database Mirroring (DBM) or Always On Availability Groups (AGs). Amazon RDS monitors and maintains the health of your Multi-AZ deployment. If problems occur, Amazon RDS automatically repairs unhealthy database instances, reestablishes synchronization, and initiates failovers. Multi-AZ deployments provide increased availability, data durability, and fault tolerance for database instances. In the event of planned database maintenance or unplanned service disruption, Amazon RDS automatically fails over to the up-to-date secondary database instance. This functionality lets database operations resume quickly without manual intervention. The primary and standby instances use the same endpoint, whose physical network address transitions to the secondary replica as part of the failover process. You don't have to reconfigure your application when a failover occurs. This option provides the maximum possible availability for the database layer while minimizing operational and management overhead. Incorrect options: Migrate the data to Amazon EC2 instance hosted SQL Server database. Deploy the Amazon EC2 instances in a Multi-AZ configuration - Hosting SQL Server database on Amazon EC2 instance involves significant operational and management overhead in terms of OS patching, database patching, etc. So this option is incorrect. Migrate the data to Amazon RDS for SQL Server database in a cross-region read-replica configuration - Amazon RDS Read Replicas enable you to create one or more read-only copies of your database instance within the same AWS Region or in a different AWS Region. Read replicas are used to enhance the read scalability of a database. You cannot use read replicas to improve the availability of a database. Therefore this option is incorrect. Migrate the data to Amazon RDS for SQL Server database in a cross-region Multi-AZ deployment - Amazon RDS Multi-AZ deployments provide enhanced availability for database instances within a single AWS Region. There is no such thing as a cross-region Multi-AZ deployment. Hence this option is incorrect. References: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_SQLServerMultiAZ.html https://aws.amazon.com/about-aws/whats-new/2018/01/amazon-rds-read-replicas-now-support-multi-az-deployments/",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A Big Data processing company has created a distributed data processing framework that performs best if the network performance between the processing machines is high. The application has to be deployed on AWS, and the company is only looking at performance as the key measure. As a Solutions Architect, which deployment do you recommend?",
      "options": [
        "A. Use Spot Instances",
        "B. Use a Spread placement group",
        "C. Optimize the Amazon EC2 kernel using EC2 User Data",
        "D. Use a Cluster placement group"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: When you launch a new Amazon EC2 instance, the EC2 service attempts to place the instance in such a way that all of your instances are spread out across underlying hardware to minimize correlated failures. You can use placement groups to influence the placement of a group of interdependent instances to meet the needs of your workload. Depending on the type of workload, you can create a placement group using one of the following placement strategies: Cluster – packs instances close together inside an Availability Zone (AZ). This strategy enables workloads to achieve the low-latency network performance necessary for tightly-coupled node-to-node communication that is typical of HPC applications. Partition – spreads your instances across logical partitions such that groups of instances in one partition do not share the underlying hardware with groups of instances in different partitions. This strategy is typically used by large distributed and replicated workloads, such as Hadoop, Cassandra, and Kafka. Spread – strictly places a small group of instances across distinct underlying hardware to reduce correlated failures. There is no charge for creating a placement group. Use a Cluster placement group A cluster placement group is a logical grouping of instances within a single Availability Zone (AZ). A cluster placement group can span peered VPCs in the same Region. Instances in the same cluster placement group enjoy a higher per-flow throughput limit of up to 10 Gbps for TCP/IP traffic and are placed in the same high-bisection bandwidth segment of the network. Cluster placement groups are recommended for applications that benefit from low network latency, high network throughput, or both. They are also recommended when the majority of the network traffic is between the instances in the group. To provide the lowest latency and the highest packet-per-second network performance for your placement group, choose an instance type that supports enhanced networking. Image of Cluster placement group: via - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html Image of Partition placement group: via - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html Image of Spread placement group: via - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html Incorrect options: Use Spot Instances - A Spot Instance is an unused Amazon EC2 instance that is available for less than the On-Demand price. Because Spot Instances enable you to request unused Amazon EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly. Spot Instances are a cost-effective choice if you can be flexible about when your applications run and if your applications can be interrupted. Since performance is the key criteria, this is not the right choice. Optimize the Amazon EC2 kernel using EC2 User Data - Optimizing the Amazon EC2 kernel won't help with network performance as it's bounded by the EC2 instance type mainly. Therefore, this option is incorrect. Use a Spread placement group - A spread placement group is a group of instances that are each placed on distinct racks, with each rack having its own network and power source. The instances are placed across distinct underlying hardware to reduce correlated failures. A spread placement group can span multiple Availability Zones (AZs) in the same Region. You can have a maximum of seven running instances per Availability Zone (AZ) per group. Reference: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A photo hosting service publishes a collection of beautiful mountain images, every month, that aggregate over 50 gigabytes in size and downloaded all around the world. The content is currently hosted on Amazon EFS and distributed by Elastic Load Balancing (ELB) and Amazon EC2 instances. The website is experiencing high load each month and very high network costs. As a Solutions Architect, what can you recommend that won't force an application refactor and reduce network costs and Amazon EC2 load drastically?",
      "options": [
        "A. Host the master pack onto Amazon S3 for faster access",
        "B. Create an Amazon CloudFront distribution",
        "C. Upgrade the Amazon EC2 instances",
        "D. Enable Elastic Load Balancing (ELB) caching"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Create an Amazon CloudFront distribution Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment. Amazon CloudFront points of presence (POPs) (edge locations) make sure that popular content can be served quickly to your viewers. Amazon CloudFront also has regional edge caches that bring more of your content closer to your viewers, even when the content is not popular enough to stay at a POP, to help improve performance for that content. Regional edge caches help with all types of content, particularly content that tends to become less popular over time. Examples include user-generated content, such as video, photos, or artwork; e-commerce assets such as product photos and videos; and news and event-related content that might suddenly find new popularity. For the given use case, you need to create an Amazon CloudFront distribution to add a caching layer in front of your ELB. That caching layer will be very effective as the image pack is a static file, and therefore it would save the network costs significantly without requiring an application refactor. How Amazon CloudFront delivers content to your users: via - https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/HowCloudFrontWorks.html Incorrect options: Host the master pack onto Amazon S3 for faster access - Hosting the master pack into Amazon S3 will result in application code refactoring. So, this option is not correct. Upgrade the Amazon EC2 instances - Upgrading the Amazon EC2 instances won't help save network cost. Hence, this option is incorrect for the given scenario. Enable Elastic Load Balancing (ELB) caching - ELB does not have any caching capability. This option is just added as a distractor. Reference: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/HowCloudFrontWorks.html",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An e-commerce company tracks user clicks on its flagship website and performs analytics to provide near-real-time product recommendations. An Amazon EC2 instance receives data from the website and sends the data to an Amazon Aurora Database instance. Another Amazon EC2 instance continuously checks the changes in the database and executes SQL queries to provide recommendations. Now, the company wants a redesign to decouple and scale the infrastructure. The solution must ensure that data can be analyzed in real-time without any data loss even when the company sees huge traffic spikes. What would you recommend as an AWS Certified Solutions Architect - Associate?",
      "options": [
        "A. Leverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Amazon QuickSight which can query the data in real time. Lastly, the analyzed feed is output into Kinesis Data Firehose to persist the data on Amazon S3",
        "B. Leverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Amazon Kinesis Data Analytics which can query the data in real time. Lastly, the analyzed feed is output into Amazon Kinesis Data Firehose to persist the data on Amazon S3",
        "C. Leverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Amazon Kinesis Data Firehose to persist the data on Amazon S3. Lastly, use Amazon Athena to analyze the data in real time",
        "D. Leverage Amazon SQS to capture the data from the website. Configure a fleet of Amazon EC2 instances under an Auto scaling group to process messages from the Amazon SQS queue and trigger the scaling policy based on the number of pending messages in the queue. Perform real-time analytics using a third-party library on the Amazon EC2 instances"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Leverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Amazon Kinesis Data Analytics which can query the data in real time. Lastly, the analyzed feed is output into Amazon Kinesis Data Firehose to persist the data on Amazon S3 You can use Amazon Kinesis Data Streams to build custom applications that process or analyze streaming data for specialized needs. Amazon Kinesis Data Streams manages the infrastructure, storage, networking, and configuration needed to stream your data at the level of your data throughput. You don't have to worry about provisioning, deployment, or ongoing maintenance of hardware, software, or other services for your data streams. For the given use case, you can use Amazon Kinesis Data Analytics to transform and analyze incoming streaming data from Kinesis Data Streams in real time. Kinesis Data Analytics takes care of everything required to run streaming applications continuously, and scales automatically to match the volume and throughput of your incoming data. With Amazon Kinesis Data Analytics, there are no servers to manage, no minimum fee or setup cost, and you only pay for the resources your streaming applications consume. Amazon Kinesis Data Analytics: via - https://aws.amazon.com/kinesis/ Amazon Kinesis Data Firehose is an extract, transform, and load (ETL) service that reliably captures, transforms and delivers streaming data to data lakes, data stores, and analytics services. For the given use case, post the real-time analysis, the output feed from Kinesis Data Analytics is output into Kinesis Data Firehose which dumps the data into Amazon S3 without any data loss. Amazon Kinesis Data Firehose: via - https://aws.amazon.com/kinesis/ Incorrect options: Leverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Amazon QuickSight which can query the data in real time. Lastly, the analyzed feed is output into Kinesis Data Firehose to persist the data on Amazon S3 - Amazon QuickSight cannot use Amazon Kinesis Data Streams as a source. In addition, Amazon QuickSight cannot be used for real-time streaming data analysis from its source. Therefore this option is incorrect. Leverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Amazon Kinesis Data Firehose to persist the data on Amazon S3. Lastly, use Amazon Athena to analyze the data in real time - Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run. Athena cannot be used to analyze data in real time. Therefore this option is incorrect. Leverage Amazon SQS to capture the data from the website. Configure a fleet of Amazon EC2 instances under an Auto scaling group to process messages from the Amazon SQS queue and trigger the scaling policy based on the number of pending messages in the queue. Perform real-time analytics using a third-party library on the Amazon EC2 instances - Even though using Amazon SQS with Amazon EC2 instances can decouple the architecture, however, performing real-time analytics using a third party library on the Amazon EC2 instances is not the best fit solution for the given use case. The Amazon Kinesis family of services is the better fit for the given scenario as these services allow streaming data ingestion, real-time analysis, and reliable data delivery to the data sink. References: https://aws.amazon.com/kinesis/ https://aws.amazon.com/quicksight/resources/faqs/",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "For security purposes, a development team has decided to deploy the Amazon EC2 instances in a private subnet. The team plans to use VPC endpoints so that the instances can access some AWS services securely. The members of the team would like to know about the two AWS services that support Gateway Endpoints. As a solutions architect, which of the following services would you suggest for this requirement? (Select two)",
      "options": [
        "A. Amazon Simple Queue Service (Amazon SQS)",
        "B. Amazon Simple Notification Service (Amazon SNS)",
        "C. Amazon DynamoDB",
        "D. Amazon S3",
        "E. Amazon Kinesis"
      ],
      "correct_answer": "C",
      "explanation": "Correct options: Amazon S3 Amazon DynamoDB A VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by AWS PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Instances in your VPC do not require public IP addresses to communicate with resources in the service. Traffic between your VPC and the other service does not leave the Amazon network. Endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between instances in your VPC and services without imposing availability risks or bandwidth constraints on your network traffic. There are two types of VPC endpoints: Interface Endpoints and Gateway Endpoints. An Interface Endpoint is an Elastic Network Interface with a private IP address from the IP address range of your subnet that serves as an entry point for traffic destined to a supported service. A Gateway Endpoint is a gateway that you specify as a target for a route in your route table for traffic destined to a supported AWS service. The following AWS services are supported: Amazon S3 and Amazon DynamoDB. You can use two types of VPC endpoints to access Amazon S3: gateway endpoints and interface endpoints. A gateway endpoint is a gateway that you specify in your route table to access Amazon S3 from your VPC over the AWS network. Interface endpoints extend the functionality of gateway endpoints by using private IP addresses to route requests to Amazon S3 from within your VPC, on premises, or from a VPC in another AWS Region using VPC peering or AWS Transit Gateway. You must remember that these two services use a VPC gateway endpoint. The rest of the AWS services use VPC interface endpoints. Gateway VPC endpoints: via - https://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html Incorrect options: Amazon Simple Queue Service (Amazon SQS) Amazon Simple Notification Service (Amazon SNS) Amazon Kinesis As mentioned in the description above, these three options use interface endpoints, so these are incorrect. Reference: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "You are working as an AWS architect for a weather tracking facility. You are asked to set up a Disaster Recovery (DR) mechanism with minimum costs. In case of failure, the facility can only bear data loss of approximately 15 minutes without jeopardizing the forecasting models. As a Solutions Architect, which DR method will you suggest?",
      "options": [
        "A. Pilot Light",
        "B. Backup and Restore",
        "C. Warm Standby",
        "D. Multi-Site"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Pilot Light The term pilot light is often used to describe a DR scenario in which a minimal version of an environment is always running in the cloud. The idea of the pilot light is an analogy that comes from the gas heater. In a gas heater, a small flame that’s always on can quickly ignite the entire furnace to heat up a house. This scenario is similar to a backup-and-restore scenario. For example, with AWS you can maintain a pilot light by configuring and running the most critical core elements of your system in AWS. For the given use-case, a small part of the backup infrastructure is always running simultaneously syncing mutable data (such as databases or documents) so that there is no loss of critical data. When the time comes for recovery, you can rapidly provision a full-scale production environment around the critical core. For Pilot light, RPO/RTO is in 10s of minutes, so this is the correct solution. Four Disaster Recovery scenarios: via - https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/plan-for-disaster-recovery-dr.html Incorrect options: Backup and Restore - In most traditional environments, data is backed up to tape and sent off-site regularly. If you use this method, it can take a long time to restore your system in the event of a disruption or disaster. Amazon S3 is an ideal destination for backup data that might be needed quickly to perform a restore. Transferring data to and from Amazon S3 is typically done through the network and is therefore accessible from any location. There are many commercial and open-source backup solutions that integrate with Amazon S3. You can use AWS Import/Export to transfer very large data sets by shipping storage devices directly to AWS. For longer-term data storage where retrieval times of several hours are adequate, there is Amazon Glacier, which has the same durability model as Amazon S3. Amazon Glacier is a low-cost alternative starting from $0.01/GB per month. Amazon Glacier and Amazon S3 can be used in conjunction to produce a tiered backup solution. Even though Backup and Restore method is cheaper, it has an RPO in hours, so this option is not the right fit. Warm Standby - The term warm standby is used to describe a DR scenario in which a scaled-down version of a fully functional environment is always running in the cloud. A warm standby solution extends the pilot light elements and preparation. It further decreases the recovery time because some services are always running. By identifying your business-critical systems, you can fully duplicate these systems on AWS and have them always on. This option is costlier compared to Pilot Light. Multi-Site - A multi-site solution runs on AWS as well as on your existing on-site infrastructure in an active-active configuration. The data replication method that you employ will be determined by the recovery point that you choose, either Recovery Time Objective (the maximum allowable downtime before degraded operations are restored) or Recovery Point Objective (the maximum allowable time window whereby you will accept the loss of transactions during the DR process). This option is more costly compared to Pilot Light. References: https://aws.amazon.com/blogs/architecture/disaster-recovery-dr-architecture-on-aws-part-iii-pilot-light-and-warm-standby/ https://aws.amazon.com/blogs/publicsector/rapidly-recover-mission-critical-systems-in-a-disaster/ https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/plan-for-disaster-recovery-dr.html https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/disaster-recovery-dr-objectives.html",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "The engineering team at an e-commerce company has been tasked with migrating to a serverless architecture. The team wants to focus on the key points of consideration when using AWS Lambda as a backbone for this architecture. As a Solutions Architect, which of the following options would you identify as correct for the given requirement? (Select three)",
      "options": [
        "A. By default, AWS Lambda functions always operate from an AWS-owned VPC and hence have access to any public internet address or public AWS APIs. Once an AWS Lambda function is VPC-enabled, it will need a route through a Network Address Translation gateway (NAT gateway) in a public subnet to access public resources",
        "B. AWS Lambda allocates compute power in proportion to the memory you allocate to your function. AWS, thus recommends to over provision your function time out settings for the proper performance of AWS Lambda functions",
        "C. The bigger your deployment package, the slower your AWS Lambda function will cold-start. Hence, AWS suggests packaging dependencies as a separate package from the actual AWS Lambda package",
        "D. Since AWS Lambda functions can scale extremely quickly, it's a good idea to deploy a Amazon CloudWatch Alarm that notifies your team when function metrics such as ConcurrentExecutions or Invocations exceeds the expected threshold",
        "E. If you intend to reuse code in more than one AWS Lambda function, you should consider creating an AWS Lambda Layer for the reusable code",
        "F. Serverless architecture and containers complement each other but you cannot package and deploy AWS Lambda functions as container images"
      ],
      "correct_answer": "A",
      "explanation": "Correct options: By default, AWS Lambda functions always operate from an AWS-owned VPC and hence have access to any public internet address or public AWS APIs. Once an AWS Lambda function is VPC-enabled, it will need a route through a Network Address Translation gateway (NAT gateway) in a public subnet to access public resources AWS Lambda functions always operate from an AWS-owned VPC. By default, your function has the full ability to make network requests to any public internet address — this includes access to any of the public AWS APIs. For example, your function can interact with AWS DynamoDB APIs to PutItem or Query for records. You should only enable your functions for VPC access when you need to interact with a private resource located in a private subnet. An Amazon RDS instance is a good example. Once your function is VPC-enabled, all network traffic from your function is subject to the routing rules of your VPC/Subnet. If your function needs to interact with a public resource, you will need a route through a NAT gateway in a public subnet. When to VPC-Enable an AWS Lambda Function: via - https://aws.amazon.com/blogs/architecture/best-practices-for-developing-on-aws-lambda/ Since AWS Lambda functions can scale extremely quickly, it's a good idea to deploy a Amazon CloudWatch Alarm that notifies your team when function metrics such as ConcurrentExecutions or Invocations exceeds the expected threshold Since AWS Lambda functions can scale extremely quickly, this means you should have controls in place to notify you when you have a spike in concurrency. A good idea is to deploy an Amazon CloudWatch Alarm that notifies your team when function metrics such as ConcurrentExecutions or Invocations exceeds your threshold. You should create an AWS Budget so you can monitor costs on a daily basis. If you intend to reuse code in more than one AWS Lambda function, you should consider creating an AWS Lambda Layer for the reusable code You can configure your AWS Lambda function to pull in additional code and content in the form of layers. A layer is a ZIP archive that contains libraries, a custom runtime, or other dependencies. With layers, you can use libraries in your function without needing to include them in your deployment package. Layers let you keep your deployment package small, which makes development easier. A function can use up to 5 layers at a time. You can create layers, or use layers published by AWS and other AWS customers. Layers support resource-based policies for granting layer usage permissions to specific AWS accounts, AWS Organizations, or all accounts. The total unzipped size of the function and all layers can't exceed the unzipped deployment package size limit of 250 megabytes. Incorrect options: AWS Lambda allocates compute power in proportion to the memory you allocate to your function. AWS, thus recommends to over provision your function time out settings for the proper performance of AWS Lambda functions - AWS Lambda allocates compute power in proportion to the memory you allocate to your function. This means you can over-provision memory to run your functions faster and potentially reduce your costs. However, AWS recommends that you should not over provision your function time out settings. Always understand your code performance and set a function time out accordingly. Overprovisioning function timeout often results in Lambda functions running longer than expected and unexpected costs. The bigger your deployment package, the slower your AWS Lambda function will cold-start. Hence, AWS suggests packaging dependencies as a separate package from the actual AWS Lambda package - This statement is incorrect and acts as a distractor. All the dependencies are also packaged into the single Lambda deployment package. Serverless architecture and containers complement each other but you cannot package and deploy AWS Lambda functions as container images - This statement is incorrect. You can now package and deploy AWS Lambda functions as container images. References: https://aws.amazon.com/blogs/architecture/best-practices-for-developing-on-aws-lambda/ https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html https://aws.amazon.com/blogs/aws/new-for-aws-lambda-container-image-support/",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An Elastic Load Balancer has marked all the Amazon EC2 instances in the target group as unhealthy. Surprisingly, when a developer enters the IP address of the Amazon EC2 instances in the web browser, he can access the website. What could be the reason the instances are being marked as unhealthy? (Select two)",
      "options": [
        "A. The security group of the Amazon EC2 instance does not allow for traffic from the security group of the Application Load Balancer",
        "B. The route for the health check is misconfigured",
        "C. The Amazon Elastic Block Store (Amazon EBS) volumes have been improperly mounted",
        "D. Your web-app has a runtime that is not supported by the Application Load Balancer",
        "E. You need to attach elastic IP address (EIP) to the Amazon EC2 instances"
      ],
      "correct_answer": "A",
      "explanation": "Correct options: The security group of the Amazon EC2 instance does not allow for traffic from the security group of the Application Load Balancer The route for the health check is misconfigured An Application Load Balancer periodically sends requests to its registered targets to test their status. These tests are called health checks. Each load balancer node routes requests only to the healthy targets in the enabled Availability Zones (AZs) for the load balancer. Each load balancer node checks the health of each target, using the health check settings for the target groups with which the target is registered. If a target group contains only unhealthy registered targets, the load balancer nodes route requests across its unhealthy targets. You must ensure that your load balancer can communicate with registered targets on both the listener port and the health check port. Whenever you add a listener to your load balancer or update the health check port for a target group used by the load balancer to route requests, you must verify that the security groups associated with the load balancer allow traffic on the new port in both directions. Application Load Balancer Configuration for Security Groups and Health Check Routes: via - https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-update-security-groups.html Incorrect options: The Amazon Elastic Block Store (Amazon EBS) volumes have been improperly mounted - You can access the website using the IP address which means there is no issue with the Amazon EBS volumes. So this option is not correct. Your web-app has a runtime that is not supported by the Application Load Balancer - There is no connection between a web app runtime and the application load balancer. This option has been added as a distractor. You need to attach elastic IP address (EIP) to the Amazon EC2 instances - This option is a distractor as Elastic IPs do not need to be assigned to Amazon EC2 instances while using an Application Load Balancer. References: https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-update-security-groups.html https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A Big Data analytics company writes data and log files in Amazon S3 buckets. The company now wants to stream the existing data files as well as any ongoing file updates from Amazon S3 to Amazon Kinesis Data Streams. As a Solutions Architect, which of the following would you suggest as the fastest possible way of building a solution for this requirement?",
      "options": [
        "A. Configure Amazon EventBridge events for the bucket actions on Amazon S3. An AWS Lambda function can then be triggered from the Amazon EventBridge event that will send the necessary data to Amazon Kinesis Data Streams",
        "B. Leverage Amazon S3 event notification to trigger an AWS Lambda function for the file create event. The AWS Lambda function will then send the necessary data to Amazon Kinesis Data Streams",
        "C. Amazon S3 bucket actions can be directly configured to write data into Amazon Simple Notification Service (Amazon SNS). Amazon SNS can then be used to send the updates to Amazon Kinesis Data Streams",
        "D. Leverage AWS Database Migration Service (AWS DMS) as a bridge between Amazon S3 and Amazon Kinesis Data Streams"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: Leverage AWS Database Migration Service (AWS DMS) as a bridge between Amazon S3 and Amazon Kinesis Data Streams You can achieve this by using AWS Database Migration Service (AWS DMS). AWS DMS enables you to seamlessly migrate data from supported sources to relational databases, data warehouses, streaming platforms, and other data stores in AWS cloud. The given requirement needs the functionality to be implemented in the least possible time. You can use AWS DMS for such data-processing requirements. AWS DMS lets you expand the existing application to stream data from Amazon S3 into Amazon Kinesis Data Streams for real-time analytics without writing and maintaining new code. AWS DMS supports specifying Amazon S3 as the source and streaming services like Kinesis and Amazon Managed Streaming of Kafka (Amazon MSK) as the target. AWS DMS allows migration of full and change data capture (CDC) files to these services. AWS DMS performs this task out of box without any complex configuration or code development. You can also configure an AWS DMS replication instance to scale up or down depending on the workload. AWS DMS supports Amazon S3 as the source and Kinesis as the target, so data stored in an S3 bucket is streamed to Kinesis. Several consumers, such as AWS Lambda, Amazon Kinesis Data Firehose, Amazon Kinesis Data Analytics, and the Kinesis Consumer Library (KCL), can consume the data concurrently to perform real-time analytics on the dataset. Each AWS service in this architecture can scale independently as needed. Architecture of the proposed solution: via - https://aws.amazon.com/blogs/big-data/streaming-data-from-amazon-s3-to-amazon-kinesis-data-streams-using-aws-dms/ Incorrect options: Configure Amazon EventBridge events for the bucket actions on Amazon S3. An AWS Lambda function can then be triggered from the Amazon EventBridge event that will send the necessary data to Amazon Kinesis Data Streams - You will need to enable AWS Cloudtrail trail to use object-level actions as a trigger for Amazon EventBridge events. Also, using AWS Lambda functions would require significant custom development to write the data into Amazon Kinesis Data Streams, so this option is not the right fit. Leverage Amazon S3 event notification to trigger an AWS Lambda function for the file create event. The AWS Lambda function will then send the necessary data to Amazon Kinesis Data Streams - Using AWS Lambda functions would require significant custom development to write the data into Amazon Kinesis Data Streams, so this option is not the right fit. Amazon S3 bucket actions can be directly configured to write data into Amazon Simple Notification Service (Amazon SNS). Amazon SNS can then be used to send the updates to Amazon Kinesis Data Streams - Amazon S3 cannot directly write data into Amazon SNS, although it can certainly use Amazon S3 event notifications to send an event to Amazon SNS. Also, Amazon SNS cannot directly send messages to Amazon Kinesis Data Streams. So this option is incorrect. Reference: https://aws.amazon.com/blogs/big-data/streaming-data-from-amazon-s3-to-amazon-kinesis-data-streams-using-aws-dms/",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A Pharmaceuticals company is looking for a simple solution to connect its VPCs and on-premises networks through a central hub. As a Solutions Architect, which of the following would you suggest as the solution that requires the LEAST operational overhead?",
      "options": [
        "A. Use AWS Transit Gateway to connect the Amazon VPCs to the on-premises networks",
        "B. Use Transit VPC Solution to connect the Amazon VPCs to the on-premises networks",
        "C. Partially meshed VPC peering can be used to connect the Amazon VPCs to the on-premises networks",
        "D. Fully meshed VPC peering can be used to connect the Amazon VPCs to the on-premises networks"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Use AWS Transit Gateway to connect the Amazon VPCs to the on-premises networks The AWS Transit Gateway allows customers to connect their Amazon VPCs and their on-premises networks to a single gateway. As your number of workloads running on AWS increases, you need to be able to scale your networks across multiple accounts and Amazon VPCs to keep up with the growth. With AWS Transit Gateway, you only have to create and manage a single connection from the central gateway into each Amazon VPC, on-premises data center, or remote office across your network. AWS Transit Gateway acts as a hub that controls how traffic is routed among all the connected networks, which act like spokes. This hub and spoke model simplifies management and reduces operational costs because each network only has to connect to the Transit Gateway and not to every other network. AWS Transit Gateway: via - https://aws.amazon.com/transit-gateway/ Incorrect options: Use Transit VPC Solution to connect the Amazon VPCs to the on-premises networks - The Transit VPC can be used to enable connectivity between various VPC’s in different regions and customer data centers. You can use this to connect multiple VPCs that are geographically disparate and/or running in separate AWS accounts, to a common VPC that serves as a global network transit center. This network topology simplifies network management and minimizes the number of connections that you need to set up. Transit VPC: via - https://aws.amazon.com/transit-gateway/ Transit VPC is not the right solution for this use-case as Transit Gateway provides several advantages over Transit VPC: 1. Transit Gateway abstracts away the complexity of maintaining VPN connections with hundreds of VPCs. 2. Transit Gateway removes the need to manage and scale Amazon EC2 based software appliances. AWS is responsible for managing all resources needed to route traffic. 3. Transit Gateway removes the need to manage high availability by providing a highly available and redundant Multi-AZ infrastructure. 4. Transit Gateway improves bandwidth for inter-VPC communication to burst speeds of 50 Gbps per Availability Zone (AZ). 5. Transit Gateway streamlines user costs to a simple per hour per/GB transferred model. 6. Transit Gateway decreases latency by removing Amazon EC2 proxies and the need for VPN encapsulation. Partially meshed VPC peering can be used to connect the Amazon VPCs to the on-premises networks Fully meshed VPC peering can be used to connect the Amazon VPCs to the on-premises networks The simplest way to connect two VPCs is to use VPC Peering. In this setup, a connection enables full bidirectional connectivity between the VPCs. This peering connection is used to route traffic between the VPCs. VPCs across accounts and AWS Regions can also be peered together. VPC peering only incurs costs for traffic traveling over the connection (there is no hourly infrastructure fee). VPC peering is point-to-point connectivity, and it does not support transitive routing. If you are using VPC peering, on-premises connectivity (VPN and/or Direct Connect) must be made to each VPC. Resources in a VPC cannot reach on-premises using the hybrid connectivity of a peered VPC. VPC peering is best used when resources in one VPC must communicate with resources in another VPC, the environment of both VPCs is controlled and secured, and the number of VPCs to be connected is less than 10 (to allow for the individual management of each connection). VPC peering offers the lowest overall cost when compared to other options for inter-VPC connectivity. Network setup using VPC Peering: via - https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/vpc-peering.html You cannot use VPC Peering to establish on-premises connectivity with AWS Cloud, so both these options are incorrect. References: https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/transit-gateway-vs-transit-vpc.html https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/vpc-peering.html",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An e-commerce company has copied 1 petabyte of data from its on-premises data center to an Amazon S3 bucket in the us-west-1 Region using an AWS Direct Connect link. The company now wants to set up a one-time copy of the data to another Amazon S3 bucket in the us-east-1 Region. The on-premises data center does not allow the use of AWS Snowball. As a Solutions Architect, which of the following options can be used to accomplish this goal? (Select two)",
      "options": [
        "A. Copy data from the source bucket to the destination bucket using the aws S3 sync command",
        "B. Use AWS Snowball Edge device to copy the data from one Region to another Region",
        "C. Copy data from the source Amazon S3 bucket to a target Amazon S3 bucket using the S3 console",
        "D. Set up Amazon S3 batch replication to copy objects across Amazon S3 buckets in another Region using S3 console and then delete the replication configuration",
        "E. Set up Amazon S3 Transfer Acceleration (Amazon S3TA) to copy objects across Amazon S3 buckets in different Regions using S3 console"
      ],
      "correct_answer": "A",
      "explanation": "Correct options: Copy data from the source bucket to the destination bucket using the aws S3 sync command The aws S3 sync command uses the CopyObject APIs to copy objects between Amazon S3 buckets. The sync command lists the source and target buckets to identify objects that are in the source bucket but that aren't in the target bucket. The command also identifies objects in the source bucket that have different LastModified dates than the objects that are in the target bucket. The sync command on a versioned bucket copies only the current version of the object—previous versions aren't copied. By default, this preserves object metadata, but the access control lists (ACLs) are set to FULL_CONTROL for your AWS account, which removes any additional ACLs. If the operation fails, you can run the sync command again without duplicating previously copied objects. You can use the command like so: aws s3 sync s3://DOC-EXAMPLE-BUCKET-SOURCE s3://DOC-EXAMPLE-BUCKET-TARGET Set up Amazon S3 batch replication to copy objects across Amazon S3 buckets in another Region using S3 console and then delete the replication configuration Amazon S3 Batch Replication provides you a way to replicate objects that existed before a replication configuration was in place, objects that have previously been replicated, and objects that have failed replication. This is done through the use of a Batch Operations job. You should note that batch replication differs from live replication which continuously and automatically replicates new objects across Amazon S3 buckets. You cannot directly use the AWS S3 console to configure cross-Region replication for existing objects. By default, replication only supports copying new Amazon S3 objects after it is enabled using the AWS S3 console. Replication enables automatic, asynchronous copying of objects across Amazon S3 buckets. Buckets that are configured for object replication can be owned by the same AWS account or by different accounts. Object may be replicated to a single destination bucket or multiple destination buckets. Destination buckets can be in different AWS Regions or within the same Region as the source bucket. Once done, you can delete the replication configuration, as it ensures that batch replication is only used for this one-time data copy operation. If you want to enable live replication for existing objects for your bucket, you must contact AWS Support and raise a support ticket. This is required to ensure that replication is configured correctly. Incorrect options: Use AWS Snowball Edge device to copy the data from one Region to another Region - As the given requirement is about copying the data from one AWS Region to another AWS Region, so AWS Snowball Edge cannot be used here. AWS Snowball Edge Storage Optimized is the optimal data transfer choice if you need to securely and quickly transfer terabytes to petabytes of data to AWS. You can use AWS Snowball Edge Storage Optimized if you have a large backlog of data to transfer or if you frequently collect data that needs to be transferred to AWS and your storage is in an area where high-bandwidth internet connections are not available or cost-prohibitive. AWS Snowball Edge can operate in remote locations or harsh operating environments, such as factory floors, oil and gas rigs, mining sites, hospitals, and on moving vehicles. Copy data from the source Amazon S3 bucket to a target Amazon S3 bucket using the S3 console - AWS S3 console cannot be used to copy 1 petabytes of data from one bucket to another as it's not feasible. You should note that this option is different from using the replication options on the AWS console, since here you are using the copy and paste options provided on the AWS console, which is suggested for small or medium data volume. You should use S3 sync for the requirement of one-time copy of data. Set up Amazon S3 Transfer Acceleration (Amazon S3TA) to copy objects across Amazon S3 buckets in different Regions using S3 console - Amazon S3 Transfer Acceleration (Amazon S3TA) is a bucket-level feature that enables fast, easy, and secure transfers of files over long distances between your client and an Amazon S3 bucket. You cannot use Transfer Acceleration to copy objects across Amazon S3 buckets in different Regions using Amazon S3 console. References: https://aws.amazon.com/premiumsupport/knowledge-center/move-objects-s3-bucket/ https://aws.amazon.com/snowball/faqs/ https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication.html",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A startup's cloud infrastructure consists of a few Amazon EC2 instances, Amazon RDS instances and Amazon S3 storage. A year into their business operations, the startup is incurring costs that seem too high for their business requirements. Which of the following options represents a valid cost-optimization solution?",
      "options": [
        "A. Use Amazon S3 Storage class analysis to get recommendations for transitions of objects to Amazon S3 Glacier storage classes to reduce storage costs. You can also automate moving these objects into lower-cost storage tier using Lifecycle Policies",
        "B. Use AWS Cost Explorer Resource Optimization to get a report of Amazon EC2 instances that are either idle or have low utilization and use AWS Compute Optimizer to look at instance type recommendations",
        "C. Use AWS Trusted Advisor checks on Amazon EC2 Reserved Instances to automatically renew reserved instances (RI). AWS Trusted advisor also suggests Amazon RDS idle database instances",
        "D. Use AWS Compute Optimizer recommendations to help you choose the optimal Amazon EC2 purchasing options and help reserve your instance capacities at reduced costs"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Use AWS Cost Explorer Resource Optimization to get a report of Amazon EC2 instances that are either idle or have low utilization and use AWS Compute Optimizer to look at instance type recommendations AWS Cost Explorer helps you identify under-utilized Amazon EC2 instances that may be downsized on an instance by instance basis within the same instance family, and also understand the potential impact on your AWS bill by taking into account your Reserved Instances and Savings Plans. AWS Compute Optimizer recommends optimal AWS Compute resources for your workloads to reduce costs and improve performance by using machine learning to analyze historical utilization metrics. Compute Optimizer helps you choose the optimal Amazon EC2 instance types, including those that are part of an Amazon EC2 Auto Scaling group, based on your utilization data. Incorrect options: Use Amazon S3 Storage class analysis to get recommendations for transitions of objects to Amazon S3 Glacier storage classes to reduce storage costs. You can also automate moving these objects into lower-cost storage tier using Lifecycle Policies - By using Amazon S3 Analytics Storage Class analysis you can analyze storage access patterns to help you decide when to transition the right data to the right storage class. This new Amazon S3 analytics feature observes data access patterns to help you determine when to transition less frequently accessed STANDARD storage to the STANDARD_IA (IA, for infrequent access) storage class. Storage class analysis does not give recommendations for transitions to the ONEZONE_IA or S3 Glacier storage classes. Use AWS Trusted Advisor checks on Amazon EC2 Reserved Instances to automatically renew reserved instances (RI). AWS Trusted advisor also suggests Amazon RDS idle database instances - AWS Trusted Advisor checks for Amazon EC2 Reserved Instances that are scheduled to expire within the next 30 days or have expired in the preceding 30 days. Reserved Instances do not renew automatically; you can continue using an Amazon EC2 instance covered by the reservation without interruption, but you will be charged On-Demand rates. AWS Trusted advisor does not have a feature to auto-renew Reserved Instances. Use AWS Compute Optimizer recommendations to help you choose the optimal Amazon EC2 purchasing options and help reserve your instance capacities at reduced costs - AWS Compute Optimizer recommends optimal AWS Compute resources for your workloads to reduce costs and improve performance by using machine learning to analyze historical utilization metrics. Over-provisioning compute can lead to unnecessary infrastructure cost and under-provisioning compute can lead to poor application performance. Compute Optimizer helps you choose the optimal Amazon EC2 instance types, including those that are part of an Amazon EC2 Auto Scaling group, based on your utilization data. It does not recommend instance purchase options. References: https://aws.amazon.com/compute-optimizer/ https://aws.amazon.com/premiumsupport/technology/trusted-advisor/best-practice-checklist/ https://docs.aws.amazon.com/AmazonS3/latest/dev/analytics-storage-class.html",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A retail company uses AWS Cloud to manage its technology infrastructure. The company has deployed its consumer-focused web application on Amazon EC2-based web servers and uses Amazon RDS PostgreSQL database as the data store. The PostgreSQL database is set up in a private subnet that allows inbound traffic from selected Amazon EC2 instances. The database also uses AWS Key Management Service (AWS KMS) for encrypting data at rest. Which of the following steps would you recommend to facilitate end-to-end security for the data-in-transit while accessing the database?",
      "options": [
        "A. Use IAM authentication to access the database instead of the database user's access credentials",
        "B. Configure Amazon RDS to use SSL for data in transit",
        "C. Create a new security group that blocks SSH from the selected Amazon EC2 instances into the database",
        "D. Create a new network access control list (network ACL) that blocks SSH from the entire Amazon EC2 subnet into the database"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Configure Amazon RDS to use SSL for data in transit You can use Secure Socket Layer / Transport Layer Security (SSL/TLS) connections to encrypt data in transit. Amazon RDS creates an SSL certificate and installs the certificate on the DB instance when the instance is provisioned. For MySQL, you launch the MySQL client using the --ssl_ca parameter to reference the public key to encrypt connections. Using SSL, you can encrypt a PostgreSQL connection between your applications and your PostgreSQL DB instances. You can also force all connections to your PostgreSQL DB instance to use SSL. via - https://aws.amazon.com/rds/features/security/ Incorrect options: Use IAM authentication to access the database instead of the database user's access credentials - You can authenticate to your database instance using AWS Identity and Access Management (IAM) database authentication. IAM database authentication works with MySQL and PostgreSQL. With this authentication method, you don't need to use a password when you connect to a database instance. Instead, you use an authentication token. IAM authentication is just another way to authenticate the user's credentials while accessing the database. It would not significantly enhance the security in a way that enabling SSL does by facilitating the in-transit encryption for the database. Create a new security group that blocks SSH from the selected Amazon EC2 instances into the database Create a new network access control list (network ACL) that blocks SSH from the entire Amazon EC2 subnet into the database Both these options are added as distractors. You cannot SSH into an Amazon RDS database instance. References: https://aws.amazon.com/rds/features/security/ https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_MySQL.html#MySQL.Concepts.SSLSupport https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_PostgreSQL.html#PostgreSQL.Concepts.General.SSL https://aws.amazon.com/blogs/database/using-iam-authentication-to-connect-with-pgadmin-amazon-aurora-postgresql-or-amazon-rds-for-postgresql/",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A media company uses Amazon ElastiCache Redis to enhance the performance of its Amazon RDS database layer. The company wants a robust disaster recovery strategy for its caching layer that guarantees minimal downtime as well as minimal data loss while ensuring good application performance. Which of the following solutions will you recommend to address the given use-case?",
      "options": [
        "A. Opt for Multi-AZ configuration with automatic failover functionality to help mitigate failure",
        "B. Schedule daily automatic backups at a time when you expect low resource utilization for your cluster",
        "C. Schedule manual backups using Redis append-only file (AOF)",
        "D. Add read-replicas across multiple availability zones (AZs) to reduce the risk of potential data loss because of failure"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Opt for Multi-AZ configuration with automatic failover functionality to help mitigate failure Multi-AZ is the best option when data retention, minimal downtime, and application performance are a priority. Data-loss potential - Low. Multi-AZ provides fault tolerance for every scenario, including hardware-related issues. Performance impact - Low. Of the available options, Multi-AZ provides the fastest time to recovery, because there is no manual procedure to follow after the process is implemented. Cost - Low to high. Multi-AZ is the lowest-cost option. Use Multi-AZ when you can't risk losing data because of hardware failure or you can't afford the downtime required by other options in your response to an outage. Incorrect options: Schedule daily automatic backups at a time when you expect low resource utilization for your cluster - Data loss potential is high, almost up to a day's worth of data. Hence, this is not the right option. Schedule manual backups using Redis append-only file (AOF) - Manual backups using AOF are retained indefinitely and are useful for testing and archiving. You can schedule manual backups to occur up to 20 times per node within any 24-hour period. Although AOF provides a measure of fault tolerance, it can't protect your data from a hardware-related cache node failure, so there is a risk of data loss. Add read-replicas across multiple availability zones (AZs) to reduce the risk of potential data loss because of failure - To scale read capacity, Amazon ElastiCache allows you to add up to five read replicas across multiple availability zones. Read replicas are used to ease out read traffic from the primary database and cannot be used as a complete fault-tolerant solution in itself. Reference: https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/FaultTolerance.html",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An IT company has a large number of clients opting to build their application programming interface (API) using Docker containers. To facilitate the hosting of these containers, the company is looking at various orchestration services available with AWS. As a Solutions Architect, which of the following solutions will you suggest? (Select two)",
      "options": [
        "A. Use Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate for serverless orchestration of the containerized services",
        "B. Use Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 for serverless orchestration of the containerized services",
        "C. Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate for serverless orchestration of the containerized services",
        "D. Use Amazon EMR for serverless orchestration of the containerized services",
        "E. Use Amazon SageMaker for serverless orchestration of the containerized services"
      ],
      "correct_answer": "A",
      "explanation": "Correct options: Use Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate for serverless orchestration of the containerized services Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate for serverless orchestration of the containerized services Building APIs with Docker containers has been gaining momentum over the years. For hosting and exposing these container-based APIs, they need a solution which supports HTTP requests routing, autoscaling, and high availability. In some cases, user authorization is also needed. For this purpose, many organizations are orchestrating their containerized services with Amazon Elastic Container Service (Amazon ECS) or Amazon Elastic Kubernetes Service (Amazon EKS), while hosting their containers on Amazon EC2 or AWS Fargate. Then, they can add scalability and high availability with Service Auto Scaling (in Amazon ECS) or Horizontal Pod Auto Scaler (in Amazon EKS), and they expose the services through load balancers. When you use Amazon ECS as an orchestrator (with EC2 or Fargate launch type), you also have the option to expose your services with Amazon API Gateway and AWS Cloud Map instead of a load balancer. AWS Cloud Map is used for service discovery: no matter how Amazon ECS tasks scale, AWS Cloud Map service names would point to the right set of Amazon ECS tasks. Then, API Gateway HTTP APIs can be used to define API routes and point them to the corresponding AWS Cloud Map services. Incorrect options: Use Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 for serverless orchestration of the containerized services - Amazon EC2 can be used to host the container services. Amazon EC2 needs hosting and management of the instance, hence does not come under serverless solution. Fargate can be used for serverless container solutions. Use Amazon EMR for serverless orchestration of the containerized services - Amazon EMR is a web service that enables businesses, researchers, data analysts, and developers to easily and cost-effectively process vast amounts of data. It utilizes a hosted Hadoop framework running on the web-scale infrastructure of Amazon EC2 and Amazon S3. EMR is not a docker orchestration service, as required for the use case. Use Amazon SageMaker for serverless orchestration of the containerized services - Amazon SageMaker helps data scientists and developers to prepare, build, train, and deploy high-quality machine learning (ML) models quickly by bringing together a broad set of capabilities purpose-built for ML. A powerful tool, SageMaker is not a docker orchestration service, as required for the use case. Reference: https://aws.amazon.com/blogs/architecture/field-notes-serverless-container-based-apis-with-amazon-ecs-and-amazon-api-gateway/",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A systems administrator is creating IAM policies and attaching them to IAM identities. After creating the necessary identity-based policies, the administrator is now creating resource-based policies. Which is the only resource-based policy that the IAM service supports?",
      "options": [
        "A. AWS Organizations Service Control Policies (SCP)",
        "B. Trust policy",
        "C. Access control list (ACL)",
        "D. Permissions boundary"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: You manage access in AWS by creating policies and attaching them to IAM identities (users, groups of users, or roles) or AWS resources. A policy is an object in AWS that, when associated with an identity or resource, defines their permissions. Resource-based policies are JSON policy documents that you attach to a resource such as an Amazon S3 bucket. These policies grant the specified principal permission to perform specific actions on that resource and define under what conditions this applies. Trust policy Trust policies define which principal entities (accounts, users, roles, and federated users) can assume the role. An IAM role is both an identity and a resource that supports resource-based policies. For this reason, you must attach both a trust policy and an identity-based policy to an IAM role. The IAM service supports only one type of resource-based policy called a role trust policy, which is attached to an IAM role. Incorrect options: AWS Organizations Service Control Policies (SCP) - If you enable all features of AWS organization, then you can apply service control policies (SCPs) to any or all of your accounts. SCPs are JSON policies that specify the maximum permissions for an organization or organizational unit (OU). The SCP limits permissions for entities in member accounts, including each AWS account root user. An explicit deny in any of these policies overrides the allow. Access control list (ACL) - Access control lists (ACLs) are service policies that allow you to control which principals in another account can access a resource. ACLs cannot be used to control access for a principal within the same account. Amazon S3, AWS WAF, and Amazon VPC are examples of services that support ACLs. Permissions boundary - AWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature for using a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. An entity's permissions boundary allows it to perform only the actions that are allowed by both its identity-based policies and its permissions boundaries. References: https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html#policies_resource-based https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A CRM company has a software as a service (SaaS) application that feeds updates to other in-house and third-party applications. The SaaS application and the in-house applications are being migrated to use AWS services for this inter-application communication. As a Solutions Architect, which of the following would you suggest to asynchronously decouple the architecture?",
      "options": [
        "A. Use Amazon Simple Queue Service (Amazon SQS) to decouple the architecture",
        "B. Use Amazon EventBridge to decouple the system architecture",
        "C. Use Amazon Simple Notification Service (Amazon SNS) to communicate between systems and decouple the architecture",
        "D. Use Elastic Load Balancing (ELB) for effective decoupling of system architecture"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Use Amazon EventBridge to decouple the system architecture Both Amazon EventBridge and Amazon SNS can be used to develop event-driven applications, but for this use case, EventBridge is the right fit. Amazon EventBridge is recommended when you want to build an application that reacts to events from SaaS applications and/or AWS services. Amazon EventBridge is the only event-based service that integrates directly with third-party SaaS partners. Amazon EventBridge also automatically ingests events from over 90 AWS services without requiring developers to create any resources in their account. Further, Amazon EventBridge uses a defined JSON-based structure for events and allows you to create rules that are applied across the entire event body to select events to forward to a target. Amazon EventBridge currently supports over 15 AWS services as targets, including AWS Lambda, Amazon SQS, Amazon SNS, and Amazon Kinesis Streams and Firehose, among others. At launch, Amazon EventBridge is has limited throughput (see Service Limits) which can be increased upon request, and typical latency of around half a second. How Amazon EventBridge works: via - https://aws.amazon.com/eventbridge/ Incorrect options: Use Amazon Simple Notification Service (Amazon SNS) to communicate between systems and decouple the architecture - As discussed above, Amazon SNS can be used for event-based services. But, our use case needs integration with third-party SaaS services, hence Amazon EventBridge is the right choice, as Amazon SNS does not support third-party services integration. Use Amazon Simple Queue Service (Amazon SQS) to decouple the architecture - Amazon SQS is a message queuing service from amazon and works well for decoupling applications. It does not directly integrate with third-party SaaS services. Use Elastic Load Balancing (ELB) for effective decoupling of system architecture - Elastic Load Balancing (ELB) offers a synchronous decoupling of applications, which is not the right fit for the current use case. References: https://aws.amazon.com/eventbridge/ https://aws.amazon.com/sns/",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An enterprise has decided to move its secondary workloads such as backups and archives to AWS cloud. The CTO wishes to move the data stored on physical tapes to Cloud, without changing their current tape backup workflows. The company holds petabytes of data on tapes and needs a cost-optimized solution to move this data to cloud. What is an optimal solution that meets these requirements while keeping the costs to a minimum?",
      "options": [
        "A. Use Tape Gateway, which can be used to move on-premises tape data onto AWS Cloud. Then, Amazon S3 archiving storage classes can be used to store data cost-effectively for years",
        "B. Use AWS DataSync, which makes it simple and fast to move large amounts of data online between on-premises storage and AWS Cloud. Data moved to Cloud can then be stored cost-effectively in Amazon S3 archiving storage classes",
        "C. Use AWS Direct Connect, a cloud service solution that makes it easy to establish a dedicated network connection from on-premises to AWS to transfer data. Once this is done, Amazon S3 can be used to store data at lesser costs",
        "D. Use AWS VPN connection between the on-premises datacenter and your Amazon VPC. Once this is established, you can use Amazon Elastic File System (Amazon EFS) to get a scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Use Tape Gateway, which can be used to move on-premises tape data onto AWS Cloud. Then, Amazon S3 archiving storage classes can be used to store data cost-effectively for years Tape Gateway enables you to replace using physical tapes on-premises with virtual tapes in AWS without changing existing backup workflows. Tape Gateway supports all leading backup applications and caches virtual tapes on-premises for low-latency data access. Tape Gateway encrypts data between the gateway and AWS for secure data transfer and compresses data while transitioning virtual tapes between Amazon S3 and Amazon S3 Glacier, or Amazon S3 Glacier Deep Archive, to minimize storage costs. Tape Gateway compresses and stores archived virtual tapes in the lowest-cost Amazon S3 storage classes, Amazon S3 Glacier and Amazon S3 Glacier Deep Archive. This makes it feasible for you to retain long-term data in the AWS Cloud at a very low cost. With Tape Gateway, you only pay for what you consume, with no minimum commitments and no upfront fees. Tape Gateway stores your virtual tapes in S3 buckets managed by the AWS Storage Gateway service, so you don’t have to manage your own Amazon S3 storage. Tape Gateway integrates with all leading backup applications allowing you to start using cloud storage for on-premises backup and archive without any changes to your backup and archive workflows. Tape Gateway Overview: via - https://aws.amazon.com/storagegateway/vtl/ Incorrect options: Use AWS DataSync, which makes it simple and fast to move large amounts of data online between on-premises storage and AWS Cloud. Data moved to Cloud can then be stored cost-effectively in Amazon S3 archiving storage classes - AWS DataSync supports only NFS and SMB file types and hence is not the right choice for the given use case. Use AWS Direct Connect, a cloud service solution that makes it easy to establish a dedicated network connection from on-premises to AWS to transfer data. Once this is done, Amazon S3 can be used to store data at lesser costs - AWS Direct Connect is used when customers need to retain on-premises structure because of compliance reasons and have moved the rest of the architecture to AWS Cloud. These businesses generally have an on-going requirement for low latency access to AWS Cloud and hence are willing to spend on installing the physical lines needed for this connection. The given use-case needs a cost-optimized solution and they do not have an ongoing requirement for high availability bandwidth. Use AWS VPN connection between the on-premises datacenter and your Amazon VPC. Once this is established, you can use Amazon Elastic File System (Amazon EFS) to get a scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources - VPN connection is used when businesses have an on-going requirement for connectivity from the on-premises data center to AWS Cloud. Amazon EFS is a managed file system by AWS and cannot be used for archiving on-premises tape data onto AWS Cloud. References: https://aws.amazon.com/storagegateway/vtl/ https://aws.amazon.com/storagegateway/faqs/",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A music-sharing company uses a Network Load Balancer to direct traffic to 5 Amazon EC2 instances managed by an Auto Scaling group. When a very popular song is released, the Auto Scaling Group scales to 100 instances and the company incurs high network and compute fees. The company wants a solution to reduce the costs without changing any of the application code. What do you recommend?",
      "options": [
        "A. Move the songs to Amazon S3 Glacier",
        "B. Leverage AWS Storage Gateway",
        "C. Use an Amazon CloudFront distribution",
        "D. Move the songs to Amazon S3"
      ],
      "correct_answer": "C",
      "explanation": "Correct option: Use an Amazon CloudFront distribution Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment. Amazon CloudFront points of presence (POPs) (edge locations) make sure that popular content can be served quickly to your viewers. CloudFront also has regional edge caches that bring more of your content closer to your viewers, even when the content is not popular enough to stay at a POP, to help improve performance for that content. Regional edge caches help with all types of content, particularly content that tends to become less popular over time. Examples include user-generated content, such as video, photos, or artwork; e-commerce assets such as product photos and videos; and news and event-related content that might suddenly find new popularity. Amazon CloudFront is the right answer because we can put it in front of our Auto Scaling group and leverage a Global Caching feature that will help us distribute the content reliably with dramatically reduced costs (the ASG won't need to scale as much). Incorrect options: Leverage AWS Storage Gateway - AWS Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage. The service provides three different types of gateways – Tape Gateway, File Gateway, and Volume Gateway – that seamlessly connect on-premises applications to cloud storage, caching data locally for low-latency access. AWS Storage Gateway cannot be used for distributing files to end-users, so this option is ruled out. Move the songs to Amazon S3 - Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. Using Amazon S3 would imply changing the application code, so this option is ruled out. Move the songs to Amazon S3 Glacier - Amazon S3 Glacier and S3 Glacier Deep Archive are secure, durable, and extremely low-cost Amazon S3 cloud storage classes for data archiving and long-term backup. They are designed to deliver 99.999999999% durability and provide comprehensive security and compliance capabilities that can help meet even the most stringent regulatory requirements. Amazon Glacier is not applicable as the files are frequently requested (Glacier has retrieval times ranging from a few minutes to hours), so this option is also ruled out. References: https://aws.amazon.com/cloudfront/ https://aws.amazon.com/storagegateway/",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A company has recently created a new department to handle their services workload. An IT team has been asked to create a custom VPC to isolate the resources created in this new department. They have set up the public subnet and internet gateway (IGW). However, they are not able to ping the Amazon EC2 instances with elastic IP address (EIP) launched in the newly created VPC. As a Solutions Architect, the team has requested your help. How will you troubleshoot this scenario? (Select two)",
      "options": [
        "A. Disable Source / Destination check on the Amazon EC2 instance",
        "B. Check if the security groups allow ping from the source",
        "C. Contact AWS support to map your VPC with subnet",
        "D. Create a secondary internet gateway to attach with public subnet and move the current internet gateway to private and write route tables",
        "E. Check if the route table is configured with internet gateway"
      ],
      "correct_answer": "B",
      "explanation": "Correct options: Check if the route table is configured with internet gateway An internet gateway (IGW) is a horizontally scaled, redundant, and highly available VPC component that allows communication between instances in your VPC and the internet. An internet gateway serves two purposes: to provide a target in your VPC route tables for internet-routable traffic, and to perform network address translation (NAT) for instances that have been assigned public IPv4 addresses. An internet gateway supports IPv4 and IPv6 traffic. To enable access to or from the internet for instances in a subnet in a VPC, you must do the following: 1. Attach an internet gateway to your VPC. 2. Add a route to your subnet's route table that directs internet-bound traffic to the internet gateway. 3. Ensure that instances in your subnet have a globally unique IP address 4. Ensure that your network access control lists and security group rules allow the relevant traffic to flow to and from your instance. A route table contains a set of rules, called routes, that are used to determine where network traffic from your subnet or gateway is directed. After creating an IGW, make sure the route tables are updated. Additionally, ensure the security group allows the ICMP protocol for ping requests. Check if the security groups allow ping from the source A security group acts as a virtual firewall that controls the traffic for one or more instances. When you launch an instance, you can specify one or more security groups; otherwise, AWS uses the default security group. You can add rules to each security group that allow traffic to or from its associated instances. You can modify the rules for a security group at any time; the new rules are automatically applied to all instances that are associated with the security group. To decide whether to allow traffic to reach an instance, all the rules from all the security groups that are associated with the instance are evaluated. The following are the characteristics of security group rules: 1. By default, security groups allow all outbound traffic. 2. Security group rules are always permissive; you can't create rules that deny access. 3. Security groups are stateful Incorrect options: Disable Source / Destination check on the Amazon EC2 instance - The Source/Destination Check attribute controls whether source/destination checking is enabled on the instance. Disabling this attribute enables an instance to handle network traffic that isn't specifically destined for the instance. For example, instances running services such as network address translation, routing, or a firewall should set this value to disabled. The default value is enabled. Source/Destination Check is not relevant to the question and it has been added as a distractor. Create a secondary internet gateway to attach with public subnet and move the current internet gateway to private and write route tables - There is no such thing as a secondary IGW. This option is added as a distractor. Contact AWS support to map your VPC with subnet - You cannot contact AWS support to map your VPC with the subnet. References: https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Route_Tables.html https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html#change_source_dest_check",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A company runs a popular dating website on the AWS Cloud. As a Solutions Architect, you've designed the architecture of the website to follow a serverless pattern on the AWS Cloud using Amazon API Gateway and AWS Lambda. The backend uses an Amazon RDS PostgreSQL database. Currently, the application uses a username and password combination to connect the AWS Lambda function to the Amazon RDS database. You would like to improve the security at the authentication level by leveraging short-lived credentials. What will you choose? (Select two)",
      "options": [
        "A. Embed a credential rotation logic in the AWS Lambda, retrieving them from SSM",
        "B. Use IAM authentication from AWS Lambda to Amazon RDS PostgreSQL",
        "C. Restrict the Amazon RDS database security group to the AWS Lambda's security group",
        "D. Deploy AWS Lambda in a VPC",
        "E. Attach an AWS Identity and Access Management (IAM) role to AWS Lambda"
      ],
      "correct_answer": "B",
      "explanation": "Correct options: Use IAM authentication from AWS Lambda to Amazon RDS PostgreSQL Attach an AWS Identity and Access Management (IAM) role to AWS Lambda You can authenticate to your database instance using AWS Identity and Access Management (IAM) database authentication. IAM database authentication works with MySQL and PostgreSQL. With this authentication method, you don't need to use a password when you connect to a database instance. Instead, you use an authentication token. An authentication token is a unique string of characters that Amazon RDS generates on request. Authentication tokens are generated using AWS Signature Version 4. Each token has a lifetime of 15 minutes. You don't need to store user credentials in the database, because authentication is managed externally using IAM. You can also still use standard database authentication. IAM database authentication provides the following benefits: 1. Network traffic to and from the database is encrypted using Secure Sockets Layer (SSL). 2. You can use IAM to centrally manage access to your database resources, instead of managing access individually on each DB instance. 3. For applications running on Amazon EC2, you can use profile credentials specific to your Amazon EC2 instance to access your database instead of a password, for greater security. Incorrect options: AWS Systems Manager Parameter Store (aka SSM Parameter Store) provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, Amazon EC2 instance IDs, Amazon Machine Image (AMI) IDs, and license codes as parameter values. You can store values as plain text or encrypted data. Embed a credential rotation logic in the AWS Lambda, retrieving them from SSM - Retrieving credentials from SSM is overkill for the expected solution and hence this is not a correct option. Restrict the Amazon RDS database security group to the AWS Lambda's security group Deploy AWS Lambda in a VPC This question is very tricky because all answers do indeed increase security. But the question is related to authentication mechanisms, and as such, deploying an AWS Lambda in a VPC or tightening security groups does not change the authentication layer. IAM authentication to Amazon RDS is supported, which must be achieved by attaching an IAM role the AWS Lambda function References: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.html https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A CRM web application was written as a monolith in PHP and is facing scaling issues because of performance bottlenecks. The CTO wants to re-engineer towards microservices architecture and expose their application from the same load balancer, linked to different target groups with different URLs: checkout.mycorp.com, www.mycorp.com, yourcorp.com/profile and yourcorp.com/search. The CTO would like to expose all these URLs as HTTPS endpoints for security purposes. As a solutions architect, which of the following would you recommend as a solution that requires MINIMAL configuration effort?",
      "options": [
        "A. Use Secure Sockets Layer certificate (SSL certificate) with SNI",
        "B. Use a wildcard Secure Sockets Layer certificate (SSL certificate)",
        "C. Use an HTTP to HTTPS redirect",
        "D. Change the Elastic Load Balancing (ELB) SSL Security Policy"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Use Secure Sockets Layer certificate (SSL certificate) with SNI You can host multiple TLS secured applications, each with its own TLS certificate, behind a single load balancer. To use SNI, all you need to do is bind multiple certificates to the same secure listener on your load balancer. ALB will automatically choose the optimal TLS certificate for each client. ALB’s smart certificate selection goes beyond SNI. In addition to containing a list of valid domain names, certificates also describe the type of key exchange and cryptography that the server supports, as well as the signature algorithm (SHA2, SHA1, MD5) used to sign the certificate. With SNI support AWS makes it easy to use more than one certificate with the same ALB. The most common reason you might want to use multiple certificates is to handle different domains with the same load balancer. It’s always been possible to use wildcard and subject-alternate-name (SAN) certificates with ALB, but these come with limitations. Wildcard certificates only work for related subdomains that match a simple pattern and while SAN certificates can support many different domains, the same certificate authority has to authenticate each one. That means you have to reauthenticate and reprovision your certificate every time you add a new domain. Incorrect options: Use a wildcard Secure Sockets Layer certificate (SSL certificate) - As the use case requires different domain names, so you cannot use a wildcard SSL certificate. Use an HTTP to HTTPS redirect - This will not provide multiple secure endpoints for different URLs such as checkout.mycorp.com or www.mycorp.com, therefore it is incorrect for the given use-case. Change the Elastic Load Balancing (ELB) SSL Security Policy - Elastic Load Balancing (ELB) SSL Security Policy will not provide multiple secure endpoints for different URLs such as checkout.mycorp.com or www.mycorp.com, therefore it is incorrect for the given use-case. References: https://aws.amazon.com/blogs/aws/new-application-load-balancer-sni/ https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-security-policy-table.html",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A financial services firm has traditionally operated with an on-premise data center and would like to create a disaster recovery strategy leveraging the AWS Cloud. As a Solutions Architect, you would like to ensure that a scaled-down version of a fully functional environment is always running in the AWS cloud, and in case of a disaster, the recovery time is kept to a minimum. Which disaster recovery strategy is that?",
      "options": [
        "A. Warm Standby",
        "B. Pilot Light",
        "C. Backup and Restore",
        "D. Multi Site"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Warm Standby The term warm standby is used to describe a DR scenario in which a scaled-down version of a fully functional environment is always running in the cloud. A warm standby solution extends the pilot light elements and preparation. It further decreases the recovery time because some services are always running. By identifying your business-critical systems, you can fully duplicate these systems on AWS and have them always on. Incorrect options: Backup and Restore - In most traditional environments, data is backed up to tape and sent off-site regularly. If you use this method, it can take a long time to restore your system in the event of a disruption or disaster. Amazon S3 is an ideal destination for backup data that might be needed quickly to perform a restore. Transferring data to and from Amazon S3 is typically done through the network, and is therefore accessible from any location. Many commercial and open-source backup solutions integrate with Amazon S3. Pilot Light - The term pilot light is often used to describe a DR scenario in which a minimal version of an environment is always running in the cloud. The idea of the pilot light is an analogy that comes from the gas heater. In a gas heater, a small flame that’s always on can quickly ignite the entire furnace to heat up a house. This scenario is similar to a backup-and-restore scenario. For example, with AWS you can maintain a pilot light by configuring and running the most critical core elements of your system in AWS. When the time comes for recovery, you can rapidly provision a full-scale production environment around the critical core. Multi Site - A multi-site solution runs in AWS as well as on your existing on-site infrastructure, in an active-active configuration. The data replication method that you employ will be determined by the recovery point that you choose. References: https://d1.awsstatic.com/whitepapers/aws-disaster-recovery.pdf https://d1.awsstatic.com/asset-repository/products/CloudEndure/CloudEndure_Affordable_Enterprise-Grade_Disaster_Recovery_Using_AWS.pdf",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A leading e-commerce company runs its IT infrastructure on AWS Cloud. The company has a batch job running at 7AM daily on an Amazon RDS database. It processes shipping orders for the past day, and usually gets around 2000 records that need to be processed sequentially in a batch job via a shell script. The processing of each record takes about 3 seconds. What platform do you recommend to run this batch job?",
      "options": [
        "A. AWS Glue",
        "B. Amazon Elastic Compute Cloud (Amazon EC2)",
        "C. AWS Lambda",
        "D. Amazon Kinesis Data Streams"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Amazon Elastic Compute Cloud (Amazon EC2) Amazon Elastic Compute Cloud (Amazon EC2) is a web service that provides secure, resizable compute capacity in the cloud. It is designed to make web-scale cloud computing easier for developers. Amazon EC2’s simple web service interface allows you to obtain and configure capacity with minimal friction. It provides you with complete control of your computing resources and lets you run on Amazon’s proven computing environment. AWS Batch can be used to plan, schedule, and execute your batch computing workloads on Amazon EC2 Instances. Amazon EC2 is the right choice as it can accommodate batch processing and run customized scripts, as is the needed requirement. Incorrect options: AWS Glue - AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. AWS Glue job is meant to be used for batch ETL data processing. Glue is for performing ETL, but cannot run custom shell scripts and hence not the right choice here. Amazon Kinesis Data Streams - Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The throughput of an Amazon Kinesis data stream is designed to scale without limits via increasing the number of shards within a data stream. However, Kinesis works great with real-time data, we are looking at batch processing, so Kinesis is not an option. AWS Lambda - AWS Lambda lets you run code without provisioning or managing servers. AWS Lambda functions can be configured to run up to 15 minutes per execution. You can set the timeout to any value between 1 second and 15 minutes. The total runtime for the given use-case is 100 minutes (2000*3=6000 seconds = 100 minutes) but the Lambda would time out after 15 minutes, so this option is incorrect. References: https://aws.amazon.com/ec2/features/ https://aws.amazon.com/lambda/faqs/",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "You have an Amazon S3 bucket that contains files in two different folders - s3://my-bucket/images and s3://my-bucket/thumbnails. When an image is first uploaded and new, it is viewed several times. But after 45 days, analytics prove that image files are on average rarely requested, but the thumbnails still are. After 180 days, you would like to archive the image files and the thumbnails. Overall you would like the solution to remain highly available to prevent disasters happening against a whole Availability Zone (AZ). How can you implement an efficient cost strategy for your Amazon S3 bucket? (Select two)",
      "options": [
        "A. Create a Lifecycle Policy to transition objects to Amazon S3 One Zone IA using a prefix after 45 days",
        "B. Create a Lifecycle Policy to transition objects to Amazon S3 Standard IA using a prefix after 45 days",
        "C. Create a Lifecycle Policy to transition all objects to Amazon S3 Glacier after 180 days",
        "D. Create a Lifecycle Policy to transition all objects to Amazon S3 Standard IA after 45 days",
        "E. Create a Lifecycle Policy to transition objects to Amazon S3 Glacier using a prefix after 180 days"
      ],
      "correct_answer": "B",
      "explanation": "Correct options: To manage your S3 objects, so they are stored cost-effectively throughout their lifecycle, configure their Amazon S3 Lifecycle. An S3 Lifecycle configuration is a set of rules that define actions that Amazon S3 applies to a group of objects. There are two types of actions: Transition actions — Define when objects transition to another storage class. For example, you might choose to transition objects to the S3 Standard-IA storage class 30 days after you created them, or archive objects to the S3 Glacier storage class one year after creating them. Expiration actions — Define when objects expire. Amazon S3 deletes expired objects on your behalf. Create a Lifecycle Policy to transition objects to Amazon S3 Standard IA using a prefix after 45 days Amazon S3 Standard-IA is for data that is accessed less frequently but requires rapid access when needed. Amazon S3 Standard-IA offers high durability, high throughput, and low latency of S3 Standard, with a low per gigabyte storage price and per gigabyte retrieval fee. This combination of low cost and high performance makes Amazon S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files. The minimum storage duration charge is 30 days. As the use-case mentions that after 45 days, image files are rarely requested, but the thumbnails still are. So you need to use a prefix while configuring the Lifecycle Policy so that only objects in the s3://my-bucket/images are transitioned to Standard IA and not all the objects in the bucket. Create a Lifecycle Policy to transition all objects to Amazon S3 Glacier after 180 days Amazon S3 Glacier and S3 Glacier Deep Archive are secure, durable, and extremely low-cost Amazon S3 cloud storage classes for data archiving and long-term backup. They are designed to deliver 99.999999999% durability, and provide comprehensive security and compliance capabilities that can help meet even the most stringent regulatory requirements. Incorrect options: Create a Lifecycle Policy to transition all objects to Amazon S3 Standard IA after 45 days - As discussed above, you need to use a prefix while configuring the Lifecycle Policy so that only objects in the s3://my-bucket/images are transitioned to Amazon S3 Standard IA and not all the objects in the bucket. Create a Lifecycle Policy to transition objects to Amazon S3 Glacier using a prefix after 180 days - After 180 days, you can move all the objects to Amazon S3 Glacier storage as per the use case. Glacier doesn't need prefixes for the given use-case. Create a Lifecycle Policy to transition objects to Amazon S3 One Zone IA using a prefix after 45 days - Amazon S3 Standard-IA is for data that is accessed less frequently but requires rapid access when needed. Amazon S3 Standard-IA offers high durability, high throughput, and low latency of Amazon S3 Standard, with a low per GB storage price and per GB retrieval fee. This combination of low cost and high performance makes S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files. The minimum storage duration charge is 30 days. Amazon S3 One Zone-IA is for data that is accessed less frequently but requires rapid access when needed. Unlike other S3 Storage Classes which store data in a minimum of three Availability Zones (AZs), S3 One Zone-IA stores data in a single AZ and costs 20% less than S3 Standard-IA. The minimum storage duration charge is 30 days. Finally, Amazon S3 One Zone IA will not achieve the necessary availability in case an Availability Zone (AZ) goes down. References: https://aws.amazon.com/s3/storage-classes/ https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "You have developed a new REST API leveraging the Amazon API Gateway, AWS Lambda and Amazon Aurora database services. Most of the workload on the website is read-heavy. The data rarely changes and it is acceptable to serve users outdated data for about 24 hours. Recently, the website has been experiencing high load and the costs incurred on the Aurora database have been very high. How can you easily reduce the costs while improving performance, with minimal changes?",
      "options": [
        "A. Add Amazon Aurora Read Replicas",
        "B. Enable Amazon API Gateway Caching",
        "C. Enable AWS Lambda In Memory Caching",
        "D. Switch to using an Application Load Balancer"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Enable Amazon API Gateway Caching Amazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the \"front door\" for applications to access data, business logic, or functionality from your backend services. Using API Gateway, you can create RESTful APIs and WebSocket APIs that enable real-time two-way communication applications. API Gateway supports containerized and serverless workloads, as well as web applications. You can enable Amazon API caching in Amazon API Gateway to cache your endpoint's responses. With caching, you can reduce the number of calls made to your endpoint and also improve the latency of requests to your API. When you enable caching for a stage, API Gateway caches responses from your endpoint for a specified time-to-live (TTL) period, in seconds. Amazon API Gateway then responds to the request by looking up the endpoint response from the cache instead of requesting your endpoint. The default TTL value for API caching is 300 seconds. The maximum TTL value is 3600 seconds. TTL=0 means caching is disabled. Using API Gateway Caching feature is the answer for the use case, as we can accept stale data for about 24 hours. Incorrect options: Add Amazon Aurora Read Replicas - Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 64TB per database instance. It delivers high performance and availability with up to 15 low-latency read replicas, point-in-time recovery, continuous backup to Amazon S3, and replication across three Availability Zones (AZs). Amazon Aurora Read Replicas are independent endpoints in an Aurora DB cluster, best used for scaling read operations and increasing availability. Up to 15 Aurora Replicas can be distributed across the Availability Zones that a DB cluster spans within an AWS Region. The DB cluster volume is made up of multiple copies of the data for the DB cluster. However, the data in the cluster volume is represented as a single, logical volume to the primary instance and to Aurora Replicas in the DB cluster. Adding Aurora Read Replicas would greatly increase the cost, therefore this option is ruled out. Switch to using an Application Load Balancer - An Application Load Balancer functions at the application layer, the seventh layer of the Open Systems Interconnection (OSI) model. After the load balancer receives a request, it evaluates the listener rules in priority order to determine which rule to apply, and then selects a target from the target group for the rule action. You can configure listener rules to route requests to different target groups based on the content of the application traffic. Switching to a Load Balancer would not improve the current status as we need a caching mechanism. Enable AWS Lambda In Memory Caching - AWS Lambda has no native in-memory caching capability. AWS Lambda is a serverless compute capacity. This option is incorrect and has been added as a distractor. Reference: https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A small rental company had 5 employees, all working under the same AWS cloud account. These employees deployed their applications built for various functions- including billing, operations, finance, etc. Each of these employees has been operating in their own VPC. Now, there is a need to connect these VPCs so that the applications can communicate with each other. Which of the following is the MOST cost-effective solution for this use-case?",
      "options": [
        "A. Use an AWS Direct Connect connection",
        "B. Use a VPC peering connection",
        "C. Use an Internet Gateway",
        "D. Use a Network Address Translation gateway (NAT gateway)"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Use a VPC peering connection A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses. Instances in either VPC can communicate with each other as if they are within the same network. You can create a VPC peering connection between your own VPCs, or with a VPC in another AWS account. The VPCs can be in different regions (also known as an inter-region VPC peering connection). VPC Peering helps connect two VPCs and is not transitive. To connect VPCs together, the best available option is to use VPC peering. More on VPC Peering: via - https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html Incorrect options: Use an Internet Gateway - An internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between instances in your VPC and the internet. It, therefore, imposes no availability risks or bandwidth constraints on your network traffic. Internet Gateway is not meant for connecting between VPCs. Use an AWS Direct Connect connection - AWS Direct Connect is a cloud service solution that makes it easy to establish a dedicated network connection from your premises to AWS. AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations. Using industry-standard 802.1q VLANs, this dedicated connection can be partitioned into multiple virtual interfaces. For the given use-case, direct connect gateway is overkill and is not as cost-optimal as using VPC peering. Use a Network Address Translation gateway (NAT gateway) - You can use a network address translation (NAT) gateway to enable instances in a private subnet to connect to the internet or other AWS services, but prevent the internet from initiating a connection with those instances. You are charged for creating and using a NAT gateway in your account. NAT gateway hourly usage and data processing rates apply. NAT Gateway is not used for connection between VPCs. Reference: https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "You are working as a Solutions Architect for a photo processing company that has a proprietary algorithm to compress an image without any loss in quality. Because of the efficiency of the algorithm, your clients are willing to wait for a response that carries their compressed images back. You also want to process these jobs asynchronously and scale quickly, to cater to the high demand. Additionally, you also want the job to be retried in case of failures. Which combination of choices do you recommend to minimize cost and comply with the requirements? (Select two)",
      "options": [
        "A. Amazon Simple Notification Service (Amazon SNS)",
        "B. Amazon EC2 Spot Instances",
        "C. Amazon Simple Queue Service (Amazon SQS)",
        "D. Amazon EC2 Reserved Instances (RIs)",
        "E. Amazon EC2 On-Demand Instances"
      ],
      "correct_answer": "B",
      "explanation": "Correct options: Amazon EC2 Spot Instances A Spot Instance is an unused Amazon EC2 instance that is available for less than the On-Demand price. Because Spot Instances enable you to request unused Amazon EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly. The hourly price for a Spot Instance is called a Spot price. The Spot price of each instance type in each Availability Zone (AZ) is set by Amazon EC2 and adjusted gradually based on the long-term supply of and demand for Spot Instances. Your Spot Instance runs whenever capacity is available and the maximum price per hour for your request exceeds the Spot price. To process these jobs, due to the unpredictable nature of their volume, and the desire to save on costs, spot Instances are recommended as compared to on-demand instances. As spot instances are cheaper than reserved instances and do not require long term commitment, spot instances are a better fit for the given use-case. Amazon EC2 Instance purchasing options: via - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-purchasing-options.html Amazon Simple Queue Service (Amazon SQS) Amazon Simple Queue Service (Amazon SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. Amazon SQS FIFO (First-In-First-out) queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent. Amazon SQS will allow you to buffer the image compression requests and process them asynchronously. It also has a direct built-in mechanism for retries and scales seamlessly. Incorrect options: Amazon Simple Notification Service (Amazon SNS) - Amazon Simple Notification Service (Amazon SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. Amazon SNS provides topics for high-throughput, push-based, many-to-many messaging. SNS is not the right fit for this use-case, since its not a queuing mechanism. Amazon EC2 Reserved Instances (RIs) - Reserved instances (RIs) reduce your Amazon EC2 costs by making a commitment to a consistent instance configuration, including instance type and Region, for a term of 1 or 3 years. For the given use case, this kind of annual commitment might not be a desirable option. Amazon EC2 On-Demand Instances - With On-Demand Instances, you pay for compute capacity by the second with no long-term commitments. You have full control over its lifecycle—you decide when to launch, stop, hibernate, start, reboot, or terminate it. There is no long-term commitment required when you purchase On-Demand Instances. You pay only for the seconds that your On-Demand Instances are running. AWS recommends that you use On-Demand Instances for applications with short-term, irregular workloads that cannot be interrupted. References: https://aws.amazon.com/sqs/ https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "You started a new job as a solutions architect at a company that has both AWS experts and people learning AWS. Recently, a developer misconfigured a newly created Amazon RDS database which resulted in a production outage. How can you ensure that Amazon RDS specific best practices are incorporated into a reusable infrastructure template to be used by all your AWS users?",
      "options": [
        "A. Store your recommendations in a custom AWS Trusted Advisor rule",
        "B. Create an AWS Lambda function which sends emails when it finds misconfigured Amazon RDS databases",
        "C. Use AWS CloudFormation to manage Amazon RDS databases",
        "D. Attach an IAM policy to interns preventing them from creating an Amazon RDS database"
      ],
      "correct_answer": "C",
      "explanation": "Correct option: Use AWS CloudFormation to manage Amazon RDS databases AWS CloudFormation provides a common language for you to model and provision AWS and third-party application resources in your cloud environment. AWS CloudFormation allows you to use programming languages or a simple text file to model and provision, in an automated and secure manner, all the resources needed for your applications across all regions and accounts. This gives you a single source of truth for your AWS and third-party resources. AWS CloudFormation allows you to keep your infrastructure as code and re-use the best practices around your company for configuration parameters. Therefore, this is the correct option for the given use-case. Incorrect options: Store your recommendations in a custom AWS Trusted Advisor rule - AWS Trusted Advisor is an online tool that provides you real-time guidance to help you provision your resources following AWS best practices. Whether establishing new workflows, developing applications, or as part of ongoing improvement, take advantage of the recommendations provided by AWS Trusted Advisor regularly to help keep your solutions provisioned optimally. AWS Trusted Advisor just provides recommendations rather than creating reusable infrastructure templates. Create an AWS Lambda function which sends emails when it finds misconfigured Amazon RDS databases - Using an AWS Lambda function to scan for a misconfigured Amazon RDS database is a reactive mechanism. It does not help in creating reusable infrastructure templates. Attach an IAM policy to interns preventing them from creating an Amazon RDS database - Using an IAM policy to prevent interns from creating an Amazon RDS database does not solve the problem of allowing any user to create resources by leveraging reusable infrastructure templates. So, this option is ruled out. References: https://aws.amazon.com/premiumsupport/technology/trusted-advisor/ https://aws.amazon.com/cloudformation/",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A company wants to adopt a hybrid cloud infrastructure where it uses some AWS services such as Amazon S3 alongside its on-premises data center. The company wants a dedicated private connection between the on-premise data center and AWS. In case of failures though, the company needs to guarantee uptime and is willing to use the public internet for an encrypted connection. What do you recommend? (Select two)",
      "options": [
        "A. Use AWS Direct Connect connection as a primary connection",
        "B. Use AWS Site-to-Site VPN as a primary connection",
        "C. Use Egress Only Internet Gateway as a backup connection",
        "D. Use AWS Site-to-Site VPN as a backup connection",
        "E. Use AWS Direct Connect connection as a backup connection"
      ],
      "correct_answer": "A",
      "explanation": "Correct options: Use AWS Direct Connect connection as a primary connection AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations. Using industry-standard 802.1q VLANs, this dedicated connection can be partitioned into multiple virtual interfaces. AWS Direct Connect does not involve the Internet; instead, it uses dedicated, private network connections between your intranet and Amazon VPC. Use AWS Site-to-Site VPN as a backup connection AWS Site-to-Site VPN enables you to securely connect your on-premises network or branch office site to your Amazon Virtual Private Cloud (Amazon VPC). You can securely extend your data center or branch office network to the cloud with an AWS Site-to-Site VPN connection. A VPC VPN Connection utilizes IPSec to establish encrypted network connectivity between your intranet and Amazon VPC over the Internet. VPN Connections can be configured in minutes and are a good solution if you have an immediate need, have low to modest bandwidth requirements, and can tolerate the inherent variability in Internet-based connectivity. AWS Direct Connect as a primary connection guarantees great performance and security (as the connection is private). Using Direct Connect as a backup solution would work but probably carries a risk it would fail as well. As we don't mind going over the public internet (which is reliable, but less secure as connections are going over the public route), we should use a Site to Site VPN which offers an encrypted connection to handle failover scenarios. Incorrect options: Use Egress Only Internet Gateway as a backup connection - An Egress-Only Internet Gateway is a horizontally scaled, redundant, and highly available VPC component that allows outbound communication over IPv6 from instances in your VPC to the Internet, and prevents the Internet from initiating an IPv6 connection with your instances. Egress-Only Internet Gateway cannot be used to connect on-premises data centers to AWS Cloud. Use AWS Site-to-Site VPN as a primary connection - AWS Site-to-Site VPN as a primary connection is not advisable since the use of internet-based connection is only for failover scenarios, as stated in the problem. Use AWS Direct Connect connection as a backup connection - AWS Direct Connect connection is a highly secure, physical connection. It is also a costly solution and hence does not make much sense to set up the connection and keep it only as a backup. References: https://aws.amazon.com/directconnect/ https://aws.amazon.com/vpn/",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "As an e-sport tournament hosting company, you have servers that need to scale and be highly available. Therefore you have deployed an Elastic Load Balancing (ELB) with an Auto Scaling group (ASG) across 3 Availability Zones (AZs). When e-sport tournaments are running, the servers need to scale quickly. And when tournaments are done, the servers can be idle. As a general rule, you would like to be highly available, have the capacity to scale and optimize your costs. What do you recommend? (Select two)",
      "options": [
        "A. Set the minimum capacity to 1",
        "B. Set the minimum capacity to 3",
        "C. Set the minimum capacity to 2",
        "D. Use Dedicated hosts for the minimum capacity",
        "E. Use Reserved Instances (RIs) for the minimum capacity"
      ],
      "correct_answer": "C",
      "explanation": "Correct options: Set the minimum capacity to 2 An Auto Scaling group contains a collection of Amazon EC2 instances that are treated as a logical grouping for automatic scaling and management. An Auto Scaling group also enables you to use Amazon EC2 Auto Scaling features such as health check replacements and scaling policies. Both maintaining the number of instances in an Auto Scaling group and automatic scaling are the core functionality of the Amazon EC2 Auto Scaling service. You configure the size of your Auto Scaling group by setting the minimum, maximum, and desired capacity. The minimum and maximum capacity are required to create an Auto Scaling group, while the desired capacity is optional. If you do not define your desired capacity upfront, it defaults to your minimum capacity. An Auto Scaling group is elastic as long as it has different values for minimum and maximum capacity. All requests to change the Auto Scaling group's desired capacity (either by manual scaling or automatic scaling) must fall within these limits. Here, even though our ASG is deployed across 3 Availability Zones (AZs), the minimum capacity to be highly available is 2. When we specify 2 as the minimum capacity, the ASG would create these 2 instances in separate Availability Zones (AZs). If demand goes up, the ASG would spin up a new instance in the third Availability Zone (AZ). Later as the demand subsides, the ASG would scale-in and the instance count would be back to 2. Use Reserved Instances (RIs) for the minimum capacity Reserved Instances (RIs) provide you with significant savings on your Amazon EC2 costs compared to On-Demand Instance pricing. Reserved Instances are not physical instances, but rather a billing discount applied to the use of On-Demand Instances in your account. These On-Demand Instances must match certain attributes, such as instance type and Region, to benefit from the billing discount. Since minimum capacity will always be maintained, it is cost-effective to choose reserved instances than any other option. In case of an Availability Zone (AZ) outage, the instance in that Availability Zone (AZ) would go down however the other instance would still be available. The ASG would provision the replacement instance in the third Availability Zone (AZ) to keep the minimum count to 2. Incorrect options: Set the minimum capacity to 1 - This is not failure proof, since only one instance will be maintained consistently and this will be from only one Availability Zone (AZ). Set the minimum capacity to 3 - This is not a cost-effective option, as two instances in two different Availability Zones (AZs) are enough to make the architecture disaster-proof. Use Dedicated hosts for the minimum capacity - As there is no use-case to utilize existing per-socket, per-core, or per-VM software licenses or to run the instance on a dedicated physical host, so the option to use dedicated hosts for the minimum capacity is ruled out. References: https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html https://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-capacity-limits.html",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "The engineering team at a company is running batch workloads on AWS Cloud. The team has embedded Amazon RDS database connection strings within each web server hosting the flagship application. After failing a security audit, the team is looking at a different approach to store the database secrets securely and automatically rotate the database credentials. Which of the following solutions would you recommend to meet this requirement?",
      "options": [
        "A. AWS Systems Manager Parameter Store",
        "B. AWS Secrets Manager",
        "C. AWS Systems Manager",
        "D. AWS Key Management Service (KMS)"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: AWS Secrets Manager AWS Secrets Manager enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. Users and applications retrieve secrets with a call to Secrets Manager APIs, eliminating the need to hardcode sensitive information in plain text. Secrets Manager offers secret rotation with built-in integration for Amazon RDS, Amazon Redshift, and Amazon DocumentDB. Benefits of AWS Secrets Manager: via - https://aws.amazon.com/secrets-manager/ Incorrect options: AWS Systems Manager Parameter Store - AWS Systems Manager Parameter Store provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, and license codes as parameter values. AWS SSM Parameter Store cannot be used to automatically rotate the database credentials. AWS Systems Manager - AWS Systems Manager gives you visibility and control of your infrastructure on AWS. Systems Manager provides a unified user interface so you can view operational data from multiple AWS services and allows you to automate operational tasks across your AWS resources. AWS Systems Manager cannot be used to store your secrets securely and automatically rotate the database credentials. AWS Key Management Service (KMS) - AWS Key Management Service (KMS) makes it easy for you to create and manage cryptographic keys and control their use across a wide range of AWS services and in your applications. KMS cannot be used to store your secrets securely and automatically rotate the database credentials. References: https://aws.amazon.com/secrets-manager/ https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A company's business logic is built on several microservices that are running in the on-premises data center. They currently communicate using a message broker that supports the MQTT protocol. The company is looking at migrating these applications and the message broker to AWS Cloud without changing the application logic. Which technology allows you to get a managed message broker that supports the MQTT protocol?",
      "options": [
        "A. Amazon MQ",
        "B. Amazon Simple Queue Service (Amazon SQS)",
        "C. Amazon Simple Notification Service (Amazon SNS)",
        "D. Amazon Kinesis Data Streams"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Amazon MQ Amazon MQ is a managed message broker service for Apache ActiveMQ that makes it easy to set up and operate message brokers in the cloud. Message brokers allow different software systems–often using different programming languages, and on different platforms–to communicate and exchange information. If an organization is using messaging with existing applications and wants to move the messaging service to the cloud quickly and easily, AWS recommends Amazon MQ for such a use case. Connecting your current applications to Amazon MQ is easy because it uses industry-standard APIs and protocols for messaging, including JMS, NMS, AMQP, STOMP, MQTT, and WebSocket. Therefore, the only possible answer is Amazon MQ. Incorrect option: Amazon Simple Queue Service (Amazon SQS) - Amazon Simple Queue Service (Amazon SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS eliminates the complexity and overhead associated with managing and operating message-oriented middleware and empowers developers to focus on differentiating work. Using Amazon SQS, you can send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be available. Amazon Kinesis Data Streams - Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. Amazon Simple Notification Service (Amazon SNS) - Amazon Simple Notification Service (Amazon SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. Amazon SNS provides topics for high-throughput, push-based, many-to-many messaging. Amazon SNS, Amazon SQS, and Amazon Kinesis are AWS's proprietary technologies and do not come with MQTT compatibility. References: https://aws.amazon.com/amazon-mq/",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A developer in your company has set up a classic 2 tier architecture consisting of an Application Load Balancer and an Auto Scaling group (ASG) managing a fleet of Amazon EC2 instances. The Application Load Balancer is deployed in a subnet of size 10.0.1.0/24 and the Auto Scaling group is deployed in a subnet of size 10.0.4.0/22. As a solutions architect, you would like to adhere to the security pillar of the well-architected framework. How do you configure the security group of the Amazon EC2 instances to only allow traffic coming from the Application Load Balancer?",
      "options": [
        "A. Add a rule to authorize the security group of the Application Load Balancer",
        "B. Add a rule to authorize the CIDR 10.0.4.0/22",
        "C. Add a rule to authorize the security group of the Auto Scaling group",
        "D. Add a rule to authorize the CIDR 10.0.1.0/24"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: An Auto Scaling group (ASG) contains a collection of Amazon EC2 instances that are treated as a logical grouping for automatic scaling and management. An Auto Scaling group also enables you to use Amazon EC2 Auto Scaling features such as health check replacements and scaling policies. Both maintaining the number of instances in an Auto Scaling group and automatic scaling are the core functionality of the Amazon EC2 Auto Scaling service. Add a rule to authorize the security group of the Application Load Balancer A security group acts as a virtual firewall that controls the traffic for one or more instances. When you launch an instance, you can specify one or more security groups; otherwise, we use the default security group. You can add rules to each security group that allow traffic to or from its associated instances. You can modify the rules for a security group at any time; the new rules are automatically applied to all instances that are associated with the security group. When deciding to allow traffic to reach an instance, all the rules from all the security groups that are associated with the instance are evaluated. The following are the characteristics of security group rules: 1. By default, security groups allow all outbound traffic. 2. Security group rules are always permissive; you can't create rules that deny access. 3. Security groups are stateful Application Load Balancer (ALB) operates at the request level (layer 7), routing traffic to targets – Amazon EC2 instances, containers, IP addresses and Lambda functions based on the content of the request. Ideal for advanced load balancing of HTTP and HTTPS traffic, Application Load Balancer provides advanced request routing targeted at delivery of modern application architectures, including microservices and container-based applications. Incorrect option: Add a rule to authorize the CIDR 10.0.4.0/22 Add a rule to authorize the security group of the Auto Scaling group Add a rule to authorize the CIDR 10.0.1.0/24 Adding the entire CIDR of the Application Load Balancer would work, but wouldn't guarantee that only the Auto Scaling group can access the Amazon EC2 instances that are part of the Auto Scaling group. Here, the right solution is to add a rule on the Auto Scaling group security group to allow incoming traffic only from the security group configured for the Application Load Balancer. Reference: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "The engineering team at a leading e-commerce company is anticipating a surge in the traffic because of a flash sale planned for the weekend. You have estimated the web traffic to be 10x. The content of your website is highly dynamic and changes very often. As a Solutions Architect, which of the following options would you recommend to make sure your infrastructure scales for that day?",
      "options": [
        "A. Use an Amazon Route 53 Multi Value record",
        "B. Use an Amazon CloudFront distribution in front of your website",
        "C. Use an Auto Scaling Group",
        "D. Deploy the website on Amazon S3"
      ],
      "correct_answer": "C",
      "explanation": "Correct option: Use an Auto Scaling Group An Auto Scaling group (ASG) contains a collection of Amazon EC2 instances that are treated as a logical grouping for automatic scaling and management. An Auto Scaling group also enables you to use Amazon EC2 Auto Scaling features such as health check replacements and scaling policies. Both maintaining the number of instances in an Auto Scaling group and automatic scaling are the core functionality of the Amazon EC2 Auto Scaling service. The size of an Auto Scaling group depends on the number of instances that you set as the desired capacity. You can adjust its size to meet demand, either manually or by using automatic scaling. An Auto Scaling group starts by launching enough instances to meet its desired capacity. It maintains this number of instances by performing periodic health checks on the instances in the group. The Auto Scaling group continues to maintain a fixed number of instances even if an instance becomes unhealthy. If an instance becomes unhealthy, the group terminates the unhealthy instance and launches another instance to replace it. Auto Scaling group is the correct answer here. Incorrect option: Use an Amazon CloudFront distribution in front of your website - Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment. You can use Amazon CloudFront to improve the performance of your website. Amazon CloudFront makes your website files (such as HTML, images, and video) available from data centers around the world (called edge locations). When a visitor requests a file from your website, CloudFront automatically redirects the request to a copy of the file at the nearest edge location. This results in faster download times than if the visitor had requested the content from a data center that is located farther away. Amazon CloudFront is not a good solution here as the content is highly dynamic, and Amazon CloudFront will cache things. Deploy the website on Amazon S3 - You can use Amazon S3 to host a static website. On a static website, individual web pages include static content. They might also contain client-side scripts. To host a static website on Amazon S3, you configure an Amazon S3 bucket for website hosting and then upload your website content to the bucket. When you configure a bucket as a static website, you enable static website hosting, set permissions, and add an index document. Depending on your website requirements, you can also configure other options, including redirects, web traffic logging, and custom error documents. Dynamic applications cannot be deployed to Amazon S3. This option has been added as a distractor. Use an Amazon Route 53 Multi Value record - Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. Use Multi Value answer routing policy when you want Route 53 to respond to DNS queries with up to eight healthy records selected at random. Amazon Route 53 does not help in scaling your application. This option has been added as a distractor. References: https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A niche social media application allows users to connect with sports athletes. As a solutions architect, you've designed the architecture of the application to be fully serverless using Amazon API Gateway and AWS Lambda. The backend uses an Amazon DynamoDB table. Some of the star athletes using the application are highly popular, and therefore Amazon DynamoDB has increased the read capacity units (RCUs). Still, the application is experiencing a hot partition problem. What can you do to improve the performance of Amazon DynamoDB and eliminate the hot partition problem without a lot of application refactoring?",
      "options": [
        "A. Use Amazon ElastiCache",
        "B. Use Amazon DynamoDB Streams",
        "C. Use Amazon DynamoDB DAX",
        "D. Use Amazon DynamoDB Global Tables"
      ],
      "correct_answer": "C",
      "explanation": "Correct option: Use Amazon DynamoDB DAX Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-Region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications. Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement – from milliseconds to microseconds – even at millions of requests per second. DAX does all the heavy lifting required to add in-memory acceleration to your DynamoDB tables, without requiring developers to manage cache invalidation, data population, or cluster management. DAX will be transparent and won't require an application refactoring, and will cache the \"hotkeys\". Therefore, this is the correct option. Incorrect options: Use Amazon DynamoDB Global Tables - Amazon DynamoDB Global Tables builds upon DynamoDB’s global footprint to provide you with a fully managed, multi-region, and multi-master database that provides fast, local, read and write performance for massively scaled, global applications. Global Tables replicates your Amazon DynamoDB tables automatically across your choice of AWS regions. But Global Tables cannot address the hotkey issue. Use Amazon DynamoDB Streams - Amazon DynamoDB stream is an ordered flow of information about changes to items in a DynamoDB table. When you enable a stream on a table, DynamoDB captures information about every modification to data items in the table. Whenever an application creates, updates, or deletes items in the table, DynamoDB Streams writes a stream record with the primary key attributes of the items that were modified. A stream record contains information about a data modification to a single item in a DynamoDB table. DynamoDB Streams cannot address the hotkey issue. Use Amazon ElastiCache - Amazon ElastiCache allows you to seamlessly set up, run, and scale popular open-Source compatible in-memory data stores in the cloud. Build data-intensive apps or boost the performance of your existing databases by retrieving data from high throughput and low latency in-memory data stores. Amazon ElastiCache is a popular choice for real-time use cases like Caching, Session Stores, Gaming, Geospatial Services, Real-Time Analytics, and Queuing. ElastiCache could also be a solution, but it will require a lot of refactoring work on the AWS Lambda side. References: https://aws.amazon.com/dynamodb/dax/ https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "Your company runs a web portal to match developers to clients who need their help. As a solutions architect, you've designed the architecture of the website to be fully serverless with Amazon API Gateway and AWS Lambda. The backend uses Amazon DynamoDB table. You would like to automatically congratulate your developers on important milestones, such as - their first paid contract. All the contracts are stored in Amazon DynamoDB. Which Amazon DynamoDB feature can you use to implement this functionality such that there is LEAST delay in sending automatic notifications?",
      "options": [
        "A. Amazon DynamoDB DAX + Amazon API Gateway",
        "B. Amazon EventBridge events + AWS Lambda",
        "C. Amazon Simple Queue Service (Amazon SQS) + AWS Lambda",
        "D. Amazon DynamoDB Streams + AWS Lambda"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-Region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications. Amazon DynamoDB Streams + AWS Lambda Amazon DynamoDB stream is an ordered flow of information about changes to items in Amazon DynamoDB table. When you enable a stream on a table, DynamoDB captures information about every modification to data items in the table. Whenever an application creates, updates, or deletes items in the table, DynamoDB Streams writes a stream record with the primary key attributes of the items that were modified. A stream record contains information about a data modification to a single item in a DynamoDB table. Amazon DynamoDB Streams will contain a stream of all the changes that happen to an Amazon DynamoDB table. It can be chained with an AWS Lambda function that will be triggered to react to these changes, one of which is the developer's milestone. Therefore, this is the correct option. Incorrect options: Amazon DynamoDB DAX + Amazon API Gateway - Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement – from milliseconds to microseconds – even at millions of requests per second. DAX does all the heavy lifting required to add in-memory acceleration to your DynamoDB tables, without requiring developers to manage cache invalidation, data population, or cluster management. DAX is a caching layer and Amazon API Gateway is used to deploy APIs at scale, so this won't help. Amazon Simple Queue Service (Amazon SQS) + AWS Lambda - Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. Amazon SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. Amazon SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent. Amazon SQS and AWS Lambda could also work, but one would need to write extra logic to send messages to SQS, whereas our data already lives on Amazon DynamoDB so Amazon DynamoDB Streams is a much better choice. Amazon EventBridge events + AWS Lambda - You cannot use Amazon DynamoDB as a target for an Amazon EventBridge event, so this option is ruled out. References: https://aws.amazon.com/dynamodb/dax/ https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "As part of the on-premises data center migration to AWS Cloud, a company is looking at using multiple AWS Snow Family devices to move their on-premises data. Which AWS Snow Family service offers the feature of storage clustering?",
      "options": [
        "A. AWS Snowcone",
        "B. AWS Snowmobile Storage Compute",
        "C. AWS Snowmobile",
        "D. AWS Snowball Edge Compute Optimized"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: AWS Snowball Edge Compute Optimized AWS Snowball is a data migration and edge computing device that comes in two device options: Compute Optimized and Storage Optimized. AWS Snowball Edge Storage Optimized devices provide 40 vCPUs of compute capacity coupled with 80 terabytes of usable block or Amazon S3-compatible object storage. It is well-suited for local storage and large-scale data transfer. AWS Snowball Edge Compute Optimized devices provide 52 vCPUs, 42 terabytes of usable block or object storage, and an optional GPU for use cases such as advanced machine learning and full-motion video analysis in disconnected environments. Customers can use these two options for data collection, machine learning and processing, and storage in environments with intermittent connectivity (such as manufacturing, industrial, and transportation) or in extremely remote locations (such as military or maritime operations) before shipping it back to AWS. These devices may also be rack mounted and clustered together to build larger, temporary installations. Therefore, both AWS Snowball Edge Storage Optimized and AWS Snowball Edge Compute Optimized offer the storage clustering feature. via - https://aws.amazon.com/snow/#Feature_comparison Incorrect options: AWS Snowcone - AWS Snowcone is the smallest member of the AWS Snow Family of edge computing and data transfer devices. Snowcone is portable, rugged, and secure. You can use Snowcone to collect, process, and move data to AWS, either offline by shipping the device or online with AWS DataSync. Snowcone does not offer a storage Clustering option. AWS Snowmobile - AWS Snowmobile moves up to 100 PB of data in a 45-foot long ruggedized shipping container and is ideal for multi-petabyte or Exabyte-scale digital media migrations and data centers shutdowns. AWS Snowmobile arrives at the customer site and appears as a network-attached data store for more secure, high-speed data transfer. After data is transferred to Snowmobile, it is driven back to an AWS Region where the data is loaded into Amazon S3. AWS Snowmobile does not offer a storage Clustering option. AWS Snowmobile Storage Compute - This is a made-up option, given only as a distractor. References: https://aws.amazon.com/snow/#Feature_comparison https://aws.amazon.com/snow/",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A development team has configured Elastic Load Balancing for host-based routing. The idea is to support multiple subdomains and different top-level domains. The rule *.example.com matches which of the following?",
      "options": [
        "A. example.com",
        "B. example.test.com",
        "C. EXAMPLE.COM",
        "D. test.example.com"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: test.example.com You can use host conditions to define rules that route requests based on the hostname in the host header (also known as host-based routing). This enables you to support multiple subdomains and different top-level domains using a single load balancer. A hostname is not case-sensitive, can be up to 128 characters in length, and can contain any of the following characters: 1. A–Z, a–z, 0–9 2. - . 3. * (matches 0 or more characters) 4. ? (matches exactly 1 character) You must include at least one \".\" character. You can include only alphabetical characters after the final \".\" character. The rule *.example.com matches test.example.com but doesn't match example.com. Incorrect options: example.com example.test.com EXAMPLE.COM These three options contradict the explanation provided above, so these options are incorrect. Reference: https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A company uses Application Load Balancers in multiple AWS Regions. The Application Load Balancers receive inconsistent traffic that varies throughout the year. The engineering team at the company needs to allow the IP addresses of the Application Load Balancers in the on-premises firewall to enable connectivity. Which of the following represents the MOST scalable solution with minimal configuration changes?",
      "options": [
        "A. Migrate all Application Load Balancers in different Regions to the Network Load Balancers. Configure the on-premises firewall's rule to allow the Elastic IP addresses of all the Network Load Balancers",
        "B. Set up a Network Load Balancer in one Region. Register the private IP addresses of the Application Load Balancers in different Regions with the Network Load Balancer. Configure the on-premises firewall's rule to allow the Elastic IP address attached to the Network Load Balancer",
        "C. Develop an AWS Lambda script to get the IP addresses of the Application Load Balancers in different Regions. Configure the on-premises firewall's rule to allow the IP addresses of the Application Load Balancers",
        "D. Set up AWS Global Accelerator. Register the Application Load Balancers in different Regions to the AWS Global Accelerator. Configure the on-premises firewall's rule to allow static IP addresses associated with the AWS Global Accelerator"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: Set up AWS Global Accelerator. Register the Application Load Balancers in different Regions to the AWS Global Accelerator. Configure the on-premises firewall's rule to allow static IP addresses associated with the AWS Global Accelerator AWS Global Accelerator is a networking service that helps you improve the availability and performance of the applications that you offer to your global users. AWS Global Accelerator is easy to set up, configure, and manage. It provides static IP addresses that provide a fixed entry point to your applications and eliminate the complexity of managing specific IP addresses for different AWS Regions and Availability Zones. Associate the static IP addresses provided by AWS Global Accelerator to regional AWS resources or endpoints, such as Network Load Balancers, Application Load Balancers, Amazon EC2 Instances, and Elastic IP addresses. The IP addresses are anycast from AWS edge locations so they provide onboarding to the AWS global network close to your users. Simplified and resilient traffic routing for multi-Region applications using AWS Global Accelerator: via - https://aws.amazon.com/global-accelerator/ Incorrect options: Migrate all Application Load Balancers in different Regions to the Network Load Balancers. Configure the on-premises firewall's rule to allow the Elastic IP addresses of all the Network Load Balancers - Although you could potentially migrate the Application Load Balancers to Network Load Balancers, this option requires changes to the on-premises firewall's configuration rules, hence this is not the right fit for the given use-case. It is more optimal to manage the two static IPs provided by the AWS Global Accelerator for configuring the firewall. Set up a Network Load Balancer in one Region. Register the private IP addresses of the Application Load Balancers in different Regions with the Network Load Balancer. Configure the on-premises firewall's rule to allow the Elastic IP address attached to the Network Load Balancer - Using a single Network Load Balancer is not possible across AWS regions since an Network Load Balancer is Region bound. Multiple Network Load Balancers have to be registered for the on-premises firewall. Develop an AWS Lambda script to get the IP addresses of the Application Load Balancers in different Regions. Configure the on-premises firewall's rule to allow the IP addresses of the Application Load Balancers - This option requires on-going changes to the on-premises firewall's configuration rules because the IP addresses of the Application Load Balancers would keep changing. Hence this is not the right fit for the given use-case. It is more optimal to configure the firewall with a one-time change for the two static IPs provided by the AWS Global Accelerator. Reference: https://aws.amazon.com/global-accelerator/faqs/",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "Amazon Route 53 is configured to route traffic to two Network Load Balancer nodes belonging to two Availability Zones (AZs): AZ-A and AZ-B. Cross-zone load balancing is disabled. AZ-A has four targets and AZ-B has six targets. Which of the below statements is true about traffic distribution to the target instances from Amazon Route 53?",
      "options": [
        "A. Each of the six targets in AZ-B receives 10% of the traffic",
        "B. Each of the four targets in AZ-A receives 8% of the traffic",
        "C. Each of the four targets in AZ-A receives 12.5% of the traffic",
        "D. Each of the four targets in AZ-A receives 10% of the traffic"
      ],
      "correct_answer": "C",
      "explanation": "Correct option: Each of the four targets in AZ-A receives 12.5% of the traffic The nodes for your load balancer distribute requests from clients to registered targets. When cross-zone load balancing is enabled, each load balancer node distributes traffic across the registered targets in all enabled Availability Zones (AZs). When cross-zone load balancing is disabled, each load balancer node distributes traffic only across the registered targets in its Availability Zone (AZ). Amazon Route 53 will distribute traffic such that each load balancer node receives 50% of the traffic from the clients. If cross-zone load balancing is disabled: 1. Each of the four targets in AZ-A receives 12.5% of the traffic. 2. Each of the six targets in AZ-B receives 8.3% of the traffic. This is because each load balancer node can route its 50% of the client traffic only to targets in its Availability Zone (AZ). Incorrect options: Each of the six targets in AZ-B receives 10% of the traffic - As mentioned above in the correct explanation, each of the six targets in AZ-B receives 8.3% of the traffic. Each of the four targets in AZ-A receives 8% of the traffic - As mentioned above in the correct explanation, each of the four targets in AZ-A receives 12.5% of the traffic. Each of the four targets in AZ-A receives 10% of the traffic - As mentioned above in the correct explanation, each of the four targets in AZ-A receives 12.5% of the traffic. Reference: https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An IT company runs a high-performance computing (HPC) workload on AWS. The workload requires high network throughput and low-latency network performance along with tightly coupled node-to-node communications. The Amazon EC2 instances are properly sized for compute and storage capacity and are launched using default options. Which of the following solutions can be used to improve the performance of the workload?",
      "options": [
        "A. Select the appropriate capacity reservation while launching Amazon EC2 instances",
        "B. Select a cluster placement group while launching Amazon EC2 instances",
        "C. Select dedicated instance tenancy while launching Amazon EC2 instances",
        "D. Select an Elastic Inference accelerator while launching Amazon EC2 instances"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Select a cluster placement group while launching Amazon EC2 instances When you launch a new Amazon EC2 instance, the EC2 service attempts to place the instance in such a way that all of your instances are spread out across underlying hardware to minimize correlated failures. You can use placement groups to influence the placement of a group of interdependent instances to meet the needs of your workload. Cluster placement group packs instances close together inside an Availability Zone (AZ). This strategy enables workloads to achieve the low-latency network performance necessary for tightly coupled node-to-node communication that is typical of HPC applications. More on Cluster placement groups: via - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html Incorrect options: Select the appropriate capacity reservation while launching Amazon EC2 instances - Capacity Reservations enable you to reserve compute capacity for your Amazon EC2 instances in a specific Availability Zone (AZ) for any duration. This gives you the ability to create and manage Capacity Reservations independently from the billing discounts offered by Savings Plans or regional Reserved Instances (RIs). By creating Capacity Reservations, you ensure that you always have access to Amazon EC2 capacity when you need it, for as long as you need it. Capacity Reservations cannot impact the performance of the underlying Amazon EC2 instances. Select dedicated instance tenancy while launching Amazon EC2 instances - Dedicated Instances are Amazon EC2 instances that run in a VPC on hardware that's dedicated to a single customer. Your Dedicated instances are physically isolated at the host hardware level from instances that belong to other AWS accounts. Dedicated Instances are useful from a compliance perspective, but are not meant for performance improvement. Select an Elastic Inference accelerator while launching Amazon EC2 instances - Amazon Elastic Inference accelerators are GPU-powered hardware devices that are designed to work with any Amazon EC2 instance, Amazon Sagemaker instance, or Amazon ECS task to accelerate deep learning inference workloads at a low cost. They are for workloads that need deep learning. Also, AWS PrivateLink VPC Endpoints are needed for Elastic Inference accelerators, which makes it unsuitable for the current scenario. Reference: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A social media company wants the capability to dynamically alter the size of a geographic area from which traffic is routed to a specific server resource. Which feature of Amazon Route 53 can help achieve this functionality?",
      "options": [
        "A. Geolocation routing",
        "B. Latency-based routing",
        "C. Geoproximity routing",
        "D. Weighted routing"
      ],
      "correct_answer": "C",
      "explanation": "Correct option: Geoproximity routing Geoproximity routing lets Amazon Route 53 route traffic to your resources based on the geographic location of your users and your resources. You can also optionally choose to route more traffic or less to a given resource by specifying a value, known as a bias. A bias expands or shrinks the size of the geographic region from which traffic is routed to a resource. To optionally change the size of the geographic region from which Amazon Route 53 routes traffic to a resource, specify the applicable value for the bias: 1. To expand the size of the geographic region from which Amazon Route 53 routes traffic to a resource, specify a positive integer from 1 to 99 for the bias. Amazon Route 53 shrinks the size of adjacent regions. To shrink the size of the geographic region from which Amazon Route 53 routes traffic to a resource, specify a negative bias of -1 to -99. Amazon Route 53 expands the size of adjacent regions. More on how bias works in Geoproximity routing: via - https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html Incorrect options: Geolocation routing - Geolocation routing lets you choose the resources that serve your traffic based on the geographic location of your users, meaning the location that DNS queries originate from. For example, you might want all queries from Europe to be routed to an Elastic Load Balancing (ELB) load balancer in the Frankfurt region. When you use geolocation routing, you can localize your content and present some or all of your website in the language of your users. You can also use geolocation routing to restrict the distribution of content to only the locations in which you have distribution rights. Another possible use is for balancing load across endpoints in a predictable, easy-to-manage way so that each user location is consistently routed to the same endpoint. Latency-based routing - If your application is hosted in multiple AWS Regions, you can improve performance for your users by serving their requests from the AWS Region that provides the lowest latency. To use latency-based routing, you create latency records for your resources in multiple AWS Regions. When Amazon Route 53 receives a DNS query for your domain or subdomain (example.com or acme.example.com), it determines which AWS Regions you've created latency records for, determines which region gives the user the lowest latency, and then selects a latency record for that region. Amazon Route 53 responds with the value from the selected record, such as the IP address for a web server. Weighted routing - Weighted routing lets you associate multiple resources with a single domain name (example.com) or subdomain name (acme.example.com) and choose how much traffic is routed to each resource. This can be useful for a variety of purposes, including load balancing and testing new versions of software. To configure weighted routing, you create records that have the same name and type for each of your resources. You assign each record a relative weight that corresponds with how much traffic you want to send to each resource. Amazon Route 53 sends traffic to a resource based on the weight that you assign to the record as a proportion of the total weight for all records in the group. Reference: https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A ride-sharing company wants to use an Amazon DynamoDB table for data storage. The table will not be used during the night hours whereas the read and write traffic will often be unpredictable during day hours. When traffic spikes occur they will happen very quickly. Which of the following will you recommend as the best-fit solution?",
      "options": [
        "A. Set up Amazon DynamoDB table in the provisioned capacity mode with auto-scaling enabled",
        "B. Set up Amazon DynamoDB table in the on-demand capacity mode",
        "C. Set up Amazon DynamoDB table with a global secondary index",
        "D. Set up Amazon DynamoDB global table in the provisioned capacity mode"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Set up Amazon DynamoDB table in the on-demand capacity mode Amazon DynamoDB has two read/write capacity modes for processing reads and writes on your tables: On-demand Provisioned (default, free-tier eligible) Amazon DynamoDB on-demand is a flexible billing option capable of serving thousands of requests per second without capacity planning. DynamoDB on-demand offers pay-per-request pricing for read and write requests so that you pay only for what you use. The on-demand mode is a good option if any of the following are true: You create new tables with unknown workloads. You have unpredictable application traffic. You prefer the ease of paying for only what you use. If you choose provisioned mode, you specify the number of reads and writes per second that you require for your application. You can use auto-scaling to adjust your table’s provisioned capacity automatically in response to traffic changes. This helps you govern your DynamoDB use to stay at or below a defined request rate to obtain cost predictability. Provisioned mode is a good option if any of the following are true: You have predictable application traffic. You run applications whose traffic is consistent or ramps gradually. You can forecast capacity requirements to control costs. via - https://aws.amazon.com/blogs/database/amazon-dynamodb-auto-scaling-performance-and-cost-optimization-at-any-scale/ With on-demand, Amazon DynamoDB instantly allocates capacity as it is needed. There is no concept of provisioned capacity, and there is no delay waiting for Amazon CloudWatch thresholds or the subsequent table updates. On-demand is ideal for bursty, new, or unpredictable workloads whose traffic can spike in seconds or minutes, and when underprovisioned capacity would impact the user experience. On-demand is a perfect solution if your team is moving to a NoOps or serverless environment. The given use case clearly states that when the traffic spikes occur they happen very quickly, thereby implying an unpredictable traffic pattern, therefore the on-demand capacity mode is the correct option for the given use case. Incorrect options: Set up Amazon DynamoDB table in the provisioned capacity mode with auto-scaling enabled - As mentioned in the explanation above, you should use the provisioned capacity mode (even with auto-scaling) only when you have predictable application traffic. When you create Amazon DynamoDB table, auto-scaling is the default capacity setting, but you can also enable auto-scaling on any table that does not have it active. Behind the scenes, as illustrated in the following diagram, DynamoDB auto scaling uses a scaling policy in Application Auto Scaling. To configure auto-scaling in DynamoDB, you set the minimum and maximum levels of read and write capacity in addition to the target utilization percentage. Auto-scaling uses Amazon CloudWatch to monitor a table’s read and write capacity metrics. To do so, it creates CloudWatch alarms that track consumed capacity. Set up Amazon DynamoDB table with a global secondary index - A global secondary index (GSI) is an index with a partition key and a sort key that can be different from those on the base table. A global secondary index is considered \"global\" because queries on the index can span all of the data in the base table, across all partitions. A global secondary index is stored in its own partition space away from the base table and scales separately from the base table. GSI cannot be used to handle an unpredictable load on a DynamoDB table. Set up Amazon DynamoDB global table in the provisioned capacity mode - Global tables build on the global Amazon DynamoDB footprint to provide you with a fully managed, multi-Region, and multi-active database that delivers fast, local, read and write performance for massively scaled, global applications. Global tables replicate your DynamoDB tables automatically across your choice of AWS Regions. Global tables eliminate the difficult work of replicating data between Regions and resolving update conflicts, enabling you to focus on your application's business logic. In addition, global tables enable your applications to stay highly available even in the unlikely event of isolation or degradation of an entire Region. Amazon DynamoDB global tables: via - https://aws.amazon.com/dynamodb/global-tables/ Amazon DynamoDB global table cannot be used to handle an unpredictable load on a Amazon DynamoDB table. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html https://aws.amazon.com/blogs/database/amazon-dynamodb-auto-scaling-performance-and-cost-optimization-at-any-scale/ https://aws.amazon.com/dynamodb/global-tables/",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "The engineering team at a social media company has recently migrated to AWS Cloud from its on-premises data center. The team is evaluating Amazon CloudFront to be used as a CDN for its flagship application. The team has hired you as an AWS Certified Solutions Architect – Associate to advise on Amazon CloudFront capabilities on routing, security, and high availability. Which of the following would you identify as correct regarding Amazon CloudFront? (Select three)",
      "options": [
        "A. Use AWS Key Management Service (AWS KMS) encryption in Amazon CloudFront to protect sensitive data for specific content",
        "B. Use geo restriction to configure Amazon CloudFront for high-availability and failover",
        "C. Amazon CloudFront can route to multiple origins based on the price class",
        "D. Amazon CloudFront can route to multiple origins based on the content type",
        "E. Use an origin group with primary and secondary origins to configure Amazon CloudFront for high-availability and failover",
        "F. Use field level encryption in Amazon CloudFront to protect sensitive data for specific content"
      ],
      "correct_answer": "D",
      "explanation": "Correct options: Amazon CloudFront can route to multiple origins based on the content type You can configure a single Amazon CloudFront web distribution to serve different types of requests from multiple origins. For example, if you are building a website that serves static content from an Amazon Simple Storage Service (Amazon S3) bucket and dynamic content from a load balancer, you can serve both types of content from a Amazon CloudFront web distribution. Use an origin group with primary and secondary origins to configure Amazon CloudFront for high-availability and failover You can set up Amazon CloudFront with origin failover for scenarios that require high availability. To get started, you create an origin group with two origins: a primary and a secondary. If the primary origin is unavailable or returns specific HTTP response status codes that indicate a failure, CloudFront automatically switches to the secondary origin. To set up origin failover, you must have a distribution with at least two origins. Next, you create an origin group for your distribution that includes two origins, setting one as the primary. Finally, you create or update a cache behavior to use the origin group. via - https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html Use field level encryption in Amazon CloudFront to protect sensitive data for specific content Field-level encryption allows you to enable your users to securely upload sensitive information to your web servers. The sensitive information provided by your users is encrypted at the edge, close to the user, and remains encrypted throughout your entire application stack. This encryption ensures that only applications that need the data—and have the credentials to decrypt it—are able to do so. To use field-level encryption, when you configure your Amazon CloudFront distribution, specify the set of fields in POST requests that you want to be encrypted, and the public key to use to encrypt them. You can encrypt up to 10 data fields in a request. (You can’t encrypt all of the data in a request with field-level encryption; you must specify individual fields to encrypt.) via - https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/field-level-encryption.html Incorrect options: Use AWS Key Management Service (AWS KMS) encryption in Amazon CloudFront to protect sensitive data for specific content - This option has been added as a distractor. You can use field level encryption in Amazon CloudFront to protect sensitive data for specific content. Use geo restriction to configure Amazon CloudFront for high-availability and failover - You can use geo restriction, also known as geo blocking, to prevent users in specific geographic locations from accessing content that you're distributing through a Amazon CloudFront distribution. Geo restriction is not used to configure Amazon CloudFront for high availability and failover. Amazon CloudFront can route to multiple origins based on the price class - Amazon CloudFront edge locations are grouped into geographic regions, and AWS has grouped regions into price classes. The default price class includes all regions. Another price class includes most regions (the United States; Canada; Europe; Hong Kong, Philippines, South Korea, Taiwan, and Singapore; Japan; India; South Africa; and Middle East regions) but excludes the most expensive regions. A third price class includes only the least expensive regions (the United States, Canada, and Europe regions). CloudFront can only route to multiple origins based on content type and not on the basis of the price class. References: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/field-level-encryption.html https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PriceClass.html",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A healthcare company is evaluating storage options on Amazon S3 to meet regulatory guidelines. The data should be stored in such a way on Amazon S3 that it cannot be deleted until the regulatory time period has expired. As a solutions architect, which of the following would you recommend for the given requirement?",
      "options": [
        "A. Use Amazon S3 cross-region replication (S3 CRR)",
        "B. Use Amazon S3 Glacier Vault Lock",
        "C. Activate AWS Multi-Factor Authentication (AWS MFA) delete on the Amazon S3 bucket",
        "D. Use Amazon S3 Object Lock"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: Use Amazon S3 Object Lock Amazon S3 Object Lock is an Amazon S3 feature that allows you to store objects using a write once, read many (WORM) model. You can use WORM protection for scenarios where it is imperative that data is not changed or deleted after it has been written. Whether your business has a requirement to satisfy compliance regulations in the financial or healthcare sector, or you simply want to capture a golden copy of business records for later auditing and reconciliation, Amazon S3 Object Lock is the right tool for you. Object Lock can help prevent objects from being deleted or overwritten for a fixed amount of time or indefinitely. via - https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html via - https://aws.amazon.com/blogs/storage/protecting-data-with-amazon-s3-object-lock/ Incorrect options: Use Amazon S3 Glacier Vault Lock A vault is a container for storing archives on Glacier. When you create a vault, you specify a vault name and the AWS Region in which you want to create the vault. Since Vault Lock is only for Glacier and not for Amazon S3, so it cannot be used for the given use-case. Use Amazon S3 cross-region replication (S3 CRR) - Replication enables automatic, asynchronous copying of objects across Amazon S3 buckets. The object may be replicated to a single destination bucket or multiple destination buckets. Both source and destination buckets must have versioning enabled. By default, when Amazon S3 Replication is enabled and an object is deleted in the source bucket, Amazon S3 adds a delete marker in the source bucket only. This action protects data from malicious deletions. If you have delete marker replication enabled, these markers are copied to the destination buckets, and Amazon S3 behaves as if the object was deleted in both source and destination buckets. However, someone with administrative access to Amazon S3 can disable cross-Region replication and then delete all versions from both source as well as the destination, so this option will not be able to safeguard your data compared to Amazon S3 Object Lock. Activate AWS Multi-Factor Authentication (AWS MFA) delete on the Amazon S3 bucket - When working with Amazon S3 Versioning in Amazon S3 buckets, you can optionally add another layer of security by configuring a bucket to enable MFA (multi-factor authentication) delete. When you do this, the bucket owner must include two forms of authentication in any request to delete a version or change the versioning state of the bucket. Only the root account can enable MFA delete. MFA delete cannot be used for the given use case because it just represents an additional security layer and can be disabled by anyone having access to the root account credentials. References: https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html https://aws.amazon.com/blogs/storage/protecting-data-with-amazon-s3-object-lock/",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A company wants to grant access to an Amazon S3 bucket to users in its own AWS account as well as to users in another AWS account. Which of the following options can be used to meet this requirement?",
      "options": [
        "A. Use either a bucket policy or a user policy to grant permission to users in its account as well as to users in another account",
        "B. Use a bucket policy to grant permission to users in its account as well as to users in another account",
        "C. Use a user policy to grant permission to users in its account as well as to users in another account",
        "D. Use permissions boundary to grant permission to users in its account as well as to users in another account"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Use a bucket policy to grant permission to users in its account as well as to users in another account A bucket policy is a type of resource-based policy that can be used to grant permissions to the principal that is specified in the policy. Principals can be in the same account as the resource or in other accounts. For cross-account permissions to other AWS accounts or users in another account, you must use a bucket policy. via - https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html Incorrect options: Use either a bucket policy or a user policy to grant permission to users in its account as well as to users in another account Use a user policy to grant permission to users in its account as well as to users in another account If an AWS account that owns a bucket wants to grant permission to users in its own AWS account, it can use either a bucket policy or a user policy. The user policies are for managing permissions for users in their own AWS account and NOT for users in other AWS accounts. Therefore both these options are incorrect. Use permissions boundary to grant permission to users in its account as well as to users in another account - Use a managed policy as the permissions boundary for an IAM entity (user or role). That policy defines the maximum permissions that the identity-based policies can grant to an entity, but does not grant permissions, so this option is not correct for the given use case. via - https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html References: https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html",
      "reference": "Source: Practice Test #4 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A DevOps engineer at an IT company was recently added to the admin group of the company's AWS account. The AdministratorAccess managed policy is attached to this group. Can you identify the AWS tasks that the DevOps engineer CANNOT perform even though he has full Administrator privileges (Select two)?",
      "options": [
        "A. Delete the IAM user for his manager",
        "B. Delete an Amazon S3 bucket from the production environment",
        "C. Configure an Amazon S3 bucket to enable AWS Multi-Factor Authentication (AWS MFA) delete",
        "D. Close the company's AWS account",
        "E. Change the password for his own IAM user account"
      ],
      "correct_answer": "C",
      "explanation": "Correct options: Configure an Amazon S3 bucket to enable AWS Multi-Factor Authentication (AWS MFA) delete Close the company's AWS account An IAM user with full administrator access can perform almost all AWS tasks except a few tasks designated only for the root account user. Some of the AWS tasks that only a root account user can do are as follows: change account name or root password or root email address, change AWS support plan, close AWS account, enable AWS Multi-Factor Authentication (AWS MFA) on S3 bucket delete, create Cloudfront key pair, register for GovCloud. Even though the DevOps engineer is part of the admin group, he cannot configure an Amazon S3 bucket to enable AWS MFA delete or close the company's AWS account. Incorrect Options: Delete the IAM user for his manager Delete an Amazon S3 bucket from the production environment [@@-E The DevOps engineer is part of the admin group, so he can delete any IAM user, delete the Amazon S3 bucket, and change the password for his own IAM user account. For the complete list of AWS tasks that require AWS account root user credentials, please review this reference link: Reference: https://docs.aws.amazon.com/general/latest/gr/aws_tasks-that-require-root.html",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An online gaming company wants to block access to its application from specific countries; however, the company wants to allow its remote development team (from one of the blocked countries) to have access to the application. The application is deployed on Amazon EC2 instances running under an Application Load Balancer with AWS Web Application Firewall (AWS WAF). As a solutions architect, which of the following solutions can be combined to address the given use-case? (Select two)",
      "options": [
        "A. Create a deny rule for the blocked countries in the network access control list (network ACL) associated with each of the Amazon EC2 instances",
        "B. Use Application Load Balancer geo match statement listing the countries that you want to block",
        "C. Use Application Load Balancer IP set statement that specifies the IP addresses that you want to allow through",
        "D. Use AWS WAF geo match statement listing the countries that you want to block",
        "E. Use AWS WAF IP set statement that specifies the IP addresses that you want to allow through"
      ],
      "correct_answer": "D",
      "explanation": "Correct options: Use AWS WAF geo match statement listing the countries that you want to block Use AWS WAF IP set statement that specifies the IP addresses that you want to allow through AWS WAF is a web application firewall that helps protect your web applications or APIs against common web exploits that may affect availability, compromise security, or consume excessive resources. AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that block common attack patterns and rules that filter out specific traffic patterns you define. You can deploy AWS WAF on Amazon CloudFront as part of your CDN solution, the Application Load Balancer that fronts your web servers or origin servers running on Amazon EC2, or Amazon API Gateway for your APIs. AWS WAF - How it Works?: via - https://aws.amazon.com/waf/ To block specific countries, you can create a AWS WAF geo match statement listing the countries that you want to block, and to allow traffic from IPs of the remote development team, you can create a WAF IP set statement that specifies the IP addresses that you want to allow through. You can combine the two rules as shown below: Incorrect options: Create a deny rule for the blocked countries in the network access control list (network ACL) associated with each of the Amazon EC2 instances - A network access control list (network ACL) is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets. A network access control list (network ACL) does not have the capability to block traffic based on geographic match conditions. Use Application Load Balancer geo match statement listing the countries that you want to block Use Application Load Balancer IP set statement that specifies the IP addresses that you want to allow through An Application Load Balancer operates at the request level (layer 7), routing traffic to targets – Amazon EC2 instances, containers, IP addresses, and AWS Lambda functions based on the content of the request. Ideal for advanced load balancing of HTTP and HTTPS traffic, Application Load Balancer provides advanced request routing targeted at delivery of modern application architectures, including microservices and container-based applications. An Application Load Balancer cannot block or allow traffic based on geographic match conditions or IP based conditions. Both these options have been added as distractors. References: https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-geo-match.html https://aws.amazon.com/blogs/security/how-to-use-aws-waf-to-filter-incoming-traffic-from-embargoed-countries/",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A financial services company is moving its IT infrastructure to AWS Cloud and wants to enforce adequate data protection mechanisms on Amazon Simple Storage Service (Amazon S3) to meet compliance guidelines. The engineering team has hired you as a solutions architect to build a solution for this requirement. Can you help the team identify the INCORRECT option from the choices below?",
      "options": [
        "A. Amazon S3 can protect data at rest using Server-Side Encryption",
        "B. Amazon S3 can protect data at rest using Client-Side Encryption",
        "C. Amazon S3 can encrypt data in transit using HTTPS (TLS)",
        "D. Amazon S3 can encrypt object metadata by using Server-Side Encryption"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: Amazon S3 can encrypt object metadata by using Server-Side Encryption Amazon S3 is a simple key-value store designed to store as many objects as you want. You store these objects in one or more buckets, and each object can be up to 5 TB in size. An object consists of the following: Key – The name that you assign to an object. You use the object key to retrieve the object. Version ID – Within a bucket, a key and version ID uniquely identify an object. Value – The content that you are storing. Metadata – A set of name-value pairs with which you can store information regarding the object. Subresources – Amazon S3 uses the subresource mechanism to store object-specific additional information. Access Control Information – You can control access to the objects you store in Amazon S3. Metadata, which can be included with the object, is not encrypted while being stored on Amazon S3. Therefore, AWS recommends that customers not place sensitive information in Amazon S3 metadata. Incorrect options: Amazon S3 can protect data at rest using Server-Side Encryption - This is possible and AWS provides three different ways of doing this - Server-side encryption with Amazon S3‐managed keys (SSE-S3), Server-side encryption with customer master keys stored in AWS Key Management Service (SSE-KMS), Server-side encryption with customer-provided keys (SSE-C). Amazon S3 can protect data at rest using Client-Side Encryption - This is a possible scenario too. You can encrypt data on the client-side and upload the encrypted data to Amazon S3. In this case, the client manages the encryption process, the encryption keys, and related tools. Amazon S3 can encrypt data in transit using HTTPS (TLS) - This is also possible and you can use HTTPS (TLS) to help prevent potential attackers from eavesdropping on or manipulating network traffic using person-in-the-middle or similar attacks. References: https://docs.aws.amazon.com/AmazonS3/latest/dev/security-best-practices.html#server-side https://d1.awsstatic.com/whitepapers/aws-security-whitepaper.pdf?did=wp_card&trk=wp_card",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A streaming solutions company is building a video streaming product by using an Application Load Balancer (ALB) that routes the requests to the underlying Amazon EC2 instances. The engineering team has noticed a peculiar pattern. The Application Load Balancer removes an instance from its pool of healthy instances whenever it is detected as unhealthy but the Auto Scaling group fails to kick-in and provision the replacement instance. What could explain this anomaly?",
      "options": [
        "A. The Auto Scaling group is using Amazon EC2 based health check and the Application Load Balancer is using ALB based health check",
        "B. The Auto Scaling group is using ALB based health check and the Application Load Balancer is using Amazon EC2 based health check",
        "C. Both the Auto Scaling group and Application Load Balancer are using ALB based health check",
        "D. Both the Auto Scaling group and Application Load Balancer are using Amazon EC2 based health check"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: The Auto Scaling group is using Amazon EC2 based health check and the Application Load Balancer is using ALB based health check An Auto Scaling group contains a collection of Amazon EC2 instances that are treated as a logical grouping for automatic scaling and management. Auto Scaling Group Overview: via - https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html Application Load Balancer automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, and AWS Lambda functions. It can handle the varying load of your application traffic in a single Availability Zone or across multiple Availability Zones. If the Auto Scaling group (ASG) is using EC2 as the health check type and the Application Load Balancer (ALB) is using its in-built health check, there may be a situation where the ALB health check fails because the health check pings fail to receive a response from the instance. At the same time, ASG health check can come back as successful because it is based on EC2 based health check. Therefore, in this scenario, the ALB will remove the instance from its inventory, however, the Auto Scaling Group will fail to provide the replacement instance. This can lead to the scaling issues mentioned in the problem statement. Incorrect options: The Auto Scaling group is using ALB based health check and the Application Load Balancer is using Amazon EC2 based health check - Application Load Balancer cannot use EC2 based health checks, so this option is incorrect. Both the Auto Scaling group and Application Load Balancer are using ALB based health check - It is recommended to use ALB based health checks for both Auto Scaling group and Application Load Balancer. If both the Auto Scaling group and Application Load Balancer use ALB based health checks, then you will be able to avoid the scenario mentioned in the question. Both the Auto Scaling group and Application Load Balancer are using Amazon EC2 based health check - Application Load Balancer cannot use EC2 based health checks, so this option is incorrect. References: https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-health-checks.html https://docs.aws.amazon.com/autoscaling/ec2/userguide/health-checks-overview.html https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-elb-healthcheck.html",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A retail company maintains an AWS Direct Connect connection to AWS and has recently migrated its data warehouse to AWS. The data analysts at the company query the data warehouse using a visualization tool. The average size of a query returned by the data warehouse is 60 megabytes and the query responses returned by the data warehouse are not cached in the visualization tool. Each webpage returned by the visualization tool is approximately 600 kilobytes. Which of the following options offers the LOWEST data transfer egress cost for the company?",
      "options": [
        "A. Deploy the visualization tool in the same AWS region as the data warehouse. Access the visualization tool over the internet at a location in the same region",
        "B. Deploy the visualization tool on-premises. Query the data warehouse directly over an AWS Direct Connect connection at a location in the same AWS region",
        "C. Deploy the visualization tool in the same AWS region as the data warehouse. Access the visualization tool over a Direct Connect connection at a location in the same region",
        "D. Deploy the visualization tool on-premises. Query the data warehouse over the internet at a location in the same AWS region"
      ],
      "correct_answer": "C",
      "explanation": "Correct option: Deploy the visualization tool in the same AWS region as the data warehouse. Access the visualization tool over a Direct Connect connection at a location in the same region AWS Direct Connect is a networking service that provides an alternative to using the internet to connect to AWS. Using AWS Direct Connect, data that would have previously been transported over the internet is delivered through a private network connection between your on-premises data center and AWS. For the given use case, the main pricing parameter while using the AWS Direct Connect connection is the Data Transfer Out (DTO) from AWS to the on-premises data center. DTO refers to the cumulative network traffic that is sent through AWS Direct Connect to destinations outside of AWS. This is charged per gigabyte (GB), and unlike capacity measurements, DTO refers to the amount of data transferred, not the speed. via - https://aws.amazon.com/directconnect/pricing/ Each query response is 60 megabytes in size and each webpage for the visualization tool is 600 kilobytes in size. If you deploy the visualization tool in the same AWS region as the data warehouse, then you only need to pay for the 600 kilobytes of DTO charges for the webpage. Therefore this option is correct. However, if you deploy the visualization tool on-premises, then you need to pay for the 60 MB of DTO charges for the query response from the data warehouse to the visualization tool. Incorrect options: Deploy the visualization tool in the same AWS region as the data warehouse. Access the visualization tool over the internet at a location in the same region Deploy the visualization tool on-premises. Query the data warehouse over the internet at a location in the same AWS region Data transfer pricing over AWS Direct Connect is lower than data transfer pricing over the internet, so both of these options are incorrect. Deploy the visualization tool on-premises. Query the data warehouse directly over an AWS Direct Connect connection at a location in the same AWS region - As mentioned in the explanation above, if you deploy the visualization tool on-premises, then you need to pay for the 60 megabytes of DTO charges for the query response from the data warehouse to the visualization tool. So this option is incorrect. References: https://aws.amazon.com/directconnect/pricing/ https://aws.amazon.com/getting-started/hands-on/connect-data-center-to-aws/services-costs/ https://aws.amazon.com/directconnect/faqs/",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "The engineering team at an e-commerce company uses an AWS Lambda function to write the order data into a single DB instance Amazon Aurora cluster. The team has noticed that many order- writes to its Aurora cluster are getting missed during peak load times. The diagnostics data has revealed that the database is experiencing high CPU and memory consumption during traffic spikes. The team also wants to enhance the availability of the Aurora DB. Which of the following steps would you combine to address the given scenario? (Select two)",
      "options": [
        "A. Increase the concurrency of the AWS Lambda function so that the order-writes do not get missed during traffic spikes",
        "B. Handle all read operations for your application by connecting to the reader endpoint of the Amazon Aurora cluster so that Aurora can spread the load for read-only connections across the Aurora replica",
        "C. Create a standby Aurora instance in another Availability Zone to improve the availability as the standby can serve as a failover target",
        "D. Create a replica Aurora instance in another Availability Zone to improve the availability as the replica can serve as a failover target",
        "E. Use Amazon EC2 instances behind an Application Load Balancer to write the order data into Amazon Aurora cluster"
      ],
      "correct_answer": "B",
      "explanation": "Correct options: Handle all read operations for your application by connecting to the reader endpoint of the Amazon Aurora cluster so that Aurora can spread the load for read-only connections across the Aurora replica When you create a second, third, and so on DB instance in an Aurora-provisioned DB cluster, Aurora automatically sets up replication from the writer DB instance to all the other DB instances. These other DB instances are read-only and are known as Aurora Replicas. Aurora Replicas have two main purposes. You can issue queries to them to scale the read operations for your application. You typically do so by connecting to the reader endpoint of the cluster. That way, Aurora can spread the load for read-only connections across as many Aurora Replicas as you have in the cluster. Aurora Replicas also help to increase availability. If the writer instance in a cluster becomes unavailable, Aurora automatically promotes one of the reader instances to take its place as the new writer. via - https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html Create a replica Aurora instance in another Availability Zone to improve the availability as the replica can serve as a failover target If the primary instance in a DB cluster using single-master replication fails, Aurora automatically fails over to a new primary instance in one of two ways: By promoting an existing Aurora Replica to the new primary instance By creating a new primary instance via - https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.AuroraHighAvailability.html Incorrect options: Create a standby Aurora instance in another Availability Zone to improve the availability as the standby can serve as a failover target - There are no standby instances in Aurora. Aurora performs an automatic failover to a read replica when a problem is detected. So this option is incorrect. Read replicas, Multi-AZ deployments, and multi-region deployments: via - https://aws.amazon.com/rds/features/read-replicas/ Increase the concurrency of the AWS Lambda function so that the order-writes do not get missed during traffic spikes - Increasing the concurrency of the AWS Lambda function would not resolve the issue since the bottleneck is at the database layer, as exhibited by the high CPU and memory consumption for the Aurora instance. This option has been added as a distractor. Use Amazon EC2 instances behind an Application Load Balancer to write the order data into Amazon Aurora cluster - Using Amazon EC2 instances behind an Application Load Balancer would not resolve the issue since the bottleneck is at the database layer, as exhibited by the high CPU and memory consumption for the Aurora instance. This option has been added as a distractor. References: https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.AuroraHighAvailability.html https://aws.amazon.com/rds/features/read-replicas/",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "Which of the following is true regarding cross-zone load balancing as seen in Application Load Balancer versus Network Load Balancer?",
      "options": [
        "A. By default, cross-zone load balancing is disabled for both Application Load Balancer and Network Load Balancer",
        "B. By default, cross-zone load balancing is enabled for both Application Load Balancer and Network Load Balancer",
        "C. By default, cross-zone load balancing is enabled for Application Load Balancer and disabled for Network Load Balancer",
        "D. By default, cross-zone load balancing is disabled for Application Load Balancer and enabled for Network Load Balancer"
      ],
      "correct_answer": "C",
      "explanation": "Correct option: By default, cross-zone load balancing is enabled for Application Load Balancer and disabled for Network Load Balancer By default, cross-zone load balancing is enabled for Application Load Balancer and disabled for Network Load Balancer. When cross-zone load balancing is enabled, each load balancer node distributes traffic across the registered targets in all the enabled Availability Zones. When cross-zone load balancing is disabled, each load balancer node distributes traffic only across the registered targets in its Availability Zone. How cross-zone load balancing works: via - https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html Incorrect Options: By default, cross-zone load balancing is disabled for both Application Load Balancer and Network Load Balancer By default, cross-zone load balancing is enabled for both Application Load Balancer and Network Load Balancer By default, cross-zone load balancing is disabled for Application Load Balancer and enabled for Network Load Balancer Per the default cross-zone load balancing settings described earlier in the explanation, these three options are incorrect. Reference: https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A media company wants to get out of the business of owning and maintaining its own IT infrastructure. As part of this digital transformation, the media company wants to archive about 5 petabytes of data in its on-premises data center to durable long term storage. As a solutions architect, what is your recommendation to migrate this data in the MOST cost-optimal way?",
      "options": [
        "A. Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the AWS Snowball Edge data into Amazon S3 Glacier",
        "B. Setup AWS direct connect between the on-premises data center and AWS Cloud. Use this connection to transfer the data into Amazon S3 Glacier",
        "C. Setup AWS Site-to-Site VPN connection between the on-premises data center and AWS Cloud. Use this connection to transfer the data into Amazon S3 Glacier",
        "D. Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the AWS Snowball Edge data into Amazon S3 and create a lifecycle policy to transition the data into Amazon S3 Glacier"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the AWS Snowball Edge data into Amazon S3 and create a lifecycle policy to transition the data into Amazon S3 Glacier AWS Snowball Edge Storage Optimized is the optimal choice if you need to securely and quickly transfer dozens of terabytes to petabytes of data to AWS. It provides up to 80 TB of usable HDD storage, 40 vCPUs, 1 TB of SATA SSD storage, and up to 40 Gb network connectivity to address large scale data transfer and pre-processing use cases. The data stored on AWS Snowball Edge device can be copied into Amazon S3 bucket and later transitioned into Amazon S3 Glacier via a lifecycle policy. You can't directly copy data from AWS Snowball Edge devices into Amazon S3 Glacier. Incorrect options: Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the AWS Snowball Edge data into Amazon S3 Glacier - As mentioned earlier, you can't directly copy data from AWS Snowball Edge devices into Amazon S3 Glacier. Hence, this option is incorrect. Setup AWS direct connect between the on-premises data center and AWS Cloud. Use this connection to transfer the data into Amazon S3 Glacier - AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations. Using industry-standard 802.1q VLANs, this dedicated connection can be partitioned into multiple virtual interfaces. Direct Connect involves significant monetary investment and takes more than a month to set up, therefore it's not the correct fit for this use-case where just a one-time data transfer has to be done. Setup AWS Site-to-Site VPN connection between the on-premises data center and AWS Cloud. Use this connection to transfer the data into Amazon S3 Glacier - AWS Site-to-Site VPN enables you to securely connect your on-premises network or branch office site to your Amazon Virtual Private Cloud (Amazon VPC). VPN Connections are a good solution if you have an immediate need, and have low to modest bandwidth requirements. Because of the high data volume for the given use-case, Site-to-Site VPN is not the correct choice. Reference: https://aws.amazon.com/snowball/",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A company has noticed that its application performance has deteriorated after a new Auto Scaling group was deployed a few days back. Upon investigation, the team found out that the Launch Configuration selected for the Auto Scaling group is using the incorrect instance type that is not optimized to handle the application workflow. As a solutions architect, what would you recommend to provide a long term resolution for this issue?",
      "options": [
        "A. Modify the launch configuration to use the correct instance type and continue to use the existing Auto Scaling group",
        "B. Create a new launch configuration to use the correct instance type. Modify the Auto Scaling group to use this new launch configuration. Delete the old launch configuration as it is no longer needed",
        "C. No need to modify the launch configuration. Just modify the Auto Scaling group to use the correct instance type",
        "D. No need to modify the launch configuration. Just modify the Auto Scaling group to use more number of existing instance types. More instances may offset the loss of performance"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Create a new launch configuration to use the correct instance type. Modify the Auto Scaling group to use this new launch configuration. Delete the old launch configuration as it is no longer needed A launch configuration is an instance configuration template that an Auto Scaling group uses to launch Amazon EC2 instances. When you create a launch configuration, you specify information for the instances. Include the ID of the Amazon Machine Image (AMI), the instance type, a key pair, one or more security groups, and a block device mapping. It is not possible to modify a launch configuration once it is created. The correct option is to create a new launch configuration to use the correct instance type. Then modify the Auto Scaling group to use this new launch configuration. Lastly to clean-up, just delete the old launch configuration as it is no longer needed. Incorrect options: Modify the launch configuration to use the correct instance type and continue to use the existing Auto Scaling group - As mentioned earlier, it is not possible to modify a launch configuration once it is created. Hence, this option is incorrect. No need to modify the launch configuration. Just modify the Auto Scaling group to use the correct instance type - You cannot use an Auto Scaling group to directly modify the instance type of the underlying instances. Hence, this option is incorrect. No need to modify the launch configuration. Just modify the Auto Scaling group to use more number of existing instance types. More instances may offset the loss of performance - Using the Auto Scaling group to increase the number of instances to cover up for the performance loss is not recommended as it does not address the root cause of the problem. The Machine Learning workflow requires a certain instance type that is optimized to handle Machine Learning computations. Hence, this option is incorrect. Reference: https://docs.aws.amazon.com/autoscaling/ec2/userguide/LaunchConfiguration.html",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A leading video streaming provider is migrating to AWS Cloud infrastructure for delivering its content to users across the world. The company wants to make sure that the solution supports at least a million requests per second for its Amazon EC2 server farm. As a solutions architect, which type of Elastic Load Balancing would you recommend as part of the solution stack?",
      "options": [
        "A. Application Load Balancer",
        "B. Network Load Balancer",
        "C. Classic Load Balancer",
        "D. Infrastructure Load Balancer"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Network Load Balancer Network Load Balancer is best suited for use-cases involving low latency and high throughput workloads that involve scaling to millions of requests per second. Network Load Balancer operates at the connection level (Layer 4), routing connections to targets - Amazon EC2 instances, microservices, and containers – within Amazon Virtual Private Cloud (Amazon VPC) based on IP protocol data. Incorrect options: Application Load Balancer - Application Load Balancer operates at the request level (layer 7), routing traffic to targets – EC2 instances, containers, IP addresses, and Lambda functions based on the content of the request. Ideal for advanced load balancing of HTTP and HTTPS traffic, Application Load Balancer provides advanced request routing targeted at delivery of modern application architectures, including microservices and container-based applications. Application Load Balancer is not a good fit for the low latency and high throughput scenario mentioned in the given use-case. Classic Load Balancer - Classic Load Balancer provides basic load balancing across multiple Amazon EC2 instances and operates at both the request level and connection level. Classic Load Balancer is intended for applications that were built within the EC2-Classic network. Classic Load Balancer is not a good fit for the low latency and high throughput scenario mentioned in the given use-case. Infrastructure Load Balancer - There is no such thing as Infrastructure Load Balancer and this option just acts as a distractor. Reference: https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An IT company has built a custom data warehousing solution for a retail organization by using Amazon Redshift. As part of the cost optimizations, the company wants to move any historical data (any data older than a year) into Amazon S3, as the daily analytical reports consume data for just the last one year. However the analysts want to retain the ability to cross-reference this historical data along with the daily reports. The company wants to develop a solution with the LEAST amount of effort and MINIMUM cost. As a solutions architect, which option would you recommend to facilitate this use-case?",
      "options": [
        "A. Setup access to the historical data via Amazon Athena. The analytics team can run historical data queries on Amazon Athena and continue the daily reporting on Amazon Redshift. In case the reports need to be cross-referenced, the analytics team need to export these in flat files and then do further analysis",
        "B. Use the Amazon Redshift COPY command to load the Amazon S3 based historical data into Amazon Redshift. Once the ad-hoc queries are run for the historic data, it can be removed from Amazon Redshift",
        "C. Use Amazon Redshift Spectrum to create Amazon Redshift cluster tables pointing to the underlying historical data in Amazon S3. The analytics team can then query this historical data to cross-reference with the daily reports from Redshift",
        "D. Use AWS Glue ETL job to load the Amazon S3 based historical data into Redshift. Once the ad-hoc queries are run for the historic data, it can be removed from Amazon Redshift"
      ],
      "correct_answer": "C",
      "explanation": "Correct option: Use Amazon Redshift Spectrum to create Amazon Redshift cluster tables pointing to the underlying historical data in Amazon S3. The analytics team can then query this historical data to cross-reference with the daily reports from Redshift Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large scale data set storage and analysis. Using Amazon Redshift Spectrum, you can efficiently query and retrieve structured and semistructured data from files in Amazon S3 without having to load the data into Amazon Redshift tables. Amazon Redshift Spectrum resides on dedicated Amazon Redshift servers that are independent of your cluster. Redshift Spectrum pushes many compute-intensive tasks, such as predicate filtering and aggregation, down to the Redshift Spectrum layer. Thus, Amazon Redshift Spectrum queries use much less of your cluster's processing capacity than other queries. Redshift Spectrum Overview: via - https://aws.amazon.com/blogs/big-data/amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required/ Incorrect options: Setup access to the historical data via Amazon Athena. The analytics team can run historical data queries on Amazon Athena and continue the daily reporting on Amazon Redshift. In case the reports need to be cross-referenced, the analytics team need to export these in flat files and then do further analysis - Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to set up or manage, and customers pay only for the queries they run. You can use Athena to process logs, perform ad-hoc analysis, and run interactive queries. Providing access to historical data via Athena would mean that historical data reconciliation would become difficult as the daily report would still be produced via Redshift. Such a setup is cumbersome to maintain on a day to day basis. Hence the option to use Athena is ruled out. Use the Amazon Redshift COPY command to load the Amazon S3 based historical data into Amazon Redshift. Once the ad-hoc queries are run for the historic data, it can be removed from Amazon Redshift Use AWS Glue ETL job to load the Amazon S3 based historical data into Redshift. Once the ad-hoc queries are run for the historic data, it can be removed from Amazon Redshift Loading historical data into Amazon Redshift via COPY command or AWS Glue ETL job would cost heavy for a one-time ad-hoc process. The same result can be achieved more cost-efficiently by using Amazon Redshift Spectrum. Therefore both these options to load historical data into Redshift are also incorrect for the given use-case. References: https://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum.html#c-spectrum-overview https://aws.amazon.com/blogs/big-data/ amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required/",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "The engineering team at an online fashion retailer uses AWS Cloud to manage its technology infrastructure. The Amazon EC2 server fleet is behind an Application Load Balancer and the fleet strength is managed by an Auto Scaling group. Based on the historical data, the team is anticipating a huge traffic spike during the upcoming Thanksgiving sale. As an AWS solutions architect, what feature of the Auto Scaling group would you leverage so that the potential surge in traffic can be preemptively addressed?",
      "options": [
        "A. Auto Scaling group target tracking scaling policy",
        "B. Auto Scaling group scheduled action",
        "C. Auto Scaling group step scaling policy",
        "D. Auto Scaling group lifecycle hook"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Auto Scaling group scheduled action The engineering team can create a scheduled action for the Auto Scaling group to pre-emptively provision additional instances for the sale duration. This makes sure that adequate instances are ready before the sale goes live. The scheduled action tells Amazon EC2 Auto Scaling to perform a scaling action at specified times. To create a scheduled scaling action, you specify the start time when the scaling action should take effect, and the new minimum, maximum, and desired sizes for the scaling action. At the specified time, Amazon EC2 Auto Scaling updates the group with the values for minimum, maximum, and desired size that are specified by the scaling action. Incorrect options: Auto Scaling group target tracking scaling policy - With target tracking scaling policies, you choose a scaling metric and set a target value. Application Auto Scaling creates and manages the Amazon CloudWatch alarms that trigger the scaling policy and calculates the scaling adjustment based on the metric and the target value. Auto Scaling group step scaling policy - With step scaling, you choose scaling metrics and threshold values for the Amazon CloudWatch alarms that trigger the scaling process as well as define how your scalable target should be scaled when a threshold is in breach for a specified number of evaluation periods. Both the target tracking as well as step scaling policies entail a lag wherein the instances will be provisioned only when the underlying Amazon CloudWatch alarms go off. Therefore these two options are not pre-emptive in nature and ruled out for the given use-case. Auto Scaling group lifecycle hook - Auto Scaling group lifecycle hooks enable you to perform custom actions as the Auto Scaling group launches or terminates instances. For example, you could install or configure software on newly launched instances, or download log files from an instance before it terminates. Lifecycle hooks cannot be used to pre-emptively provision additional instances for a specific period such as the sale duration. Reference: https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A company needs a massive PostgreSQL database and the engineering team would like to retain control over managing the patches, version upgrades for the database, and consistent performance with high IOPS. The team wants to install the database on an Amazon EC2 instance with the optimal storage type on the attached Amazon EBS volume. As a solutions architect, which of the following configurations would you suggest to the engineering team?",
      "options": [
        "A. Amazon EC2 with Amazon EBS volume of General Purpose SSD (gp2) type",
        "B. Amazon EC2 with Amazon EBS volume of Provisioned IOPS SSD (io1) type",
        "C. Amazon EC2 with Amazon EBS volume of Throughput Optimized HDD (st1) type",
        "D. Amazon EC2 with Amazon EBS volume of cold HDD (sc1) type"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Amazon EC2 with Amazon EBS volume of Provisioned IOPS SSD (io1) type Amazon EBS provides the following volume types, which differ in performance characteristics and price so that you can tailor your storage performance and cost to the needs of your applications. The volumes types fall into two categories: SSD-backed volumes optimized for transactional workloads involving frequent read/write operations with small I/O size, where the dominant performance attribute is IOPS HDD-backed volumes optimized for large streaming workloads where throughput (measured in MiB/s) is a better performance measure than IOPS Provision IOPS type supports critical business applications that require sustained IOPS performance, or more than 16,000 IOPS or 250 MiB/s of throughput per volume. Examples are large database workloads, such as: MongoDB Cassandra Microsoft SQL Server MySQL PostgreSQL Oracle Therefore, Amazon EC2 with Amazon EBS volume of Provisioned IOPS SSD (io1) type is the right fit for the given use-case. Please see this detailed overview of the volume types for Amazon EBS volumes. via - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html Incorrect options: Amazon EC2 with Amazon EBS volume of General Purpose SSD (gp2) type Amazon EC2 with Amazon EBS volume of Throughput Optimized HDD (st1) type Amazon EC2 with Amazon EBS volume of cold HDD (sc1) type Per the explanation in the detailed overview provided above, these three options are incorrect. Reference: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A cyber security company is running a mission critical application using a single Spread placement group of Amazon EC2 instances. The company needs 15 Amazon EC2 instances for optimal performance. How many Availability Zones (AZs) will the company need to deploy these Amazon EC2 instances per the given use-case?",
      "options": [
        "A. 3",
        "B. 7",
        "C. 14",
        "D. 15"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: 3 When you launch a new Amazon EC2 instance, the EC2 service attempts to place the instance in such a way that all of your instances are spread out across underlying hardware to minimize correlated failures. You can use placement groups to influence the placement of a group of interdependent instances to meet the needs of your workload. Depending on the type of workload, you can create a placement group using one of the following placement strategies: Cluster placement group Partition placement group Spread placement group. A Spread placement group is a group of instances that are each placed on distinct racks, with each rack having its own network and power source. Spread placement groups are recommended for applications that have a small number of critical instances that should be kept separate from each other. Launching instances in a spread placement group reduces the risk of simultaneous failures that might occur when instances share the same racks. A spread placement group can span multiple Availability Zones in the same Region. You can have a maximum of seven running instances per Availability Zone per group. Therefore, to deploy 15 Amazon EC2 instances in a single Spread placement group, the company needs to use 3 Availability Zones. Spread placement group overview: via - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html Incorrect options: 7 14 15 These three options contradict the details provided in the explanation above, so these options are incorrect. Reference: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "The data engineering team at an e-commerce company has set up a workflow to ingest the clickstream data into the raw zone of the Amazon S3 data lake. The team wants to run some SQL based data sanity checks on the raw zone of the data lake. What AWS services would you recommend for this use-case such that the solution is cost-effective and easy to maintain?",
      "options": [
        "A. Load the incremental raw zone data into Amazon Redshift on an hourly basis and run the SQL based sanity checks",
        "B. Load the incremental raw zone data into Amazon RDS on an hourly basis and run the SQL based sanity checks",
        "C. Load the incremental raw zone data into an Amazon EMR based Spark Cluster on an hourly basis and use SparkSQL to run the SQL based sanity checks",
        "D. Use Amazon Athena to run SQL based analytics against Amazon S3 data"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: Use Amazon Athena to run SQL based analytics against Amazon S3 data Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon S3 using standard SQL. Amazon Athena is serverless, so there is no infrastructure to set up or manage, and customers pay only for the queries they run. You can use Athena to process logs, perform ad-hoc analysis, and run interactive queries. Amazon Athena Benefits: via - https://aws.amazon.com/athena/ Incorrect options: Load the incremental raw zone data into Amazon Redshift on an hourly basis and run the SQL based sanity checks - Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large scale data set storage and analysis. As the development team would have to maintain and monitor the Amazon Redshift cluster size and would require significant development time to set up the processes to consume the data periodically, so this option is ruled out. Load the incremental raw zone data into an Amazon EMR based Spark Cluster on an hourly basis and use SparkSQL to run the SQL based sanity checks - Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. Amazon EMR uses Hadoop, an open-source framework, to distribute your data and processing across a resizable cluster of Amazon EC2 instances. Using an Amazon EMR cluster would imply managing the underlying infrastructure so it’s ruled out because the correct solution for the given use-case should require the least amount of development effort and ongoing maintenance. Load the incremental raw zone data into Amazon RDS on an hourly basis and run the SQL based sanity checks - Loading the incremental data into Amazon RDS implies data migration jobs will have to be written via a AWS Lambda function or an Amazon EC2 based process. This goes against the requirement that the solution should involve the least amount of development effort and ongoing maintenance. Hence this option is not correct. Reference: https://aws.amazon.com/athena/",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A global media company uses a fleet of Amazon EC2 instances (behind an Application Load Balancer) to power its video streaming application. To improve the performance of the application, the engineering team has also created an Amazon CloudFront distribution with the Application Load Balancer as the custom origin. The security team at the company has noticed a spike in the number and types of SQL injection and cross-site scripting attack vectors on the application. As a solutions architect, which of the following solutions would you recommend as the MOST effective in countering these malicious attacks?",
      "options": [
        "A. Use Amazon Route 53 with Amazon CloudFront distribution",
        "B. Use AWS Firewall Manager with CloudFront distribution",
        "C. Use AWS Security Hub with Amazon CloudFront distribution",
        "D. Use AWS Web Application Firewall (AWS WAF) with Amazon CloudFront distribution"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: Use AWS Web Application Firewall (AWS WAF) with Amazon CloudFront distribution AWS WAF is a web application firewall that helps protect your web applications or APIs against common web exploits that may affect availability, compromise security, or consume excessive resources. AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that block common attack patterns, such as SQL injection or cross-site scripting, and rules that filter out specific traffic patterns you define. How AWS WAF Works: via - https://aws.amazon.com/waf/ A web access control list (web ACL) gives you fine-grained control over the web requests that your Amazon CloudFront distribution, Amazon API Gateway API, or Application Load Balancer responds to. When you create a web ACL, you can specify one or more Amazon CloudFront distributions that you want AWS WAF to inspect. AWS WAF starts to allow, block, or count web requests for those distributions based on the conditions that you identify in the web ACL. Therefore, combining AWS WAF with Amazon CloudFront can prevent SQL injection and cross-site scripting attacks. So this is the correct option. Incorrect options: Use Amazon Route 53 with Amazon CloudFront distribution - Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. You cannot use Route 53 to prevent SQL injection and cross-site scripting attacks. So this option is incorrect. Use AWS Security Hub with Amazon CloudFront distribution - AWS Security Hub gives you a comprehensive view of your high-priority security alerts and security posture across your AWS accounts. With Security Hub, you have a single place that aggregates, organizes, and prioritizes your security alerts, or findings, from multiple AWS services, such as Amazon GuardDuty, Amazon Inspector, Amazon Macie, AWS Identity and Access Management (IAM) Access Analyzer, and AWS Firewall Manager, as well as from AWS Partner solutions. You cannot use Security Hub to prevent SQL injection and cross-site scripting attacks. So this option is incorrect. Use AWS Firewall Manager with CloudFront distribution - AWS Firewall Manager is a security management service that allows you to centrally configure and manage firewall rules across your accounts and applications in AWS Organization. You cannot use AWS Firewall Manager to prevent SQL injection and cross-site scripting attacks. So this option is incorrect. References: https://aws.amazon.com/waf/features/ https://docs.aws.amazon.com/waf/latest/developerguide/web-acl.html https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "The infrastructure team at a company maintains 5 different VPCs (let's call these VPCs A, B, C, D, E) for resource isolation. Due to the changed organizational structure, the team wants to interconnect all VPCs together. To facilitate this, the team has set up VPC peering connection between VPC A and all other VPCs in a hub and spoke model with VPC A at the center. However, the team has still failed to establish connectivity between all VPCs. As a solutions architect, which of the following would you recommend as the MOST resource-efficient and scalable solution?",
      "options": [
        "A. Establish VPC peering connections between all VPCs",
        "B. Use AWS transit gateway to interconnect the VPCs",
        "C. Use an internet gateway to interconnect the VPCs",
        "D. Use a VPC endpoint to interconnect the VPCs"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Use AWS transit gateway to interconnect the VPCs An AWS transit gateway is a network transit hub that you can use to interconnect your virtual private clouds (VPC) and on-premises networks. AWS Transit Gateway Overview: via - https://docs.aws.amazon.com/vpc/latest/tgw/what-is-transit-gateway.html A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses. Transitive Peering does not work for VPC peering connections. So, if you have a VPC peering connection between VPC A and VPC B (pcx-aaaabbbb), and between VPC A and VPC C (pcx-aaaacccc). Then, there is no VPC peering connection between VPC B and VPC C. Instead of using VPC peering, you can use an AWS Transit Gateway that acts as a network transit hub, to interconnect your VPCs or connect your VPCs with on-premises networks. Therefore this is the correct option. VPC Peering Connections Overview: via - https://docs.aws.amazon.com/vpc/latest/peering/vpc-peering-basics.html Incorrect options: Use an internet gateway to interconnect the VPCs - An internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between instances in your VPC and the internet. It, therefore, imposes no availability risks or bandwidth constraints on your network traffic. You cannot use an internet gateway to interconnect your VPCs and on-premises networks, hence this option is incorrect. Use a VPC endpoint to interconnect the VPCs - A VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by AWS PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. You cannot use a VPC endpoint to interconnect your VPCs and on-premises networks, hence this option is incorrect. Establish VPC peering connections between all VPCs - Establishing VPC peering between all VPCs is an inelegant and clumsy way to establish connectivity between all VPCs. Instead, you should use a Transit Gateway that acts as a network transit hub to interconnect your VPCs and on-premises networks. References: https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html https://docs.aws.amazon.com/vpc/latest/tgw/what-is-transit-gateway.html",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A company wants to publish an event into an Amazon Simple Queue Service (Amazon SQS) queue whenever a new object is uploaded on Amazon S3. Which of the following statements are true regarding this functionality?",
      "options": [
        "A. Both Standard Amazon SQS queue and FIFO SQS queue are allowed as an Amazon S3 event notification destination",
        "B. Neither Standard Amazon SQS queue nor FIFO SQS queue are allowed as an Amazon S3 event notification destination",
        "C. Only Standard Amazon SQS queue is allowed as an Amazon S3 event notification destination, whereas FIFO SQS queue is not allowed",
        "D. Only FIFO Amazon SQS queue is allowed as an Amazon S3 event notification destination, whereas Standard SQS queue is not allowed"
      ],
      "correct_answer": "C",
      "explanation": "Correct option: Only Standard Amazon SQS queue is allowed as an Amazon S3 event notification destination, whereas FIFO SQS queue is not allowed The Amazon S3 notification feature enables you to receive notifications when certain events happen in your bucket. To enable notifications, you must first add a notification configuration that identifies the events you want Amazon S3 to publish and the destinations where you want Amazon S3 to send the notifications. Amazon S3 supports the following destinations where it can publish events: Amazon Simple Notification Service (Amazon SNS) topic Amazon Simple Queue Service (Amazon SQS) queue AWS Lambda Currently, the Standard Amazon SQS queue is only allowed as an Amazon S3 event notification destination, whereas the FIFO SQS queue is not allowed. Incorrect options: Both Standard Amazon SQS queue and FIFO SQS queue are allowed as an Amazon S3 event notification destination Neither Standard Amazon SQS queue nor FIFO SQS queue are allowed as an Amazon S3 event notification destination Only FIFO Amazon SQS queue is allowed as an Amazon S3 event notification destination, whereas Standard SQS queue is not allowed These three options contradict the details provided in the explanation above. To summarize, the Standard Amazon SQS queue is only allowed as an Amazon S3 event notification destination, whereas the FIFO SQS queue is not allowed. Hence these three options are incorrect. Reference: https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A pharma company is working on developing a vaccine for the COVID-19 virus. The researchers at the company want to process the reference healthcare data in a highly available as well as HIPAA compliant in-memory database that supports caching results of SQL queries. As a solutions architect, which of the following AWS services would you recommend for this task?",
      "options": [
        "A. Amazon DynamoDB",
        "B. Amazon ElastiCache for Redis/Memcached",
        "C. Amazon DynamoDB Accelerator (DAX)",
        "D. Amazon DocumentDB"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Amazon ElastiCache for Redis/Memcached Amazon ElastiCache Overview: via - https://aws.amazon.com/elasticache/redis-vs-memcached/ Amazon ElastiCache for Redis is a blazing fast in-memory data store that provides sub-millisecond latency to power internet-scale real-time applications. Amazon ElastiCache for Redis is a great choice for real-time transactional and analytical processing use cases such as caching, chat/messaging, gaming leaderboards, geospatial, machine learning, media streaming, queues, real-time analytics, and session store. ElastiCache for Redis supports replication, high availability, and cluster sharding right out of the box. Amazon ElastiCache for Memcached is a Memcached-compatible in-memory key-value store service that can be used as a cache or a data store. Amazon ElastiCache for Memcached is a great choice for implementing an in-memory cache to decrease access latency, increase throughput, and ease the load off your relational or NoSQL database. Session stores are easy to create with Amazon ElastiCache for Memcached. Both Amazon ElastiCache for Redis and Amazon ElastiCache for Memcached are HIPAA Eligible. Therefore, this is the correct option. Exam Alert: Please review this comparison sheet for Redis vs Memcached features: via - https://aws.amazon.com/elasticache/redis-vs-memcached/ Incorrect Options: Amazon DynamoDB Accelerator (DAX) - Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications. DAX is a DynamoDB-compatible caching service that enables you to benefit from fast in-memory performance for demanding applications. DAX does not support SQL query caching. Amazon DynamoDB - Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-region, multi-master, durable database with built-in security, backup and restore, and in-memory caching (via DAX) for internet-scale applications. Amazon DynamoDB is not an in-memory database, so this option is incorrect. Amazon DocumentDB - Amazon DocumentDB is a fast, scalable, highly available, and fully managed document database service that supports MongoDB workloads. As a document database, Amazon DocumentDB makes it easy to store, query, and index JSON data. Amazon DocumentDB is not an in-memory database, so this option is incorrect. References: https://aws.amazon.com/about-aws/whats-new/2017/11/amazon-elasticache-for-redis-is-now-hipaa-eligible-to-help-you-power-secure-healthcare-applications-with-sub-millisecond-latency/ https://aws.amazon.com/elasticache/redis/ https://aws.amazon.com/about-aws/whats-new/2022/08/amazon-elasticache-memcached-hipaa-eligible/ https://aws.amazon.com/blogs/database/automating-sql-caching-for-amazon-elasticache-and-amazon-rds/",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A financial services firm uses a high-frequency trading system and wants to write the log files into Amazon S3. The system will also read these log files in parallel on a near real-time basis. The engineering team wants to address any data discrepancies that might arise when the trading system overwrites an existing log file and then tries to read that specific log file. Which of the following options BEST describes the capabilities of Amazon S3 relevant to this scenario?",
      "options": [
        "A. A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the previous data",
        "B. A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 does not return any data",
        "C. A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the new data",
        "D. A process replaces an existing object and immediately tries to read it. Amazon S3 always returns the latest version of the object"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: A process replaces an existing object and immediately tries to read it. Amazon S3 always returns the latest version of the object Amazon S3 delivers strong read-after-write consistency automatically, without changes to performance or availability, without sacrificing regional isolation for applications, and at no additional cost. After a successful write of a new object or an overwrite of an existing object, any subsequent read request immediately receives the latest version of the object. Amazon S3 also provides strong consistency for list operations, so after a write, you can immediately perform a listing of the objects in a bucket with any changes reflected. Strong read-after-write consistency helps when you need to immediately read an object after a write. For example, strong read-after-write consistency when you often read and list immediately after writing objects. To summarize, all Amazon S3 GET, PUT, and LIST operations, as well as operations that change object tags, ACLs, or metadata, are strongly consistent. What you write is what you will read, and the results of a LIST will be an accurate reflection of what’s in the bucket. Incorrect options: A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the previous data A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 does not return any data A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the new data These three options contradict the earlier details provided in the explanation. References: https://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html#ConsistencyModel https://aws.amazon.com/s3/faqs/",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A junior developer is learning to build websites using HTML, CSS, and JavaScript. He has created a static website and then deployed it on Amazon S3. Now he can't seem to figure out the endpoint for his super cool website. As a solutions architect, can you help him figure out the allowed formats for the Amazon S3 website endpoints? (Select two)",
      "options": [
        "A. http://s3-website-Region.bucket-name.amazonaws.com",
        "B. http://s3-website.Region.bucket-name.amazonaws.com",
        "C. http://bucket-name.s3-website.Region.amazonaws.com",
        "D. http://bucket-name.Region.s3-website.amazonaws.com",
        "E. http://bucket-name.s3-website-Region.amazonaws.com"
      ],
      "correct_answer": "C",
      "explanation": "Correct options: http://bucket-name.s3-website.Region.amazonaws.com http://bucket-name.s3-website-Region.amazonaws.com To host a static website on Amazon S3, you configure an Amazon S3 bucket for website hosting and then upload your website content to the bucket. When you configure a bucket as a static website, you enable static website hosting, set permissions, and add an index document. Depending on your website requirements, you can also configure other options, including redirects, web traffic logging, and custom error documents. When you configure your bucket as a static website, the website is available at the AWS Region-specific website endpoint of the bucket. Depending on your Region, your Amazon S3 website endpoints follow one of these two formats. s3-website dash (-) Region ‐ http://bucket-name.s3-website.Region.amazonaws.com s3-website dot (.) Region ‐ http://bucket-name.s3-website-Region.amazonaws.com These URLs return the default index document that you configure for the website. Incorrect options: http://s3-website-Region.bucket-name.amazonaws.com http://s3-website.Region.bucket-name.amazonaws.com http://bucket-name.Region.s3-website.amazonaws.com These three options do not meet the specifications for the Amazon S3 website endpoints format, so these are incorrect. Reference: https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteEndpoints.html",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A medium-sized business has a taxi dispatch application deployed on an Amazon EC2 instance. Because of an unknown bug, the application causes the instance to freeze regularly. Then, the instance has to be manually restarted via the AWS management console. Which of the following is the MOST cost-optimal and resource-efficient way to implement an automated solution until a permanent fix is delivered by the development team?",
      "options": [
        "A. Setup an Amazon CloudWatch alarm to monitor the health status of the instance. In case of an Instance Health Check failure, Amazon CloudWatch Alarm can publish to an Amazon Simple Notification Service (Amazon SNS) event which can then trigger an AWS lambda function. The AWS lambda function can use Amazon EC2 API to reboot the instance",
        "B. Use Amazon EventBridge events to trigger an AWS Lambda function to check the instance status every 5 minutes. In the case of Instance Health Check failure, the AWS lambda function can use Amazon EC2 API to reboot the instance",
        "C. Setup an Amazon CloudWatch alarm to monitor the health status of the instance. In case of an Instance Health Check failure, an EC2 Reboot CloudWatch Alarm Action can be used to reboot the instance",
        "D. Use Amazon EventBridge events to trigger an AWS Lambda function to reboot the instance status every 5 minutes"
      ],
      "correct_answer": "C",
      "explanation": "Correct option: Setup an Amazon CloudWatch alarm to monitor the health status of the instance. In case of an Instance Health Check failure, an EC2 Reboot CloudWatch Alarm Action can be used to reboot the instance Using Amazon CloudWatch alarm actions, you can create alarms that automatically stop, terminate, reboot, or recover your Amazon EC2 instances. You can use the stop or terminate actions to help you save money when you no longer need an instance to be running. You can use the reboot and recover actions to automatically reboot those instances or recover them onto new hardware if a system impairment occurs. You can create an Amazon CloudWatch alarm that monitors an Amazon EC2 instance and automatically reboots the instance. The reboot alarm action is recommended for Instance Health Check failures (as opposed to the recover alarm action, which is suited for System Health Check failures). Incorrect options: Setup an Amazon CloudWatch alarm to monitor the health status of the instance. In case of an Instance Health Check failure, Amazon CloudWatch Alarm can publish to an Amazon Simple Notification Service (Amazon SNS) event which can then trigger an AWS lambda function. The AWS lambda function can use Amazon EC2 API to reboot the instance Use Amazon EventBridge events to trigger an AWS Lambda function to check the instance status every 5 minutes. In the case of Instance Health Check failure, the AWS lambda function can use Amazon EC2 API to reboot the instance Use Amazon EventBridge events to trigger an AWS Lambda function to reboot the instance status every 5 minutes Using Amazon EventBridge event or Amazon CloudWatch alarm to trigger an AWS lambda function, directly or indirectly, is wasteful of resources. You should just use the EC2 Reboot CloudWatch Alarm Action to reboot the instance. So all the options that trigger the AWS lambda function are incorrect. Reference: https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/UsingAlarmActions.html",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A DevOps engineer at an organization is debugging issues related to an Amazon EC2 instance. The engineer has SSH'ed into the instance and he needs to retrieve the instance public IP from within a shell script running on the instance command line. Can you identify the correct URL path to get the instance public IP?",
      "options": [
        "A. http://169.254.169.254/latest/user-data/public-ipv4",
        "B. http://169.254.169.254/latest/meta-data/public-ipv4",
        "C. http://254.169.254.169/latest/meta-data/public-ipv4",
        "D. http://254.169.254.169/latest/user-data/public-ipv4"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: http://169.254.169.254/latest/meta-data/public-ipv4 Instance metadata is the data about your instance that you can use to configure or manage the running instance. Instance user data is the data that you specified in the form of a configuration script while launching your instance. The following URL paths can be used to get the instance meta data and user data from within the instance: http://169.254.169.254/latest/meta-data/ http://169.254.169.254/latest/user-data/ Further, you can get the instance public IP via the URL - http://169.254.169.254/latest/meta-data/public-ipv4 Incorrect options: http://169.254.169.254/latest/user-data/public-ipv4 http://254.169.254.169/latest/meta-data/public-ipv4 http://254.169.254.169/latest/user-data/public-ipv4 These three options do not meet the specification for the URL path to get the instance public IP, so these are incorrect. Reference: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-add-user-data.html",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "Computer vision researchers at a university are trying to optimize the I/O bound processes for a proprietary algorithm running on Amazon EC2 instances. The ideal storage would facilitate high-performance IOPS when doing file processing in a temporary storage space before uploading the results back into Amazon S3. As a solutions architect, which of the following AWS storage options would you recommend as the MOST performant as well as cost-optimal?",
      "options": [
        "A. Use Amazon EC2 instances with Instance Store as the storage option",
        "B. Use Amazon EC2 instances with Amazon EBS General Purpose SSD (gp2) as the storage option",
        "C. Use Amazon EC2 instances with Amazon EBS Provisioned IOPS SSD (io1) as the storage option",
        "D. Use Amazon EC2 instances with Amazon EBS Throughput Optimized HDD (st1) as the storage option"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Use Amazon EC2 instances with Instance Store as the storage option An instance store provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host computer. Instance store is ideal for the temporary storage of information that changes frequently, such as buffers, caches, scratch data, and other temporary content, or for data that is replicated across a fleet of instances, such as a load-balanced pool of web servers. Some instance types use NVMe or SATA-based solid-state drives (SSD) to deliver high random I/O performance. This is a good option when you need storage with very low latency, but you don't need the data to persist when the instance terminates or you can take advantage of fault-tolerant architectures. As Instance Store delivers high random I/O performance, it can act as a temporary storage space, and these volumes are included as part of the instance's usage cost, therefore this is the correct option. Amazon EC2 Instance Store: via - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html Incorrect options: Use Amazon EC2 instances with Amazon EBS General Purpose SSD (gp2) as the storage option - General Purpose SSD (gp2) volumes offer cost-effective storage that is ideal for a broad range of workloads. These volumes deliver single-digit millisecond latencies and the ability to burst to 3,000 IOPS for extended periods. Between a minimum of 100 IOPS (at 33.33 GiB and below) and a maximum of 16,000 IOPS (at 5,334 GiB and above), baseline performance scales linearly at 3 IOPS per GiB of volume size. AWS designs gp2 volumes to deliver its provisioned performance 99% of the time. A gp2 volume can range in size from 1 GiB to 16 TiB. Amazon EBS gp2 is persistent storage and costlier than Instance Stores (the cost of the storage volume is in addition to that of the Amazon EC2 instance), therefore this option is not correct. Use Amazon EC2 instances with Amazon EBS Provisioned IOPS SSD (io1) as the storage option - Provisioned IOPS SSD (io1) volumes are designed to meet the needs of I/O-intensive workloads, particularly database workloads, that are sensitive to storage performance and consistency. Unlike gp2, which uses a bucket and credit model to calculate performance, an io1 volume allows you to specify a consistent IOPS rate when you create the volume, and Amazon EBS delivers the provisioned performance 99.9 percent of the time. Amazon EBS io1 is persistent storage and costlier than Instance Stores (the cost of the storage volume is in addition to that of the Amazon EC2 instance), therefore this option is not correct. Use Amazon EC2 instances with Amazon EBS Throughput Optimized HDD (st1) as the storage option - Throughput Optimized HDD (st1) are low-cost HDD volumes designed for frequently accessed, throughput-intensive workloads such as Big data and Data warehouses. Amazon EBS st1 is persistent storage and costlier than Instance Stores (the cost of the storage volume is in addition to that of the Amazon EC2 instance), therefore this option is not correct. References: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AmazonEBS.html https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "Reporters at a news agency upload/download video files (about 500 megabytes each) to/from an Amazon S3 bucket as part of their daily work. As the agency has started offices in remote locations, it has resulted in poor latency for uploading and accessing data to/from the given Amazon S3 bucket. The agency wants to continue using a serverless storage solution such as Amazon S3 but wants to improve the performance. As a solutions architect, which of the following solutions do you propose to address this issue? (Select two)",
      "options": [
        "A. Create new Amazon S3 buckets in every region where the agency has a remote office, so that each office can maintain its storage for the media assets",
        "B. Move Amazon S3 data into Amazon Elastic File System (Amazon EFS) created in a US region, connect to Amazon EFS file system from Amazon EC2 instances in other AWS regions using an inter-region VPC peering connection",
        "C. Use Amazon CloudFront distribution with origin as the Amazon S3 bucket. This would speed up uploads as well as downloads for the video files",
        "D. Enable Amazon S3 Transfer Acceleration (Amazon S3TA) for the Amazon S3 bucket. This would speed up uploads as well as downloads for the video files",
        "E. Spin up Amazon EC2 instances in each region where the agency has a remote office. Create a daily job to transfer Amazon S3 data into Amazon EBS volumes attached to the Amazon EC2 instances"
      ],
      "correct_answer": "C",
      "explanation": "Correct options: Use Amazon CloudFront distribution with origin as the Amazon S3 bucket. This would speed up uploads as well as downloads for the video files Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, within a developer-friendly environment. When an object from Amazon S3 that is set up with Amazon CloudFront CDN is requested, the request would come through the Edge Location transfer paths only for the first request. Thereafter, it would be served from the nearest edge location to the users until it expires. So in this way, you can speed up uploads as well as downloads for the video files. Following is a good reference blog for a deep-dive: https://aws.amazon.com/blogs/aws/amazon-cloudfront-content-uploads-post-put-other-methods/ Enable Amazon S3 Transfer Acceleration (Amazon S3TA) for the Amazon S3 bucket. This would speed up uploads as well as downloads for the video files Amazon S3 Transfer Acceleration (Amazon S3TA) can speed up content transfers to and from Amazon S3 by as much as 50-500% for long-distance transfer of larger objects. Transfer Acceleration takes advantage of Amazon CloudFront’s globally distributed edge locations. As the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path. So this option is also correct. Amazon S3TA: via - https://aws.amazon.com/s3/transfer-acceleration/ Incorrect options: Create new Amazon S3 buckets in every region where the agency has a remote office, so that each office can maintain its storage for the media assets - Creating new Amazon S3 buckets in every region is not an option, since the agency maintains centralized storage. Hence this option is incorrect. Move Amazon S3 data into Amazon Elastic File System (Amazon EFS) created in a US region, connect to Amazon EFS file system from Amazon EC2 instances in other AWS regions using an inter-region VPC peering connection Spin up Amazon EC2 instances in each region where the agency has a remote office. Create a daily job to transfer Amazon S3 data into Amazon EBS volumes attached to the Amazon EC2 instances Both these options using Amazon EC2 instances are not correct for the given use-case, as the agency wants a serverless storage solution. References: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/DownloadDistS3AndCustomOrigins.html https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html https://aws.amazon.com/s3/transfer-acceleration/ https://aws.amazon.com/blogs/aws/amazon-cloudfront-content-uploads-post-put-other-methods/",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A company wants to store business-critical data on Amazon Elastic Block Store (Amazon EBS) volumes which provide persistent storage independent of Amazon EC2 instances. During a test run, the development team found that on terminating an Amazon EC2 instance, the attached Amazon EBS volume was also lost, which was contrary to their assumptions. As a solutions architect, could you explain this issue?",
      "options": [
        "A. The Amazon EBS volumes were not backed up on Amazon S3 storage, resulting in the loss of volume",
        "B. The Amazon EBS volumes were not backed up on Amazon EFS file system storage, resulting in the loss of volume",
        "C. On termination of an Amazon EC2 instance, all the attached Amazon EBS volumes are always terminated",
        "D. The Amazon EBS volume was configured as the root volume of Amazon EC2 instance. On termination of the instance, the default behavior is to also terminate the attached root volume"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: The Amazon EBS volume was configured as the root volume of Amazon EC2 instance. On termination of the instance, the default behavior is to also terminate the attached root volume Amazon Elastic Block Store (EBS) is an easy to use, high-performance block storage service designed for use with Amazon Elastic Compute Cloud (EC2) for both throughput and transaction-intensive workloads at any scale. When you launch an instance, the root device volume contains the image used to boot the instance. You can choose between AMIs backed by Amazon EC2 instance store and AMIs backed by Amazon EBS. By default, the root volume for an AMI backed by Amazon EBS is deleted when the instance terminates. You can change the default behavior to ensure that the volume persists after the instance terminates. Non-root EBS volumes remain available even after you terminate an instance to which the volumes were attached. Therefore, this option is correct. Incorrect options: The Amazon EBS volumes were not backed up on Amazon S3 storage, resulting in the loss of volume The Amazon EBS volumes were not backed up on Amazon EFS file system storage, resulting in the loss of volume Amazon EBS volumes do not need to back up the data on Amazon S3 or Amazon EFS filesystem. Both these options are added as distractors. On termination of an Amazon EC2 instance, all the attached Amazon EBS volumes are always terminated - As mentioned earlier, non-root Amazon EBS volumes remain available even after you terminate an instance to which the volumes were attached. Hence this option is incorrect. Reference: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/RootDeviceStorage.html",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A financial services company has to retain the activity logs for each of their customers to meet compliance guidelines. Depending on the business line, the company wants to retain the logs for 5-10 years in highly available and durable storage on AWS. The overall data size is expected to be in Petabytes. In case of an audit, the data would need to be accessible within a timeframe of up to 48 hours. Which AWS storage option is the MOST cost-effective for the given compliance requirements?",
      "options": [
        "A. Amazon S3 Glacier Deep Archive",
        "B. Amazon S3 Glacier",
        "C. Third party tape storage",
        "D. Amazon S3 Standard storage"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Amazon S3 Glacier Deep Archive Amazon S3 Glacier and Amazon S3 Glacier Deep Archive are secure, durable, and extremely low-cost Amazon S3 cloud storage classes for data archiving and long-term backup. They are designed to deliver 99.999999999% durability, and provide comprehensive security and compliance capabilities that can help meet even the most stringent regulatory requirements. Amazon S3 Glacier Deep Archive is a new Amazon S3 storage class that provides secure and durable object storage for long-term retention of data that is accessed once or twice in a year. From just $0.00099 per GB-month (less than one-tenth of one cent, or about $1 per TB-month), Amazon S3 Glacier Deep Archive offers the lowest cost storage in the cloud, at prices significantly lower than storing and maintaining data in on-premises magnetic tape libraries or archiving data off-site. Amazon S3 Glacier Deep Archive is up to 75% less expensive than Amazon S3 Glacier and provides retrieval within 12 hours using the Standard retrieval speed. You may also reduce retrieval costs by selecting Bulk retrieval, which will return data within 48 hours. Therefore, Amazon S3 Glacier Deep Archive is the correct choice. Amazon S3 Glacier vs Amazon S3 Glacier Deep Archive: via - https://aws.amazon.com/s3/faqs/ Incorrect options: Amazon S3 Glacier - As mentioned earlier, Amazon S3 Glacier Deep Archive is up to 75% less expensive than Amazon S3 Glacier and provides retrieval within 12 hours. So using Amazon S3 Glacier is not the correct choice. Third party tape storage Amazon S3 Standard storage Given the relaxed retrieval times, Amazon S3 standard storage would be much costlier than the Amazon S3 Glacier Deep Archive, so Amazon S3 standard storage is not the correct option. Using Third-party tape storage is ruled out as the company wants to use an AWS storage service. Therefore, both of these options are incorrect. Reference: https://aws.amazon.com/s3/faqs/",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A company wants to ensure high availability for its Amazon RDS database. The development team wants to opt for Multi-AZ deployment and they would like to understand what happens when the primary instance of the Multi-AZ configuration goes down. As a Solutions Architect, which of the following will you identify as the outcome of the scenario?",
      "options": [
        "A. An email will be sent to the System Administrator asking for manual intervention",
        "B. The CNAME record will be updated to point to the standby database",
        "C. The URL to access the database will change to the standby database",
        "D. The application will be down until the primary database has recovered itself"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: The CNAME record will be updated to point to the standby database Amazon RDS provides high availability and failover support for DB instances using Multi-AZ deployments. Amazon RDS uses several different technologies to provide failover support. Multi-AZ deployments for MariaDB, MySQL, Oracle, and PostgreSQL DB instances use Amazon's failover technology. SQL Server DB instances use SQL Server Database Mirroring (DBM) or Always On Availability Groups (AGs). In a Multi-AZ deployment, Amazon RDS automatically provisions and maintains a synchronous standby replica in a different Availability Zone. The primary DB instance is synchronously replicated across Availability Zones to a standby replica to provide data redundancy, eliminate I/O freezes, and minimize latency spikes during system backups. Running a DB instance with high availability can enhance availability during planned system maintenance, and help protect your databases against DB instance failure and Availability Zone disruption. Failover is automatically handled by Amazon RDS so that you can resume database operations as quickly as possible without administrative intervention. When failing over, Amazon RDS simply flips the canonical name record (CNAME) for your DB instance to point at the standby, which is in turn promoted to become the new primary. Multi-AZ means the URL is the same, the failover is automated, and the CNAME will automatically be updated to point to the standby database. Incorrect options: The URL to access the database will change to the standby database - As discussed above, URL remains the same. An email will be sent to the System Administrator asking for manual intervention - This option is incorrect and it has been added as a distractor. The application will be down until the primary database has recovered itself - This option is incorrect and it has been added as a distractor. References: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html https://aws.amazon.com/rds/faqs/",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "Your company is evolving towards a microservice approach for their website. The company plans to expose the website from the same load balancer, linked to different target groups with different URLs, that are similar to these - checkout.mycorp.com, www.mycorp.com, mycorp.com/profile, and mycorp.com/search. As a Solutions Architect, which Load Balancer type do you recommend to achieve this routing feature with MINIMUM configuration and development effort?",
      "options": [
        "A. Create an NGINX based load balancer on an Amazon EC2 instance to have advanced routing capabilities",
        "B. Create a Network Load Balancer",
        "C. Create a Classic Load Balancer",
        "D. Create an Application Load Balancer"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: Create an Application Load Balancer Application Load Balancer can automatically distribute incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and Lambda functions. It can handle the varying load of your application traffic in a single Availability Zone or across multiple Availability Zones. If your application is composed of several individual services, an Application Load Balancer can route a request to a service based on the content of the request. Here are the different types - Host-based Routing: You can route a client request based on the Host field of the HTTP header allowing you to route to multiple domains from the same load balancer. You can use host conditions to define rules that route requests based on the hostname in the host header (also known as host-based routing). This enables you to support multiple domains using a single load balancer. Example hostnames: example.com test.example.com *.example.com The rule *.example.com matches test.example.com but doesn't match example.com. Path-based Routing: You can route a client request based on the URL path of the HTTP header. You can use path conditions to define rules that route requests based on the URL in the request (also known as path-based routing). Example path patterns: /img/* /img//pics The path pattern is used to route requests but does not alter them. For example, if a rule has a path pattern of /img/, the rule would forward a request for /img/picture.jpg to the specified target group as a request for /img/picture.jpg. The path pattern is applied only to the path of the URL, not to its query parameters. HTTP header-based routing: You can route a client request based on the value of any standard or custom HTTP header. HTTP method-based routing: You can route a client request based on any standard or custom HTTP method. Query string parameter-based routing: You can route a client request based on query string or query parameters. Source IP address CIDR-based routing: You can route a client request based on source IP address CIDR from where the request originates. Path based routing and host based routing are only available for the Application Load Balancer (ALB). Therefore this is the correct option for the given use-case. Incorrect options: Create an NGINX based load balancer on an Amazon EC2 instance to have advanced routing capabilities - Although it is technically possible to set up NGINX based load balancer, however, this option involves a lot of configuration effort, so this option is ruled out for the given use-case. So, deploying an NGINX load balancer on Amazon EC2 would work but would suffer management and scaling issues. Create a Network Load Balancer - Network Load Balancer is best suited for use-cases involving low latency and high throughput workloads that involve scaling to millions of requests per second. Network Load Balancer operates at the connection level (Layer 4), routing connections to targets - Amazon EC2 instances, microservices, and containers – within Amazon Virtual Private Cloud (Amazon VPC) based on IP protocol data. Create a Classic Load Balancer - Classic Load Balancer provides basic load balancing across multiple Amazon EC2 instances and operates at both the request level and connection level. Classic Load Balancer is intended for applications that were built within the EC2-Classic network. As mentioned in the description above, these two options are incorrect for the given use-case. References: https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html https://aws.amazon.com/blogs/aws/new-host-based-routing-support-for-aws-application-load-balancers/",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A developer in your team has set up a classic 3 tier architecture composed of an Application Load Balancer, an Auto Scaling group managing a fleet of Amazon EC2 instances, and an Amazon Aurora database. As a Solutions Architect, you would like to adhere to the security pillar of the well-architected framework. How do you configure the security group of the Aurora database to only allow traffic coming from the Amazon EC2 instances?",
      "options": [
        "A. Add a rule authorizing the Amazon EC2 security group",
        "B. Add a rule authorizing the Amazon Aurora security group",
        "C. Add a rule authorizing the Auto Scaling group subnets CIDR",
        "D. Add a rule authorizing the Elastic Load Balancing security group"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Add a rule authorizing the Amazon EC2 security group A security group acts as a virtual firewall that controls the traffic for one or more instances. When you launch an instance, you can specify one or more security groups; otherwise, we use the default security group. You can add rules to each security group that allow traffic to or from its associated instances. You can modify the rules for a security group at any time; the new rules are automatically applied to all instances that are associated with the security group. When we decide whether to allow traffic to reach an instance, we evaluate all the rules from all the security groups that are associated with the instance. The following are the characteristics of security group rules: By default, security groups allow all outbound traffic. Security group rules are always permissive; you can't create rules that deny access. Security groups are stateful. For the given scenario, the Amazon EC2 instances that are part of the Auto Scaling Group are the ones accessing the database layer. The correct response is to add a rule to the security group attached to Aurora authorizing the Amazon EC2 instance's security group. Incorrect options: Add a rule authorizing the Amazon Aurora security group - Adding a rule, authorizing the Aurora security group, is just a distractor. Since it has no bearing on traffic allowed from the Amazon EC2 instances. Add a rule authorizing the Auto Scaling group subnets CIDR - Authorizing the entire CIDR of the ASG's subnets is overkill and would allow non-Auto Scaling Group instances, access Aurora if they were part of the same CIDR. Add a rule authorizing the Elastic Load Balancing security group - Adding a rule authorizing the ELB security group would dilute the security for the Aurora databases because only the Amazon EC2 instances that are part of the Auto Scaling Group are the ones accessing the database layer. Therefore, it is not the correct option. Reference: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A development team has deployed a microservice to the Amazon Elastic Container Service (Amazon ECS). The application layer is in a Docker container that provides both static and dynamic content through an Application Load Balancer. With increasing load, the Amazon ECS cluster is experiencing higher network usage. The development team has looked into the network usage and found that 90% of it is due to distributing static content of the application. As a Solutions Architect, what do you recommend to improve the application's network usage and decrease costs?",
      "options": [
        "A. Distribute the static content through Amazon EFS",
        "B. Distribute the static content through Amazon S3",
        "C. Distribute the dynamic content through Amazon EFS",
        "D. Distribute the dynamic content through Amazon S3"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Distribute the static content through Amazon S3 You can use Amazon S3 to host a static website. On a static website, individual web pages include static content. They might also contain client-side scripts. To host a static website on Amazon S3, you configure an Amazon S3 bucket for website hosting and then upload your website content to the bucket. When you configure a bucket as a static website, you must enable website hosting, set permissions, and create and add an index document. Depending on your website requirements, you can also configure redirects, web traffic logging, and a custom error document. Distributing the static content through Amazon S3 allows us to offload most of the network usage to Amazon S3 and free up our applications running on Amazon ECS. Incorrect options: Distribute the dynamic content through Amazon S3 - By contrast, a dynamic website relies on server-side processing, including server-side scripts such as PHP, JSP, or ASP.NET. Amazon S3 does not support server-side scripting, but AWS has other resources for hosting dynamic websites. Distribute the static content through Amazon EFS Distribute the dynamic content through Amazon EFS Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. Using Amazon EFS for static or dynamic content will not change anything as static content on EFS would still have to be distributed by the Amazon ECS instances. References: https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An e-commerce company uses a two-tier architecture with application servers in the public subnet and an Amazon RDS MySQL DB in a private subnet. The development team can use a bastion host in the public subnet to access the MySQL database and run queries from the bastion host. However, end-users are reporting application errors. Upon inspecting application logs, the team notices several \"could not connect to server: connection timed out\" error messages. Which of the following options represent the root cause for this issue?",
      "options": [
        "A. The security group configuration for the database instance does not have the correct rules to allow inbound connections from the application servers",
        "B. The database user credentials (username and password) configured for the application are incorrect",
        "C. The security group configuration for the application servers does not have the correct rules to allow inbound connections from the database instance",
        "D. The database user credentials (username and password) configured for the application do not have the required privilege for the given database"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: The security group configuration for the database instance does not have the correct rules to allow inbound connections from the application servers You should use security groups to control the inbound and outbound traffic for your database instance. For your application servers, create a security group with inbound rules that use the IP addresses of the client application as the source. This security group allows your client application to connect to your application servers. Then create a second security group for your database instance and create a new rule by specifying the security group that you created earlier as the source for this database-specific security group. via - https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.RDSSecurityGroups.html Incorrect options: The security group configuration for the application servers does not have the correct rules to allow inbound connections from the database instance - As mentioned in the explanation above, the application servers don't need inbound connections from the database instance, rather the database instance needs the correct inbound rule with application servers' security group as the source. The database user credentials (username and password) configured for the application are incorrect The database user credentials (username and password) configured for the application do not have the required privilege for the given database These two options have been added as a distractor since the error mentions a \"connection timeout\" issue rather than an \"access denied\" error. References: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.RDSSecurityGroups.html https://aws.amazon.com/premiumsupport/knowledge-center/rds-cannot-connect/",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A company is looking for a technology that allows its mobile app users to connect through a Google login and have the capability to turn on AWS Multi-Factor Authentication (AWS MFA) to have maximum security. Ideally, the solution should be fully managed by AWS. Which technology do you recommend for managing the users' accounts?",
      "options": [
        "A. Write an AWS Lambda function with Auth0 3rd party integration",
        "B. Amazon Cognito",
        "C. Enable the AWS Google Login Service",
        "D. AWS Identity and Access Management (AWS IAM)"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Amazon Cognito Amazon Cognito lets you add user sign-up, sign-in, and access control to your web and mobile apps quickly and easily. Amazon Cognito scales to millions of users and supports sign-in with social identity providers, such as Facebook, Google, and Amazon, and enterprise identity providers via SAML 2.0. Here Cognito is the best technology choice for managing mobile user accounts. Amazon Cognito Features: via - https://aws.amazon.com/cognito/details/ Incorrect options: Write an AWS Lambda function with Auth0 3rd party integration - AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume. Using Lambda would require code maintenance for user management functionality, therefore this option is ruled out. AWS Identity and Access Management (AWS IAM) - AWS Identity and Access Management (IAM) enables you to manage access to AWS services and resources securely. Using IAM, you can create and manage AWS users and groups, and use permissions to allow and deny their access to AWS resources. IAM cannot be used to manage mobile user accounts. Enable the AWS Google Login Service - There is no such thing as AWS Google Login service. This option is just added as a distractor. Reference: https://aws.amazon.com/cognito/",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "Your firm has implemented a multi-tiered networking structure within the VPC - with two public and two private subnets. The public subnets are used to deploy the Application Load Balancers, while the two private subnets are used to deploy the application on Amazon EC2 instances. The development team wants the Amazon EC2 instances to have access to the internet. The solution has to be fully managed by AWS and needs to work over IPv4. What will you recommend?",
      "options": [
        "A. NAT Gateways deployed in your public subnet",
        "B. NAT Instances deployed in your public subnet",
        "C. Internet Gateways deployed in your private subnet",
        "D. Egress-Only Internet Gateways deployed in your private subnet"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: NAT Gateways deployed in your public subnet You can use a network address translation (NAT) gateway to enable instances in a private subnet to connect to the internet or other AWS services, but prevent the internet from initiating a connection with those instances. A NAT gateway has the following characteristics and limitations: A NAT gateway supports 5 Gbps of bandwidth and automatically scales up to 45 Gbps. You can associate exactly one Elastic IP address with a NAT gateway. A NAT gateway supports the following protocols: TCP, UDP, and ICMP. You cannot associate a security group with a NAT gateway. You can use a network access control list (network ACL) to control the traffic to and from the subnet in which the NAT gateway is located. A NAT gateway can support up to 55,000 simultaneous connections to each unique destination. Therefore you must use a NAT Gateway in your public subnet in order to provide internet access to your instances in your private subnets. You are charged for creating and using a NAT gateway in your account. NAT gateway hourly usage and data processing rates apply. Comparison of NAT instances and NAT gateways: via - https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html Incorrect options: NAT Instances deployed in your public subnet - You can use a network address translation (NAT) instance in a public subnet in your VPC to enable instances in the private subnet to initiate outbound IPv4 traffic to the Internet or other AWS services, but prevent the instances from receiving inbound traffic initiated by someone on the Internet. Amazon provides Amazon Linux AMIs that are configured to run as NAT instances. These AMIs include the string amzn-ami-vpc-nat in their names, so you can search for them in the Amazon EC2 console. NAT Instances would work but won't scale and you would have to manage them (as they're nothing but Amazon EC2 instances). Internet Gateways deployed in your private subnet - An internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between instances in your VPC and the internet. It, therefore, imposes no availability risks or bandwidth constraints on your network traffic. Internet Gateways must be deployed in a public subnet, hence not an option here. Egress-Only Internet Gateways deployed in your private subnet - An Egress-Only Internet Gateway is a horizontally scaled, redundant, and highly available VPC component that allows outbound communication over IPv6 from instances in your VPC to the Internet, and prevents the Internet from initiating an IPv6 connection with your instances. Egress-Only Internet Gateways are for IPv6, not IPv4. Therefore, this option is incorrect. References: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html https://docs.aws.amazon.com/vpc/latest/userguide/VPC_NAT_Instance.html",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An Internet-of-Things (IoT) company is looking for a database solution on AWS Cloud that has Auto Scaling capabilities and is highly available. The database should be able to handle any changes in data attributes over time, in case the company updates the data feed from its IoT devices. The database must provide the capability to output a continuous stream with details of any changes to the underlying data. As a Solutions Architect, which database will you recommend?",
      "options": [
        "A. Amazon DynamoDB",
        "B. Amazon Redshift",
        "C. Amazon Aurora",
        "D. Amazon Relational Database Service (Amazon RDS)"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Amazon DynamoDB Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-Region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications. DynamoDB can handle more than 10 trillion requests per day and can support peaks of more than 20 million requests per second. DynamoDB is serverless with no servers to provision, patch, or manage and no software to install, maintain, or operate. A Amazon DynamoDB stream is an ordered flow of information about changes to items in a DynamoDB table. When you enable a stream on a table, Amazon DynamoDB captures information about every modification to data items in the table. Whenever an application creates, updates, or deletes items in the table, DynamoDB Streams writes a stream record with the primary key attributes of the items that were modified. A stream record contains information about a data modification to a single item in a DynamoDB table. You can configure the stream so that the stream records capture additional information, such as the \"before\" and \"after\" images of modified items. Amazon DynamoDB is horizontally scalable, has a DynamoDB streams capability and is multi-AZ by default. On top of it, we can adjust the RCU and WCU automatically using Auto Scaling. This is the right choice for current requirements. Incorrect options: Amazon Relational Database Service (Amazon RDS) - Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching and backups. Schema changes on relational databases are not straight forward and are hard to maintain if the schema requirements change often. Amazon Aurora - Amazon Aurora is a MySQL and PostgreSQL-compatible relational database built for the cloud, that combines the performance and availability of traditional enterprise databases with the simplicity and cost-effectiveness of open source databases. Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 64TB per database instance. Aurora is not an in-memory database. Schema changes on relational databases are not straight forward and are hard to maintain if the schema requirements change often. Amazon Redshift - Amazon Redshift is a fully-managed petabyte-scale cloud based data warehouse product designed for large scale data set storage and analysis. It is a powerful warehousing service from Amazon. The current requirement, however, is not looking for a warehousing solution and hence Redshift is not an option here. References: https://aws.amazon.com/dynamodb/ https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An Internet-of-Things (IoT) company is planning on distributing a master sensor in people's homes to measure the key metrics from its smart devices. In order to provide adjustment commands for these devices, the company would like to have a streaming system that supports ordered data based on the sensor's key, and also sustains high throughput messages (thousands of messages per second). As a solutions architect, which of the following AWS services would you recommend for this use-case?",
      "options": [
        "A. Amazon Simple Queue Service (Amazon SQS)",
        "B. Amazon Simple Notification Service (Amazon SNS)",
        "C. Amazon Kinesis Data Streams",
        "D. AWS Lambda"
      ],
      "correct_answer": "C",
      "explanation": "Correct option: Amazon Kinesis Data Streams Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The throughput of an Amazon Kinesis data stream is designed to scale without limits via increasing the number of shards within a data stream. However, there are certain limits you should keep in mind while using Amazon Kinesis Data Streams: A Kinesis data stream stores records from 24 hours by default, up to 8760 hours (365 days). The maximum size of a data blob (the data payload before Base64-encoding) within one record is 1 megabyte (MB). Each shard can support up to 1000 PUT records per second. Kinesis is the right answer here, as by providing a partition key in your message, you can guarantee ordered messages for a specific sensor, even if your stream is sharded. Incorrect options: Amazon Simple Queue Service (Amazon SQS) - Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. Amazon SQS eliminates the complexity and overhead associated with managing and operating message-oriented middleware, and empowers developers to focus on differentiating work. Using Amazon SQS, you can send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be available. Kinesis is better for streaming data since queues aren't meant for real-time streaming of data. Amazon Simple Notification Service (Amazon SNS) - Amazon Simple Notification Service (Amazon SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. Amazon SNS provides topics for high-throughput, push-based, many-to-many messaging. SNS cannot be used for data streaming. Therefore this option is not the best fit for the given use-case. AWS Lambda - AWS Lambda lets you run code without provisioning or managing servers. It cannot be used for production-grade serverless log analytics. Lambda isn't meant to retain data either. Therefore this option is not the best fit for the given use-case. Reference: https://aws.amazon.com/kinesis/data-streams/faqs/",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "You have built an application that is deployed with Elastic Load Balancing and an Auto Scaling Group. As a Solutions Architect, you have configured aggressive Amazon CloudWatch alarms, making your Auto Scaling Group (ASG) scale in and out very quickly, renewing your fleet of Amazon EC2 instances on a daily basis. A production bug appeared two days ago, but the team is unable to SSH into the instance to debug the issue, because the instance has already been terminated by the Auto Scaling Group. The log files are saved on the Amazon EC2 instance. How will you resolve the issue and make sure it doesn't happen again?",
      "options": [
        "A. Install an Amazon CloudWatch Logs agents on the Amazon EC2 instances to send logs to Amazon CloudWatch",
        "B. Disable the Termination from the Auto Scaling Group any time a user reports an issue",
        "C. Make a snapshot of the Amazon EC2 instance just before it gets terminated",
        "D. Use AWS Lambda to regularly SSH into the Amazon EC2 instances and copy the log files to Amazon S3"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Install an Amazon CloudWatch Logs agents on the Amazon EC2 instances to send logs to Amazon CloudWatch You can use the Amazon CloudWatch Logs agent installer on an existing Amazon EC2 instance to install and configure the Amazon CloudWatch Logs agent. After installation is complete, logs automatically flow from the instance to the log stream you create while installing the agent. The agent confirms that it has started and it stays running until you disable it. Here, the natural and by far the easiest solution would be to use the Amazon CloudWatch Logs agents on the Amazon EC2 instances to automatically send log files into Amazon CloudWatch, so we can analyze them in the future easily should any problem arise. To control whether an Auto Scaling group can terminate a particular instance when scaling in, use instance scale-in protection. You can enable the instance scale-in protection setting on an Auto Scaling group or on an individual Auto Scaling instance. When the Auto Scaling group launches an instance, it inherits the instance scale-in protection setting of the Auto Scaling group. You can change the instance scale-in protection setting for an Auto Scaling group or an Auto Scaling instance at any time. Incorrect options: Disable the Termination from the Auto Scaling Group any time a user reports an issue - Disabling the Termination from the Auto Scaling Group would prevent our Auto Scaling Group from being Elastic and impact our costs. Therefore this option is incorrect. Make a snapshot of the Amazon EC2 instance just before it gets terminated - Making a snapshot of the Amazon EC2 instance before it gets terminated could work but it's tedious, not elastic and very expensive, since our interest is just the log files. Therefore this option is not the best fit for the given use-case. You can back up the data on your Amazon EBS volumes to Amazon S3 by taking point-in-time snapshots. Snapshots are incremental backups, which means that only the blocks on the device that have changed after your most recent snapshot are saved. This minimizes the time required to create the snapshot and saves on storage costs by not duplicating data. Use AWS Lambda to regularly SSH into the Amazon EC2 instances and copy the log files to Amazon S3 - AWS Lambda lets you run code without provisioning or managing servers. It cannot be used for production-grade serverless log analytics. Using AWS Lambda would be extremely hard to use for this task. Therefore this option is not the best fit for the given use-case. Reference: https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/QuickStartEC2Instance.html",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An e-commerce company uses Amazon Simple Queue Service (Amazon SQS) queues to decouple their application architecture. The engineering team has observed message processing failures for some customer orders. As a solutions architect, which of the following solutions would you recommend for handling such message failures?",
      "options": [
        "A. Use a temporary queue to handle message processing failures",
        "B. Use a dead-letter queue to handle message processing failures",
        "C. Use short polling to handle message processing failures",
        "D. Use long polling to handle message processing failures"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Use a dead-letter queue to handle message processing failures Dead-letter queues can be used by other queues (source queues) as a target for messages that can't be processed (consumed) successfully. Dead-letter queues are useful for debugging your application or messaging system because they let you isolate problematic messages to determine why their processing doesn't succeed. Sometimes, messages can’t be processed because of a variety of possible issues, such as when a user comments on a story but it remains unprocessed because the original story itself is deleted by the author while the comments were being posted. In such a case, the dead-letter queue can be used to handle message processing failures. How do dead-letter queues work?: via - https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html Incorrect options: Use a temporary queue to handle message processing failures - The most common use case for temporary queues is the request-response messaging pattern (for example, processing a login request), where a requester creates a temporary queue for receiving each response message. To avoid creating an Amazon SQS queue for each response message, the Temporary Queue Client lets you create and delete multiple temporary queues without making any Amazon SQS API calls. Temporary queues cannot be used to handle message processing failures. Use short polling to handle message processing failures Use long polling to handle message processing failures Amazon SQS provides short polling and long polling to receive messages from a queue. By default, queues use short polling. With short polling, Amazon SQS sends the response right away, even if the query found no messages. With long polling, Amazon SQS sends a response after it collects at least one available message, up to the maximum number of messages specified in the request. Amazon SQS sends an empty response only if the polling wait time expires. Neither short polling nor long polling can be used to handle message processing failures. Reference: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "The engineering team at a retail company manages 3 Amazon EC2 instances that make read-heavy database requests to the Amazon RDS for the PostgreSQL database instance. As an AWS Certified Solutions Architect - Associate, you have been tasked to make the database instance resilient from a disaster recovery perspective. Which of the following features will help you in disaster recovery of the database? (Select two)",
      "options": [
        "A. Use cross-Region Read Replicas",
        "B. Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups across multiple Regions",
        "C. Use Amazon RDS Provisioned IOPS (SSD) Storage in place of General Purpose (SSD) Storage",
        "D. Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups in a single AWS Region",
        "E. Use the database cloning feature of the Amazon RDS Database cluster"
      ],
      "correct_answer": "A",
      "explanation": "Correct options: Use cross-Region Read Replicas In addition to using Read Replicas to reduce the load on your source database instance, you can also use Read Replicas to implement a DR solution for your production DB environment. If the source DB instance fails, you can promote your Read Replica to a standalone source server. Read Replicas can also be created in a different Region than the source database. Using a cross-Region Read Replica can help ensure that you get back up and running if you experience a regional availability issue. Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups across multiple Regions Amazon RDS provides high availability and failover support for database instances using Multi-AZ deployments. Amazon RDS uses several different technologies to provide failover support. Multi-AZ deployments for MariaDB, MySQL, Oracle, and PostgreSQL DB instances use Amazon's failover technology. The automated backup feature of Amazon RDS enables point-in-time recovery for your database instance. Amazon RDS will back up your database and transaction logs and store both for a user-specified retention period. If it’s a Multi-AZ configuration, backups occur on standby to reduce the I/O impact on the primary. Amazon RDS supports Cross-Region Automated Backups. Manual snapshots and Read Replicas are also supported across multiple Regions. Incorrect options: Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups in a single AWS Region - This is an incorrect statement. Automated backups can be created across AWS Regions. Use Amazon RDS Provisioned IOPS (SSD) Storage in place of General Purpose (SSD) Storage - Amazon RDS Provisioned IOPS Storage is an SSD-backed storage option designed to deliver fast, predictable, and consistent I/O performance. This storage type enhances the performance of the RDS database, but this isn't a disaster recovery option. Use the database cloning feature of the Amazon RDS Database cluster - This option has been added as a distractor. Database cloning is only available for Amazon Aurora and not for Amazon RDS. References: https://aws.amazon.com/rds/features/ https://aws.amazon.com/blogs/database/implementing-a-disaster-recovery-strategy-with-amazon-rds/ https://aws.amazon.com/about-aws/whats-new/2021/07/amazon-rds-cross-region-automated-backups-regional-expansion/",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A retail company wants to establish encrypted network connectivity between its on-premises data center and AWS Cloud. The company wants to get the solution up and running in the fastest possible time and it should also support encryption in transit. As a solutions architect, which of the following solutions would you suggest to the company?",
      "options": [
        "A. Use AWS Direct Connect to establish encrypted network connectivity between the on-premises data center and AWS Cloud",
        "B. Use AWS Data Sync to establish encrypted network connectivity between the on-premises data center and AWS Cloud",
        "C. Use AWS Secrets Manager to establish encrypted network connectivity between the on-premises data center and AWS Cloud",
        "D. Use AWS Site-to-Site VPN to establish encrypted network connectivity between the on-premises data center and AWS Cloud"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: Use AWS Site-to-Site VPN to establish encrypted network connectivity between the on-premises data center and AWS Cloud AWS Site-to-Site VPN enables you to securely connect your on-premises network or branch office site to your Amazon Virtual Private Cloud (Amazon VPC). You can securely extend your data center or branch office network to the cloud with an AWS Site-to-Site VPN connection. A VPC VPN Connection utilizes IPSec to establish encrypted network connectivity between your on-premises network and Amazon VPC over the Internet. IPsec is a protocol suite for securing IP communications by authenticating and encrypting each IP packet in a data stream. Incorrect options: Use AWS Direct Connect to establish encrypted network connectivity between the on-premises data center and AWS Cloud - AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations. Using industry-standard 802.1q VLANs, this dedicated connection can be partitioned into multiple virtual interfaces. AWS Direct Connect does not encrypt your traffic that is in transit. To encrypt the data in transit that traverses AWS Direct Connect, you must use the transit encryption options for that service. As AWS Direct Connect does not support encrypted network connectivity between an on-premises data center and AWS Cloud, therefore this option is incorrect. Use AWS Data Sync to establish encrypted network connectivity between the on-premises data center and AWS Cloud - AWS DataSync makes it simple and fast to move large amounts of data online between on-premises storage and AWS. AWS DataSync eliminates or automatically handles many of these tasks, including scripting copy jobs, scheduling, and monitoring transfers, validating data, and optimizing network utilization. As AWS Data Sync cannot be used to establish network connectivity between an on-premises data center and AWS Cloud, therefore this option is incorrect. Use AWS Secrets Manager to establish encrypted network connectivity between the on-premises data center and AWS Cloud - AWS Secrets Manager helps you protect secrets needed to access your applications, services, and IT resources. The service enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. As AWS Secrets Manager cannot be used to establish network connectivity between an on-premises data center and AWS Cloud, therefore this option is incorrect. References: https://docs.aws.amazon.com/vpn/latest/s2svpn/internetwork-traffic-privacy.html https://docs.aws.amazon.com/directconnect/latest/UserGuide/encryption-in-transit.html",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A pharmaceutical company is considering moving to AWS Cloud to accelerate the research and development process. Most of the daily workflows would be centered around running batch jobs on Amazon EC2 instances with storage on Amazon Elastic Block Store (Amazon EBS) volumes. The CTO is concerned about meeting HIPAA compliance norms for sensitive data stored on Amazon EBS. Which of the following options outline the correct capabilities of an encrypted Amazon EBS volume? (Select three)",
      "options": [
        "A. Data at rest inside the volume is encrypted",
        "B. Data moving between the volume and the instance is NOT encrypted",
        "C. Any snapshot created from the volume is encrypted",
        "D. Any snapshot created from the volume is NOT encrypted",
        "E. Data moving between the volume and the instance is encrypted",
        "F. Data at rest inside the volume is NOT encrypted"
      ],
      "correct_answer": "A",
      "explanation": "Correct options: Data at rest inside the volume is encrypted Any snapshot created from the volume is encrypted Data moving between the volume and the instance is encrypted Amazon Elastic Block Store (Amazon EBS) provides block-level storage volumes for use with Amazon EC2 instances. When you create an encrypted Amazon EBS volume and attach it to a supported instance type, data stored at rest on the volume, data moving between the volume and the instance, snapshots created from the volume and volumes created from those snapshots are all encrypted. It uses AWS Key Management Service (AWS KMS) customer master keys (CMK) when creating encrypted volumes and snapshots. Encryption operations occur on the servers that host Amazon EC2 instances, ensuring the security of both data-at-rest and data-in-transit between an instance and its attached Amazon EBS storage. Therefore, the incorrect options are: Data moving between the volume and the instance is NOT encrypted Any snapshot created from the volume is NOT encrypted Data at rest inside the volume is NOT encrypted Reference: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A silicon valley based healthcare startup uses AWS Cloud for its IT infrastructure. The startup stores patient health records on Amazon Simple Storage Service (Amazon S3). The engineering team needs to implement an archival solution based on Amazon S3 Glacier to enforce regulatory and compliance controls on data access. As a solutions architect, which of the following solutions would you recommend?",
      "options": [
        "A. Use Amazon S3 Glacier vault to store the sensitive archived data and then use a vault lock policy to enforce compliance controls",
        "B. Use Amazon S3 Glacier to store the sensitive archived data and then use an Amazon S3 lifecycle policy to enforce compliance controls",
        "C. Use Amazon S3 Glacier vault to store the sensitive archived data and then use an Amazon S3 Access Control List to enforce compliance controls",
        "D. Use Amazon S3 Glacier to store the sensitive archived data and then use an Amazon S3 Access Control List to enforce compliance controls"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Use Amazon S3 Glacier vault to store the sensitive archived data and then use a vault lock policy to enforce compliance controls Amazon S3 Glacier is a secure, durable, and extremely low-cost Amazon S3 cloud storage class for data archiving and long-term backup. It is designed to deliver 99.999999999% durability, and provide comprehensive security and compliance capabilities that can help meet even the most stringent regulatory requirements. An Amazon S3 Glacier vault is a container for storing archives. When you create a vault, you specify a vault name and the AWS Region in which you want to create the vault. Amazon S3 Glacier Vault Lock allows you to easily deploy and enforce compliance controls for individual Amazon S3 Glacier vaults with a vault lock policy. You can specify controls such as “write once read many” (WORM) in a vault lock policy and lock the policy from future edits. Therefore, this is the correct option. Incorrect options: Use Amazon S3 Glacier to store the sensitive archived data and then use an Amazon S3 lifecycle policy to enforce compliance controls - You can use lifecycle policy to define actions you want Amazon S3 to take during an object's lifetime. For example, use a lifecycle policy to transition objects to another storage class, archive them, or delete them after a specified period. It cannot be used to enforce compliance controls. Therefore, this option is incorrect. Use Amazon S3 Glacier vault to store the sensitive archived data and then use an Amazon S3 Access Control List to enforce compliance controls- Amazon S3 access control lists (ACLs) enable you to manage access to buckets and objects. It cannot be used to enforce compliance controls. Therefore, this option is incorrect. Use Amazon S3 Glacier to store the sensitive archived data and then use an Amazon S3 Access Control List to enforce compliance controls - Amazon S3 access control lists (ACLs) enable you to manage access to buckets and objects. It cannot be used to enforce compliance controls. Therefore, this option is incorrect. References: https://docs.aws.amazon.com/amazonglacier/latest/dev/working-with-vaults.html https://docs.aws.amazon.com/amazonglacier/latest/dev/vault-lock.html https://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-lifecycle.html",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A big data analytics company is using Amazon Kinesis Data Streams (KDS) to process IoT data from the field devices of an agricultural sciences company. Multiple consumer applications are using the incoming data streams and the engineers have noticed a performance lag for the data delivery speed between producers and consumers of the data streams. As a solutions architect, which of the following would you recommend for improving the performance for the given use-case?",
      "options": [
        "A. Swap out Amazon Kinesis Data Streams with Amazon SQS Standard queues",
        "B. Swap out Amazon Kinesis Data Streams with Amazon SQS FIFO queues",
        "C. Use Enhanced Fanout feature of Amazon Kinesis Data Streams",
        "D. Swap out Amazon Kinesis Data Streams with Amazon Kinesis Data Firehose"
      ],
      "correct_answer": "C",
      "explanation": "Correct option: Use Enhanced Fanout feature of Amazon Kinesis Data Streams Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. By default, the 2MB/second/shard output is shared between all of the applications consuming data from the stream. You should use enhanced fan-out if you have multiple consumers retrieving data from a stream in parallel. With enhanced fan-out developers can register stream consumers to use enhanced fan-out and receive their own 2MB/second pipe of read throughput per shard, and this throughput automatically scales with the number of shards in a stream. Amazon Kinesis Data Streams Fanout: via - https://aws.amazon.com/blogs/aws/kds-enhanced-fanout/ Incorrect options: Swap out Amazon Kinesis Data Streams with Amazon Kinesis Data Firehose - Amazon Kinesis Data Firehose is the easiest way to reliably load streaming data into data lakes, data stores, and analytics tools. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, transform, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security. Amazon Kinesis Data Firehose can only write to Amazon S3, Amazon Redshift, Amazon Elasticsearch or Splunk. You can't have applications consuming data streams from Amazon Kinesis Data Firehose, that's the job of Amazon Kinesis Data Streams. Therefore this option is not correct. Swap out Amazon Kinesis Data Streams with Amazon SQS Standard queues Swap out Amazon Kinesis Data Streams with Amazon SQS FIFO queues Amazon Simple Queue Service (Amazon SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. Amazon SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. Amazon SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent. As multiple applications are consuming the same stream concurrently, both Amazon SQS Standard and Amazon SQS FIFO are not the right fit for the given use-case. Exam Alert: Please understand the differences between the capabilities of Amazon Kinesis Data Streams vs Amazon SQS, as you may be asked scenario-based questions on this topic in the exam. via - https://aws.amazon.com/kinesis/data-streams/faqs/ References: https://aws.amazon.com/blogs/aws/kds-enhanced-fanout/ https://aws.amazon.com/kinesis/data-streams/faqs/",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A Customer relationship management (CRM) application is facing user experience issues with users reporting frequent sign-in requests from the application. The application is currently hosted on multiple Amazon EC2 instances behind an Application Load Balancer. The engineering team has identified the root cause as unhealthy servers causing session data to be lost. The team would like to implement a distributed in-memory cache-based session management solution. As a solutions architect, which of the following solutions would you recommend?",
      "options": [
        "A. Use Amazon RDS for distributed in-memory cache based session management",
        "B. Use Amazon Elasticache for distributed in-memory cache based session management",
        "C. Use Application Load Balancer sticky sessions",
        "D. Use Amazon DynamoDB for distributed in-memory cache based session management"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Use Amazon Elasticache for distributed in-memory cache based session management Amazon ElastiCache can be used as a distributed in-memory cache for session management. Amazon ElastiCache allows you to seamlessly set up, run, and scale popular open-Source compatible in-memory data stores in the cloud. Session stores can be set up using both Memcached or Redis for ElastiCache. Amazon ElastiCache for Redis is a great choice for real-time transactional and analytical processing use cases such as caching, chat/messaging, gaming leaderboards, geospatial, machine learning, media streaming, queues, real-time analytics, and session store. Amazon ElastiCache for Memcached is a Memcached-compatible in-memory key-value store service that can be used as a cache or a data store. Session stores are easy to create with Amazon ElastiCache for Memcached. How Amazon ElastiCache Works: via - https://aws.amazon.com/elasticache/ Incorrect options: Use Amazon RDS for distributed in-memory cache based session management - Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It cannot be used as a distributed in-memory cache for session management, hence this option is incorrect. Use Amazon DynamoDB for distributed in-memory cache based session management - Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. Amazon DynamoDB is a NoSQL database and is not the right fit for a distributed in-memory cache-based session management solution. Use Application Load Balancer sticky sessions - Although sticky sessions enable each user to interact with one server and one server only, however, in case of an unhealthy server, all the session data is gone as well. Therefore Amazon Elasticache powered distributed in-memory cache-based session management is a better solution. References: https://aws.amazon.com/getting-started/hands-on/building-fast-session-caching-with-amazon-elasticache-for-redis/ https://aws.amazon.com/elasticache/",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A silicon valley based startup helps its users legally sign highly confidential contracts. To meet the compliance guidelines, the startup must ensure that the signed contracts are encrypted using the AES-256 algorithm via an encryption key that is generated as well as managed internally. The startup is now migrating to AWS Cloud and would like the data to be encrypted on AWS. The startup wants to continue using their existing encryption key generation as well as key management mechanism. What do you recommend?",
      "options": [
        "A. SSE-KMS",
        "B. SSE-S3",
        "C. SSE-C",
        "D. Client-Side Encryption"
      ],
      "correct_answer": "C",
      "explanation": "Correct option: SSE-C With Server-Side Encryption with Customer-Provided Keys (SSE-C), you manage the encryption keys and Amazon S3 manages the encryption, as it writes to disks, and decryption when you access your objects. With SSE-C, the startup can still generate and manage the encryption key but let AWS do the encryption. Therefore, this is the correct option. Incorrect options: SSE-KMS - AWS Key Management Service (AWS KMS) is a service that combines secure, highly available hardware and software to provide a key management system scaled for the cloud. When you use server-side encryption with AWS KMS (SSE-KMS), you can specify a customer-managed CMK that you have already created. But, you never get to know the actual key here. SSE-S3 - When you use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key. However, this option does not provide the ability to audit trail the usage of the encryption keys. Client-Side Encryption - Client-side encryption is the act of encrypting data before sending it to Amazon S3. To enable client-side encryption, you have the following options: Use a AWS KMS key stored in AWS Key Management Service (AWS KMS), Use a master key you store within your application. Since the customer wants to use AWS provided facility, this is not an option. References: https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingClientSideEncryption.html https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A financial services company runs its flagship web application on AWS. The application serves thousands of users during peak hours. The company needs a scalable near-real-time solution to share hundreds of thousands of financial transactions with multiple internal applications. The solution should also remove sensitive details from the transactions before storing the cleansed transactions in a document database for low-latency retrieval. As an AWS Certified Solutions Architect Associate, which of the following would you recommend?",
      "options": [
        "A. Batch process the raw transactions data into Amazon S3 flat files. Use S3 events to trigger an AWS Lambda function to remove sensitive data from the raw transactions in the flat file and then store the cleansed transactions in Amazon DynamoDB. Leverage DynamoDB Streams to share the transactions data with the internal applications",
        "B. Feed the streaming transactions into Amazon Kinesis Data Streams. Leverage AWS Lambda integration to remove sensitive data from every transaction and then store the cleansed transactions in Amazon DynamoDB. The internal applications can consume the raw transactions off the Amazon Kinesis Data Stream",
        "C. Feed the streaming transactions into Amazon Kinesis Data Firehose. Leverage AWS Lambda integration to remove sensitive data from every transaction and then store the cleansed transactions in Amazon DynamoDB. The internal applications can consume the raw transactions off the Amazon Kinesis Data Firehose",
        "D. Persist the raw transactions into Amazon DynamoDB. Configure a rule in Amazon DynamoDB to update the transaction by removing sensitive data whenever any new raw transaction is written. Leverage Amazon DynamoDB Streams to share the transactions data with the internal applications"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Feed the streaming transactions into Amazon Kinesis Data Streams. Leverage AWS Lambda integration to remove sensitive data from every transaction and then store the cleansed transactions in Amazon DynamoDB. The internal applications can consume the raw transactions off the Amazon Kinesis Data Stream You can use Amazon Kinesis Data Streams to build custom applications that process or analyze streaming data for specialized needs. Amazon Kinesis Data Streams manages the infrastructure, storage, networking, and configuration needed to stream your data at the level of your data throughput. You don't have to worry about provisioning, deployment, or ongoing maintenance of hardware, software, or other services for your data streams. How Amazon Kinesis Data Streams Work: via - https://aws.amazon.com/kinesis/data-streams/ Amazon Kinesis Data Streams Key Concepts: via - https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html For the given use case, you can stream the raw financial transactions into Amazon Kinesis Data Streams, which in turn, are processed by the AWS Lambda function that is set up as one of the consumers of the data stream. The Lambda would remove sensitive data from every transaction and then store the cleansed transactions in Amazon DynamoDB. The internal applications can be configured as the other consumers of the data stream and ingest the raw transactions Incorrect options: Batch process the raw transactions data into Amazon S3 flat files. Use S3 events to trigger an AWS Lambda function to remove sensitive data from the raw transactions in the flat file and then store the cleansed transactions in Amazon DynamoDB. Leverage DynamoDB Streams to share the transactions data with the internal applications- The use case requires a near-real-time solution for cleansing, processing and storing the transactions, so using a batch process would be incorrect. Feed the streaming transactions into Amazon Kinesis Data Firehose. Leverage AWS Lambda integration to remove sensitive data from every transaction and then store the cleansed transactions in Amazon DynamoDB. The internal applications can consume the raw transactions off the Amazon Kinesis Data Firehose - Amazon Kinesis Data Firehose is an extract, transform, and load (ETL) service that reliably captures, transforms, and delivers streaming data to data lakes, data stores, and analytics services. via - https://aws.amazon.com/kinesis/data-firehose/ You cannot set up multiple consumers for Amazon Kinesis Data Firehose delivery streams as it can dump data in a single data repository at a time, so this option is incorrect. Persist the raw transactions into Amazon DynamoDB. Configure a rule in Amazon DynamoDB to update the transaction by removing sensitive data whenever any new raw transaction is written. Leverage Amazon DynamoDB Streams to share the transactions data with the internal applications - There is no such rule within Amazon DynamoDB that can auto-update every time a new item is written in a DynamoDB table. You would need to use a Amazon DynamoDB trigger to invoke an external service like a Lambda function on every new write, which can then cleanse and update the item. In addition, this process introduces inefficiency in the workflow as the same item is written and then updated for cleansing purposes. Therefore this option is incorrect. References: https://aws.amazon.com/kinesis/data-streams/ https://aws.amazon.com/kinesis/data-firehose/ https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "The engineering team at a weather tracking company wants to enhance the performance of its relational database and is looking for a caching solution that supports geospatial data. As a solutions architect, which of the following solutions will you suggest?",
      "options": [
        "A. Use Amazon ElastiCache for Memcached",
        "B. Use Amazon ElastiCache for Redis",
        "C. Use Amazon DynamoDB Accelerator (DAX)",
        "D. Use AWS Global Accelerator"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Use Amazon ElastiCache for Redis Amazon ElastiCache is a web service that makes it easy to set up, manage, and scale a distributed in-memory data store or cache environment in the cloud. Redis, which stands for Remote Dictionary Server, is a fast, open-source, in-memory key-value data store for use as a database, cache, message broker, and queue. Redis now delivers sub-millisecond response times enabling millions of requests per second for real-time applications in Gaming, Ad-Tech, Financial Services, Healthcare, and IoT. Redis is a popular choice for caching, session management, gaming, leaderboards, real-time analytics, geospatial, ride-hailing, chat/messaging, media streaming, and pub/sub apps. All Redis data resides in the server’s main memory, in contrast to databases such as PostgreSQL, Cassandra, MongoDB and others that store most data on disk or on SSDs. In comparison to traditional disk based databases where most operations require a roundtrip to disk, in-memory data stores such as Redis don’t suffer the same penalty. They can therefore support an order of magnitude more operations and faster response times. The result is – blazing fast performance with average read or write operations taking less than a millisecond and support for millions of operations per second. Redis has purpose-built commands for working with real-time geospatial data at scale. You can perform operations like finding the distance between two elements (for example people or places) and finding all elements within a given distance of a point. Incorrect options: Use Amazon ElastiCache for Memcached - Both Redis and MemCached are in-memory, open-source data stores. Memcached, a high-performance distributed memory cache service, is designed for simplicity while Redis offers a rich set of features that make it effective for a wide range of use cases. Memcached does not offer support for geospatial data. via - https://aws.amazon.com/elasticache/redis-vs-memcached/ Use Amazon DynamoDB Accelerator (DAX) - Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for Amazon DynamoDB. DAX does not support relational databases. Use AWS Global Accelerator - AWS Global Accelerator is a networking service that helps you improve the availability and performance of the applications that you offer to your global users. This option has been added as a distractor, it has nothing to do with database caching. Reference: https://aws.amazon.com/elasticache/redis-vs-memcached/",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "You have just terminated an instance in the us-west-1a Availability Zone (AZ). The attached Amazon EBS volume is now available for attachment to other instances. An intern launches a new Linux Amazon EC2 instance in the us-west-1b Availability Zone (AZ) and is attempting to attach the Amazon EBS volume. The intern informs you that it is not possible and needs your help. Which of the following explanations would you provide to them?",
      "options": [
        "A. The required IAM permissions are missing",
        "B. Amazon EBS volumes are region locked",
        "C. Amazon EBS volumes are Availability Zone (AZ) locked",
        "D. The Amazon EBS volume is encrypted"
      ],
      "correct_answer": "C",
      "explanation": "Correct option: Amazon EBS volumes are Availability Zone (AZ) locked An Amazon EBS volume is a durable, block-level storage device that you can attach to your instances. After you attach a volume to an instance, you can use it as you would use a physical hard drive. Amazon EBS volumes are flexible. For current-generation volumes attached to current-generation instance types, you can dynamically increase size, modify the provisioned IOPS capacity, and change volume type on live production volumes. When you create an Amazon EBS volume, it is automatically replicated within its Availability Zone to prevent data loss due to the failure of any single hardware component. You can attach an Amazon EBS volume to an Amazon EC2 instance in the same Availability Zone (AZ). Incorrect options: Amazon EBS volumes are region locked - It's confined to an Availability Zone (AZ) and not by region. The required IAM permissions are missing - This is a possibility as well but if permissions are not an issue then you are still confined to an availability zone (AZ). The Amazon EBS volume is encrypted - This doesn't affect the ability to attach an Amazon EBS volume. Reference: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumes.html",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "While troubleshooting, a cloud architect realized that the Amazon EC2 instance is unable to connect to the internet using the Internet Gateway. Which conditions should be met for internet connectivity to be established? (Select two)",
      "options": [
        "A. The instance's subnet is not associated with any route table",
        "B. The network access control list (network ACL) associated with the subnet must have rules to allow inbound and outbound traffic",
        "C. The instance's subnet is associated with multiple route tables with conflicting configurations",
        "D. The route table in the instance’s subnet should have a route to an Internet Gateway",
        "E. The subnet has been configured to be public and has no access to the internet"
      ],
      "correct_answer": "B",
      "explanation": "Correct options: The network access control list (network ACL) associated with the subnet must have rules to allow inbound and outbound traffic The network access control list (network ACL) that is associated with the subnet must have rules to allow inbound and outbound traffic on port 80 (for HTTP traffic) and port 443 (for HTTPs traffic). This is a necessary condition for Internet Gateway connectivity. The route table in the instance’s subnet should have a route to an Internet Gateway A route table contains a set of rules, called routes, that are used to determine where network traffic from your subnet or gateway is directed. The route table in the instance’s subnet should have a route defined to the Internet Gateway. Incorrect options: The instance's subnet is not associated with any route table - This is an incorrect statement. A subnet is implicitly associated with the main route table if it is not explicitly associated with a particular route table. So, a subnet is always associated with some route table. The instance's subnet is associated with multiple route tables with conflicting configurations - This is an incorrect statement. A subnet can only be associated with one route table at a time. The subnet has been configured to be public and has no access to the internet - This is an incorrect statement. Public subnets have access to the internet via Internet Gateway. Reference: https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Route_Tables.html",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "As a Solutions Architect, you have been hired to work with the engineering team at a company to create a REST API using the serverless architecture. Which of the following solutions will you recommend to move the company to the serverless architecture?",
      "options": [
        "A. Amazon API Gateway exposing AWS Lambda Functionality",
        "B. AWS Fargate with AWS Lambda at the front",
        "C. Public-facing Application Load Balancer with Amazon Elastic Container Service (Amazon ECS) on Amazon EC2",
        "D. Amazon Route 53 with Amazon EC2 as backend"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Amazon API Gateway exposing AWS Lambda Functionality Amazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the \"front door\" for applications to access data, business logic, or functionality from your backend services. How Amazon API Gateway Works: via - https://aws.amazon.com/api-gateway/ AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume. How AWS Lambda function works: via - https://aws.amazon.com/lambda/ Amazon API Gateway can expose AWS Lambda functionality through RESTful APIs. Both are serverless options offered by AWS and hence the right choice for this scenario, considering all the functionality they offer. Incorrect options: AWS Fargate with AWS Lambda at the front - AWS Lambda cannot directly handle RESTful API requests. You can invoke an AWS Lambda function over HTTPS by defining a custom RESTful API using Amazon API Gateway. So, AWS Fargate with AWS Lambda as the front-facing service is a wrong combination, though both Fargate and Lambda are serverless. Public-facing Application Load Balancer with Amazon Elastic Container Service (Amazon ECS) on Amazon EC2 - Amazon ECS on Amazon EC2 does not come under serverless and hence cannot be considered for this use case. Amazon Route 53 with Amazon EC2 as backend - Amazon EC2 is not a serverless service and hence cannot be considered for this use case. References: https://aws.amazon.com/serverless/ https://aws.amazon.com/api-gateway/",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A health-care company manages its web application on Amazon EC2 instances running behind Auto Scaling group (ASG). The company provides ambulances for critical patients and needs the application to be reliable. The workload of the company can be managed on 2 Amazon EC2 instances and can peak up to 6 instances when traffic increases. As a Solutions Architect, which of the following configurations would you select as the best fit for these requirements?",
      "options": [
        "A. The Auto Scaling group should be configured with the minimum capacity set to 4, with 2 instances each in two different Availability Zones. The maximum capacity of the Auto Scaling group should be set to 6",
        "B. The Auto Scaling group should be configured with the minimum capacity set to 2, with 1 instance each in two different Availability Zones. The maximum capacity of the Auto Scaling group should be set to 6",
        "C. The Auto Scaling group should be configured with the minimum capacity set to 2 and the maximum capacity set to 6 in a single Availability Zone",
        "D. The Auto Scaling group should be configured with the minimum capacity set to 4, with 2 instances each in two different AWS Regions. The maximum capacity of the Auto Scaling group should be set to 6"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: The Auto Scaling group should be configured with the minimum capacity set to 4, with 2 instances each in two different Availability Zones. The maximum capacity of the Auto Scaling group should be set to 6 You configure the size of your Auto Scaling group by setting the minimum, maximum, and desired capacity. The minimum and maximum capacity are required to create an Auto Scaling group, while the desired capacity is optional. If you do not define your desired capacity upfront, it defaults to your minimum capacity. Amazon EC2 Auto Scaling enables you to take advantage of the safety and reliability of geographic redundancy by spanning Auto Scaling groups across multiple Availability Zones within a Region. When one Availability Zone becomes unhealthy or unavailable, Auto Scaling launches new instances in an unaffected Availability Zone. When the unhealthy Availability Zone returns to a healthy state, Auto Scaling automatically redistributes the application instances evenly across all of the designated Availability Zones. Since the application is extremely critical and needs to have a reliable architecture to support it, the Amazon EC2 instances should be maintained in at least two Availability Zones (AZs) for uninterrupted service. Amazon EC2 Auto Scaling attempts to distribute instances evenly between the Availability Zones that are enabled for your Auto Scaling group. This is why the minimum capacity should be 4 instances and not 2. Auto Scaling group will launch 2 instances each in both the AZs and this redundancy is needed to keep the service available always. Incorrect options: The Auto Scaling group should be configured with the minimum capacity set to 2, with 1 instance each in two different Availability Zones. The maximum capacity of the Auto Scaling group should be set to 6 The Auto Scaling group should be configured with the minimum capacity set to 2 and the maximum capacity set to 6 in a single Availability Zone The explanation above gives the correct rationale for minimum capacity as well as the instance distribution across AZs, so both these options are incorrect. The Auto Scaling group should be configured with the minimum capacity set to 4, with 2 instances each in two different AWS Regions. The maximum capacity of the Auto Scaling group should be set to 6 - An Auto Scaling group can contain Amazon EC2 instances in one or more Availability Zones within the same region. However, Auto Scaling groups cannot span multiple Regions. Reference: https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A solutions architect has been tasked to design a low-latency solution for a static, single-page application, accessed by users through a custom domain name. The solution must be serverless, provide in-transit data encryption and needs to be cost-effective. Which AWS services can be combined to build the simplest possible solution for the company's requirement?",
      "options": [
        "A. Host the application on Amazon EC2 instance with instance store volume for high performance and low latency access to users",
        "B. Configure Amazon S3 to store the static data and use AWS Fargate for hosting the application",
        "C. Use Amazon S3 to host the static website and Amazon CloudFront to distribute the content for low latency access",
        "D. Host the application on AWS Fargate and front it with Elastic Load Balancing for an improved performance"
      ],
      "correct_answer": "C",
      "explanation": "Correct option: Use Amazon S3 to host the static website and Amazon CloudFront to distribute the content for low latency access To host a static website on Amazon S3, you configure an Amazon S3 bucket for website hosting and then upload your website content to the bucket. When you configure a bucket as a static website, you must enable website hosting, set permissions, and create and add an index document. Depending on your website requirements, you can also configure redirects, web traffic logging, and a custom error document. After you configure your bucket as a static website, you can access the bucket through the AWS Region-specific Amazon S3 website endpoints for your bucket. Website endpoints are different from the endpoints where you send REST API requests. Amazon S3 doesn't support HTTPS access for website endpoints. If you want to use HTTPS, you can use CloudFront to serve a static website hosted on Amazon S3. You can use Amazon CloudFront to improve the performance of your website. CloudFront makes your website files (such as HTML, images, and video) available from data centers around the world (called edge locations). When a visitor requests a file from your website, Amazon CloudFront automatically redirects the request to a copy of the file at the nearest edge location. This results in faster download times than if the visitor had requested the content from a data center that is located farther away. Amazon CloudFront caches content at edge locations for a period of time that you specify. If a visitor requests content that has been cached for longer than the expiration date, Amazon CloudFront checks the origin server to see if a newer version of the content is available. If a newer version is available, Amazon CloudFront copies the new version to the edge location. Changes that you make to the original content are replicated to edge locations as visitors request the content. Incorrect options: Host the application on Amazon EC2 instance with instance store volume for high performance and low latency access to users - Since the use case speaks about a serverless solution, Amazon EC2 cannot be the answer, since Amazon EC2 is not serverless. Host the application on AWS Fargate and front it with Elastic Load Balancing for an improved performance - AWS Fargate is a serverless compute engine for containers that works with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS). Elastic Load Balancing can spread the incoming requests across a fleet of Amazon EC2 instances. This added complexity is not needed since we are looking at a static single-page webpage. Configure Amazon S3 to store the static data and use AWS Fargate for hosting the application - AWS Fargate is overkill for hosting a static single-page webpage. Reference: https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A gaming company is doing pre-launch testing for its new product. The company runs its production database on an Aurora MySQL DB cluster and the performance testing team wants access to multiple test databases that must be re-created from production data. The company has hired you as an AWS Certified Solutions Architect - Associate to deploy a solution to create these test databases quickly with the LEAST required effort. What would you suggest to address this use case?",
      "options": [
        "A. Use database cloning to create multiple clones of the production database and use each clone as a test database",
        "B. Enable database Backtracking on the production database and let the testing team use the production database",
        "C. Take a backup of the Aurora MySQL database instance using the mysqldump utility, create multiple new test database instances and restore each test database from the backup",
        "D. Set up binlog replication in the Aurora MySQL database instance to create multiple new test database instances"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Use database cloning to create multiple clones of the production database and use each clone as a test database You can quickly create clones of an Aurora DB by using the database cloning feature. In addition, database cloning uses a copy-on-write protocol, in which data is copied only at the time the data changes, either on the source database or the clone database. Cloning is much faster than a manual snapshot of the DB cluster. For the given use case, the most optimal solution is to clone the DB cluster. This would allow the performance testing team to have quick access to the production data in an isolated way. The team can iterate over the various test phases by deleting existing test databases and then cloning the production DB to create new test databases. You cannot clone databases across AWS regions. The clone databases must be created in the same region as the source databases. Currently, you are limited to 15 clones based on a copy, including clones based on other clones. After that, only copies can be created. However, each copy can also have up to 15 clones. via - https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html via - https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html Incorrect options: Enable database Backtracking on the production database and let the testing team use the production database - Using Backtracking, you can \"rewind\" the DB cluster to any time you specify. One of the major advantages of backtracking is that it can rewind the DB cluster much faster compared to restoring a DB cluster via point-in-time restore (PITR) or via a manual DB cluster snapshot, which can take hours. Backtracking a DB cluster doesn't require a new DB cluster and rewinds the DB cluster in minutes. However, as the given use-case is around pre-release testing, it does not make sense to use production DB itself for testing even if backtracking is enabled. The right solution is to use clones of the production DB for testing. via - https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Backtrack.html Take a backup of the Aurora MySQL database instance using the mysqldump utility, create multiple new test database instances and restore each test database from the backup - As the use-case mandates the least effort for database administration, therefore this option is not correct since using the mysqldump utility requires several manual steps to take a backup of a DB and restore into another DB. Set up binlog replication in the Aurora MySQL database instance to create multiple new test database instances - As the use-case mandates the least effort for database administration, therefore this option is not correct since using the binlog replication requires several steps such as creating a snapshot of your replication source, loading the snapshot into your replica target, etc. References: https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Backtrack.html https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Replication.MySQL.html",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A company hires experienced specialists to analyze the customer service calls attended by its call center representatives. Now, the company wants to move to AWS Cloud and is looking at an automated solution to analyze customer service calls for sentiment analysis via ad-hoc SQL queries. As a Solutions Architect, which of the following solutions would you recommend?",
      "options": [
        "A. Use Amazon Kinesis Data Streams to read the audio files and machine learning (ML) algorithms to convert the audio files into text and run customer sentiment analysis",
        "B. Use Amazon Kinesis Data Streams to read the audio files and Amazon Alexa to convert them into text. Amazon Kinesis Data Analytics can be used to analyze these files and Amazon Quicksight can be used to visualize and display the output",
        "C. Use Amazon Transcribe to convert audio files to text and Amazon Athena to perform SQL based analysis to understand the underlying customer sentiments",
        "D. Use Amazon Transcribe to convert audio files to text and Amazon Quicksight to perform SQL based analysis on these text files to understand the underlying patterns. Visualize and display them onto user Dashboards for reporting purposes"
      ],
      "correct_answer": "C",
      "explanation": "Correct option: Use Amazon Transcribe to convert audio files to text and Amazon Athena to perform SQL based analysis to understand the underlying customer sentiments Amazon Transcribe is an automatic speech recognition (ASR) service that makes it easy to convert audio to text. One key feature of the service is called speaker identification, which you can use to label each individual speaker when transcribing multi-speaker audio files. You can specify Amazon Transcribe to identify 2–10 speakers in the audio clip. Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run. To leverage Athena, you can simply point to your data in Amazon S3, define the schema, and start querying using standard SQL. Most results are delivered within seconds. Analyzing multi-speaker audio files using Amazon Transcribe and Amazon Athena: via - https://aws.amazon.com/blogs/machine-learning/automating-the-analysis-of-multi-speaker-audio-files-using-amazon-transcribe-and-amazon-athena Incorrect options: Use Amazon Kinesis Data Streams to read the audio files and machine learning (ML) algorithms to convert the audio files into text and run customer sentiment analysis - Amazon Kinesis can be used to stream real-time data for further analysis and storage. Kinesis Data Streams cannot read audio files. You will still need to use AWS Transcribe for ASR services. Use Amazon Kinesis Data Streams to read the audio files and Amazon Alexa to convert them into text. Amazon Kinesis Data Analytics can be used to analyze these files and Amazon Quicksight can be used to visualize and display the output - Amazon Kinesis Data Streams cannot read audio files. Amazon Alexa cannot be used as an Automatic Speech Recognition (ASR) service, though Alexa internally uses ASR for its working. Use Amazon Transcribe to convert audio files to text and Amazon Quicksight to perform SQL based analysis on these text files to understand the underlying patterns. Visualize and display them onto user Dashboards for reporting purposes - Amazon Quicksight is used for the visual representation of data through dashboards. However, it is not an SQL query based analysis tool like Amazon Athena. So, this option is incorrect. References: https://aws.amazon.com/blogs/machine-learning/automating-the-analysis-of-multi-speaker-audio-files-using-amazon-transcribe-and-amazon-athena https://aws.amazon.com/athena",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An application hosted on Amazon EC2 contains sensitive personal information about all its customers and needs to be protected from all types of cyber-attacks. The company is considering using the AWS Web Application Firewall (AWS WAF) to handle this requirement. Can you identify the correct solution leveraging the capabilities of AWS WAF?",
      "options": [
        "A. Create Amazon CloudFront distribution for the application on Amazon EC2 instances. Deploy AWS WAF on Amazon CloudFront to provide the necessary safety measures",
        "B. Configure an Application Load Balancer (ALB) to balance the workload for all the Amazon EC2 instances. Configure Amazon CloudFront to distribute from an Application Load Balancer since AWS WAF cannot be directly configured on ALB. This configuration not only provides necessary safety but is scalable too",
        "C. AWS WAF can be directly configured on Amazon EC2 instances for ensuring the security of the underlying application data",
        "D. AWS WAF can be directly configured only on an Application Load Balancer or an Amazon API Gateway. One of these two services can then be configured with Amazon EC2 to build the needed secure architecture"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Create Amazon CloudFront distribution for the application on Amazon EC2 instances. Deploy AWS WAF on Amazon CloudFront to provide the necessary safety measures When you use AWS WAF with Amazon CloudFront, you can protect your applications running on any HTTP webserver, whether it's a webserver that's running in Amazon Elastic Compute Cloud (Amazon EC2) or a web server that you manage privately. You can also configure Amazon CloudFront to require HTTPS between CloudFront and your own webserver, as well as between viewers and Amazon CloudFront. AWS WAF is tightly integrated with Amazon CloudFront and the Application Load Balancer (ALB), services that AWS customers commonly use to deliver content for their websites and applications. When you use AWS WAF on Amazon CloudFront, your rules run in all AWS Edge Locations, located around the world close to your end-users. This means security doesn’t come at the expense of performance. Blocked requests are stopped before they reach your web servers. When you use AWS WAF on Application Load Balancer, your rules run in the region and can be used to protect internet-facing as well as internal load balancers. Incorrect options: Configure an Application Load Balancer (ALB) to balance the workload for all the Amazon EC2 instances. Configure Amazon CloudFront to distribute from an Application Load Balancer since AWS WAF cannot be directly configured on ALB. This configuration not only provides necessary safety but is scalable too - This statement is wrong. You can configure AWS WAF on Application Load Balancers (ALB). AWS WAF can be directly configured on Amazon EC2 instances for ensuring the security of the underlying application data - AWS WAF can be deployed on Amazon CloudFront, the Application Load Balancer (ALB), and Amazon API Gateway. It cannot be configured directly on an Amazon EC2 instance. AWS WAF can be directly configured only on an Application Load Balancer or an Amazon API Gateway. One of these two services can then be configured with Amazon EC2 to build the needed secure architecture - This statement is only partially correct. AWS WAF can also be deployed on Amazon CloudFront service. References: https://aws.amazon.com/waf/faqs/ https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A company's cloud architect has set up a solution that uses Amazon Route 53 to configure the DNS records for the primary website with the domain pointing to the Application Load Balancer (ALB). The company wants a solution where users will be directed to a static error page, configured as a backup, in case of unavailability of the primary website. Which configuration will meet the company's requirements, while keeping the changes to a bare minimum?",
      "options": [
        "A. Set up Amazon Route 53 active-passive type of failover routing policy. If Amazon Route 53 health check determines the Application Load Balancer endpoint as unhealthy, the traffic will be diverted to a static error page, hosted on Amazon S3 bucket",
        "B. Set up Amazon Route 53 active-active type of failover routing policy. If Amazon Route 53 health check determines the Application Load Balancer endpoint as unhealthy, the traffic will be diverted to a static error page, hosted on Amazon S3 bucket",
        "C. Use Amazon Route 53 Latency-based routing. Create a latency record to point to the Amazon S3 bucket that holds the error page to be displayed",
        "D. Use Amazon Route 53 Weighted routing to give minimum weight to Amazon S3 bucket that holds the error page to be displayed. In case of primary failure, the requests get routed to the error page"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Set up Amazon Route 53 active-passive type of failover routing policy. If Amazon Route 53 health check determines the Application Load Balancer endpoint as unhealthy, the traffic will be diverted to a static error page, hosted on Amazon S3 bucket Use an active-passive failover configuration when you want a primary resource or group of resources to be available the majority of the time and you want a secondary resource or group of resources to be on standby in case all the primary resources become unavailable. When responding to queries, Amazon Route 53 includes only healthy primary resources. If all the primary resources are unhealthy, Route 53 begins to include only the healthy secondary resources in response to DNS queries. Incorrect options: Set up Amazon Route 53 active-active type of failover routing policy. If Amazon Route 53 health check determines the Application Load Balancer endpoint as unhealthy, the traffic will be diverted to a static error page, hosted on Amazon S3 bucket - This option has been added as a distractor as there is no such thing as an active-active failover routing policy in Amazon Route 53. You can configure active-active failover using any routing policy (or combination of routing policies) other than failover routing policy and you configure active-passive failover only using the failover routing policy. In active-active failover configuration, all the records that have the same name, the same type (such as A or AAAA), and the same routing policy (such as weighted or latency) are active unless Amazon Route 53 considers them unhealthy. Amazon Route 53 can respond to a DNS query using any healthy record. Use Amazon Route 53 Latency-based routing. Create a latency record to point to the Amazon S3 bucket that holds the error page to be displayed - If your application is hosted in multiple AWS Regions, you can improve performance for your users by serving their requests from the AWS Region that provides the lowest latency - this is Latency-based routing and is not helpful for the current use case. Use Amazon Route 53 Weighted routing to give minimum weight to Amazon S3 bucket that holds the error page to be displayed. In case of primary failure, the requests get routed to the error page - Weighted routing lets you associate multiple resources with a single domain name (example.com) or subdomain name (acme.example.com) and choose how much traffic is routed to each resource. This can be useful for a variety of purposes, including load balancing and testing new versions of the software. This is not useful for the current use case. References: https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-types.html#dns-failover-types-active-passive https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-latency",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An application with global users across AWS Regions had suffered an issue when the Elastic Load Balancing (ELB) in a Region malfunctioned thereby taking down the traffic with it. The manual intervention cost the company significant time and resulted in major revenue loss. What should a solutions architect recommend to reduce internet latency and add automatic failover across AWS Regions?",
      "options": [
        "A. Set up AWS Direct Connect as the backbone for each of the AWS Regions where the application is deployed",
        "B. Create Amazon S3 buckets in different AWS Regions and configure Amazon CloudFront to pick the nearest edge location to the user",
        "C. Set up an Amazon Route 53 geoproximity routing policy to route traffic",
        "D. Set up AWS Global Accelerator and add endpoints to cater to users in different geographic locations"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: Set up AWS Global Accelerator and add endpoints to cater to users in different geographic locations As your application architecture grows, so does the complexity, with longer user-facing IP lists and more nuanced traffic routing logic. AWS Global Accelerator solves this by providing you with two static IPs that are anycast from our globally distributed edge locations, giving you a single entry point to your application, regardless of how many AWS Regions it’s deployed in. This allows you to add or remove origins, Availability Zones or Regions without reducing your application availability. Your traffic routing is managed manually, or in console with endpoint traffic dials and weights. If your application endpoint has a failure or availability issue, AWS Global Accelerator will automatically redirect your new connections to a healthy endpoint within seconds. By using AWS Global Accelerator, you can: Associate the static IP addresses provided by AWS Global Accelerator to regional AWS resources or endpoints, such as Network Load Balancers, Application Load Balancers, EC2 Instances, and Elastic IP addresses. The IP addresses are anycast from AWS edge locations so they provide onboarding to the AWS global network close to your users. Easily move endpoints between Availability Zones or AWS Regions without needing to update your DNS configuration or change client-facing applications. Dial traffic up or down for a specific AWS Region by configuring a traffic dial percentage for your endpoint groups. This is especially useful for testing performance and releasing updates. Control the proportion of traffic directed to each endpoint within an endpoint group by assigning weights across the endpoints. AWS Global Accelerator for Multi-Region applications: via - https://aws.amazon.com/global-accelerator/ Incorrect options: Set up AWS Direct Connect as the backbone for each of the AWS Regions where the application is deployed - AWS Direct Connect can reduce latency to great extent. Direct Connect is used to connect on-premises systems to AWS Cloud for extremely low latency use cases. It cannot be used to serve users directly. Create Amazon S3 buckets in different AWS Regions and configure Amazon CloudFront to pick the nearest edge location to the user - If most of the content is static, we can configure Amazon CloudFront to improve performance. In the current scenario, the architecture has ELBs, Amazon EC2 instances too that need to be covered in the automatic failover plan. Set up an Amazon Route 53 geoproximity routing policy to route traffic - Geoproximity routing lets Amazon Route 53 route traffic to your resources based on the geographic location of your users and your resources. Unlike AWS Global Accelerator, managing and routing to different instances, ELBs and other AWS resources will become an operational overhead as the resource count reaches into the hundreds. With inbuilt features like Static anycast IP addresses, fault tolerance using network zones, Global performance-based routing, TCP Termination at the Edge - AWS Global Accelerator is the right choice for multi-region, low latency use cases. References: https://aws.amazon.com/global-accelerator/features/ https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-geoproximity",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A mobile chat application uses Amazon DynamoDB as its database service to provide low latency chat updates. A new developer has joined the team and is reviewing the configuration settings for Amazon DynamoDB which have been tweaked for certain technical requirements. AWS CloudTrail service has been enabled on all the resources used for the project. Yet, Amazon DynamoDB encryption details are nowhere to be found. Which of the following options can explain the root cause for the given issue?",
      "options": [
        "A. By default, all Amazon DynamoDB tables are encrypted under AWS managed Keys, which do not write to AWS CloudTrail logs",
        "B. By default, all Amazon DynamoDB tables are encrypted using AWS owned keys, which do not write to AWS CloudTrail logs",
        "C. By default, all Amazon DynamoDB tables are encrypted under Customer managed keys, which do not write to AWS CloudTrail logs",
        "D. By default, all Amazon DynamoDB tables are encrypted using Data keys, which do not write to AWS CloudTrail logs"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: By default, all Amazon DynamoDB tables are encrypted using AWS owned keys, which do not write to AWS CloudTrail logs AWS owned keys are not stored in your AWS account. They are part of a collection of KMS keys that AWS owns and manages for use in multiple AWS accounts. AWS services can use AWS owned keys to protect your data. AWS owned keys used by DynamoDB are rotated every year (approximately 365 days). You cannot view, manage, or use AWS owned keys, or audit their use. However, you do not need to do any work or change any programs to protect the keys that encrypt your data. You are not charged a monthly fee or a usage fee for use of AWS owned keys, and they do not count against AWS KMS quotas for your account. All DynamoDB tables are encrypted. There is no option to enable or disable encryption for new or existing tables. By default, all tables are encrypted under an AWS owned key in the DynamoDB service account. However, you can select an option to encrypt some or all of your tables under a customer managed key or the AWS managed key for DynamoDB in your account. Incorrect options: By default, all Amazon DynamoDB tables are encrypted under AWS managed Keys, which do not write to AWS CloudTrail logs By default, all Amazon DynamoDB tables are encrypted under Customer managed keys, which do not write to AWS CloudTrail logs By default, all Amazon DynamoDB tables are encrypted using Data keys, which do not write to AWS CloudTrail logs These three options contradict the explanation provided above, so these options are incorrect. References: https://docs.aws.amazon.com/kms/latest/developerguide/services-dynamodb.html https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#master_keys",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "You are a cloud architect at an IT company. The company has multiple enterprise customers that manage their own mobile applications that capture and send data to Amazon Kinesis Data Streams. They have been getting a ProvisionedThroughputExceededException exception. You have been contacted to help and upon analysis, you notice that messages are being sent one by one at a high rate. Which of the following options will help with the exception while keeping costs at a minimum?",
      "options": [
        "A. Use batch messages",
        "B. Decrease the Stream retention duration",
        "C. Increase the number of shards",
        "D. Use Exponential Backoff"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Use batch messages Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more. Amazon Kinesis Data Streams Overview: via - https://aws.amazon.com/kinesis/data-streams/ When a host needs to send many records per second (RPS) to Amazon Kinesis, simply calling the basic PutRecord API action in a loop is inadequate. To reduce overhead and increase throughput, the application must batch records and implement parallel HTTP requests. This will increase the efficiency overall and ensure you are optimally using the shards. Incorrect options: Use Exponential Backoff - While this may help in the short term, as soon as the request rate increases, you will see the ProvisionedThroughputExceededException exception again. Increase the number of shards - Increasing shards could be a short term fix but will substantially increase the cost, so this option is ruled out. Decrease the Stream retention duration - This operation may result in data loss and won't help with the exceptions, so this option is incorrect. References: https://aws.amazon.com/blogs/big-data/implementing-efficient-and-reliable-producers-with-the-amazon-kinesis-producer-library/ https://aws.amazon.com/kinesis/data-streams/",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A media company is evaluating the possibility of moving its IT infrastructure to the AWS Cloud. The company needs at least 10 terabytes of storage with the maximum possible I/O performance for processing certain files which are mostly large videos. The company also needs close to 450 terabytes of very durable storage for storing media content and almost double of it, i.e. 900 terabytes for archival of legacy data. As a Solutions Architect, which set of services will you recommend to meet these requirements?",
      "options": [
        "A. Amazon S3 standard storage for maximum performance, Amazon S3 Intelligent-Tiering for intelligent, durable storage, and Amazon S3 Glacier Deep Archive for archival storage",
        "B. Amazon EC2 instance store for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage",
        "C. Amazon EBS for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage",
        "D. Amazon EC2 instance store for maximum performance, AWS Storage Gateway for on-premises durable data access and Amazon S3 Glacier Deep Archive for archival storage"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Amazon EC2 instance store for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage An instance store provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host computer. Instance store is ideal for the temporary storage of information that changes frequently, such as buffers, caches, scratch data, and other temporary content, or for data that is replicated across a fleet of instances, such as a load-balanced pool of web servers. You can specify instance store volumes for an instance only when you launch it. You can't detach an instance store volume from one instance and attach it to a different instance. Some instance types use NVMe or SATA-based solid-state drives (SSD) to deliver high random I/O performance. This is a good option when you need storage with very low latency, but you don't need the data to persist when the instance terminates or you can take advantage of fault-tolerant architectures. Amazon S3 Standard offers high durability, availability, and performance object storage for frequently accessed data. Because it delivers low latency and high throughput, Amazon S3 Standard is appropriate for a wide variety of use cases, including cloud applications, dynamic websites, content distribution, mobile and gaming applications, and big data analytics. Amazon S3 Glacier is a secure, durable, and low-cost storage class for data archiving. You can reliably store any amount of data at costs that are competitive with or cheaper than on-premises solutions. To keep costs low yet suitable for varying needs, Amazon S3 Glacier provides three retrieval options that range from a few minutes to hours. You can upload objects directly to Amazon S3 Glacier, or use S3 Lifecycle policies to transfer data between any of the Amazon S3 Storage Classes for active data (S3 Standard, S3 Intelligent-Tiering, S3 Standard-IA, and S3 One Zone-IA) and S3 Glacier. Incorrect options: Amazon S3 standard storage for maximum performance, Amazon S3 Intelligent-Tiering for intelligent, durable storage, and Amazon S3 Glacier Deep Archive for archival storage - Amazon EC2 instance store volumes provide the best I/O performance for low latency requirement, as in the current use case. The Amazon S3 Intelligent-Tiering storage class is designed to optimize costs by automatically moving data to the most cost-effective access tier, without performance impact or operational overhead. Amazon S3 Glacier Deep Archive is Amazon S3’s lowest-cost storage class and supports long-term retention and digital preservation for data that may be accessed once or twice a year. It is designed for customers — particularly those in highly-regulated industries, such as the Financial Services, Healthcare, and Public Sectors — that retain data sets for 7-10 years or longer to meet regulatory compliance requirements. Amazon EBS for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage - Amazon Elastic Block Store (Amazon EBS) provides block-level storage volumes for use with EC2 instances. Amazon EBS volumes are particularly well-suited for use as the primary storage for file systems, databases, or for any applications that require fine granular updates and access to raw, unformatted, block-level storage. For high I/O performance, instance store volumes are a better option. Amazon EC2 instance store for maximum performance, AWS Storage Gateway for on-premises durable data access and Amazon S3 Glacier Deep Archive for archival storage - AWS Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage. AWS Storage Gateway will be the right answer if the customer wanted to retain the on-premises data storage and just move the applications to AWS Cloud. In the absence of such requirements, instance store is a better option for high performance and Amazon S3 for durable storage. Reference: https://aws.amazon.com/s3/storage-classes/",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "The content division at a digital media agency has an application that generates a large number of files on Amazon S3, each approximately 10 megabytes in size. The agency mandates that the files be stored for 5 years before they can be deleted. The files are frequently accessed in the first 30 days of the object creation but are rarely accessed after the first 30 days. The files contain critical business data that is not easy to reproduce, therefore, immediate accessibility is always required. Which solution is the MOST cost-effective for the given use case?",
      "options": [
        "A. Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 Glacier Flexible Retrieval 30 days after object creation. Delete the files 5 years after object creation",
        "B. Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 Standard-IA 30 days after object creation. Archive the files to Amazon S3 Glacier Deep Archive 5 years after object creation",
        "C. Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 One Zone-IA 30 days after object creation. Delete the files 5 years after object creation",
        "D. Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 Standard-IA 30 days after object creation. Delete the files 5 years after object creation"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 Standard-IA 30 days after object creation. Delete the files 5 years after object creation Amazon S3 Standard-IA class is for data that is accessed less frequently but requires rapid access when needed. Amazon S3 Standard-IA offers the high durability, high throughput, and low latency of S3 Standard, with a low per gigabyte storage price and per GB retrieval charge. via - https://aws.amazon.com/s3/storage-classes/ For the given use case, you can set up an Amazon S3 lifecycle configuration and create a transition action to move objects from Amazon S3 Standard to Amazon S3 Standard-IA 30 days after object creation. You can set up an expiration action to delete the object 5 years after object creation. via - https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html via - https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html Incorrect options: Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 Glacier Flexible Retrieval 30 days after object creation. Delete the files 5 years after object creation - Amazon S3 Glacier Flexible Retrieval storage class has the best case retrieval time of the order of minutes, so this option is incorrect for the given requirement. Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 Standard-IA 30 days after object creation. Archive the files to Amazon S3 Glacier Deep Archive 5 years after object creation - The files can simply be deleted 5 years after object creation instead of archiving the files to Amazon S3 Glacier Deep Archive. There is no need to incur the cost of archival. Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 One Zone-IA 30 days after object creation. Delete the files 5 years after object creation - Unlike other Amazon S3 Storage Classes which store data in a minimum of three Availability Zones (AZs), Amazon S3 One Zone-IA stores data in a single AZ and costs 20% less than Amazon S3 Standard-IA. Amazon S3 One Zone-IA is a good choice for storing secondary backup copies of on-premises data or easily re-creatable data. The given scenario clearly states that the business-critical data is not easy to reproduce, so this option is incorrect. References: https://aws.amazon.com/s3/storage-classes/ https://aws.amazon.com/s3/storage-classes/glacier/ https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A startup has created a cost-effective backup solution in another AWS Region. The application is running in warm standby mode and has Application Load Balancer (ALB) to support it from the front. The current failover process is manual and requires updating the DNS alias record to point to the secondary Application Load Balancer in another Region in case of failure of the primary Application Load Balancer. As a Solutions Architect, what will you recommend to automate the failover process?",
      "options": [
        "A. Enable an ALB health check",
        "B. Enable an Amazon EC2 instance health check",
        "C. Configure AWS Trusted Advisor to check on unhealthy instances",
        "D. Enable an Amazon Route 53 health check"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: Enable an Amazon Route 53 health check Determining the health of an ELB endpoint is more complex than health checking a single IP address. For example, what if your application is running fine on Amazon EC2, but the load balancer itself isn't reachable? Or if your load balancer and your Amazon EC2 instances are working correctly, but a bug in your code causes your application to crash? Or how about if the Amazon EC2 instances in one Availability Zone of a multi-AZ ELB are experiencing problems? Amazon Route 53 DNS Failover handles all of these failure scenarios by integrating with ELB behind the scenes. Once enabled, Route 53 automatically configures and manages health checks for individual ELB nodes. Amazon Route 53 also takes advantage of the Amazon EC2 instance health checking that ELB performs (information on configuring your ELB health checks is available here). By combining the results of health checks of your Amazon EC2 instances and your ELBs, Amazon Route 53 DNS Failover can evaluate the health of the load balancer and the health of the application running on the Amazon EC2 instances behind it. In other words, if any part of the stack goes down, Amazon Route 53 detects the failure and routes traffic away from the failed endpoint. Using Amazon Route 53 DNS Failover, you can run your primary application simultaneously in multiple AWS regions around the world and failover across regions. Your end-users will be routed to the closest (by latency), healthy region for your application. Amazon Route 53 automatically removes from service any region where your application is unavailable - it will pull an endpoint out of service if there is region-wide connectivity or operational issue, if your application goes down in that region, or if your ELB or Amazon EC2 instances go down in that region. Incorrect options: Enable an ALB health check - ELB health check verifies that a specified TCP port on an instance is accepting connections or a specified page has returned an error code of 200. It is not useful for the given failover scenario. Enable an Amazon EC2 instance health check - Instance status checks monitor the software and network configuration of your instance. It is not intelligent enough to understand if the application on the instance is working correctly. Hence, this is not the right choice for the given use-case. Configure AWS Trusted Advisor to check on unhealthy instances - AWS Trusted Advisor examines the health check configuration for Auto Scaling groups. If Elastic Load Balancing is being used for an Auto Scaling group, the recommended configuration is to enable an Elastic Load Balancing health check. AWS Trusted Advisor recommends certain configuration changes by comparing your system configurations to AWS Best practices. It cannot handle a failover the way Amazon Route 53 does. References: https://aws.amazon.com/blogs/aws/amazon-route-53-elb-integration-dns-failover/ https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-system-instance-status-check.html https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html https://aws.amazon.com/premiumsupport/technology/trusted-advisor/best-practice-checklist/",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A medical devices company uses Amazon S3 buckets to store critical data. Hundreds of buckets are used to keep the data segregated and well organized. Recently, the development team noticed that the lifecycle policies on the Amazon S3 buckets have not been applied optimally, resulting in higher costs. As a Solutions Architect, can you recommend a solution to reduce storage costs on Amazon S3 while keeping the IT team's involvement to a minimum?",
      "options": [
        "A. Configure Amazon EFS to provide a fast, cost-effective and sharable storage service",
        "B. Use Amazon S3 Intelligent-Tiering storage class to optimize the Amazon S3 storage costs",
        "C. Use Amazon S3 One Zone-Infrequent Access, to reduce the costs on Amazon S3 storage",
        "D. Use Amazon S3 Outposts storage class to reduce the costs on Amazon S3 storage by storing the data on-premises"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Use Amazon S3 Intelligent-Tiering storage class to optimize the Amazon S3 storage costs The Amazon S3 Intelligent-Tiering storage class is designed to optimize costs by automatically moving data to the most cost-effective access tier, without performance impact or operational overhead. It works by storing objects in two access tiers: one tier that is optimized for frequent access and another lower-cost tier that is optimized for infrequent access. For a small monthly monitoring and automation fee per object, Amazon S3 monitors access patterns of the objects in Amazon S3 Intelligent-Tiering and moves the ones that have not been accessed for 30 consecutive days to the infrequent access tier. If an object in the infrequent access tier is accessed, it is automatically moved back to the frequent access tier. There are no retrieval fees when using the Amazon S3 Intelligent-Tiering storage class, and no additional tiering fees when objects are moved between access tiers. It is the ideal storage class for long-lived data with access patterns that are unknown or unpredictable. Amazon S3 Storage Classes can be configured at the object level and a single bucket can contain objects stored in Amazon S3 Standard, Amazon S3 Intelligent-Tiering, Amazon S3 Standard-IA, and Amazon S3 One Zone-IA. You can upload objects directly to Amazon S3 Intelligent-Tiering, or use S3 Lifecycle policies to transfer objects from Amazon S3 Standard and Amazon S3 Standard-IA to Amazon S3 Intelligent-Tiering. You can also archive objects from Amazon S3 Intelligent-Tiering to Amazon S3 Glacier. Incorrect options: Configure Amazon EFS to provide a fast, cost-effective and sharable storage service - Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. Amazon EFS offers sharable service, unlike Amazon Elastic Block Storage (EBS) that cannot be shared by instances. Amazon EFS is costlier than storing data in Amazon S3. Also, Amazon EFS needs an Amazon EC2 instance or an AWS Direct Connect network connection. Hence, this is not the correct option. Use Amazon S3 One Zone-Infrequent Access, to reduce the costs on Amazon S3 storage - Amazon S3 One Zone-IA is for data that is accessed less frequently but requires rapid access when needed. Unlike other Amazon S3 Storage Classes which store data in a minimum of three Availability Zones (AZs), Amazon S3 One Zone-IA stores data in a single AZ and costs 20% less than Amazon S3 Standard-IA. Amazon S3 One Zone-IA is ideal for customers who want a lower-cost option for infrequently accessed data but do not require the availability and resilience of Amazon S3 Standard or Amazon S3 Standard-IA. Not a right option, since data stored is business-critical and cannot be risked by using Amazon S3 One Zone-IA. Use Amazon S3 Outposts storage class to reduce the costs on Amazon S3 storage by storing the data on-premises - This is a distractor as Amazon S3 on Outposts (S3 Outposts) delivers object storage to your on-premises AWS Outposts environment. It is used in conjunction with AWS Outposts and has no relevance to the current use case. Reference: https://aws.amazon.com/s3/storage-classes/",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A leading media company wants to do an accelerated online migration of hundreds of terabytes of files from their on-premises data center to Amazon S3 and then establish a mechanism to access the migrated data for ongoing updates from the on-premises applications. As a solutions architect, which of the following would you select as the MOST performant solution for the given use-case?",
      "options": [
        "A. Use AWS DataSync to migrate existing data to Amazon S3 as well as access the Amazon S3 data for ongoing updates",
        "B. Use File Gateway configuration of AWS Storage Gateway to migrate data to Amazon S3 and then use Amazon S3 Transfer Acceleration (Amazon S3TA) for ongoing updates from the on-premises applications",
        "C. Use Amazon S3 Transfer Acceleration (Amazon S3TA) to migrate existing data to Amazon S3 and then use AWS DataSync for ongoing updates from the on-premises applications",
        "D. Use AWS DataSync to migrate existing data to Amazon S3 and then use File Gateway to retain access to the migrated data for ongoing updates from the on-premises applications"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: Use AWS DataSync to migrate existing data to Amazon S3 and then use File Gateway to retain access to the migrated data for ongoing updates from the on-premises applications AWS DataSync is an online data transfer service that simplifies, automates, and accelerates copying large amounts of data to and from AWS storage services over the internet or AWS Direct Connect. AWS DataSync fully automates and accelerates moving large active datasets to AWS, up to 10 times faster than command-line tools. It is natively integrated with Amazon S3, Amazon EFS, Amazon FSx for Windows File Server, Amazon CloudWatch, and AWS CloudTrail, which provides seamless and secure access to your storage services, as well as detailed monitoring of the transfer. DataSync uses a purpose-built network protocol and scale-out architecture to transfer data. A single AWS DataSync agent is capable of saturating a 10 Gbps network link. AWS DataSync fully automates the data transfer. It comes with retry and network resiliency mechanisms, network optimizations, built-in task scheduling, monitoring via the AWS DataSync API and Console, and Amazon CloudWatch metrics, events, and logs that provide granular visibility into the transfer process. AWS DataSync performs data integrity verification both during the transfer and at the end of the transfer. How AWS DataSync Works: via - https://aws.amazon.com/datasync/ AWS Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage. The service provides three different types of gateways – Tape Gateway, File Gateway, and Volume Gateway – that seamlessly connect on-premises applications to cloud storage, caching data locally for low-latency access. File gateway offers SMB or NFS-based access to data in Amazon S3 with local caching. The combination of AWS DataSync and File Gateway is the correct solution. AWS DataSync enables you to automate and accelerate online data transfers to AWS storage services. File Gateway then provides your on-premises applications with low latency access to the migrated data. Incorrect options: Use AWS DataSync to migrate existing data to Amazon S3 as well as access the Amazon S3 data for ongoing updates - AWS DataSync is used to easily transfer data to and from AWS with up to 10x faster speeds. It is used to transfer data and cannot be used to facilitate ongoing updates to the migrated files from the on-premises applications. Use File Gateway configuration of AWS Storage Gateway to migrate data to Amazon S3 and then use Amazon S3 Transfer Acceleration (Amazon S3TA) for ongoing updates from the on-premises applications - File Gateway can be used to move on-premises data to AWS Cloud, but it not an optimal solution for high volumes. Migration services such as AWS DataSync are best suited for this purpose. Amazon S3 Transfer Acceleration cannot facilitate ongoing updates to the migrated files from the on-premises applications. Use Amazon S3 Transfer Acceleration (Amazon S3TA) to migrate existing data to Amazon S3 and then use AWS DataSync for ongoing updates from the on-premises applications - If your application is already integrated with the Amazon S3 API, and you want higher throughput for transferring large files to Amazon S3, Amazon S3 Transfer Acceleration can be used. However AWS DataSync cannot be used to facilitate ongoing updates to the migrated files from the on-premises applications. Reference: https://aws.amazon.com/datasync/features/",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A multi-national company is looking at optimizing their AWS resources across various countries and regions. They want to understand the best practices on cost optimization, performance, and security for their system architecture spanning across multiple business units. Which AWS service is the best fit for their requirements?",
      "options": [
        "A. AWS Config",
        "B. AWS Trusted Advisor",
        "C. AWS Management Console",
        "D. AWS Systems Manager"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: AWS Trusted Advisor AWS Trusted Advisor is an online tool that draws upon best practices learned from AWS’s aggregated operational history of serving hundreds of thousands of AWS customers. AWS Trusted Advisor inspects your AWS environment and makes recommendations for saving money, improving system performance, or closing security gaps. It scans your AWS infrastructure and compares it to AWS Best practices in five categories (Cost Optimization, Performance, Security, Fault Tolerance, Service limits) and then provides recommendations. How AWS Trusted Advisor Works: via - https://aws.amazon.com/premiumsupport/technology/trusted-advisor/ Incorrect options: AWS Config - AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. With Config, you can review changes in configurations and relationships between AWS resources, dive into detailed resource configuration histories, and determine your overall compliance against the configurations specified in your internal guidelines. You can use Config to answer questions such as - “What did my AWS resource look like at xyz point in time?”. It does not offer any feedback about architectural best practices. AWS Management Console - The AWS Management Console is a web application that comprises and refers to a broad collection of service consoles for managing Amazon Web Services. You log into your AWS account using the AWS Management console. It does not offer any feedback about architectural best practices. AWS Systems Manager - AWS Systems Manager is an AWS service that you can use to view and control your infrastructure on AWS. Using the Systems Manager console, you can view operational data from multiple AWS services and automate operational tasks across your AWS resources. With Systems Manager, you can group resources, like Amazon EC2 instances, Amazon S3 buckets, or Amazon RDS instances, by application, view operational data for monitoring and troubleshooting, and take action on your groups of resources. It does not offer any feedback about architectural best practices. Reference: https://aws.amazon.com/premiumsupport/technology/trusted-advisor/",
      "reference": "Source: Practice Test #5 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A software engineering intern at a company is documenting the features offered by Amazon EC2 Spot instances and Spot fleets. Can you help the intern by selecting the correct options that identify the key characteristics of these two types of Spot entities? (Select two)",
      "options": [
        "A. Spot instances are spare Amazon EC2 capacity that can save you up 90% off of On-Demand prices. Spot instances can be interrupted by Amazon EC2 for capacity requirements with a 2-minute notification",
        "B. Spot fleets allow you to request Amazon EC2 Spot instances for 1 to 6 hours at a time to avoid being interrupted",
        "C. A Spot fleet can consist of a set of Spot Instances and optionally On-Demand Instances that are launched to meet your target capacity",
        "D. A Spot fleet can only consist of a set of Spot Instances that are launched to meet your target capacity",
        "E. Spot fleets are spare EC2 capacity that can save you up 90% off of On-Demand prices. Spot fleets are usually interrupted by Amazon EC2 for capacity requirements with a 2-minute notification"
      ],
      "correct_answer": "A",
      "explanation": "Correct options: Spot instances are spare Amazon EC2 capacity that can save you up 90% off of On-Demand prices. Spot instances can be interrupted by Amazon EC2 for capacity requirements with a 2-minute notification Spot instances are spare Amazon EC2 capacity that can save you up 90% off of On-Demand prices that Amazon Web Services can interrupt with a 2-minute notification. Because Spot Instances enable you to request unused EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly. Spot Instances are a cost-effective choice if you can be flexible about when your applications run and if your applications can be interrupted. A Spot fleet can consist of a set of Spot Instances and optionally On-Demand Instances that are launched to meet your target capacity A Spot fleet is a collection, or fleet, of Spot Instances, and optionally On-Demand Instances. The Spot fleet attempts to launch the number of Spot Instances and On-Demand Instances to meet the target capacity that you specified in the Spot fleet request. A Spot fleet allows you to automatically request and manage multiple Spot instances that provide the lowest price per unit of capacity for your cluster or application, like a batch processing job, a Hadoop workflow, or an HPC grid computing job. via - https://docs.amazonaws.cn/en_us/AWSEC2/latest/UserGuide/how-spot-fleet-works.html Incorrect options: A Spot fleet can only consist of a set of Spot Instances that are launched to meet your target capacity Spot fleets are spare EC2 capacity that can save you up 90% off of On-Demand prices. Spot fleets are usually interrupted by Amazon EC2 for capacity requirements with a 2-minute notification These two options contradict the explanation provided above, so these options are incorrect. Spot fleets allow you to request Amazon EC2 Spot instances for 1 to 6 hours at a time to avoid being interrupted - You could use Spot blocks (now deprecated) to request Amazon EC2 Spot instances for 1 to 6 hours to avoid being interrupted. So, Spot fleets cannot be used for this purpose. References: https://www.amazonaws.cn/en/ec2/spot-instances/faqs/ https://docs.amazonaws.cn/en_us/AWSEC2/latest/UserGuide/how-spot-fleet-works.html",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A retail company's procurement application becomes slow when traffic spikes. The application has a three-tier architecture (web, application and database tier) that uses synchronous transactions. The engineering team at the company has identified certain bottlenecks in the application tier but it does not want to change the underlying application architecture. As a solutions architect, which of the following solutions would you suggest to meet the required application response times while accounting for any traffic spikes?",
      "options": [
        "A. Leverage horizontal scaling for the web and application tiers by using Auto Scaling groups and Application Load Balancer",
        "B. Leverage Amazon SQS with asynchronous AWS Lambda calls to decouple the application and data tiers",
        "C. Leverage horizontal scaling for the application's persistence layer by adding Oracle RAC on AWS",
        "D. Leverage vertical scaling for the application instance by provisioning a larger Amazon EC2 instance size"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Leverage horizontal scaling for the web and application tiers by using Auto Scaling groups and Application Load Balancer A horizontally scalable system is one that can increase capacity by adding more computers to the system. This is in contrast to a vertically scalable system, which is constrained to running its processes on only one computer; in such systems, the only way to increase performance is to add more resources into one computer in the form of faster (or more) CPUs, memory or storage. Horizontally scalable systems are oftentimes able to outperform vertically scalable systems by enabling parallel execution of workloads and distributing those across many different computers. Elastic Load Balancing is used to automatically distribute your incoming application traffic across all the Amazon EC2 instances that you are running. You can use Elastic Load Balancing to manage incoming requests by optimally routing traffic so that no one instance is overwhelmed. To use Elastic Load Balancing with your Auto Scaling group, you attach the load balancer to your Auto Scaling group to register the group with the load balancer. Your load balancer acts as a single point of contact for all incoming web traffic to your Auto Scaling group. When you use Elastic Load Balancing with your Auto Scaling group, it's not necessary to register individual Amazon EC2 instances with the load balancer. Instances that are launched by your Auto Scaling group are automatically registered with the load balancer. Likewise, instances that are terminated by your Auto Scaling group are automatically deregistered from the load balancer. This option will require fewer design changes, it's mostly configuration changes and the ability for the web/application tier to be able to communicate across instances. Hence, this is the right solution for the current use case. Incorrect options: Leverage Amazon SQS with asynchronous AWS Lambda calls to decouple the application and data tiers - This is incorrect as it uses asynchronous AWS Lambda calls and the application uses synchronous transactions. The question says there should be no change in the application architecture. Leverage horizontal scaling for the application's persistence layer by adding Oracle RAC on AWS - The issue is not with the persistence layer at all. This option has only been used as a distractor. You can deploy scalable Oracle Real Application Clusters (RAC) on Amazon EC2 using Amazon Machine Images (AMI) on AWS Marketplace. Oracle RAC is a shared-everything database cluster technology from Oracle that allows a single database (a set of data files) to be concurrently accessed and served by one or many database server instances. Leverage vertical scaling for the application instance by provisioning a larger Amazon EC2 instance size - Vertical scaling is just a band-aid solution and will not work long term. References: https://docs.aws.amazon.com/autoscaling/ec2/userguide/autoscaling-load-balancer.html https://aws.amazon.com/blogs/compute/operating-lambda-understanding-event-driven-architecture-part-1/",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A company uses a legacy on-premises reporting application that operates on gigabytes of .json files and represents years of data. The legacy application cannot handle the growing size of .json files. New .json files are added daily from various data sources to a central on-premises storage location. The company wants to continue to support the legacy application. The company has hired you as a solutions architect to build a solution that can manage ongoing data updates from your on-premises application to Amazon S3. Which of the following solutions would you suggest to address the given requirement?",
      "options": [
        "A. Set up an on-premises file gateway. Configure data sources to write the .json files to the file gateway. Point the legacy analytics application to the file gateway. The file gateway should replicate the .json files to Amazon S3",
        "B. Set up AWS DataSync on-premises. Configure AWS DataSync to continuously replicate the .json files between the company's on-premises storage and the company's Amazon S3 bucket",
        "C. Set up an on-premises volume gateway. Configure data sources to write the .json files to the volume gateway. Point the legacy analytics application to the volume gateway. The volume gateway should replicate data to Amazon S3",
        "D. Set up AWS DataSync on-premises. Configure AWS DataSync to continuously replicate the .json files between on-premises and Amazon Elastic File System (Amazon EFS). Enable replication from Amazon EFS to the company's Amazon S3 bucket"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Set up an on-premises file gateway. Configure data sources to write the .json files to the file gateway. Point the legacy analytics application to the file gateway. The file gateway should replicate the .json files to Amazon S3 A file gateway provides a simple solution for presenting one or more Amazon S3 buckets and their objects as a mountable NFS or SMB file share to one or more clients on-premises. The file gateway is deployed as a virtual machine in VMware ESXi or Microsoft Hyper-V environments on-premises, or in an Amazon Elastic Compute Cloud (Amazon EC2) instance in AWS. File gateway can also be deployed in data center and remote office locations on a Storage Gateway hardware appliance. When deployed, file gateway provides a seamless connection between on-premises NFS (v3.0 or v4.1) or SMB (v1 or v2) clients—typically applications—and Amazon S3 buckets hosted in a given AWS Region. The file gateway employs a local read/write cache to provide low-latency access to data for file share clients in the same local area network (LAN) as the file gateway. A bucket share consists of a file share hosted from a file gateway across a single Amazon S3 bucket. The file gateway virtual machine appliance currently supports up to 10 bucket shares. File Gateway Architecture: via - https://docs.aws.amazon.com/whitepapers/latest/file-gateway-hybrid-cloud-storage-architectures/file-gateway-architecture.html Incorrect options: Set up an on-premises volume gateway. Configure data sources to write the .json files to the volume gateway. Point the legacy analytics application to the volume gateway. The volume gateway should replicate data to Amazon S3 - The Volume Gateway provides block storage to your on-premises applications using iSCSI connectivity. Data on the volumes is stored in Amazon S3 and you can take point in time copies of volumes that are stored in AWS as Amazon EBS snapshots. Volume Gateway is for block storage and not for file storage, so it is not the right option. Set up AWS DataSync on-premises. Configure AWS DataSync to continuously replicate the .json files between the company's on-premises storage and the company's Amazon S3 bucket Set up AWS DataSync on-premises. Configure AWS DataSync to continuously replicate the .json files between on-premises and Amazon Elastic File System (Amazon EFS). Enable replication from Amazon EFS to the company's Amazon S3 bucket AWS recommends that you should use AWS DataSync to migrate existing data to Amazon S3, and subsequently use the File Gateway configuration of AWS Storage Gateway to retain access to the migrated data and for ongoing updates from your on-premises file-based applications. Therefore, both these options are incorrect, as they use DataSync for ongoing replication. Reference: https://docs.aws.amazon.com/whitepapers/latest/file-gateway-hybrid-cloud-storage-architectures/file-gateway-architecture.html",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A company has noticed several provisioned throughput exceptions on its Amazon DynamoDB database due to major spikes in the writes to the database. The development team wants to decouple the application layer from the database layer and dedicate a worker process to writing the data to Amazon DynamoDB. Which middleware do you recommend on using that can scale infinitely and meet these requirements in the most cost effective way?",
      "options": [
        "A. Amazon DynamoDB DAX",
        "B. Amazon Kinesis Data Streams",
        "C. Amazon Simple Notification Service (Amazon SNS)",
        "D. Amazon Simple Queue Service (Amazon SQS)"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: Amazon Simple Queue Service (Amazon SQS) Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. Amazon SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. Amazon SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent. Using Amazon SQS as a middleware will help us sustain the write throughput during write peaks and therefore this option is the best fit for the given use-case. Incorrect options: Amazon DynamoDB DAX - Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement – from milliseconds to microseconds – even at millions of requests per second. DAX does all the heavy lifting required to add in-memory acceleration to your DynamoDB tables, without requiring developers to manage cache invalidation, data population, or cluster management. DAX is used for caching reads, not to help with writes. So this option is ruled out. Amazon Kinesis Data Streams - Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The throughput of an Amazon Kinesis data stream is designed to scale without limits via increasing the number of shards within a data stream. Kinesis is used to process consistent real-time data and does not scale as cost effectively as SQS to handle spikes in traffic. Amazon Simple Notification Service (Amazon SNS) - Amazon Simple Notification Service (Amazon SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. Amazon SNS won't keep our data if it cannot be delivered, so this option is incorrect. References: https://aws.amazon.com/sqs/ https://aws.amazon.com/kinesis/data-streams/faqs/",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A team has around 200 users, each of these having an IAM user account in AWS. Currently, they all have read access to an Amazon S3 bucket. The team wants 50 among them to have write and read access to the buckets. How can you provide these users access in the least possible time, with minimal changes?",
      "options": [
        "A. Update the Amazon S3 bucket policy",
        "B. Create a group, attach the policy to the group and place the users in the group",
        "C. Create a policy and assign it manually to the 50 users",
        "D. Create an AWS Multi-Factor Authentication (AWS MFA) user with read / write access and link 50 IAM with AWS MFA"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Create a group, attach the policy to the group and place the users in the group An IAM group is a collection of IAM users. You can use groups to specify permissions for a collection of users, which can make those permissions easier to manage for those users. For example, you could have a group called Admins and give that group the types of permissions that administrators typically need. Any user in that group automatically has the permissions that are assigned to the group. If a new user joins your organization and should have administrator privileges, you can assign the appropriate permissions by adding the user to that group. Here creating a group, assigning users to that group and attaching policies to that group is the best way. Incorrect options: Update the Amazon S3 bucket policy - Updating the Amazon S3 bucket policy could work but would not scale, as the size of the S3 bucket policy is limited (Bucket policies are limited to 20 KB in size). Create a policy and assign it manually to the 50 users - An IAM user is an entity that you create in AWS. The IAM user represents the person or service who uses the IAM user to interact with AWS. Primary use for IAM users is to give people the ability to sign in to the AWS Management Console for interactive tasks and to make programmatic requests to AWS services using the API or CLI. A user in AWS consists of a name, a password to sign in to the AWS Management Console, and up to two access keys that can be used with the API or CLI. A policy is an object in AWS that, when associated with an identity or resource, defines their permissions. AWS evaluates these policies when an IAM principal (user or role) makes a request. Permissions in the policies determine whether the request is allowed or denied. Identity-based policies – Attach managed and inline policies to IAM identities (users, groups to which users belong, or roles). Identity-based policies grant permissions to an identity. Resource-based policies – Attach inline policies to resources. The most common examples of resource-based policies are Amazon S3 bucket policies and IAM role trust policies. Resource-based policies grant permissions to the principal that is specified in the policy. Principals can be in the same account as the resource or in other accounts. Creating a policy and assigning it manually to users would work but would be hard to scale and manage. Create an AWS Multi-Factor Authentication (AWS MFA) user with read / write access and link 50 IAM with AWS MFA - AWS MFA adds extra security because it requires users to provide unique authentication from an AWS supported MFA mechanism in addition to their regular sign-in credentials when they access AWS websites or services. AWS MFA cannot help in terms of granting read/write access to only 50 of the IAM users. References: https://docs.aws.amazon.com/IAM/latest/UserGuide/id.html https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "The engineering team at a company wants to create a daily big data analysis job leveraging Spark for analyzing online/offline sales and customer loyalty data to create customized reports on a client-by-client basis. The big data analysis job needs to read the data from Amazon S3 and output it back to Amazon S3. Which technology do you recommend to run the Big Data analysis job? (Select two)",
      "options": [
        "A. Amazon Redshift",
        "B. AWS Glue",
        "C. Amazon EMR",
        "D. Amazon Athena",
        "E. AWS Batch"
      ],
      "correct_answer": "B",
      "explanation": "Correct options: Amazon EMR Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. With EMR you can run Petabyte-scale analysis at less than half of the cost of traditional on-premises solutions and over 3x faster than standard Apache Spark. EMR is used for launching Hadoop / Spark clusters. For short-running jobs, you can spin up and spin down clusters and pay per second for the instances used. For long-running workloads, you can create highly available clusters that automatically scale to meet demand. Amazon EMR uses Hadoop, an open-source framework, to distribute your data and processing across a resizable cluster of Amazon EC2 instances. AWS Glue AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. AWS Glue job is meant to be used for batch ETL data processing. AWS Glue ETL jobs can use Amazon S3, data stores in a VPC, or on-premises JDBC data stores as a source. AWS Glue jobs extract data, transform it, and load the resulting data back to S3, data stores in a VPC, or on-premises JDBC data stores as a target. Incorrect options: Amazon Redshift - Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large scale data set storage and analysis. An Amazon Redshift data warehouse is a collection of computing resources called nodes, which are organized into a group called a cluster. Each cluster runs an Amazon Redshift engine and contains one or more databases. An Amazon Redshift cluster consists of nodes. Each cluster has a leader node and one or more compute nodes. The leader node receives queries from client applications, parses the queries, and develops query execution plans. The leader node then coordinates the parallel execution of these plans with the compute nodes and aggregates the intermediate results from these nodes. It then finally returns the results to the client applications. Amazon Athena - Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to set up or manage, and customers pay only for the queries they run. You can use Athena to process logs, perform ad-hoc analysis, and run interactive queries. AWS Batch - AWS Batch can be used to plan, schedule, and execute your batch computing workloads on Amazon EC2 Instances. AWS Batch dynamically provisions the optimal quantity and type of compute resources (e.g., CPU or memory optimized compute resources) based on the volume and specific resource requirements of the batch jobs submitted. References: https://aws.amazon.com/emr/ https://aws.amazon.com/blogs/big-data/how-to-access-and-analyze-on-premises-data-stores-using-aws-glue/",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An e-commerce website is migrating towards a microservices-based approach for their website and plans to expose their website from the same load balancer, linked to different target groups with different URLs: checkout.mycorp.com, www.mycorp.com, mycorp.com/products, and mycorp.com/orders. The website would like to use Amazon ECS on the backend to manage these microservices and possibly host the same container of the application multiple times on the same Amazon EC2 instance. Which feature can help you achieve this with minimal effort?",
      "options": [
        "A. Classic Load Balancer + dynamic port mapping",
        "B. Application Load Balancer + dynamic port mapping",
        "C. Network Load Balancer + dynamic port mapping",
        "D. Application Load Balancer + Reverse Proxy running as a Docker daemon on each Amazon ECS host"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Application Load Balancer + dynamic port mapping Application Load Balancer can automatically distribute incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and AWS Lambda functions. It can handle the varying load of your application traffic in a single Availability Zone or across multiple Availability Zones (AZs). Dynamic port mapping with an Application Load Balancer makes it easier to run multiple tasks on the same Amazon ECS service on an Amazon ECS cluster. Incorrect option: Application Load Balancer + Reverse Proxy running as a Docker daemon on each Amazon ECS host - Dynamic Port Mapping is available for the Application Load Balancer. A reverse proxy solution would work but would be too much work to manage. Here the Application Load Balancer has a feature that provides a direct dynamic port mapping feature and integration with the Amazon ECS service so we will leverage that. Classic Load Balancer + dynamic port mapping - Classic Load Balancer provides basic load balancing across multiple Amazon EC2 instances and operates at both the request level and connection level. Classic Load Balancer is intended for applications that were built within the Amazon EC2-Classic network. With the Classic Load Balancer, you must statically map port numbers on a container instance. The Classic Load Balancer does not allow you to run multiple copies of a task on the same instance because of the ports conflict. An Application Load Balancer uses dynamic port mapping so that you can run multiple tasks from a single service on the same container instance. Network Load Balancer + dynamic port mapping - Network Load Balancer is best suited for use-cases involving low latency and high throughput workloads that involve scaling to millions of requests per second. Network Load Balancer operates at the connection level (Layer 4), routing connections to targets - Amazon EC2 instances, microservices, and containers – within Amazon Virtual Private Cloud (Amazon VPC) based on IP protocol data. References: https://aws.amazon.com/premiumsupport/knowledge-center/dynamic-port-mapping-ecs/ https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "Your company has created a data warehouse using Amazon Redshift that is used to analyze data from Amazon S3. From the usage pattern, you have detected that after 30 days, the data is rarely queried in Amazon Redshift and it's not \"hot data\" anymore. You would like to preserve the SQL querying capability on your data and get the queries started immediately. Also, you want to adopt a pricing model that allows you to save the maximum amount of cost on Amazon Redshift. What do you recommend? (Select two)",
      "options": [
        "A. Migrate the Amazon Redshift underlying storage to Amazon S3 IA",
        "B. Create a smaller Amazon Redshift Cluster with the cold data",
        "C. Move the data to Amazon S3 Glacier Deep Archive after 30 days",
        "D. Move the data to Amazon S3 Standard IA after 30 days",
        "E. Analyze the cold data with Amazon Athena"
      ],
      "correct_answer": "D",
      "explanation": "Correct options: Move the data to Amazon S3 Standard IA after 30 days Amazon S3 Standard-IA is for data that is accessed less frequently but requires rapid access when needed. Amazon S3 Standard-IA offers high durability, high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval fee. This combination of low cost and high performance makes S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files. The minimum storage duration charge is 30 days. Analyze the cold data with Amazon Athena Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to set up or manage, and customers pay only for the queries they run. You can use Amazon Athena to process logs, perform ad-hoc analysis, and run interactive queries. Moving the data to Amazon S3 glacier will prevent us from being able to query it. Therefore, we should migrate the data to Amazon S3 Standard IA and use Amazon Athena to analyze the cold data. Incorrect options: Migrate the Amazon Redshift underlying storage to Amazon S3 IA - Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large scale data set storage and analysis. An Amazon Redshift data warehouse is a collection of computing resources called nodes, which are organized into a group called a cluster. Each cluster runs an Amazon Redshift engine and contains one or more databases. An Amazon Redshift cluster consists of nodes. Each cluster has a leader node and one or more compute nodes. The leader node receives queries from client applications, parses the queries, and develops query execution plans. The leader node then coordinates the parallel execution of these plans with the compute nodes and aggregates the intermediate results from these nodes. It then finally returns the results to the client applications. Redshift's internal storage does not have \"tiers\" of storage classes like Amazon S3, so this option is also ruled out. Create a smaller Amazon Redshift Cluster with the cold data - Creating a smaller cluster with the cold data would not decrease the storage cost of Amazon Redshift, which will only increase with time as we keep on creating data. Therefore this option is ruled out. Move the data to Amazon S3 Glacier Deep Archive after 30 days - Amazon S3 Glacier Deep Archive (GDA) is a secure, durable, and extremely low-cost Amazon S3 cloud storage class for data archiving and long-term backup. It is designed to deliver 99.999999999% durability, and provide comprehensive security and compliance capabilities that can help meet even the most stringent regulatory requirements. GDA has a first-byte latency of several hours, so this option is incorrect. References: https://aws.amazon.com/athena/ https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-clusters.html",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A retail company needs a secure connection between its on-premises data center and AWS Cloud. This connection does not need high bandwidth and will handle a small amount of traffic. The company wants a quick turnaround time to set up the connection. What is the MOST cost-effective way to establish such a connection?",
      "options": [
        "A. Set up a bastion host on Amazon EC2",
        "B. Set up AWS Direct Connect",
        "C. Set up an AWS Site-to-Site VPN connection",
        "D. Set up an Internet Gateway between the on-premises data center and AWS cloud"
      ],
      "correct_answer": "C",
      "explanation": "Correct option: Set up an AWS Site-to-Site VPN connection By default, instances that you launch into an Amazon VPC can't communicate with your own (remote) network. You can enable access to your remote network from your VPC by creating an AWS Site-to-Site VPN (Site-to-Site VPN) connection, and configuring routing to pass traffic through the connection. A VPN connection refers to the connection between your VPC and your own on-premises network. An AWS Site-to-Site VPN connection offers two VPN tunnels between a virtual private gateway or a transit gateway on the AWS side, and a customer gateway (which represents a VPN device) on the remote (on-premises) side. A virtual private gateway (VGW) is the VPN concentrator on the Amazon side of the AWS Site-to-Site VPN connection. You create a virtual private gateway and attach it to the VPC from which you want to create the AWS Site-to-Site VPN connection. How virtual private gateway works: via - https://docs.aws.amazon.com/vpn/latest/s2svpn/how_it_works.html An AWS transit gateway is a transit hub that you can use to interconnect your virtual private clouds (VPC) and on-premises networks. For more information, see Amazon VPC Transit Gateways. You can create a Site-to-Site VPN connection as an attachment on a transit gateway. How AWS transit gateway works: via - https://docs.aws.amazon.com/vpn/latest/s2svpn/how_it_works.html Incorrect options: Set up a bastion host on Amazon EC2 - A bastion host is a server whose purpose is to provide access to a private network from an external network, such as the Internet. The bastion host runs on an Amazon EC2 instance that is typically in a public subnet of your Amazon VPC. Other Amazon EC2 instances can be in a subnet that is not publicly accessible, and they are set up with a security group that allows SSH access from the security group attached to the underlying Amazon EC2 instance running the bastion host. A bastion host cannot be used to set up a connection between its on-premises data center and AWS Cloud. Set up AWS Direct Connect - AWS Direct Connect is a network service that provides an alternative to using the Internet to utilize AWS cloud services. AWS Direct Connect enables customers to have low latency, secure and private connections to AWS for workloads that require higher speed or lower latency than the internet. A Dedicated Connection is made through a 1 Gbps, 10 Gbps, or 100 Gbps Ethernet port dedicated to a single customer. AWS Direct Connect takes about a month to provision the connection, so this option is ruled out for the given use case. Set up an Internet Gateway between the on-premises data center and AWS cloud - An Internet Gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between your VPC and the internet. An Internet Gateway cannot be used to set up a connection between its on-premises data center and AWS Cloud. References: https://docs.aws.amazon.com/vpn/latest/s2svpn/how_it_works.html https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An application is hosted on multiple Amazon EC2 instances in the same Availability Zone (AZ). The engineering team wants to set up shared data access for these Amazon EC2 instances using Amazon EBS Multi-Attach volumes. Which Amazon EBS volume type is the correct choice for these Amazon EC2 instances?",
      "options": [
        "A. Provisioned IOPS SSD Amazon EBS volumes",
        "B. General-purpose SSD-based Amazon EBS volumes",
        "C. Throughput Optimized HDD Amazon EBS volumes",
        "D. Cold HDD Amazon EBS volumes"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Provisioned IOPS SSD Amazon EBS volumes Amazon EBS Multi-Attach enables you to attach a single Provisioned IOPS SSD (io1 or io2) volume to multiple instances that are in the same Availability Zone. You can attach multiple Multi-Attach enabled volumes to an instance or set of instances. Each instance to which the volume is attached has full read and write permission to the shared volume. Multi-Attach makes it easier for you to achieve higher application availability in clustered Linux applications that manage concurrent write operations. Multi-Attach is supported exclusively on Provisioned IOPS SSD volumes. Incorrect options: General-purpose SSD-based Amazon EBS volumes - These SSD-backed Amazon EBS volumes provide a balance of price and performance. AWS recommends these volumes for most workloads. These volume types are not supported for Multi-Attach functionality. Throughput Optimized HDD Amazon EBS volumes - These HDD-backed volumes provide a low-cost HDD designed for frequently accessed, throughput-intensive workloads. These volume types are not supported for Multi-Attach functionality. Cold HDD Amazon EBS volumes - These HDD-backed volumes provide a lowest-cost HDD design for less frequently accessed workloads. These volume types are not supported for Multi-Attach functionality. Reference: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes-multi.html",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A development team wants to ensure that all objects uploaded to an Amazon S3 bucket are encrypted? Which of the following options represents the correct solution?",
      "options": [
        "A. Configure the bucket policy to deny if the PutObject does not have an x-amz-server-side-encryption header set",
        "B. Configure the bucket policy to deny if the PutObject does not have an s3:x-amz-acl header set to private",
        "C. Configure the bucket policy to deny if the PutObject does not have an aws:SecureTransport header set to true",
        "D. Configure the bucket policy to deny if the PutObject does not have an s3:x-amz-acl header set"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Configure the bucket policy to deny if the PutObject does not have an x-amz-server-side-encryption header set Amazon S3 encrypts your data at the object level as it writes to disks in AWS data centers, and decrypts it for you when you access it. You can encrypt objects by using client-side encryption or server-side encryption. Client-side encryption occurs when an object is encrypted before you upload it to Amazon S3, and the keys are not managed by AWS. With server-side encryption, Amazon manages the keys in one of three ways: Server-side encryption with customer-provided encryption keys (SSE-C). SSE-S3. SSE-KMS. Server-side encryption is about data encryption at rest—that is, Amazon S3 encrypts your data at the object level as it writes it to disks in its data centers and decrypts it for you when you access it. As long as you authenticate your request and you have access permissions, there is no difference in the way you access encrypted or unencrypted objects. To encrypt an object at the time of upload, you need to add a header called x-amz-server-side-encryption to the request to tell S3 to encrypt the object using SSE-C, SSE-S3, or SSE-KMS. In order to enforce object encryption, create an Amazon S3 bucket policy that denies any S3 Put request that does not include the x-amz-server-side-encryption header. There are two possible values for the x-amz-server-side-encryption header: AES256, which tells S3 to use S3-managed keys, and aws:kms, which tells Amazon S3 to use AWS KMS–managed keys. Incorrect options: Configure the bucket policy to deny if the PutObject does not have an s3:x-amz-acl header set to private - The x-amz-acl header is used to specify an ACL in the PutObject request. Access permissions are defined using this header. Configure the bucket policy to deny if the PutObject does not have an aws:SecureTransport header set to true - By default, Amazon S3 allows both HTTP and HTTPS requests. aws:SecureTransport key is used to check if the request is sent through HTTP or HTTPS. When this key is true, it means that the request is sent through HTTPS. Configure the bucket policy to deny if the PutObject does not have an s3:x-amz-acl header set - As discussed above, the s3:x-amz-acl header is used to set permissions on the specified S3 bucket and has nothing to do with encryption. References: https://aws.amazon.com/blogs/security/how-to-prevent-uploads-of-unencrypted-objects-to-amazon-s3/ https://docs.aws.amazon.com/AmazonS3/latest/userguide/amazon-s3-policy-keys.html",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "To support critical production workloads that require maximum resiliency, a company wants to configure network connections between its Amazon VPC and the on-premises infrastructure. The company needs AWS Direct Connect connections with speeds greater than 1 Gbps. As a solutions architect, which of the following will you suggest as the best architecture for this requirement?",
      "options": [
        "A. Opt for one AWS Direct Connect connection at each of the multiple Direct Connect locations",
        "B. Opt for two separate AWS Direct Connect connections terminating on separate devices in more than one Direct Connect location",
        "C. Use AWS Managed VPN as a backup for AWS Direct Connect connections to ensure maximum resiliency",
        "D. Opt for at least two AWS Direct Connect connections terminating on different devices at a single Direct Connect location"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Opt for two separate AWS Direct Connect connections terminating on separate devices in more than one Direct Connect location Maximum resilience is achieved by separate connections terminating on separate devices in more than one location. This configuration offers customers maximum resilience to failure. As shown in the figure above, such a topology provides resilience to device failure, connectivity failure, and complete location failure. You can use Direct Connect Gateway to access any AWS Region (except AWS Regions in China) from any AWS Direct Connect locations. Maximum Resiliency for Critical Workloads: via - https://aws.amazon.com/directconnect/resiliency-recommendation/ Incorrect options: Opt for one AWS Direct Connect connection at each of the multiple Direct Connect locations - For critical production workloads that require high resiliency, it is recommended to have one connection at multiple locations. As shown in the figure below, such a topology ensures resilience to connectivity failure due to a fiber cut or a device failure as well as a complete location failure. You can use Direct Connect Gateway to access any AWS Region (except AWS Regions in China) from any AWS Direct Connect location. High Resiliency for Critical Workloads: via - https://aws.amazon.com/directconnect/resiliency-recommendation/ Opt for at least two AWS Direct Connect connections terminating on different devices at a single Direct Connect location - For non-critical production workloads and development workloads that do not require high resiliency, it is recommended to have at least two connections terminating on different devices at a single location. As shown in the figure above, such a topology helps in the case of the device failure at a location but does not help in the event of a total location failure. Non Critical Production Workloads or Development Workloads: via - https://aws.amazon.com/directconnect/resiliency-recommendation/ Use AWS Managed VPN as a backup for AWS Direct Connect connections to ensure maximum resiliency - It is important to understand that AWS Managed VPN supports up to 1.25 Gbps throughput per VPN tunnel and does not support Equal Cost Multi-Path (ECMP) for egress data path in the case of multiple AWS Managed VPN tunnels terminating on the same VGW. Thus, AWS does not recommend customers use AWS Managed VPN as a backup for AWS Direct Connect connections with speeds greater than 1 Gbps. Reference: https://aws.amazon.com/directconnect/resiliency-recommendation/",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A healthcare company wants to run its applications on single-tenant hardware to meet compliance guidelines. Which of the following is the MOST cost-effective way of isolating the Amazon EC2 instances to a single tenant?",
      "options": [
        "A. Dedicated Instances",
        "B. Spot Instances",
        "C. Dedicated Hosts",
        "D. On-Demand Instances"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Dedicated Instances Dedicated Instances are Amazon EC2 instances that run in a virtual private cloud (VPC) on hardware that's dedicated to a single customer. Dedicated Instances that belong to different AWS accounts are physically isolated at a hardware level, even if those accounts are linked to a single-payer account. However, Dedicated Instances may share hardware with other instances from the same AWS account that are not Dedicated Instances. A Dedicated Host is also a physical server that's dedicated for your use. With a Dedicated Host, you have visibility and control over how instances are placed on the server. Differences between Dedicated Hosts and Dedicated Instances: via - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-hosts-overview.html#dedicated-hosts-dedicated-instances Incorrect options: Spot Instances - A Spot Instance is an unused Amazon EC2 instance that is available for less than the On-Demand price. Your Spot Instance runs whenever capacity is available and the maximum price per hour for your request exceeds the Spot price. Any instance present with unused capacity will be allocated. Even though this is cost-effective, it does not fulfill the single-tenant hardware requirement of the client and hence is not the correct option. Dedicated Hosts - An Amazon EC2 Dedicated Host is a physical server with Amazon EC2 instance capacity fully dedicated to your use. Dedicated Hosts allow you to use your existing software licenses on Amazon EC2 instances. With a Dedicated Host, you have visibility and control over how instances are placed on the server. This option is costlier than the Dedicated Instance and hence is not the right choice for the current requirement. On-Demand Instances - With On-Demand Instances, you pay for the compute capacity by the second with no long-term commitments. You have full control over its lifecycle—you decide when to launch, stop, hibernate, start, reboot, or terminate it. Hardware isolation is not possible and on-demand has one of the costliest instance charges and hence is not the correct answer for current requirements. High Level Overview of Amazon EC2 Instance Purchase Options: via - https://aws.amazon.com/ec2/pricing/ References: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-instance.html https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-purchasing-options.html",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "You are looking to build an index of your files in Amazon S3, using Amazon RDS PostgreSQL. To build this index, it is necessary to read the first 250 bytes of each object in Amazon S3, which contains some metadata about the content of the file itself. There are over 100,000 files in your S3 bucket, amounting to 50 terabytes of data. How can you build this index efficiently?",
      "options": [
        "A. Create an application that will traverse the S3 bucket, issue a Byte Range Fetch for the first 250 bytes, and store that information in Amazon RDS",
        "B. Use the Amazon RDS Import feature to load the data from Amazon S3 to PostgreSQL, and run a SQL query to build the index",
        "C. Create an application that will traverse the Amazon S3 bucket, read all the files one by one, extract the first 250 bytes, and store that information in Amazon RDS",
        "D. Create an application that will traverse the Amazon S3 bucket, then use S3 Select Byte Range Fetch parameter to get the first 250 bytes, and store that information in Amazon RDS"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Create an application that will traverse the S3 bucket, issue a Byte Range Fetch for the first 250 bytes, and store that information in Amazon RDS Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. Using the Range HTTP header in a GET Object request, you can fetch a byte-range from an object, transferring only the specified portion. You can use concurrent connections to Amazon S3 to fetch different byte ranges from within the same object. This helps you achieve higher aggregate throughput versus a single whole-object request. Fetching smaller ranges of a large object also allows your application to improve retry times when requests are interrupted. A byte-range request is a perfect way to get the beginning of a file and ensuring we remain efficient during our scan of our Amazon S3 bucket. So this is the correct option. Incorrect options: Use the Amazon RDS Import feature to load the data from Amazon S3 to PostgreSQL, and run a SQL query to build the index - You cannot import data from Amazon S3 into Amazon RDS, so this option is incorrect. Create an application that will traverse the Amazon S3 bucket, read all the files one by one, extract the first 250 bytes, and store that information in Amazon RDS - If you build an application that loads all the files from Amazon S3, that would work, but you would read 50TB of data and that may be very expensive and slow. So this option is incorrect. Create an application that will traverse the Amazon S3 bucket, then use S3 Select Byte Range Fetch parameter to get the first 250 bytes, and store that information in Amazon RDS - Amazon S3 Select is a new Amazon S3 capability designed to pull out only the data you need from an object, which can dramatically improve the performance and reduce the cost of applications that need to access data in Amazon S3. You cannot use Byte Range Fetch parameter with S3 Select to traverse the Amazon S3 bucket and get the first bytes of a file. So this option is incorrect. Exam Alert: Please note that with Amazon S3 Select, you can scan a subset of an object by specifying a range of bytes to query using the ScanRange parameter. This capability lets you parallelize scanning the whole object by splitting the work into separate Amazon S3 Select requests for a series of non-overlapping scan ranges. Use the Amazon S3 Select ScanRange parameter and Start at (Byte) and End at (Byte). via - https://docs.aws.amazon.com/AmazonS3/latest/dev/selecting-content-from-objects.html Reference: https://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance-guidelines.html#optimizing-performance-guidelines-get-range",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "Your e-commerce application is using an Amazon RDS PostgreSQL database and an analytics workload also runs on the same database. When the analytics workload is run, your e-commerce application slows down which further affects your sales. Which of the following is the MOST cost-optimal solution to fix this issue?",
      "options": [
        "A. Create a Read Replica in the same Region as the Master database and point the analytics workload there",
        "B. Migrate the analytics application to AWS Lambda",
        "C. Enable Multi-AZ for the Amazon RDS database and run the analytics workload on the standby database",
        "D. Create a Read Replica in another Region as the Master database and point the analytics workload there"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Create a Read Replica in the same Region as the Master database and point the analytics workload there Amazon RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. For the MySQL, MariaDB, PostgreSQL, Oracle, and SQL Server database engines, Amazon RDS creates a second DB instance using a snapshot of the source DB instance. It then uses the engines' native asynchronous replication to update the read replica whenever there is a change to the source database instance. Read replicas can be within an Availability Zone, Cross-AZ, or Cross-Region. Creating a Read Replica is the answer. As we want to minimize the costs, we need to launch the Read Replica in the same Region as you are not charged for the data transfer incurred in replicating data between your source database instance and read replica within the same AWS Region. Exam Alert: Please review this comparison vis-a-vis Multi-AZ vs Read Replica for Amazon RDS: via - https://aws.amazon.com/rds/features/multi-az/ Incorrect options: Enable Multi-AZ for the Amazon RDS database and run the analytics workload on the standby database - Amazon RDS Multi-AZ deployments provide enhanced availability and durability for RDS database (DB) instances, making them a natural fit for production database workloads. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). Multi-AZ spans at least two Availability Zones within a single region. Enabling Multi-AZ helps make our database highly-available, but the standby database is not accessible and cannot be used for reads or write. It's just a database that will become primary when the other database encounters a failure. So this option is not correct. Migrate the analytics application to AWS Lambda- AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume. Running the application on AWS Lambda will not help, as it will still run against the main database and slow down our e-commerce application. Create a Read Replica in another Region as the Master database and point the analytics workload there - This is incorrect because we have to pay for inter-Region data replication charges for the Read Replica, whereas the replication of data within a single Region is free. References: https://aws.amazon.com/rds/features/multi-az/ https://aws.amazon.com/rds/features/read-replicas/",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "The development team at a company manages a Python based nightly process with a runtime of 30 minutes. The process can withstand any interruptions in its execution and start over again. The process currently runs on the on-premises infrastructure and it needs to be migrated to AWS. Which of the following options do you recommend as the MOST cost-effective solution?",
      "options": [
        "A. Run on a Spot Instance with a persistent request type",
        "B. Run on Amazon EMR",
        "C. Run on AWS Lambda",
        "D. Run on an Application Load Balancer"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Run on a Spot Instance with a persistent request type A Spot Instance is an unused Amazon EC2 instance that is available for less than the On-Demand price. Because Spot Instances enable you to request unused Amazon EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly. The hourly price for a Spot Instance is called a Spot price. The request type (one-time or persistent) determines whether the request is opened again when Amazon EC2 interrupts a Spot Instance or if you stop a Spot Instance. If the request is persistent, the request is opened again after your Spot Instance is interrupted. If the request is persistent and you stop your Spot Instance, the request only opens after you start your Spot Instance. Incorrect options: Run on an Application Load Balancer - Application Load Balancer operates at the request level (layer 7), routing traffic to targets – Amazon EC2 instances, containers, IP addresses, and AWS Lambda functions based on the content of the request. Ideal for advanced load balancing of HTTP and HTTPS traffic, Application Load Balancer provides advanced request routing targeted at delivery of modern application architectures, including microservices and container-based applications. Application Load Balancer helps distribute load for HTTP(S) requests. This option has been added as a distractor. Run on Amazon EMR - Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. Amazon EMR uses Hadoop, an open-source framework, to distribute your data and processing across a resizable cluster of Amazon EC2 instances. Amazon EMR is to run Big Data load that is meant to be run on Hadoop, this is also a distractor. Run on AWS Lambda - AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume. AWS Lambda would be the perfect fit if our script could run in less than 15 minutes, as this is the maximum timeout for AWS Lambda. Reference: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-requests.html",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A development team is looking for a solution that saves development time and deployment costs for an application that uses a high-throughput request-response message pattern. Which of the following Amazon SQS queue types is the best fit to meet this requirement?",
      "options": [
        "A. Amazon Simple Queue Service (Amazon SQS) dead-letter queues",
        "B. Amazon Simple Queue Service (Amazon SQS) FIFO queues",
        "C. Amazon Simple Queue Service (Amazon SQS) temporary queues",
        "D. Amazon Simple Queue Service (Amazon SQS) delay queues"
      ],
      "correct_answer": "C",
      "explanation": "Correct option: Amazon Simple Queue Service (Amazon SQS) temporary queues Temporary queues help you save development time and deployment costs when using common message patterns such as request-response. You can use the Temporary Queue Client to create high-throughput, cost-effective, application-managed temporary queues. The client maps multiple temporary queues—application-managed queues created on demand for a particular process—onto a single Amazon SQS queue automatically. This allows your application to make fewer API calls and have a higher throughput when the traffic to each temporary queue is low. When a temporary queue is no longer in use, the client cleans up the temporary queue automatically, even if some processes that use the client aren't shut down cleanly. The following are the benefits of temporary queues: They serve as lightweight communication channels for specific threads or processes. They can be created and deleted without incurring additional costs. They are API-compatible with static (normal) Amazon SQS queues. This means that existing code that sends and receives messages can send messages to and receive messages from virtual queues. To better support short-lived, lightweight messaging destinations, AWS recommends Amazon SQS Temporary Queue Client. This client makes it easy to create and delete many temporary messaging destinations without inflating your AWS bill. The key concept behind the client is the virtual queue. Virtual queues let you multiplex many low-traffic queues onto a single Amazon SQS queue. Creating a virtual queue only instantiates a local buffer to hold messages for consumers as they arrive; there is no API call to SQS and no costs associated with creating a virtual queue. End-to-end process for sending messages through virtual queues: via - https://aws.amazon.com/blogs/compute/simple-two-way-messaging-using-the-amazon-sqs-temporary-queue-client/ Incorrect options: Amazon Simple Queue Service (Amazon SQS) dead-letter queues - Amazon SQS supports dead-letter queues, which other queues (source queues) can target for messages that can't be processed (consumed) successfully. Dead-letter queues are useful for debugging your application or messaging system because they let you isolate problematic messages to determine why their processing doesn't succeed. Amazon SQS does not create the dead-letter queue automatically. You must first create the queue before using it as a dead-letter queue. Amazon Simple Queue Service (Amazon SQS) FIFO queues - Amazon SQS FIFO (First-In-First-Out) queues are designed to enhance messaging between applications when the order of operations and events is critical, or where duplicates can't be tolerated. FIFO queues also provide exactly-once processing but have a limited number of transactions per second (TPS). Amazon Simple Queue Service (Amazon SQS) delay queues - Delay queues let you postpone the delivery of new messages to a queue for a number of seconds, for example, when your consumer application needs additional time to process messages. If you create a delay queue, any messages that you send to the queue remain invisible to consumers for the duration of the delay period. The default (minimum) delay for a queue is 0 seconds. The maximum is 15 minutes. References: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-temporary-queues.html https://aws.amazon.com/blogs/compute/simple-two-way-messaging-using-the-amazon-sqs-temporary-queue-client/",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "The engineering team at an IT company is deploying an Online Transactional Processing (OLTP) application that needs to support relational queries. The application will have unpredictable spikes of usage that the team does not know in advance. Which database would you recommend using?",
      "options": [
        "A. Amazon Aurora Serverless",
        "B. Amazon ElastiCache",
        "C. Amazon DynamoDB with Provisioned Capacity and Auto Scaling",
        "D. Amazon DynamoDB with On-Demand Capacity"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Amazon Aurora Serverless Amazon Aurora Serverless is an on-demand, auto-scaling configuration for Amazon Aurora (MySQL-compatible and PostgreSQL-compatible editions), where the database will automatically start up, shut down, and scale capacity up or down based on your application's needs. It enables you to run your database in the cloud without managing any database instances. It's a simple, cost-effective option for infrequent, intermittent, or unpredictable workloads. The database design for an OLTP application fits the relational model, therefore you can infer an OLTP system as a Relational Database. Amazon Aurora Serverless is the perfect way to create a database that can scale down to 0 servers, and scale up to many servers, as an OLTP database. So this is the correct option. Incorrect options: Amazon DynamoDB with Provisioned Capacity and Auto Scaling Amazon DynamoDB with On-Demand Capacity Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications. Amazon DynamoDB is a NoSQL database and doesn't do relational queries, therefore it's a choice we have to eliminate, even though the two modes proposed here help us cope with an unpredictable amount of usage. So both these options are incorrect. Amazon ElastiCache - Amazon ElastiCache allows you to seamlessly set up, run, and scale popular open-Source compatible in-memory data stores in the cloud. Build data-intensive apps or boost the performance of your existing databases by retrieving data from high throughput and low latency in-memory data stores. Amazon ElastiCache is a popular choice for real-time use cases like Caching, Session Stores, Gaming, Geospatial Services, Real-Time Analytics, and Queuing. Amazon Elasticache is used as a caching layer in front of relational databases. Amazon ElastiCache is a NoSQL database and doesn't facilitate relational queries, so this option is ruled out. References: https://aws.amazon.com/rds/aurora/serverless/ https://aws.amazon.com/rds/",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "The data engineering team at a company wants to analyze Amazon S3 storage access patterns to decide when to transition the right data to the right storage class. Which of the following represents a correct option regarding the capabilities of Amazon S3 Analytics storage class analysis?",
      "options": [
        "A. Storage class analysis only provides recommendations for Standard to Standard One-Zone IA classes",
        "B. Storage class analysis only provides recommendations for Standard to Glacier Deep Archive classes",
        "C. Storage class analysis only provides recommendations for Standard to Glacier Flexible Retrieval classes",
        "D. Storage class analysis only provides recommendations for Standard to Standard IA classes"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: Storage class analysis only provides recommendations for Standard to Standard IA classes By using Amazon S3 analytics Storage Class Analysis you can analyze storage access patterns to help you decide when to transition the right data to the right storage class. This new Amazon S3 analytics feature observes data access patterns to help you determine when to transition less frequently accessed STANDARD storage to the STANDARD_IA (IA, for infrequent access) storage class. Storage class analysis only provides recommendations for Standard to Standard IA classes. After storage class analysis observes the infrequent access patterns of a filtered set of data over a period of time, you can use the analysis results to help you improve your lifecycle configurations. You can configure storage class analysis to analyze all the objects in a bucket. Or, you can configure filters to group objects together for analysis by common prefix (that is, objects that have names that begin with a common string), by object tags, or by both prefix and tags. Incorrect options: Storage class analysis only provides recommendations for Standard to Standard One-Zone IA classes Storage class analysis only provides recommendations for Standard to Glacier Deep Archive classes Storage class analysis only provides recommendations for Standard to Glacier Flexible Retrieval classes These three options contradict the explanation provided above, so these options are incorrect. References: https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html https://docs.aws.amazon.com/AmazonS3/latest/userguide/analytics-storage-class.html",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "The engineering team at a multi-national company uses AWS Firewall Manager to centrally configure and manage firewall rules across its accounts and applications using AWS Organizations. Which of the following AWS resources can the AWS Firewall Manager configure rules on? (Select three)",
      "options": [
        "A. AWS Web Application Firewall (AWS WAF)",
        "B. Amazon GuardDuty",
        "C. Amazon Inspector",
        "D. AWS Shield Advanced",
        "E. Network access control list (network ACL)",
        "F. VPC Security Groups"
      ],
      "correct_answer": "A",
      "explanation": "Correct options: AWS Web Application Firewall (AWS WAF) AWS Shield Advanced VPC Security Groups AWS Firewall Manager is a security management service which allows you to centrally configure and manage firewall rules across your accounts and applications in AWS Organizations. As new applications are created, Firewall Manager makes it easy to bring new applications and resources into compliance by enforcing a common set of security rules. Now you have a single service to build firewall rules, create security policies, and enforce them in a consistent, hierarchical manner across your entire infrastructure. Using AWS Firewall Manager, you can centrally configure AWS WAF rules, AWS Shield Advanced protection, Amazon Virtual Private Cloud (VPC) security groups, AWS Network Firewalls, and Amazon Route 53 Resolver DNS Firewall rules across accounts and resources in your organization. It does not support Network ACLs as of today. via - https://aws.amazon.com/firewall-manager/faqs/ Incorrect options: Amazon GuardDuty - Amazon GuardDuty offers threat detection that enables you to continuously monitor and protect your AWS accounts, workloads, and data stored in Amazon S3. Amazon GuardDuty analyzes continuous streams of meta-data generated from your account and network activity found in AWS CloudTrail Events, Amazon VPC Flow Logs, and DNS Logs. How Amazon GuardDuty Works: Amazon Inspector - Amazon Inspector is an automated security assessment service that helps you test the network accessibility of your Amazon EC2 instances and the security state of your applications running on the instances. Network access control list (network ACL) - A network access control list (ACL) is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets. These three options are not in the list of AWS resources supported by AWS Firewall Manager, so these options are incorrect. References: https://aws.amazon.com/firewall-manager/faqs/ https://aws.amazon.com/guardduty/ https://aws.amazon.com/inspector/",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A company has moved its business critical data to Amazon Elastic File System (Amazon EFS) which will be accessed by multiple Amazon EC2 instances. As an AWS Certified Solutions Architect - Associate, which of the following would you recommend to exercise access control such that only the permitted Amazon EC2 instances can read from the Amazon EFS file system? (Select two)",
      "options": [
        "A. Use VPC security groups to control the network traffic to and from your file system",
        "B. Use an IAM policy to control access for clients who can mount your file system with the required permissions",
        "C. Use network access control list (network ACL) to control the network traffic to and from your Amazon EC2 instance",
        "D. Use Amazon GuardDuty to curb unwanted access to Amazon EFS file system",
        "E. Set up the IAM policy root credentials to control and configure the clients accessing the Amazon EFS file system"
      ],
      "correct_answer": "A",
      "explanation": "Correct options: Use VPC security groups to control the network traffic to and from your file system Use an IAM policy to control access for clients who can mount your file system with the required permissions You control which Amazon EC2 instances can access your Amazon EFS file system by using VPC security group rules and AWS Identity and Access Management (IAM) policies. Use VPC security groups to control the network traffic to and from your file system. Attach an IAM policy to your file system to control which clients can mount your file system and with what permissions, and you may use Amazon EFS Access Points to manage application access. Control access to files and directories with POSIX-compliant user and group-level permissions. Files and directories in an Amazon EFS file system support standard Unix-style read, write, and execute permissions based on the user ID and group IDs. When an NFS client mounts an Amazon EFS file system without using an access point, the user ID and group ID provided by the client is trusted. You can also use Amazon EFS access points to override user ID and group IDs used by the NFS client. When users attempt to access files and directories, Amazon EFS checks their user IDs and group IDs to verify that each user has permission to access the objects. Incorrect options: Use network access control list (network ACL) to control the network traffic to and from your Amazon EC2 instance - Network ACLs operate at the subnet level and not at the instance level. Set up the IAM policy root credentials to control and configure the clients accessing the Amazon EFS file system - There is no such thing as an IAM policy root credentials and this statement has been added as a distractor. Use Amazon GuardDuty to curb unwanted access to Amazon EFS file system - Amazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts, workloads, and data stored in Amazon S3. It cannot be used for access control to the Amazon EFS file system. References: https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Security.html#VPC_Security_Comparison https://docs.aws.amazon.com/efs/latest/ug/accessing-fs-nfs-permissions.html https://docs.aws.amazon.com/efs/latest/ug/iam-access-control-nfs-efs.html",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "The DevOps team at an e-commerce company has deployed a fleet of Amazon EC2 instances under an Auto Scaling group (ASG). The instances under the ASG span two Availability Zones (AZ) within the us-east-1 region. All the incoming requests are handled by an Application Load Balancer (ALB) that routes the requests to the Amazon EC2 instances under the Auto Scaling Group. As part of a test run, two instances (instance 1 and 2, belonging to AZ A) were manually terminated by the DevOps team causing the Availability Zones (AZ) to have unbalanced resources. Later that day, another instance (belonging to AZ B) was detected as unhealthy by the Application Load Balancer's health check. Can you identify the correct outcomes for these events? (Select two)",
      "options": [
        "A. As the resources are unbalanced in the Availability Zones, Amazon EC2 Auto Scaling will compensate by rebalancing the Availability Zones. When rebalancing, Amazon EC2 Auto Scaling launches new instances before terminating the old ones, so that rebalancing does not compromise the performance or availability of your application",
        "B. Amazon EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it. Later, another scaling activity launches a new instance to replace the terminated instance",
        "C. Amazon EC2 Auto Scaling creates a new scaling activity for launching a new instance to replace the unhealthy instance. Later, Amazon EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it",
        "D. As the resources are unbalanced in the Availability Zones, Amazon EC2 Auto Scaling will compensate by rebalancing the Availability Zones. When rebalancing, Amazon EC2 Auto Scaling terminates old instances before launching new instances, so that rebalancing does not cause extra instances to be launched",
        "E. Amazon EC2 Auto Scaling creates a new scaling activity to terminate the unhealthy instance and launch the new instance simultaneously"
      ],
      "correct_answer": "A",
      "explanation": "Correct options: As the resources are unbalanced in the Availability Zones, Amazon EC2 Auto Scaling will compensate by rebalancing the Availability Zones. When rebalancing, Amazon EC2 Auto Scaling launches new instances before terminating the old ones, so that rebalancing does not compromise the performance or availability of your application Amazon EC2 Auto Scaling helps you ensure that you have the correct number of Amazon EC2 instances available to handle the load for your application. You create collections of EC2 instances, called Auto Scaling groups. You can specify the minimum number of instances in each Auto Scaling group, and Amazon EC2 Auto Scaling ensures that your group never goes below this size. Actions such as changing the Availability Zones (AZ) for your group or explicitly terminating or detaching instances can lead to the Auto Scaling group becoming unbalanced between Availability Zones. Amazon EC2 Auto Scaling compensates by rebalancing the Availability Zones. When rebalancing, Amazon EC2 Auto Scaling launches new instances before terminating the old ones, so that rebalancing does not compromise the performance or availability of your application. Therefore, this option is correct. Availability Zone Rebalancing Overview: via - https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html Amazon EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it. Later, another scaling activity launches a new instance to replace the terminated instance However, the scaling activity of Auto Scaling works in a different sequence compared to the rebalancing activity. Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it. Later, another scaling activity launches a new instance to replace the terminated instance. Incorrect options: Amazon EC2 Auto Scaling creates a new scaling activity for launching a new instance to replace the unhealthy instance. Later, Amazon EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it - This option contradicts the correct sequence of events outlined earlier for scaling activity created by Amazon EC2 Auto Scaling. Actually, Auto Scaling first terminates the unhealthy instance and then launches a new instance. Hence this is incorrect. As the resources are unbalanced in the Availability Zones, Amazon EC2 Auto Scaling will compensate by rebalancing the Availability Zones. When rebalancing, Amazon EC2 Auto Scaling terminates old instances before launching new instances, so that rebalancing does not cause extra instances to be launched - This option contradicts the correct sequence of events outlined earlier for rebalancing activity. When rebalancing, Amazon EC2 Auto Scaling launches new instances before terminating the old ones. Hence this is incorrect. Amazon EC2 Auto Scaling creates a new scaling activity to terminate the unhealthy instance and launch the new instance simultaneously - This is a made-up option as both the terminate and launch activities can't happen simultaneously. This option has been added as a distractor. References: https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html https://docs.aws.amazon.com/autoscaling/ec2/userguide/healthcheck.html",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A startup uses a fleet of Amazon EC2 servers to manage its CRM application. These Amazon EC2 servers are behind Elastic Load Balancing (ELB). Which of the following configurations are NOT allowed for Elastic Load Balancing?",
      "options": [
        "A. Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four instances are deployed across two Availability Zones of us-east-1 region",
        "B. Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. Two of these instances are deployed in Availability Zone A of us-east-1 region and the other two instances are deployed in Availability Zone B of us-west-1 region",
        "C. Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four instances are deployed in Availability Zone A of us-east-1 region",
        "D. Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four instances are deployed in Availability Zone B of us-west-1 region"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. Two of these instances are deployed in Availability Zone A of us-east-1 region and the other two instances are deployed in Availability Zone B of us-west-1 region Elastic Load Balancer automatically distributes incoming traffic across multiple targets – Amazon EC2 instances, containers, IP addresses, and Lambda functions – in multiple Availability Zones and ensures only healthy targets receive traffic. ELB cannot distribute incoming traffic for targets deployed in different regions. This configuration is NOT allowed for the Elastic Load Balancer and therefore this is the correct option. Incorrect options: Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four instances are deployed across two Availability Zones of us-east-1 region Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four instances are deployed in Availability Zone A of us-east-1 region Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four instances are deployed in Availability Zone B of us-west-1 region These three options are valid configurations for the Elastic Load Balancing to distribute traffic (either within an Availability Zone or between two Availability Zones). Reference: https://aws.amazon.com/elasticloadbalancing/",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "The DevOps team at a major financial services company uses Multi-Availability Zone (Multi-AZ) deployment for its MySQL Amazon RDS database in order to automate its database replication and augment data durability. The DevOps team has scheduled a maintenance window for a database engine level upgrade for the coming weekend. Which of the following is the correct outcome during the maintenance window?",
      "options": [
        "A. Any database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment triggers both the primary and standby database instances to be upgraded at the same time. However, this does not cause any downtime until the upgrade is complete",
        "B. Any database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment triggers the standby database instance to be upgraded which is then followed by the upgrade of the primary database instance. This does not cause any downtime for the duration of the upgrade",
        "C. Any database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment triggers the primary database instance to be upgraded which is then followed by the upgrade of the standby database instance. This does not cause any downtime for the duration of the upgrade",
        "D. Any database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment triggers both the primary and standby database instances to be upgraded at the same time. This causes downtime until the upgrade is complete"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: Any database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment triggers both the primary and standby database instances to be upgraded at the same time. This causes downtime until the upgrade is complete Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching, and backups. Upgrades to the database engine level require downtime. Even if your Amazon RDS DB instance uses a Multi-AZ deployment, both the primary and standby DB instances are upgraded at the same time. This causes downtime until the upgrade is complete, and the duration of the downtime varies based on the size of your database instance. Amazon RDS DB Engine Maintenance: via - https://aws.amazon.com/premiumsupport/knowledge-center/rds-required-maintenance/ Incorrect options: Any database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment triggers both the primary and standby database instances to be upgraded at the same time. However, this does not cause any downtime until the upgrade is complete - For Amazon RDS database engine level upgrade, primary and standby database instances are upgraded at the same time and it causes downtime until the upgrade is complete, hence this option is incorrect. Any database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment triggers the standby database instance to be upgraded which is then followed by the upgrade of the primary database instance. This does not cause any downtime for the duration of the upgrade - For Amazon RDS database engine level upgrade, primary and standby database instances are upgraded at the same time and it causes downtime until the upgrade is complete, hence this option is incorrect. Any database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment triggers the primary database instance to be upgraded which is then followed by the upgrade of the standby database instance. This does not cause any downtime for the duration of the upgrade - For Amazon RDS database engine level upgrade, primary and standby database instances are upgraded at the same time and it causes downtime until the upgrade is complete, hence this option is incorrect. Reference: https://aws.amazon.com/premiumsupport/knowledge-center/rds-required-maintenance/",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "The engineering team at a social media company has noticed that while some of the images stored in Amazon S3 are frequently accessed, others sit idle for a considerable span of time. As a solutions architect, what is your recommendation to build the MOST cost-effective solution?",
      "options": [
        "A. Store the images using the Amazon S3 Intelligent-Tiering storage class",
        "B. Store the images using the Amazon S3 Standard-IA storage class",
        "C. Create a data monitoring application on an Amazon EC2 instance in the same region as the bucket storing the images. The application is triggered daily via Amazon CloudWatch and it changes the storage class of infrequently accessed objects to Amazon S3 One Zone-IA and the frequently accessed objects are migrated to Amazon S3 Standard class",
        "D. Create a data monitoring application on an Amazon EC2 instance in the same region as the bucket storing the images. The application is triggered daily via Amazon CloudWatch and it changes the storage class of infrequently accessed objects to Amazon S3 Standard-IA and the frequently accessed objects are migrated to Amazon S3 Standard class"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Store the images using the Amazon S3 Intelligent-Tiering storage class The Amazon S3 Intelligent-Tiering storage class is designed to optimize costs by automatically moving data to the most cost-effective access tier, without performance impact or operational overhead. It works by storing objects in two access tiers: one tier that is optimized for frequent access and another lower-cost tier that is optimized for infrequent access. For a small monthly monitoring and automation fee per object, Amazon S3 monitors access patterns of the objects in S3 Intelligent-Tiering and moves the ones that have not been accessed for 30 consecutive days to the infrequent access tier. If an object in the infrequent access tier is accessed, it is automatically moved back to the frequent access tier. Therefore using the Amazon S3 Intelligent-Tiering storage class is the correct solution for the given problem statement. Amazon S3 Storage Classes Overview: Incorrect options: Store the images using the Amazon S3 Standard-IA storage class Amazon S3 Standard-IA is for data that is accessed less frequently but requires rapid access when needed. Amazon S3 Standard-IA offers high durability, high throughput, and low latency of Amazon S3 Standard, with a low per GB storage price and per GB retrieval fee. This combination of low cost and high performance makes Amazon S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files. The minimum storage duration charge is 30 days. As some of the objects are frequently accessed, the per GB retrieval fee for Amazon S3 Standard-IA can cause the costs to shoot up, hence this option is incorrect. Create a data monitoring application on an Amazon EC2 instance in the same region as the bucket storing the images. The application is triggered daily via Amazon CloudWatch and it changes the storage class of infrequently accessed objects to Amazon S3 One Zone-IA and the frequently accessed objects are migrated to Amazon S3 Standard class Create a data monitoring application on an Amazon EC2 instance in the same region as the bucket storing the images. The application is triggered daily via Amazon CloudWatch and it changes the storage class of infrequently accessed objects to Amazon S3 Standard-IA and the frequently accessed objects are migrated to Amazon S3 Standard class Creating a data monitoring application on an Amazon EC2 instance for managing the desired Amazon S3 storage class entails significant development cost as well as infrastructure maintenance effort. The Amazon S3 Intelligent-Tiering storage class does the job in a cost-effective way. Therefore both these options are incorrect. Reference: https://aws.amazon.com/s3/storage-classes/",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "The CTO of an online home rental marketplace wants to re-engineer the caching layer of the current architecture for its relational database. The CTO wants the caching layer to have replication and archival support built into the architecture. Which of the following AWS service offers the capabilities required for the re-engineering of the caching layer?",
      "options": [
        "A. Amazon DynamoDB Accelerator (DAX)",
        "B. Amazon ElastiCache for Memcached",
        "C. Amazon ElastiCache for Redis",
        "D. Amazon DocumentDB"
      ],
      "correct_answer": "C",
      "explanation": "Correct option: Amazon ElastiCache for Redis Amazon ElastiCache for Redis is a blazing fast in-memory data store that provides sub-millisecond latency to power internet-scale real-time applications. Amazon ElastiCache for Redis is a great choice for real-time transactional and analytical processing use cases such as caching, chat/messaging, gaming leaderboards, geospatial, machine learning, media streaming, queues, real-time analytics, and session store. ElastiCache for Redis supports replication and archival snapshots right out of the box. Hence this is the correct option. Exam Alert: Please review this comparison sheet for Redis vs Memcached features: via - https://aws.amazon.com/elasticache/redis-vs-memcached/ Incorrect options: Amazon ElastiCache for Memcached - Amazon ElastiCache for Memcached is a Memcached-compatible in-memory key-value store service that can be used as a cache or a data store. Amazon ElastiCache for Memcached is a great choice for implementing an in-memory cache to decrease access latency, increase throughput, and ease the load off your relational or NoSQL database. Session stores are easy to create with Amazon ElastiCache for Memcached. ElastiCache for Memcached does not support replication and archival snapshots, so this option is ruled out. Amazon DynamoDB Accelerator (DAX) - Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications. DAX is a DynamoDB-compatible caching service that enables you to benefit from fast in-memory performance for demanding applications. DAX cannot be used as a caching layer for a relational database. Amazon DocumentDB - Amazon DocumentDB is a fast, scalable, highly available, and fully managed document database service that supports MongoDB workloads. As a document database, Amazon DocumentDB makes it easy to store, query, and index JSON data. DocumentDB cannot be used as a caching layer for a relational database. References: https://aws.amazon.com/elasticache/redis/ https://aws.amazon.com/elasticache/redis-vs-memcached/",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A systems administration team has a requirement to run certain custom scripts only once during the launch of the Amazon Elastic Compute Cloud (Amazon EC2) instances that host their application. Which of the following represents the best way of configuring a solution for this requirement with minimal effort?",
      "options": [
        "A. Update Amazon EC2 instance configuration to ensure that the custom scripts, added as user data scripts, are run only during the boot process",
        "B. Run the custom scripts as user data scripts on the Amazon EC2 instances",
        "C. Run the custom scripts as instance metadata scripts on the Amazon EC2 instances",
        "D. Use AWS CLI to run the user data scripts only once while launching the instance"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Run the custom scripts as user data scripts on the Amazon EC2 instances When you launch an instance in Amazon EC2, you have the option of passing user data to the instance that can be used to perform common automated configuration tasks and even run scripts after the instance starts. You can pass two types of user data to Amazon EC2: shell scripts and cloud-init directives. By default, user data scripts and cloud-init directives run only during the boot cycle when you first launch an instance. Hence, no extra configuration is needed, apart from including the custom scripts in user data scripts. Incorrect options: Update Amazon EC2 instance configuration to ensure that the custom scripts, added as user data scripts, are run only during the boot process - You can update your configuration to ensure that your user data scripts and cloud-init directives run every time you restart your instance. By default, the scripts are run, only once during the boot process while first launching the instance. Run the custom scripts as instance metadata scripts on the Amazon EC2 instances- Instance metadata is data about your instance that you can use to configure or manage the running instance. Metadata cannot be used to run custom scripts. Use AWS CLI to run the user data scripts only once while launching the instance - This statement is incorrect and used only as a distractor. References: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A security consultant is designing a solution for a company that wants to provide developers with individual AWS accounts through AWS Organizations, while also maintaining standard security controls. Since the individual developers will have AWS account root user-level access to their own accounts, the consultant wants to ensure that the mandatory AWS CloudTrail configuration that is applied to new developer accounts is not modified. Which of the following actions meets the given requirements?",
      "options": [
        "A. Configure a new trail in AWS CloudTrail from within the developer accounts with the organization trails option enabled",
        "B. Set up a service control policy (SCP) that prohibits changes to AWS CloudTrail, and attach it to the developer accounts",
        "C. Set up an IAM policy that prohibits changes to AWS CloudTrail and attach it to the root user",
        "D. Set up a service-linked role for AWS CloudTrail with a policy condition that allows changes only from an Amazon Resource Name (ARN) in the master account"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Set up a service control policy (SCP) that prohibits changes to AWS CloudTrail, and attach it to the developer accounts Service control policy (SCP) is a type of organization policy that you can use to manage permissions in your organization. SCPs offer central control over the maximum available permissions for all accounts in your organization. SCPs help you to ensure your accounts stay within your organization’s access control guidelines. An SCP restricts permissions for IAM users and roles in member accounts, including the member account's root user. Any account has only those permissions permitted by every parent above it. If a permission is blocked at any level above the account, either implicitly (by not being included in an Allow policy statement) or explicitly (by being included in a Deny policy statement), a user or role in the affected account can't use that permission, even if the account administrator attaches the AdministratorAccess IAM policy with / permissions to the user. SCPs don't affect users or roles in the management account. They affect only the member accounts in your organization. Incorrect options: Configure a new trail in AWS CloudTrail from within the developer accounts with the organization trails option enabled - Configuring each developer account individually is not a viable solution to start with. In addition, any configuration changes can be undone by the user once they are logged into their individual accounts as root users. Set up an IAM policy that prohibits changes to AWS CloudTrail and attach it to the root user - The root user can modify this IAM policy itself, so this option is not correct. Set up a service-linked role for AWS CloudTrail with a policy condition that allows changes only from an Amazon Resource Name (ARN) in the master account - A service-linked role is a unique type of IAM role that is linked directly to an AWS service. Service-linked roles are predefined by the service and include all the permissions that the service requires to call other AWS services on your behalf. The linked service also defines how you create, modify, and delete a service-linked role. The linked service defines the permissions of its service-linked roles, and unless defined otherwise, only that service can assume the roles. The defined permissions include the trust policy and the permissions policy, and that permissions policy cannot be attached to any other entity such as the ARN in the master account. Reference: https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A healthcare company runs a fleet of Amazon EC2 instances in two private subnets (named PR1 and PR2) across two Availability Zones (AZs) named A1 and A2. The Amazon EC2 instances need access to the internet for operating system patch management and third-party software maintenance. To facilitate this, the engineering team at the company wants to set up two Network Address Translation gateways (NAT gateways) in a highly available configuration. Which of the following options would you suggest?",
      "options": [
        "A. Set up a total of two NAT gateways. NAT gateway N1 should be set up in private subnet PR1 in Availability Zone A1. NAT gateway N2 should be set up in private subnet PR2 in Availability Zone A2",
        "B. Set up a total of two NAT gateways. Both NAT gateways N1 and N2 should be set up in a single public subnet PU1 in any of the Availability Zones A1 or A2",
        "C. Set up a total of one NAT gateway. NAT gateway N1 should be set up in public subnet PU1 in any of the Availability Zones A1 or A2",
        "D. Set up a total of two NAT gateways. NAT gateway N1 should be set up in public subnet PU1 in Availability Zone A1. NAT gateway N2 should be set up in public subnet PU2 in Availability Zone A2"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: Set up a total of two NAT gateways. NAT gateway N1 should be set up in public subnet PU1 in Availability Zone A1. NAT gateway N2 should be set up in public subnet PU2 in Availability Zone A2 A NAT gateway is a Network Address Translation (NAT) service. You can use a NAT gateway so that instances in a private subnet can connect to services outside your VPC but external services cannot initiate a connection with those instances. For the given use case, the Amazon EC2 instances in the private subnets can connect to the internet through public NAT gateways in their respective Availability Zones (AZ). You should create public NAT gateway in the public subnet of each AZ and must associate an elastic IP address with the NAT gateway at creation. Then, you can route traffic from the NAT gateway to the internet gateway for the VPC. If you have resources in multiple Availability Zones and they share one NAT gateway, and if the NAT gateway’s Availability Zone is down, resources in the other Availability Zones lose internet access. To create a highly available or an Availability Zone independent architecture, create a NAT gateway in each Availability Zone and configure your routing to ensure that resources use the NAT gateway in the same Availability Zone. via - https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html Incorrect options: Set up a total of two NAT gateways. NAT gateway N1 should be set up in private subnet PR1 in Availability Zone A1. NAT gateway N2 should be set up in private subnet PR2 in Availability Zone A2 - For the Amazon EC2 instances in the private subnet, you can facilitate outbound internet connectivity in a highly available configuration by creating a public NAT gateway in the public subnet of each AZ. You cannot create NAT gateways in the private subnet for the given use case. Set up a total of two NAT gateways. Both NAT gateways N1 and N2 should be set up in a single public subnet PU1 in any of the Availability Zones A1 or A2 - For the Amazon EC2 instances in the private subnet, you can facilitate outbound internet connectivity in a highly available configuration by creating a public NAT gateway in the public subnet of each AZ. You cannot create both NAT gateways in a single public subnet, as this configuration would not be highly available. Set up a total of one NAT gateway. NAT gateway N1 should be set up in public subnet PU1 in any of the Availability Zones A1 or A2 - For the Amazon EC2 instances in the private subnet, you can facilitate outbound internet connectivity in a highly available configuration by creating a public NAT gateway in the public subnet of each AZ. You cannot create a single NAT gateway, as this configuration would not be highly available. Reference: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "During a review, a security team has flagged concerns over an Amazon EC2 instance querying IP addresses used for cryptocurrency mining. The Amazon EC2 instance does not host any authorized application related to cryptocurrency mining. Which AWS service can be used to protect the Amazon EC2 instances from such unauthorized behavior in the future?",
      "options": [
        "A. AWS Web Application Firewall (AWS WAF)",
        "B. AWS Shield Advanced",
        "C. AWS Firewall Manager",
        "D. Amazon GuardDuty"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: Amazon GuardDuty Amazon GuardDuty continuously monitors for malicious or unauthorized behavior to help protect your AWS resources, including your AWS accounts and access keys. Amazon GuardDuty identifies any unusual or unauthorized activity, like cryptocurrency mining or infrastructure deployments in a region that has never been used. Powered by threat intelligence and machine learning, GuardDuty is continuously evolving to help you protect your AWS environment. The cryptocurrency finding expands the service’s ability to detect Amazon EC2 instances querying IP addresses associated with the cryptocurrency-related activity. The finding type is: CryptoCurrency:EC2/BitcoinTool.B, CryptoCurrency:EC2/BitcoinTool.B!DNS. This finding informs you that the listed Amazon EC2 instance in your AWS environment is querying a domain name that is associated with Bitcoin or other cryptocurrency-related activity. Bitcoin is a worldwide cryptocurrency and digital payment system that can be exchanged for other currencies, products, and services. Bitcoin is a reward for bitcoin mining and is highly sought after by threat actors. If you use the Amazon EC2 instance to mine or manage cryptocurrency, or this instance is otherwise involved in blockchain activity, this finding could represent expected activity for your environment. If this is the case in your AWS environment, AWS recommends that you set up a suppression rule for this finding. Incorrect options: AWS Web Application Firewall (AWS WAF) - AWS WAF is a web application firewall that helps protect your web applications or APIs against common web exploits and bots that may affect availability, compromise security, or consume excessive resources. AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that control bot traffic and block common attack patterns, such as SQL injection or cross-site scripting. AWS Shield Advanced - For higher levels of protection against attacks targeting your applications running on Amazon Elastic Compute Cloud (EC2), Elastic Load Balancing (ELB), Amazon CloudFront, AWS Global Accelerator, and Amazon Route 53 resources, you can subscribe to AWS Shield Advanced. In addition to the network and transport layer protections that come with Standard, AWS Shield Advanced provides additional detection and mitigation against large and sophisticated DDoS attacks, near real-time visibility into attacks, and integration with AWS WAF, a web application firewall. AWS Shield Advanced also gives you 24x7 access to the AWS DDoS Response Team (DRT) and protection against DDoS-related spikes in your Amazon Elastic Compute Cloud (EC2), Elastic Load Balancing (ELB), Amazon CloudFront, AWS Global Accelerator, and Amazon Route 53 charges. AWS Firewall Manager - AWS Firewall Manager is a security management service that allows you to centrally configure and manage firewall rules across your accounts and applications in AWS Organizations. As new applications are created, Firewall Manager makes it easy to bring new applications and resources into compliance by enforcing a common set of security rules. Now you have a single service to build firewall rules, create security policies, and enforce them in a consistent, hierarchical manner across your entire infrastructure, from a central administrator account. None of these three services can detect unauthorized cryptocurrency mining activity on EC2 instances, so these options are incorrect. Reference: https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_finding-types-ec2.html#cryptocurrency-ec2-bitcointoolbdns",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A company is developing a document management application on AWS. The application runs on Amazon EC2 instances in multiple Availability Zones (AZs). The company requires the document store to be highly available and the documents need to be returned immediately when requested. The engineering team has configured the application to use Amazon Elastic Block Store (Amazon EBS) to store the documents but the team is willing to consider other options to meet the availability requirement. As a solutions architect, which of the following will you recommend?",
      "options": [
        "A. Set up Amazon EBS as the Amazon EC2 instance root volume and then configure the application to use Amazon S3 Glacier as the document store",
        "B. Create snapshots for the Amazon EBS volumes regularly and then build new volumes using those snapshots in additional Availability Zones",
        "C. Provision at least three Provisioned IOPS Amazon Instance Store volumes for the Amazon EC2 instances and then mount these volumes to multiple Amazon EC2 instances",
        "D. Set up Amazon EBS as the Amazon EC2 instance root volume and then configure the application to use Amazon S3 as the document store"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: Set up Amazon EBS as the Amazon EC2 instance root volume and then configure the application to use Amazon S3 as the document store Instances that use Amazon EBS for the root device automatically have an Amazon EBS volume attached. When you launch an Amazon EBS-backed instance, AWS creates an Amazon EBS volume for each Amazon EBS snapshot referenced by the AMI you use. An Amazon EBS-backed instance can be stopped and later restarted without affecting data stored in the attached volumes. Amazon S3 provides access to reliable, fast, and inexpensive data storage infrastructure. It is designed to make web-scale computing easier by enabling you to store and retrieve any amount of data, at any time, from within Amazon EC2 or anywhere on the web. S3 is highly available and can be configured to work as a document store for the given use case. Incorrect options: Set up Amazon EBS as the Amazon EC2 instance root volume and then configure the application to use Amazon S3 Glacier as the document store - As the documents need to be returned immediately when requested, Amazon S3 Glacier is not the right fit, since there is a lag of several minutes/hours when you want to read data from Glacier. Create snapshots for the Amazon EBS volumes regularly and then build new volumes using those snapshots in additional Availability Zones - You can back up the data on your Amazon EBS volumes to Amazon S3 by taking point-in-time snapshots. Snapshots are incremental backups, which means that only the blocks on the device that have changed after your most recent snapshot are saved. Hence, using Amazon EBS volumes as a primary storage solution is ineffective, and creating recurring snapshots is a management nightmare for the current use case. Provision at least three Provisioned IOPS Amazon Instance Store volumes for the Amazon EC2 instances and then mount these volumes to multiple Amazon EC2 instances - You cannot mount Instance Store volumes to multiple Amazon EC2 instances. An instance store provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host computer. References: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html https://aws.amazon.com/s3/storage-classes/",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "The engineering team at a retail company is planning to migrate to AWS Cloud from the on-premises data center. The team is evaluating Amazon Relational Database Service (Amazon RDS) as the database tier for its flagship application. The team has hired you as an AWS Certified Solutions Architect Associate to advise on Amazon RDS Multi-AZ capabilities. Which of the following would you identify as correct for Amazon RDS Multi-AZ? (Select two)",
      "options": [
        "A. To enhance read scalability, a Multi-AZ standby instance can be used to serve read requests",
        "B. Amazon RDS applies operating system updates by performing maintenance on the standby, then promoting the standby to primary and finally performing maintenance on the old primary, which becomes the new standby",
        "C. Amazon RDS automatically initiates a failover to the standby, in case primary database fails for any reason",
        "D. For automated backups, I/O activity is suspended on your primary database since backups are not taken from standby database",
        "E. Updates to your database Instance are asynchronously replicated across the Availability Zone to the standby in order to keep both in sync"
      ],
      "correct_answer": "B",
      "explanation": "Correct options: Amazon RDS applies operating system updates by performing maintenance on the standby, then promoting the standby to primary and finally performing maintenance on the old primary, which becomes the new standby Running a DB instance as a Multi-AZ deployment can further reduce the impact of a maintenance event because Amazon RDS applies operating system updates by following these steps: Perform maintenance on the standby. Promote the standby to primary. Perform maintenance on the old primary, which becomes the new standby. When you modify the database engine for your DB instance in a Multi-AZ deployment, then Amazon RDS upgrades both the primary and secondary DB instances at the same time. In this case, the database engine for the entire Multi-AZ deployment is shut down during the upgrade. Amazon RDS automatically initiates a failover to the standby, in case primary database fails for any reason You also benefit from enhanced database availability when running your DB instance as a Multi-AZ deployment. If an Availability Zone failure or DB instance failure occurs, your availability impact is limited to the time automatic failover takes to complete. Another implied benefit of running your DB instance as a Multi-AZ deployment is that DB instance failover is automatic and requires no administration. In an Amazon RDS context, this means you are not required to monitor DB instance events and initiate manual DB instance recovery in the event of an Availability Zone failure or DB instance failure. Incorrect options: For automated backups, I/O activity is suspended on your primary database since backups are not taken from standby database - The availability benefits of Multi-AZ also extend to planned maintenance. For example, with automated backups, I/O activity is no longer suspended on your primary during your preferred backup window, since backups are taken from the standby. To enhance read scalability, a Multi-AZ standby instance can be used to serve read requests - A Multi-AZ standby cannot serve read requests. Multi-AZ deployments are designed to provide enhanced database availability and durability, rather than read scaling benefits. As such, the feature uses synchronous replication between primary and standby. AWS implementation makes sure the primary and the standby are constantly in sync, but precludes using the standby for read or write operations. Updates to your database Instance are asynchronously replicated across the Availability Zone to the standby in order to keep both in sync - When you create your DB instance to run as a Multi-AZ deployment, Amazon RDS automatically provisions and maintains a synchronous “standby” replica in a different Availability Zone. Updates to your DB Instance are synchronously replicated across the Availability Zone to the standby in order to keep both in sync and protect your latest database updates against DB instance failure. Reference: https://aws.amazon.com/rds/faqs/",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A development team has noticed that one of the Amazon EC2 instances has been incorrectly configured with the 'DeleteOnTermination' attribute set to True for its root EBS volume. As a Solution's Architect, can you suggest a way to disable this flag while the instance is still running?",
      "options": [
        "A. Set the DeleteOnTermination attribute to False using the command line",
        "B. Update the attribute using AWS management console. Select the Amazon EC2 instance and then uncheck the DeleteOnTermination check box for the root EBS volume",
        "C. The attribute cannot be updated when the instance is running. Stop the instance from Amazon EC2 console and then update the flag",
        "D. Set the DisableApiTermination attribute of the instance using the API"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: When an instance terminates, the value of the DeleteOnTermination attribute for each attached EBS volume determines whether to preserve or delete the volume. By default, the DeleteOnTermination attribute is set to True for the root volume and is set to False for all other volume types. Set the DeleteOnTermination attribute to False using the command line If the instance is already running, you can set DeleteOnTermination to False using the command line. Incorrect options: Update the attribute using AWS management console. Select the Amazon EC2 instance and then uncheck the DeleteOnTermination check box for the root EBS volume - You can set the DeleteOnTermination attribute to False when you launch a new instance. It is not possible to update this attribute of a running instance from the AWS console. Set the DisableApiTermination attribute of the instance using the API - By default, you can terminate your instance using the Amazon EC2 console, command-line interface, or API. To prevent your instance from being accidentally terminated using Amazon EC2, you can enable termination protection for the instance. The DisableApiTermination attribute controls whether the instance can be terminated using the console, CLI, or API. This option cannot be used to control the delete status for the EBS volume when the instance terminates. The attribute cannot be updated when the instance is running. Stop the instance from Amazon EC2 console and then update the flag - This statement is wrong and given only as a distractor. References: https://aws.amazon.com/premiumsupport/knowledge-center/deleteontermination-ebs/ https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/terminating-instances.html#delete-on-termination-running-instance",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A company helps its customers legally sign highly confidential contracts. To meet the strong industry requirements, the company must ensure that the signed contracts are encrypted using the company's proprietary algorithm. The company is now migrating to AWS Cloud using Amazon Simple Storage Service (Amazon S3) and would like you, the solution architect, to advise them on the encryption scheme to adopt. What do you recommend?",
      "options": [
        "A. Server-side encryption with Amazon S3 managed keys (SSE-S3)",
        "B. Server-side encryption with AWS KMS keys (SSE-KMS)",
        "C. Server-side encryption with customer-provided keys (SSE-C)",
        "D. Client Side Encryption"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: Client Side Encryption Client-side encryption is the act of encrypting your data locally to help ensure its security in transit and at rest. To encrypt your objects before you send them to Amazon S3, use the Amazon S3 Encryption Client. When your objects are encrypted in this manner, your objects aren't exposed to any third party, including AWS. Amazon S3 receives your objects already encrypted; Amazon S3 does not play a role in encrypting or decrypting your objects. You can use both the Amazon S3 Encryption Client and server-side encryption to encrypt your data. When you send encrypted objects to Amazon S3, Amazon S3 doesn't recognize the objects as being encrypted, it only detects typical objects. Incorrect options: Server-side encryption with AWS KMS keys (SSE-KMS) - AWS Key Management Service (AWS KMS) is a service that combines secure, highly available hardware and software to provide a key management system scaled for the cloud. When you use server-side encryption with AWS KMS (SSE-KMS), you can specify a customer-managed CMK that you have already created. SSE-KMS provides you with an audit trail that shows when your CMK was used and by whom. Server-side encryption with Amazon S3 managed keys (SSE-S3) - When you use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key. Server-side encryption with customer-provided keys (SSE-C) - With Server-Side Encryption with Customer-Provided Keys (SSE-C), you manage the encryption keys and Amazon S3 manages the encryption, as it writes to disks, and decryption when you access your objects. References: https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingClientSideEncryption.html https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "You are using AWS Lambda to implement a batch job for a big data analytics workflow. Based on historical trends, a similar job runs for 30 minutes on average. The AWS Lambda function pulls data from Amazon S3, processes it, and then writes the results back to Amazon S3. When you deployed your AWS Lambda function, you noticed an issue where the AWS Lambda function abruptly failed after 15 minutes of execution. As a solutions architect, which of the following would you identify as the root cause of the issue?",
      "options": [
        "A. The AWS Lambda function is running out of memory",
        "B. The AWS Lambda function chosen runtime is wrong",
        "C. The AWS Lambda function is timing out",
        "D. The AWS Lambda function is missing IAM permissions"
      ],
      "correct_answer": "C",
      "explanation": "Correct option: AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume. With AWS Lambda, you can run code for virtually any type of application or backend service - all with zero administration. Just upload your code and Lambda takes care of everything required to run and scale your code with high availability. You can set up your code to automatically trigger from other AWS services or call it directly from any web or mobile app. AWS Lambda functions can be configured to run up to 15 minutes per execution. You can set the timeout to any value between 1 second and 15 minutes. The AWS Lambda function is timing out AWS Lambda functions time out after 15 minutes, and are not usually meant for long-running jobs. Incorrect options: The AWS Lambda function is running out of memory - Memory errors will not result in the abrupt termination of the function with no error message. The AWS Lambda function chosen runtime is wrong - AWS Lambda function execution will fail if there is an issue with runtime. So, this is not the issue in the current case. The AWS Lambda function is missing IAM permissions - Without enough permissions, AWS Lambda would not have been able to start its execution at all. So, permissions are not an issue here. Reference: https://aws.amazon.com/lambda/faqs/",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A startup wants to create a highly available architecture for its multi-tier application. Currently, the startup manages a single Amazon EC2 instance along with a single Amazon RDS MySQL DB instance. The startup has hired you as an AWS Certified Solutions Architect - Associate to build a solution that meets these requirements while minimizing the underlying infrastructure maintenance effort. What will you recommend?",
      "options": [
        "A. Create an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances across two Availability Zones. Configure an Application Load Balancer having a target group of these Amazon EC2 instances. Set up a read replica of the Amazon RDS MySQL DB in another Availability Zone",
        "B. Create an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances across two Availability Zones. Configure an Application Load Balancer having a target group of these Amazon EC2 instances. Set up Amazon RDS MySQL DB in a multi-AZ configuration",
        "C. Create an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances in a single Availability Zone. Configure an Application Load Balancer having a target group of these Amazon EC2 instances. Set up Amazon RDS MySQL DB in a multi-AZ configuration",
        "D. Provision a second Amazon EC2 instance in another Availability Zone. Provision a second Amazon RDS MySQL DB in another Availabililty Zone. Leverage Amazon Route 53 for equal distribution of incoming traffic to the Amazon EC2 instances. Use a custom script to sync data across the two MySQL DBs"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Create an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances across two Availability Zones. Configure an Application Load Balancer having a target group of these Amazon EC2 instances. Set up Amazon RDS MySQL DB in a multi-AZ configuration Amazon EC2 Auto Scaling is a fully managed service designed to launch or terminate Amazon EC2 instances automatically to help ensure you have the correct number of Amazon EC2 instances available to handle the load for your application. via - https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html Application Load Balancer automatically distributes your incoming traffic across multiple targets, such as Amazon EC2 instances, containers, and IP addresses, in one or more Availability Zones. It monitors the health of its registered targets, and routes traffic only to the healthy targets. via - https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html In a multi-AZ deployment, Amazon RDS automatically provisions and maintains a synchronous “standby” replica in a different Availability Zone. Updates to your DB Instance are synchronously replicated across Availability Zones to the standby to keep both in sync and protect your latest database updates against DB instance failure. via - https://aws.amazon.com/rds/features/multi-az/ To create a highly available architecture for the given use case, you need to set up an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances across two Availability Zones and then point the Application Load Balancer to the target group having the Amazon EC2 instances. Incorrect options: Create an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances across two Availability Zones. Configure an Application Load Balancer having a target group of these Amazon EC2 instances. Set up a read replica of the Amazon RDS MySQL DB in another Availability Zone - A read replica cannot be used to enhance the availability of an Amazon RDS MySQL DB. You must use the multi-AZ configuration of Amazon RDS MySQL for this use case. Create an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances in a single Availability Zone. Configure an Application Load Balancer having a target group of these Amazon EC2 instances. Set up Amazon RDS MySQL DB in a multi-AZ configuration - Having the Amazon EC2 instances in a single Availability Zone will not create a highly available solution. In the case of an outage for the entire Availability Zone, the Amazon EC2 instances would be unreachable. Hence this option is incorrect. Provision a second Amazon EC2 instance in another Availability Zone. Provision a second Amazon RDS MySQL DB in another Availabililty Zone. Leverage Amazon Route 53 for equal distribution of incoming traffic to the Amazon EC2 instances. Use a custom script to sync data across the two MySQL DBs - This option has been added as a distractor. It requires significant monitoring and development effort to keep the Amazon EC2 instances highly available as well as keep the MySQL DBs in sync. References: https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html https://aws.amazon.com/rds/features/multi-az/",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "As a Solutions Architect, you would like to completely secure the communications between your Amazon CloudFront distribution and your Amazon S3 bucket which contains the static files for your website. Users should only be able to access the Amazon S3 bucket through Amazon CloudFront and not directly. What do you recommend?",
      "options": [
        "A. Create a bucket policy to only authorize the IAM role attached to the Amazon CloudFront distribution",
        "B. Update the Amazon S3 bucket security groups to only allow traffic from the Amazon CloudFront security group",
        "C. Make the Amazon S3 bucket public",
        "D. Create an origin access identity (OAI) and update the Amazon S3 Bucket Policy"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: Create an origin access identity (OAI) and update the Amazon S3 Bucket Policy To restrict access to content that you serve from Amazon S3 buckets, you need to follow the following steps: Create a special Amazon CloudFront user called an origin access identity (OAI) and associate it with your distribution. Configure your Amazon S3 bucket permissions so that Amazon CloudFront can use the OAI to access the files in your bucket and serve them to your users. Make sure that users can’t use a direct URL to the Amazon S3 bucket to access a file there. After you take these steps, users can only access your files through Amazon CloudFront, not directly from the Amazon S3 bucket. In general, if you’re using an Amazon S3 bucket as the origin for a Amazon CloudFront distribution, you can either allow everyone to have access to the files there, or you can restrict access. If you restrict access by using, for example, Amazon CloudFront signed URLs or signed cookies, you also won’t want people to be able to view files by simply using the direct Amazon S3 URL for the file. Instead, you want them to only access the files by using the Amazon CloudFront URL, so your content remains protected. Incorrect options: Update the Amazon S3 bucket security groups to only allow traffic from the Amazon CloudFront security group - Amazon S3 buckets don't have security groups, hence this is an incorrect option. Make the Amazon S3 bucket public - If the Amazon S3 bucket is made public, it can be accessed by anyone directly. This is not the requirement. Create a bucket policy to only authorize the IAM role attached to the Amazon CloudFront distribution - You cannot attach IAM roles to the Amazon CloudFront distribution. Here you need to use an OAI. Reference: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "As a Solutions Architect, you have set up a database on a single Amazon EC2 instance that has an Amazon EBS volume of type gp2. You currently have 300 gigabytes of space on the gp2 device. The Amazon EC2 instance is of type m5.large. The database performance has recently been poor and upon looking at Amazon CloudWatch, you realize the IOPS on the Amazon EBS volume is maxing out. The disk size of the database must not change because of a licensing issue. How do you troubleshoot this issue?",
      "options": [
        "A. Stop the Amazon CloudWatch agent to improve performance",
        "B. Convert the gp2 volume to an io1",
        "C. Increase the IOPS on the gp2 volume",
        "D. Convert the Amazon EC2 instance to an i3.4xlarge"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Amazon EBS provides the following volume types, which differ in performance characteristics and price so that you can tailor your storage performance and cost to the needs of your applications. The volumes types fall into two categories: SSD-backed volumes optimized for transactional workloads involving frequent read/write operations with small I/O size, where the dominant performance attribute is IOPS HDD-backed volumes optimized for large streaming workloads where throughput (measured in MiB/s) is a better performance measure than IOPS Convert the gp2 volume to an io1 Provisioned IOPS SSD (io1) volumes are designed to meet the needs of I/O-intensive workloads, particularly database workloads, that are sensitive to storage performance and consistency. Unlike gp2, which uses a bucket and credit model to calculate performance, an io1 volume allows you to specify a consistent IOPS rate when you create the volume, and Amazon EBS delivers the provisioned performance 99.9 percent of the time. The only solution is to convert the volume into an io1 volume. This will allow us to keep the same disk size while independently increasing the IOPS for that volume. Incorrect options: Stop the Amazon CloudWatch agent to improve performance - The Amazon CloudWatch agent does not have any impact on the performance of the instance. Increase the IOPS on the gp2 volume - General Purpose SSD (gp2) volumes offer cost-effective storage that is ideal for a broad range of workloads. These volumes deliver single-digit millisecond latencies and the ability to burst to 3,000 IOPS for extended periods. Between a minimum of 100 IOPS (at 33.33 GiB and below) and a maximum of 16,000 IOPS (at 5,334 GiB and above), baseline performance scales linearly at 3 IOPS per GiB of volume size. AWS designs gp2 volumes to deliver their provisioned performance 99% of the time. A gp2 volume can range in size from 1 GiB to 16 TiB. IOPS cannot be directly increased on a gp2 volume without increasing its size, which is not possible due to the question's constraints. Convert the Amazon EC2 instance to an i3.4xlarge - Converting the Amazon EC2 instance to i3.4xlarge won't improve the Amazon EBS drive's performance. References: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html#EBSVolumeTypes_gp2 https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html#EBSVolumeTypes_piops",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "The systems administrator at a company wants to set up a highly available architecture for a bastion host solution. As a solutions architect, which of the following options would you recommend as the solution?",
      "options": [
        "A. Create a public Network Load Balancer that links to Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group",
        "B. Create a public Application Load Balancer that links to Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group",
        "C. Create a VPC Endpoint for a fleet of Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group",
        "D. Create an elastic IP address (EIP) and assign it to all Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Create a public Network Load Balancer that links to Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group Network Load Balancer is best suited for use-cases involving low latency and high throughput workloads that involve scaling to millions of requests per second. Network Load Balancer operates at the connection level (Layer 4), routing connections to targets - Amazon EC2 instances, microservices, and containers – within Amazon Virtual Private Cloud (Amazon VPC) based on IP protocol data. Including bastion hosts in your VPC environment enables you to securely connect to your Linux instances without exposing your environment to the Internet. After you set up your bastion hosts, you can access the other instances in your VPC through Secure Shell (SSH) connections on Linux. Bastion hosts are also configured with security groups to provide fine-grained ingress control. You need to remember that Bastion Hosts are using the SSH protocol, which is a TCP based protocol on port 22. They must be publicly accessible. Here, the correct answer is to use a Network Load Balancer, which supports TCP traffic, and will automatically allow you to connect to the Amazon EC2 instance in the backend. Incorrect options: Create an elastic IP address (EIP) and assign it to all Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group - An elastic IP address (EIP) can only be attached to one Amazon EC2 instance at a time, so it won't provide you a highly available setup on its own. Note that if we had two Elastic IPs and two Bastion Hosts, this would work. Create a VPC Endpoint for a fleet of Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group - A VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by AWS PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Instances in your VPC do not require public IP addresses to communicate with resources in the service. Traffic between your VPC and the other service does not leave the Amazon network. VPC Endpoints are not used on top of Amazon EC2 instances. They're a way to access AWS services privately within your VPC (without using the public internet). This is a distractor. Create a public Application Load Balancer that links to Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group - Application Load Balancer (ALB) operates at the request level (layer 7), routing traffic to targets – Amazon EC2 instances, containers, IP addresses and AWS Lambda functions based on the content of the request. Ideal for advanced load balancing of HTTP and HTTPS traffic, Application Load Balancer provides advanced request routing targeted at delivery of modern application architectures, including microservices and container-based applications. An Application Load Balancer only supports HTTP traffic, which is layer 7, while the SSH protocol is based on TCP and is layer 4. So, the Application Load Balancer doesn't work. References: https://docs.aws.amazon.com/quickstart/latest/linux-bastion/architecture.html https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "You are deploying a critical monolith application that must be deployed on a single web server, as it hasn't been created to work in distributed mode. Still, you want to make sure your setup can automatically recover from the failure of an Availability Zone (AZ). Which of the following options should be combined to form the MOST cost-efficient solution? (Select three)",
      "options": [
        "A. Create an auto-scaling group that spans across 2 Availability Zones, which min=1, max=1, desired=1",
        "B. Assign an Amazon EC2 Instance Role to perform the necessary API calls",
        "C. Create an elastic IP address (EIP) and use the Amazon EC2 user-data script to attach it",
        "D. Create a Spot Fleet request",
        "E. Create an Application Load Balancer and a target group with the instance(s) of the Auto Scaling Group",
        "F. Create an auto-scaling group that spans across 2 Availability Zones, which min=1, max=2, desired=2"
      ],
      "correct_answer": "A",
      "explanation": "Correct options: Create an auto-scaling group that spans across 2 Availability Zones, which min=1, max=1, desired=1 Amazon EC2 Auto Scaling helps you ensure that you have the correct number of Amazon EC2 instances available to handle the load for your application. You create collections of EC2 instances, called Auto Scaling groups. You can specify the minimum number of instances in each Auto Scaling group, and Amazon EC2 Auto Scaling ensures that your group never goes below this size. So we have an Auto Scaling Group with desired=1, across two AZ, so that if an instance goes down, it is automatically recreated in another AZ. So this option is correct. Create an elastic IP address (EIP) and use the Amazon EC2 user-data script to attach it Application Load Balancer (ALB) operates at the request level (layer 7), routing traffic to targets – Amazon EC2 instances, containers, IP addresses, and Lambda functions based on the content of the request. Ideal for advanced load balancing of HTTP and HTTPS traffic, Application Load Balancer provides advanced request routing targeted at delivery of modern application architectures, including microservices and container-based applications. An Elastic IP address is a static IPv4 address designed for dynamic cloud computing. An Elastic IP address is associated with your AWS account. With an Elastic IP address, you can mask the failure of an instance or software by rapidly remapping the address to another instance in your account. Now, between the ALB and the Elastic IP. If we use an ALB, things will still work, but we will have to pay for the provisioned ALB which sends traffic to only one Amazon EC2 instance. Instead, to minimize costs, we must use an Elastic IP. Assign an Amazon EC2 Instance Role to perform the necessary API calls For that Elastic IP to be attached to our Amazon EC2 instance, we must use an EC2 user data script, and our Amazon EC2 instance must have the correct IAM permissions to perform the API call, so we need an Amazon EC2 instance role. Incorrect options: Create a Spot Fleet request - A Spot Instance is an unused Amazon EC2 instance that is available for less than the On-Demand price. Because Spot Instances enable you to request unused EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly. The hourly price for a Spot Instance is called a Spot price. The Spot Fleet selects the Spot Instance pools that meet your needs and launches Spot Instances to meet the target capacity for the fleet. By default, Spot Fleets are set to maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated. Spot Fleets requests would not fit our purpose as we are looking at a critical application. Spot instances can be terminated. So this option is incorrect. Create an auto-scaling group that spans across 2 Availability Zones, which min=1, max=2, desired=2 - An Auto Scaling Group with desired=2 would create two instances, and this won't work for us as our monolith application is not made to work with two instances as per the given use-case. Create an Application Load Balancer and a target group with the instance(s) of the Auto Scaling Group - If we use an Application Load Balancer (ALB), things will still work, but we will have to pay for the provisioned ALB which sends traffic to only one Amazon EC2 instance. So this option is not correct. References: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A photo-sharing company is storing user profile pictures in an Amazon S3 bucket and an image analysis application is deployed on four Amazon EC2 instances. A solutions architect would like to trigger an image analysis procedure only on one of the four Amazon EC2 instances for each photo uploaded. What do you recommend?",
      "options": [
        "A. Create an Amazon S3 Event Notification that sends a message to an Amazon SQS queue. Make the Amazon EC2 instances read from the Amazon SQS queue",
        "B. Create an Amazon S3 Event Notification that sends a message to an Amazon SNS topic. Subscribe the Amazon EC2 instances to the Amazon SNS topic",
        "C. Create an Amazon EventBridge event that reacts to objects uploads in Amazon S3 and invokes one of the Amazon EC2 instances",
        "D. Subscribe the Amazon EC2 instances to the Amazon S3 Inventory stream"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Create an Amazon S3 Event Notification that sends a message to an Amazon SQS queue. Make the Amazon EC2 instances read from the Amazon SQS queue The Amazon S3 event notification feature enables you to receive notifications when certain events happen in your bucket. To enable notifications, you must first add a notification configuration that identifies the events you want Amazon S3 to publish and the destinations where you want Amazon S3 to send the notifications. Amazon S3 supports the following destinations where it can publish events: Amazon Simple Notification Service (Amazon SNS) topic Amazon Simple Queue Service (Amazon SQS) queue AWS Lambda Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent. Here we have to use Amazon S3 Event Notifications (which can send a message to either AWS Lambda, Amazon SNS, or Amazon SQS) to send a message to the Amazon SQS queue. By using Amazon SQS, we know only one Amazon EC2 instance among the four will pick up a message and process it. Incorrect options: Subscribe the Amazon EC2 instances to the Amazon S3 Inventory stream - Amazon S3 Inventory is a distractor. If you're curious - Amazon S3 inventory helps you manage your storage by creating lists of the objects in an Amazon S3 bucket on a defined schedule. Create an Amazon EventBridge event that reacts to objects uploads in Amazon S3 and invokes one of the Amazon EC2 instances- Amazon EventBridge events cannot invoke applications on Amazon EC2 instances, so we have to rule out that answer. Create an Amazon S3 Event Notification that sends a message to an Amazon SNS topic. Subscribe the Amazon EC2 instances to the Amazon SNS topic- Amazon Simple Notification Service (Amazon SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. Using Amazon SNS would send a message to each Amazon EC2 instance via the Amazon SNS topic, therefore making all of them work for each upload. This is not the intended behavior. Reference: https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html https://aws.amazon.com/sqs/",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "You have deployed a database technology that has a synchronous replication mode to survive disasters in data centers. The database is therefore deployed on two Amazon EC2 instances in two Availability Zones (AZs). The database must be publicly available so you have deployed the Amazon EC2 instances in public subnets. The replication protocol currently uses the Amazon EC2 public IP addresses. What can you do to decrease the replication cost?",
      "options": [
        "A. Use the Amazon EC2 instances private IP for the replication",
        "B. Assign elastic IP address (EIP) to the Amazon EC2 instances and use them for the replication",
        "C. Create a Private Link between the two Amazon EC2 instances",
        "D. Use an Elastic Fabric Adapter (EFA)"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Use the Amazon EC2 instances private IP for the replication The source of the cost is that traffic between two EC2 instances is going over the public internet, thus incurring high costs. Here, the correct answer is to use Private IP, so that the network remains private, for a minimal cost. Incorrect options: Assign elastic IP address (EIP) to the Amazon EC2 instances and use them for the replication - Using Elastic IPs will not solve the problem as the traffic will still be going over the public internet. Create a Private Link between the two Amazon EC2 instances - AWS PrivateLink simplifies the security of data shared with cloud-based applications by eliminating the exposure of data to the public Internet. AWS PrivateLink provides private connectivity between VPCs, AWS services, and on-premises applications, securely on the Amazon network. Private Link is a distractor in this question. Private Link is leveraged to create a private connection between an application that is fronted by an NLB in an account, and an Elastic Network Interface (ENI) in another account, without the need of VPC peering and allowing the connections between the two to remain within the AWS network. Use an Elastic Fabric Adapter (EFA) - The Elastic Fabric Adapter (EFA) is a network interface for Amazon EC2 instances that enables customers to run HPC applications requiring high levels of inter-instance communications, like computational fluid dynamics, weather modeling, and reservoir simulation, at scale on AWS. This option is not relevant to the given use-case. References: https://aws.amazon.com/privatelink/ https://aws.amazon.com/hpc/efa/",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "Your application is deployed on Amazon EC2 instances fronted by an Application Load Balancer. Recently, your infrastructure has come under attack. Attackers perform over 100 requests per second, while your normal users only make about 5 requests per second. How can you efficiently prevent attackers from overwhelming your application?",
      "options": [
        "A. Use an AWS Web Application Firewall (AWS WAF) and setup a rate-based rule",
        "B. Use AWS Shield Advanced and setup a rate-based rule",
        "C. Define a network access control list (network ACL) on your Application Load Balancer",
        "D. Configure Sticky Sessions on the Application Load Balancer"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Use an AWS Web Application Firewall (AWS WAF) and setup a rate-based rule AWS Web Application Firewall (AWS WAF) is a web application firewall that helps protect your web applications or APIs against common web exploits that may affect availability, compromise security, or consume excessive resources. AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that block common attack patterns, such as SQL injection or cross-site scripting, and rules that filter out specific traffic patterns you define. The correct answer is to use WAF (which has integration on top of your ALB) and define a rate-based rule. Incorrect options: Configure Sticky Sessions on the Application Load Balancer - Application Load Balancer (ALB) operates at the request level (layer 7), routing traffic to targets – Amazon EC2 instances, containers, IP addresses and Lambda functions based on the content of the request. Ideal for advanced load balancing of HTTP and HTTPS traffic, Application Load Balancer provides advanced request routing targeted at delivery of modern application architectures, including microservices and container-based applications. Sticky Sessions on your Application Load Balancer is a distractor here. Sticky sessions are a mechanism to route requests from the same client to the same target. Application Load Balancer supports sticky sessions using load balancer generated cookies. If you enable sticky sessions, the same target receives the request and can use the cookie to recover the session context. Define a network access control list (network ACL) on your Application Load Balancer - A network access control list (network ACL) does not work, as this only helps to block specific IPs. On top of things, network access control list (network ACL) is defined at the subnet level, and not for an Application Load Balancer. Use AWS Shield Advanced and setup a rate-based rule - AWS Shield is a managed Distributed Denial of Service (DDoS) protection service that safeguards applications running on AWS. AWS Shield provides always-on detection and automatic inline mitigations that minimize application downtime and latency, so there is no need to engage AWS Support to benefit from DDoS protection. There are two tiers of AWS Shield - Standard and Advanced. AWS Shield Advanced provides enhanced resource-specific detection and employs advanced mitigation and routing techniques for sophisticated or larger attacks. AWS Shield Advanced will give you DDoS protection overall, and you cannot set up rate-based rules in Shield. References: https://aws.amazon.com/waf/ https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-rate-based.html https://aws.amazon.com/shield/ https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html#sticky-sessions",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A company is experiencing stability issues with their cluster of self-managed RabbitMQ message brokers and the company now wants to explore an alternate solution on AWS. As a solutions architect, which of the following AWS services would you recommend that can provide support for quick and easy migration from RabbitMQ?",
      "options": [
        "A. Amazon Simple Notification Service (Amazon SNS)",
        "B. Amazon Simple Queue Service (Amazon SQS) Standard",
        "C. Amazon MQ",
        "D. Amazon SQS FIFO (First-In-First-Out)"
      ],
      "correct_answer": "C",
      "explanation": "Correct option: Amazon MQ Amazon MQ is a managed message broker service for Apache ActiveMQ that makes it easy to set up and operate message brokers in the cloud. Message brokers allow different software systems–often using different programming languages, and on different platforms–to communicate and exchange information. If an organization is using messaging with existing applications and wants to move the messaging service to the cloud quickly and easily, AWS recommends Amazon MQ for such a use case. So this is the correct option. Incorrect options: Amazon Simple Notification Service (Amazon SNS) - Amazon Simple Notification Service (Amazon SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. Amazon SNS provides topics for high-throughput, push-based, many-to-many messaging. SNS does not provide support for migration from RabbitMQ as its a fully managed pub/sub messaging service. Hence this option is incorrect. Amazon Simple Queue Service (Amazon SQS) Standard - Amazon SQS Standard offers a reliable, highly scalable hosted queue for storing messages as they travel between computers. Amazon SQS lets you easily move data between distributed application components and helps you build applications in which messages are processed independently (with message-level ack/fail semantics), such as automated workflows. SQS Standard does not provide support for migration from RabbitMQ. Hence this option is incorrect. Amazon SQS FIFO (First-In-First-Out) - Amazon SQS FIFO (First-In-First-Out) has all the capabilities of the standard queue. They are used when the order of operations and events is critical, or where duplicates can't be tolerated. SQS FIFO does not provide support for migration from RabbitMQ. Hence this option is incorrect. Reference: https://aws.amazon.com/amazon-mq/ https://aws.amazon.com/blogs/compute/migrating-from-rabbitmq-to-amazon-mq/",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A digital media streaming company wants to use Amazon CloudFront to distribute its content only to its service subscribers. As a solutions architect, which of the following solutions would you suggest to deliver restricted content to the bona fide end users? (Select two)",
      "options": [
        "A. Use Amazon CloudFront signed URLs",
        "B. Require HTTPS for communication between Amazon CloudFront and your custom origin",
        "C. Require HTTPS for communication between Amazon CloudFront and your S3 origin",
        "D. Forward HTTPS requests to the origin server by using the ECDSA or RSA ciphers",
        "E. Use Amazon CloudFront signed cookies"
      ],
      "correct_answer": "A",
      "explanation": "Correct options: Use Amazon CloudFront signed URLs Many companies that distribute content over the internet want to restrict access to documents, business data, media streams, or content that is intended for selected users, for example, users who have paid a fee. To securely serve this private content by using Amazon CloudFront, you can do the following: Require that your users access your private content by using special Amazon CloudFront signed URLs or signed cookies. A signed URL includes additional information, for example, expiration date and time, that gives you more control over access to your content. So this is a correct option. Use Amazon CloudFront signed cookies Amazon CloudFront signed cookies allow you to control who can access your content when you don't want to change your current URLs or when you want to provide access to multiple restricted files, for example, all of the files in the subscribers' area of a website. So this is also a correct option. Incorrect options: Require HTTPS for communication between Amazon CloudFront and your custom origin Require HTTPS for communication between Amazon CloudFront and your S3 origin Requiring HTTPS for communication between Amazon CloudFront and your custom origin (or S3 origin) only enables secure access to the underlying content. You cannot use HTTPS to restrict access to your private content. So both these options are incorrect. Forward HTTPS requests to the origin server by using the ECDSA or RSA ciphers - This option is just added as a distractor. You cannot use HTTPS to restrict access to your private content. Reference: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-urls.html https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-cookies.html",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "The engineering team at an e-commerce company wants to set up a custom domain for internal usage such as internaldomainexample.com. The team wants to use the private hosted zones feature of Amazon Route 53 to accomplish this. Which of the following settings of the VPC need to be enabled? (Select two)",
      "options": [
        "A. enableVpcSupport",
        "B. enableVpcHostnames",
        "C. enableDnsHostnames",
        "D. enableDnsDomain",
        "E. enableDnsSupport"
      ],
      "correct_answer": "C",
      "explanation": "Correct options: enableDnsHostnames enableDnsSupport A private hosted zone is a container for records for a domain that you host in one or more Amazon virtual private clouds (VPCs). You create a hosted zone for a domain (such as example.com), and then you create records to tell Amazon Route 53 how you want traffic to be routed for that domain within and among your VPCs. For each VPC that you want to associate with the Route 53 hosted zone, change the following VPC settings to true: enableDnsHostnames enableDnsSupport Incorrect options: enableVpcSupport enableVpcHostnames enableDnsDomain The options enableVpcSupport, enableVpcHostnames and enableDnsDomain have been added as distractors. Reference: https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-private-creating.html",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A company manages a High Performance Computing (HPC) application that needs to be deployed on Amazon EC2 instances. The application requires high levels of inter-node communications and high network traffic between the instances. As a solutions architect, which of the following options would you recommend to the engineering team at the company? (Select two)",
      "options": [
        "A. Deploy Amazon EC2 instances in a spread placement group",
        "B. Deploy Amazon EC2 instances in a partition placement group",
        "C. Deploy Amazon EC2 instances with Elastic Fabric Adapter (EFA)",
        "D. Deploy Amazon EC2 instances in a cluster placement group",
        "E. Deploy Amazon EC2 instances behind a Network Load Balancer"
      ],
      "correct_answer": "C",
      "explanation": "Correct options: Deploy Amazon EC2 instances with Elastic Fabric Adapter (EFA) Elastic Fabric Adapter (EFA) is a network interface for Amazon EC2 instances that enables customers to run applications requiring high levels of inter-node communications at scale on AWS. Its custom-built operating system (OS) bypass hardware interface enhances the performance of inter-instance communications, which is critical to scaling these applications. Therefore this option is correct. Deploy Amazon EC2 instances in a cluster placement group Cluster placement groups pack instances close together inside an Availability Zone. They are recommended when the majority of the network traffic is between the instances in the group. These are also recommended for applications that benefit from low network latency, high network throughput, or both. Therefore this option is one of the correct answers. Incorrect options: Deploy Amazon EC2 instances in a spread placement group - A spread placement group is a group of instances that are each placed on distinct racks, with each rack having its own network and power source. The instances are placed across distinct underlying hardware to reduce correlated failures. You can have a maximum of seven running instances per Availability Zone per group. Since the spread placement group can span across multiple Availability Zones in the same Region, it cannot support high levels of inter-node communications and high network traffic. So this option is incorrect. Deploy Amazon EC2 instances in a partition placement group - A partition placement group spreads your instances across logical partitions such that groups of instances in one partition do not share the underlying hardware with groups of instances in different partitions. This strategy is typically used by large distributed and replicated workloads, such as Hadoop, Cassandra, and Kafka. A partition placement group can have a maximum of seven partitions per Availability Zone. Since the partition placement group can have partitions in multiple Availability Zones in the same Region, it cannot support high levels of inter-node communications and high network traffic. So this option is incorrect. Deploy Amazon EC2 instances behind a Network Load Balancer - A load balancer serves as the single point of contact for clients. The load balancer distributes incoming traffic across multiple targets, such as Amazon EC2 instances. A Network Load Balancer functions at the fourth layer of the Open Systems Interconnection (OSI) model. Network Load Balancer cannot facilitate high network traffic between instances. Network Load Balancer cannot support high levels of inter-node communication between EC2 instances. This option just serves as a distractor. References: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html https://aws.amazon.com/hpc/efa/ https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A company has multiple Amazon EC2 instances operating in a private subnet which is part of a custom VPC. These instances are running an image processing application that needs to access images stored on Amazon S3. Once each image is processed, the status of the corresponding record needs to be marked as completed in a Amazon DynamoDB table. How would you go about providing private access to these AWS resources which are not part of this custom VPC?",
      "options": [
        "A. Create a gateway endpoint for Amazon S3 and add it as a target in the route table of the custom VPC. Create an interface endpoint for Amazon DynamoDB and then add it as a target in the route table of the custom VPC",
        "B. Create a separate gateway endpoint for Amazon S3 and Amazon DynamoDB each. Add two new target entries for these two gateway endpoints in the route table of the custom VPC",
        "C. Create a gateway endpoint for Amazon DynamoDB and add it as a target in the route table of the custom VPC. Create an Origin Access Identity for Amazon S3 and then connect to the S3 service using the private IP address",
        "D. Create a separate interface endpoint for Amazon S3 and Amazon DynamoDB each. Then connect to these services by adding these as targets in the route table of the custom VPC"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Create a separate gateway endpoint for Amazon S3 and Amazon DynamoDB each. Add two new target entries for these two gateway endpoints in the route table of the custom VPC Endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between instances in your VPC and services without imposing availability risks or bandwidth constraints on your network traffic. A VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by AWS PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Instances in your VPC do not require public IP addresses to communicate with resources in the service. Traffic between your VPC and the other service does not leave the Amazon network. There are two types of VPC endpoints: interface endpoints and gateway endpoints. An interface endpoint is an elastic network interface with a private IP address from the IP address range of your subnet that serves as an entry point for traffic destined to a supported service. A gateway endpoint is a gateway that you specify as a target for a route in your route table for traffic destined to a supported AWS service. The following AWS services are supported: Amazon S3 Amazon DynamoDB Incorrect options: Create a gateway endpoint for Amazon S3 and add it as a target in the route table of the custom VPC. Create an interface endpoint for Amazon DynamoDB and then add it as a target in the route table of the custom VPC Create a separate interface endpoint for Amazon S3 and Amazon DynamoDB each. Then connect to these services by adding these as targets in the route table of the custom VPC Amazon DynamoDB supports AWS PrivateLink. With AWS PrivateLink, you can simplify private network connectivity between virtual private clouds (VPCs), DynamoDB, and your on-premises data centers using interface VPC endpoints and private IP addresses. So, Amazon DynamoDB supports both interface endpoints as well as gateway endpoints. However, to use the interface endpoints, you need to connect to the given services using the private IP address, instead of creating an entry as a target in the route table of the custom VPC. Therefore, both these options are incorrect. Create a gateway endpoint for Amazon DynamoDB and add it as a target in the route table of the custom VPC. Create an Origin Access Identity for Amazon S3 and then connect to the S3 service using the private IP address - Origin Access Identity (OAI) is used within the context of Amazon CloudFront. To restrict access to content that you serve from Amazon S3 buckets, you can create a special Amazon CloudFront user called an origin access identity (OAI) and associate it with your distribution. You cannot use OAI to facilitate access to Amazon S3 from a VPC. References: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html https://aws.amazon.com/about-aws/whats-new/2024/03/amazon-dynamodb-aws-privatelink/",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A Big Data consulting company runs large distributed and replicated workloads on the on-premises data center. The company now wants to move these workloads to Amazon EC2 instances by using the placement groups feature and it wants to minimize correlated hardware failures. Which of the following represents the correct placement group configuration for the given requirement?",
      "options": [
        "A. Partition placement groups",
        "B. Cluster placement groups",
        "C. Spread placement groups",
        "D. Multi-AZ placement groups"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Partition placement groups Partition placement groups help reduce the likelihood of correlated hardware failures for your application. When using partition placement groups, Amazon EC2 divides each group into logical segments called partitions. Amazon EC2 ensures that each partition within a placement group has its own set of racks. Each rack has its own network and power source. No two partitions within a placement group share the same racks, allowing you to isolate the impact of a hardware failure within your application. The following image is a simple visual representation of a partition placement group in a single Availability Zone. It shows instances that are placed into a partition placement group with three partitions—Partition 1, Partition 2, and Partition 3. Each partition comprises multiple instances. The instances in a partition do not share racks with the instances in the other partitions, allowing you to contain the impact of a single hardware failure to only the associated partition. Partition placement groups can be used to deploy large distributed and replicated workloads, such as HDFS, HBase, and Cassandra, across distinct racks. When you launch instances into a partition placement group, Amazon EC2 tries to distribute the instances evenly across the number of partitions that you specify. You can also launch instances into a specific partition to have more control over where the instances are placed. A partition placement group can have partitions in multiple Availability Zones in the same Region. A partition placement group can have a maximum of seven partitions per Availability Zone. The number of instances that can be launched into a partition placement group is limited only by the limits of your account. Partition placement groups: via - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-partition Incorrect options: Cluster placement groups - A cluster placement group is a logical grouping of instances within a single Availability Zone. A cluster placement group can span peered VPCs in the same Region. Instances in the same cluster placement group enjoy a higher per-flow throughput limit for TCP/IP traffic and are placed in the same high-bisection bandwidth segment of the network. Cluster placement groups are recommended for applications that benefit from low network latency, high network throughput, or both. They are also recommended when the majority of the network traffic is between the instances in the group. As the instances are packed close together inside an Availability Zone, this option is not correct for the given use case. Cluster placement groups: via - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-partition Spread placement groups - A spread placement group is a group of instances that are each placed on distinct racks, with each rack having its own network and power source. Spread placement groups are recommended for applications that have a small number of critical instances that should be kept separate from each other. Launching instances in a spread placement group reduces the risk of simultaneous failures that might occur when instances share the same racks. Spread placement groups provide access to distinct racks, and are therefore suitable for mixing instance types or launching instances over time. As the use-case talks about running large distributed and replicated workloads, so it needs more instances, therefore this option is not the right fit for the given use-case. A spread placement group can span multiple Availability Zones in the same Region. You can have a maximum of seven running instances per Availability Zone per group. The following image shows seven instances in a single Availability Zone that are placed into a spread placement group. The seven instances are placed on seven different racks. Spread placement groups: via - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-partition Multi-AZ placement groups - This is a made-up option, given as a distractor. You should note that the Partition and Spread placement groups can span across multiple Availability Zones in the same Region. Reference: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A social media application lets users upload photos and perform image editing operations. The application offers two classes of service: pro and lite. The product team wants the photos submitted by pro users to be processed before those submitted by lite users. Photos are uploaded to Amazon S3 and the job information is sent to Amazon SQS. As a solutions architect, which of the following solutions would you recommend?",
      "options": [
        "A. Create two Amazon SQS standard queues: one for pro and one for lite. Set the lite queue to use short polling and the pro queue to use long polling",
        "B. Create two Amazon SQS standard queues: one for pro and one for lite. Set up Amazon EC2 instances to prioritize polling for the pro queue over the lite queue",
        "C. Create two Amazon SQS FIFO queues: one for pro and one for lite. Set the lite queue to use short polling and the pro queue to use long polling",
        "D. Create one Amazon SQS standard queue. Set the visibility timeout of the pro photos to zero. Set up Amazon EC2 instances to prioritize visibility settings so pro photos are processed first"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Create two Amazon SQS standard queues: one for pro and one for lite. Set up Amazon EC2 instances to prioritize polling for the pro queue over the lite queue AWS recommends using separate queues to provide prioritization of work. Therefore, for the given use case, you need to create an Amazon SQS standard queue for processing pro users' photos and another Amazon SQS standard queue for processing lite users' photos. Then you can configure Amazon EC2 instances to prioritize polling for the pro queue over the lite queue. via - https://aws.amazon.com/sqs/features/ Incorrect options: Create two Amazon SQS standard queues: one for pro and one for lite. Set the lite queue to use short polling and the pro queue to use long polling Create two Amazon SQS FIFO queues: one for pro and one for lite. Set the lite queue to use short polling and the pro queue to use long polling Amazon SQS long polling is a way to retrieve messages from your Amazon SQS queues. While the regular short polling returns immediately, even if the message queue being polled is empty, long-polling doesn’t return a response until a message arrives in the message queue, or the long poll times out. Since long polling or short polling cannot impact the priority of processing for the two queues, so both these options are incorrect. Create one Amazon SQS standard queue. Set the visibility timeout of the pro photos to zero. Set up Amazon EC2 instances to prioritize visibility settings so pro photos are processed first - To prevent other consumers from processing the message again, Amazon SQS sets a visibility timeout, a period of time during which Amazon SQS prevents other consumers from receiving and processing the message. The default visibility timeout for a message is 30 seconds. The minimum is 0 seconds. The maximum is 12 hours. Setting visibility timeout to zero can result in the same pro photo being processed by more than one consumer. This does not help in prioritizing the processing of pro photos over the lite photos. via - https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html References: https://aws.amazon.com/sqs/features/ https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An engineering team wants to orchestrate multiple Amazon ECS task types running on Amazon EC2 instances that are part of the Amazon ECS cluster. The output and state data for all tasks need to be stored. The amount of data output by each task is approximately 20 megabytes and there could be hundreds of tasks running at a time. As old outputs are archived, the storage size is not expected to exceed 1 terabyte. As a solutions architect, which of the following would you recommend as an optimized solution for high-frequency reading and writing?",
      "options": [
        "A. Use Amazon EFS with Provisioned Throughput mode",
        "B. Use Amazon EFS with Bursting Throughput mode",
        "C. Use Amazon DynamoDB table that is accessible by all ECS cluster instances",
        "D. Use an Amazon EBS volume mounted to the Amazon ECS cluster instances"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Amazon EFS file systems are distributed across an unconstrained number of storage servers. This distributed data storage design enables file systems to grow elastically to petabyte scale. It also enables massively parallel access from compute instances, including Amazon EC2, Amazon ECS, and AWS Lambda, to your data. Use Amazon EFS with Provisioned Throughput mode Provisioned Throughput mode is available for applications with high throughput to storage (MiB/s per TiB) ratios, or with requirements greater than those allowed by the Bursting Throughput mode. For example, say you're using Amazon EFS for development tools, web serving, or content management applications where the amount of data in your file system is low relative to throughput demands. Your file system can now get the high levels of throughput your applications require without having to pad your file system. If your file system is in the Provisioned Throughput mode, you can increase the Provisioned Throughput of your file system as often as you want. You can decrease your file system throughput in Provisioned Throughput mode as long as it's been more than 24 hours since the last decrease. Additionally, you can change between Provisioned Throughput mode and the default Bursting Throughput mode as long as it’s been more than 24 hours since the last throughput mode change. via - https://docs.aws.amazon.com/efs/latest/ug/performance.html Incorrect options: Use Amazon EFS with Bursting Throughput mode - With Bursting Throughput mode, a file system's throughput scales as the amount of data stored in the standard storage class grows. File-based workloads are typically spiky, driving high levels of throughput for short periods of time, and low levels of throughput the rest of the time. To accommodate this, Amazon EFS is designed to burst to high throughput levels for periods of time. By default, AWS recommends that you run your application in the Bursting Throughput mode. But, if you're planning to migrate large amounts of data into your file system, consider switching to Provisioned Throughput mode. The use-case mentions that the solution should be optimized for high-frequency reading and writing even when the old outputs are archived, therefore Provisioned Throughput mode is a better fit as it guarantees high levels of throughput your applications require without having to pad your file system. Use an Amazon EBS volume mounted to the Amazon ECS cluster instances - Amazon EFS has a higher throughput than Amazon EBS. In addition, Amazon EBS can be attached to multiple Amazon EC2 instances when the underlying EBS type is io1/io2 and the instance is of Nitro type. The use-case does not provide any such details, so this option is ruled out. Use Amazon DynamoDB table that is accessible by all ECS cluster instances - Amazon DynamoDB is not a fit for this scenario as each task output is 20 MB but the storage limit for each item in a Amazon DynamoDB table is 400 KB. You could write custom code to split the task output data into multiple items but it is not an optimal solution compared to using Amazon EFS in Provisioned Throughput mode. References: https://docs.aws.amazon.com/efs/latest/ug/performance.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Limits.html#limits-items",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An application running on an Amazon EC2 instance needs to access a Amazon DynamoDB table in the same AWS account. Which of the following solutions should a solutions architect configure for the necessary permissions?",
      "options": [
        "A. Set up an IAM user with the appropriate permissions to allow access to the Amazon DynamoDB table. Store the access credentials in an Amazon S3 bucket and read them from within the application code directly",
        "B. Set up an IAM user with the appropriate permissions to allow access to the Amazon DynamoDB table. Store the access credentials in the local storage and read them from within the application code directly",
        "C. Set up an IAM service role with the appropriate permissions to allow access to the Amazon DynamoDB table. Add the Amazon EC2 instance to the trust relationship policy document so that the instance can assume the role",
        "D. Set up an IAM service role with the appropriate permissions to allow access to the Amazon DynamoDB table. Configure an instance profile to assign this IAM role to the Amazon EC2 instance"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: Set up an IAM service role with the appropriate permissions to allow access to the Amazon DynamoDB table. Configure an instance profile to assign this IAM role to the Amazon EC2 instance A service role is an IAM role that a service assumes to perform actions on your behalf. Service roles provide access only within your account and cannot be used to grant access to services in other accounts. An IAM administrator can create, modify, and delete a service role from within IAM. When you create the service role, you define the trusted entity in the definition. If you are going to use the role with Amazon EC2 or another AWS service that uses Amazon EC2, you must store the role in an instance profile. An instance profile is a container for a role that can be attached to an Amazon EC2 instance when launched. An instance profile can contain only one role, and that limit cannot be increased. If you create the role using the AWS Management Console, the instance profile is created for you with the same name as the role. Incorrect options: Set up an IAM user with the appropriate permissions to allow access to the Amazon DynamoDB table. Store the access credentials in an Amazon S3 bucket and read them from within the application code directly Set up an IAM user with the appropriate permissions to allow access to the Amazon DynamoDB table. Store the access credentials in the local storage and read them from within the application code directly You should never store the IAM access credentials for a user in Amazon S3 or local storage or a database. It's a security bad practice. It is always recommended to use IAM roles to configure access to other AWS resources from Amazon EC2 instances. Therefore both these options are incorrect. Set up an IAM service role with the appropriate permissions to allow access to the Amazon DynamoDB table. Add the Amazon EC2 instance to the trust relationship policy document so that the instance can assume the role - There is no need for this option because when you create an IAM service role for Amazon EC2, the role automatically has Amazon EC2 identified as a trusted entity. Therefore this option is not correct. Configuring a Service Role: References: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-service.html https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A company's real-time streaming application is running on AWS. As the data is ingested, a job runs on the data and takes 30 minutes to complete. The workload frequently experiences high latency due to large amounts of incoming data. A solutions architect needs to design a scalable and serverless solution to enhance performance. Which combination of steps should the solutions architect take? (Select two)",
      "options": [
        "A. Set up AWS Database Migration Service (AWS DMS) to ingest the data",
        "B. Set up AWS Lambda with AWS Step Functions to process the data",
        "C. Set up Amazon Kinesis Data Streams to ingest the data",
        "D. Provision Amazon EC2 instances in an Auto Scaling group to process the data",
        "E. Set up AWS Fargate with Amazon ECS to process the data"
      ],
      "correct_answer": "C",
      "explanation": "Correct options: Set up Amazon Kinesis Data Streams to ingest the data Set up AWS Fargate with Amazon ECS to process the data Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more. AWS Fargate is a serverless compute engine for containers that works with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS). Fargate makes it easy for you to focus on building your applications. Fargate removes the need to provision and manage servers, lets you specify and pay for resources per application, and improves security through application isolation by design. For the given use case, we can use Kinesis Data Streams as the ingestion layer and the containerized ECS application on AWS Fargate as the processing layer. Both these components are serverless and can scale to offer the desired performance. Incorrect options: Set up AWS Database Migration Service (AWS DMS) to ingest the data - AWS Database Migration Service helps you migrate databases to AWS quickly and securely. DMS cannot be used for real-time data ingestion. Hence, this option is incorrect. Set up AWS Lambda with AWS Step Functions to process the data - The maximum timeout value for any AWS Lambda function is 15 minutes. When the specified timeout is reached, AWS Lambda terminates the execution of your Lambda function. Since the use case talks about a job that runs for 30 minutes, Lambda is not an option here. Provision Amazon EC2 instances in an Auto Scaling group to process the data - The given requirement is for a serverless solution to process the data. Hence, provisioning an Amazon EC2 instance is clearly not the right solution. Reference: https://aws.amazon.com/blogs/big-data/building-a-scalable-streaming-data-processor-with-amazon-kinesis-data-streams-on-aws-fargate/",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A company maintains its business-critical customer data on an on-premises system in an encrypted format. Over the years, the company has transitioned from using a single encryption key to multiple encryption keys by dividing the data into logical chunks. With the decision to move all the data to an Amazon S3 bucket, the company is now looking for a technique to encrypt each file with a different encryption key to provide maximum security to the migrated on-premises data. How will you implement this requirement without adding the overhead of splitting the data into logical groups?",
      "options": [
        "A. Store the logically divided data into different Amazon S3 buckets. Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data",
        "B. Configure a single Amazon S3 bucket to hold all data. Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data",
        "C. Use Multi-Region keys for client-side encryption in the AWS S3 Encryption Client to generate unique keys for each file of data",
        "D. Configure a single Amazon S3 bucket to hold all data. Use server-side encryption with AWS KMS (SSE-KMS) and use encryption context to generate a different key for each file/object that you store in the S3 bucket"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Configure a single Amazon S3 bucket to hold all data. Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data Server-side encryption is the encryption of data at its destination by the application or service that receives it. Amazon S3 encrypts your data at the object level as it writes it to disks in its data centers and decrypts it for you when you access it. When you use server-side encryption with Amazon S3 managed keys (SSE-S3), each object is encrypted with a unique key. As an additional safeguard, it encrypts the key itself with a root key that it regularly rotates. Note: Amazon S3 now applies server-side encryption with Amazon S3 managed keys (SSE-S3) as the base level of encryption for every bucket in Amazon S3. Starting January 5, 2023, all new object uploads to Amazon S3 will be automatically encrypted at no additional cost and with no impact on performance. Incorrect options: Store the logically divided data into different Amazon S3 buckets. Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data - Server-side encryption with Amazon S3 managed keys (SSE-S3) is the easiest way to implement the given requirement, as there is no additional overhead of splitting data. Multiple S3 buckets are redundant for this requirement. Use Multi-Region keys for client-side encryption in the AWS S3 Encryption Client to generate unique keys for each file of data - Server-side encryption is the encryption of data at its destination by the application or service that receives it. The requirement is about server-side encryption and not about client-side encryption, hence this choice is incorrect. Configure a single Amazon S3 bucket to hold all data. Use server-side encryption with AWS KMS (SSE-KMS) and use encryption context to generate a different key for each file/object that you store in the S3 bucket - An encryption context is a set of key-value pairs that contain additional contextual information about the data. When an encryption context is specified for an encryption operation, Amazon S3 must specify the same encryption context for the decryption operation. The encryption context offers another level of security for the encryption key. However, it is not useful for generating unique keys. References: https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A company is transferring a significant volume of data from on-site storage to AWS, where it will be accessed by Windows, Mac, and Linux-based Amazon EC2 instances within the same AWS region using both SMB and NFS protocols. Part of this data will be accessed regularly, while the rest will be accessed less frequently. The company requires a hosting solution for this data that minimizes operational overhead. What solution would best meet these requirements?",
      "options": [
        "A. Set up an Amazon FSx for ONTAP instance. Configure an FSx for ONTAP file system on the root volume and migrate the data to the FSx for ONTAP volume",
        "B. Set up an Amazon FSx for OpenZFS instance. Configure an FSx for OpenZFS file ystem on the root volume and migrate the data to the FSx for OpenZFS volume",
        "C. Set up an Amazon Elastic File System (Amazon EFS) volume that uses EFS Intelligent-Tiering. Use AWS DataSync to migrate the data to the EFS volume",
        "D. Set up an Amazon Elastic File System (Amazon EFS) volume that uses EFS Infrequent Access. Use AWS DataSync to migrate the data to the EFS volume"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Set up an Amazon FSx for ONTAP instance. Configure an FSx for ONTAP file system on the root volume and migrate the data to the FSx for ONTAP volume Amazon FSx for NetApp ONTAP is a storage service that allows customers to launch and run fully managed ONTAP file systems in the cloud. ONTAP is NetApp’s file system technology that provides a widely adopted set of data access and data management capabilities. Amazon FSx for NetApp ONTAP Overview via - https://aws.amazon.com/fsx/netapp-ontap/ The given use case mandates that the storage on AWS will be accessed by Windows, Mac, and Linux-based Amazon EC2 instances within the same AWS region using both SMB and NFS protocols. Amongst the Amazon FSx family, FSx for ONTAP is the only file system that supports this key requirement. via - https://aws.amazon.com/fsx/when-to-choose-fsx/ Incorrect options: Set up an Amazon Elastic File System (Amazon EFS) volume that uses EFS Intelligent-Tiering. Use AWS DataSync to migrate the data to the EFS volume Set up an Amazon Elastic File System (Amazon EFS) volume that uses EFS Infrequent Access. Use AWS DataSync to migrate the data to the EFS volume Amazon EFS is not supported on Windows instances. So, both these options are incorrect. Set up an Amazon FSx for OpenZFS instance. Configure an FSx for OpenZFS file ystem on the root volume and migrate the data to the FSx for OpenZFS volume - Amazon FSx for OpenZFS is a fully managed file storage service that lets you launch, run, and scale fully managed file systems built on the open-source OpenZFS file system. FSx for OpenZFS makes it easy to migrate your on-premises file servers without changing your applications or how you manage data, and to build new high-performance, data-intensive applications on the cloud. FSx for OpenZFS is compatible with Windows, Linux, macOS clients. It supports NFS 3, 4.0, 4.1, 4.2 protocols, however, it does NOT support the SMB protocol. References: https://aws.amazon.com/fsx/netapp-ontap/ https://aws.amazon.com/fsx/when-to-choose-fsx/ https://aws.amazon.com/fsx/openzfs/faqs/ https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/AmazonEFS.html",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A Hollywood production studio is looking at transferring their existing digital media assets of around 20 petabytes to AWS Cloud in the shortest possible timeframe. Which of the following is an optimal solution for this requirement, given that the studio's data centers are located at a remote location?",
      "options": [
        "A. AWS Snowball",
        "B. AWS Storage Gateway",
        "C. AWS Direct Connect",
        "D. AWS Snowmobile"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: AWS Snowmobile AWS Snowmobile is an Exabyte-scale data transfer service used to move extremely large amounts of data to AWS. You can transfer up to 100PB per Snowmobile, a 45-foot long ruggedized shipping container, pulled by a semi-trailer truck. Snowmobile makes it easy to move massive volumes of data to the cloud, including video libraries, image repositories, or even a complete data center migration. Transferring data with Snowmobile is more secure, fast, and cost-effective. AWS recommends using Snowmobile to migrate large datasets of 10PB or more in a single location. For datasets less than 10PB or distributed in multiple locations, you should use Snowball. Incorrect options: AWS Snowball - The AWS Snowball service uses physical storage devices to transfer large amounts of data between Amazon Simple Storage Service (Amazon S3) and client's onsite data storage location at faster-than-internet speeds. Snowball provides powerful interfaces that you can use to create jobs, track data, and track the status of your jobs through to completion. AWS recommends snowball only if you want to transfer greater than 10 TB of data between your on-premises data centers and Amazon S3. AWS Storage Gateway - AWS Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage. Used for key hybrid storage solutions that include moving tape backups to the cloud, reducing on-premises storage with cloud-backed file shares, providing low latency access to data in AWS for on-premises applications, as well as various migration, archiving, processing, and disaster recovery use cases. This is not an optimal solution since the studio's data centers are in remote locations where internet speed may not optimal, thereby increasing both cost and time for migrating 20TB of data. AWS Direct Connect - AWS Direct Connect is a network service that provides an alternative to using the Internet to connect a customer’s on-premises sites to AWS. Data is transmitted through a private network connection between AWS and a customer’s datacenter or corporate network. Direct Connect connection takes significant cost as well as time to provision. This is not the correct solution since the studio wants the data transfer to be done in the shortest possible time. Reference: https://aws.amazon.com/snowmobile/",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A big data analytics company is looking to archive the on-premises data into a POSIX compliant file storage system on AWS Cloud. The archived data would be accessed for just about a week in a year. As a solutions architect, which of the following AWS services would you recommend as the MOST cost-optimal solution?",
      "options": [
        "A. Amazon EFS Infrequent Access",
        "B. Amazon EFS Standard",
        "C. Amazon S3 Standard",
        "D. Amazon S3 Standard-IA"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Amazon EFS Infrequent Access Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed, elastic, NFS file system for use with AWS Cloud services and on-premises resources. Amazon EFS Infrequent Access (EFS IA) is a storage class that provides price/performance that is cost-optimized for files not accessed every day, with storage prices up to 92% lower compared to Amazon EFS Standard. The EFS IA storage class costs only $0.025/GB-month. To get started with EFS IA, simply enable EFS Lifecycle Management for your file system by selecting a lifecycle policy that matches your needs. How Amazon EFS Infrequent Access Works: via - https://aws.amazon.com/efs/features/infrequent-access/ Incorrect options: Amazon EFS Standard - Amazon EFS Infrequent Access is more cost-effective than EFS Standard for the given use-case, therefore this option is incorrect. Amazon S3 Standard Amazon S3 Standard-IA Both these options are object-based storage, whereas the given use-case requires a POSIX compliant file storage solution. Hence these two options are incorrect. Reference: https://aws.amazon.com/efs/features/infrequent-access/",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A Big Data company wants to optimize its daily Extract-Transform-Load (ETL) process that migrates and transforms data from its Amazon S3 based data lake to an Amazon Redshift cluster. The team wants to manage this daily job in a serverless environment. Which AWS service is the best fit to manage this process without the need to configure or manage the underlying compute resources?",
      "options": [
        "A. AWS Data Pipeline",
        "B. Amazon EMR",
        "C. AWS Glue",
        "D. AWS Database Migration Service (DMS)"
      ],
      "correct_answer": "C",
      "explanation": "Correct option: AWS Glue AWS Glue provides a managed ETL service that runs on a serverless Apache Spark environment. This allows you to focus on your ETL job and not worry about configuring and managing the underlying compute resources. AWS Glue takes a data-first approach and allows you to focus on the data properties and data manipulation to transform the data to a form where you can derive business insights. It provides an integrated data catalog that makes metadata available for ETL as well as querying via Amazon Athena and Amazon Redshift Spectrum. Create a unified catalog to find data across multiple data stores using AWS Glue: via - https://aws.amazon.com/glue/ AWS Glue automates much of the effort required for data integration. AWS Glue crawls your data sources, identifies data formats, and suggests schemas to store your data. It automatically generates the code to run your data transformations and loading processes. You can use AWS Glue to easily run and manage thousands of ETL jobs or to combine and replicate data across multiple data stores using SQL. AWS Glue runs in a serverless environment. There is no infrastructure to manage, and AWS Glue provisions, configures, and scales the resources required to run your data integration jobs. You pay only for the resources your jobs use while running. AWS Glue is the right fit since the company is looking at a managed ETL service without having the overhead of configuring, maintaining, or managing any servers. via - https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/load-data-from-amazon-s3-to-amazon-redshift-using-aws-glue.html Incorrect options: AWS Data Pipeline - AWS Data Pipeline provides a managed orchestration service that gives you greater flexibility in terms of the execution environment, access and control over the compute resources that run your code, as well as the code itself that does data processing. AWS Data Pipeline launches compute resources in your account allowing you direct access to the Amazon EC2 instances or Amazon EMR clusters. As this option provides access to the underlying EC2 instances so it's not a serverless solution. Therefore this option is incorrect for the given use case. Amazon EMR - EMR is a web service to easily and cost-effectively process vast amounts of data. EMR utilizes a hosted Hadoop framework running on the web-scale infrastructure of Amazon Elastic Compute Cloud (Amazon EC2) and Amazon Simple Storage Service (Amazon S3). As this option provides access to the underlying Amazon EC2 instances so it's not a serverless solution. Therefore this option is incorrect for the given use case. AWS Database Migration Service (DMS) - AWS Database Migration Service (DMS) helps you migrate databases to AWS easily and securely. For use cases that require a database migration from on-premises to AWS or database replication between on-premises sources and sources on AWS, AWS recommends you use AWS DMS. Once your data is in AWS, you can use AWS Glue to move, combine, replicate, and transform data from your data source into another database or data warehouse, such as Amazon Redshift. As the use-case talks about data migration and transformation between AWS services, so AWS Glue is a better fit than DMS. References: https://aws.amazon.com/glue/faqs/ https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/load-data-from-amazon-s3-to-amazon-redshift-using-aws-glue.html",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A financial services company stores confidential data on an Amazon Simple Storage Service (S3) bucket. The compliance guidelines require that files be stored with server-side encryption. The encryption used must be Advanced Encryption Standard (AES-256) and the company does not want to manage the encryption keys. Which of the following options represents the most cost-optimal solution for the given use case?",
      "options": [
        "A. Server-side encryption with Amazon S3 managed keys (SSE-S3)",
        "B. Server-side encryption with customer-provided keys (SSE-C)",
        "C. Client Side Encryption",
        "D. Server-side encryption with AWS KMS keys (SSE-KMS)"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Server-side encryption with Amazon S3 managed keys (SSE-S3) Using Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key employing strong multi-factor encryption. As an additional safeguard, it encrypts the key itself with a master key that it regularly rotates. Amazon S3 server-side encryption uses one of the strongest block ciphers available, 256-bit Advanced Encryption Standard (AES-256), to encrypt your data. There are no additional fees for using server-side encryption with Amazon S3-managed keys (SSE-S3). Incorrect options: Server-side encryption with customer-provided keys (SSE-C) - You manage the encryption keys and Amazon S3 manages the encryption as it writes to disks and decryption when you access your objects. Client Side Encryption - You can encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools. Server-side encryption with AWS KMS keys (SSE-KMS) - Similar to SSE-S3 and also provides you with an audit trail of when your key was used and by whom. Additionally, you have the option to create and manage encryption keys yourself. Although SSE-KMS provides an option where AWS manages the encryption key on your behalf, however, this entails a usage fee for the KMS key. So this option is not the best fit for the given use case. Reference: https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "The engineering team at a startup is evaluating the most optimal block storage volume type for the Amazon EC2 instances hosting its flagship application. The storage volume should support very low latency but it does not need to persist the data when the instance terminates. As a solutions architect, you have proposed using Instance Store volumes to meet these requirements. Which of the following would you identify as the key characteristics of the Instance Store volumes? (Select two)",
      "options": [
        "A. You can't detach an instance store volume from one instance and attach it to a different instance",
        "B. Instance store is reset when you stop or terminate an instance. Instance store data is preserved during hibernation",
        "C. You can specify instance store volumes for an instance when you launch or restart it",
        "D. An instance store is a network storage type",
        "E. If you create an Amazon Machine Image (AMI) from an instance, the data on its instance store volumes isn't preserved"
      ],
      "correct_answer": "A",
      "explanation": "Correct options: You can't detach an instance store volume from one instance and attach it to a different instance You can specify instance store volumes for an instance only when you launch it. You can't detach an instance store volume from one instance and attach it to a different instance. The data in an instance store persists only during the lifetime of its associated instance. If an instance reboots (intentionally or unintentionally), data in the instance store persists. If you create an Amazon Machine Image (AMI) from an instance, the data on its instance store volumes isn't preserved If you create an AMI from an instance, the data on its instance store volumes isn't preserved and isn't present on the instance store volumes of the instances that you launch from the AMI. Incorrect options: Instance store is reset when you stop or terminate an instance. Instance store data is preserved during hibernation - When you stop, hibernate, or terminate an instance, every block of storage in the instance store is reset. Therefore, this option is incorrect. You can specify instance store volumes for an instance when you launch or restart it - You can specify instance store volumes for an instance only when you launch it. An instance store is a network storage type - An instance store provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host computer. Reference: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A company is deploying a publicly accessible web application. To accomplish this, the engineering team has designed the VPC with a public subnet and a private subnet. The application will be hosted on several Amazon EC2 instances in an Auto Scaling group. The team also wants Transport Layer Security (TLS) termination to be offloaded from the Amazon EC2 instances. Which solution should a solutions architect implement to address these requirements in the most secure manner?",
      "options": [
        "A. Set up a Network Load Balancer in the public subnet. Create an Auto Scaling group in the public subnet and associate it with the Network Load Balancer",
        "B. Set up a Network Load Balancer in the public subnet. Create an Auto Scaling group in the private subnet and associate it with the Network Load Balancer",
        "C. Set up a Network Load Balancer in the private subnet. Create an Auto Scaling group in the public subnet and associate it with the Network Load Balancer",
        "D. Set up a Network Load Balancer in the private subnet. Create an Auto Scaling group in the private subnet and associate it with the Network Load Balancer"
      ],
      "correct_answer": "B",
      "explanation": "Correct option: Set up a Network Load Balancer in the public subnet. Create an Auto Scaling group in the private subnet and associate it with the Network Load Balancer A load balancer serves as the single point of contact for clients. The load balancer distributes incoming traffic across multiple targets, such as Amazon EC2 instances. This increases the availability of your application. You add one or more listeners to your load balancer. With a Network Load Balancer, you can offload the decryption/encryption of Transport Layer Security (TLS) traffic from your application servers to the Network Load Balancer, which helps you optimize the performance of your backend application servers while keeping your workloads secure. Additionally, Network Load Balancers preserve the source IP of the clients to the back-end applications, while terminating Transport Layer Security (TLS) on the load balancer. An Auto Scaling group contains a collection of Amazon EC2 instances that are treated as a logical grouping for the purposes of automatic scaling and management. An Auto Scaling group also enables you to use Amazon EC2 Auto Scaling features such as health check replacements and scaling policies. Both maintaining the number of instances in an Auto Scaling group and automatic scaling are the core functionality of the Amazon EC2 Auto Scaling service. The NLB has to be accessible over the internet and hence has to be in a public subnet and will act as a single point-of-contact for all incoming traffic. NLB will forward the incoming traffic to the Amazon EC2 instances managed by the ASG in the private subnet. Exam Alert: You should note that the Application Load Balancer also supports Transport Layer Security (TLS) offloading. The Classic Load Balancer supports SSL offloading. Incorrect options: Set up a Network Load Balancer in the public subnet. Create an Auto Scaling group in the public subnet and associate it with the Network Load Balancer - The Auto Scaling group with its target EC2 instances should be in the private subnet to avoid access to EC2 instances over the public internet. Having EC2 instances in the public subnet would weaken the security posture of the application. Hence, this option is incorrect. Set up a Network Load Balancer in the private subnet. Create an Auto Scaling group in the public subnet and associate it with the Network Load Balancer Set up a Network Load Balancer in the private subnet. Create an Auto Scaling group in the private subnet and associate it with the Network Load Balancer NLB should be in the public subnet as it represents the internet-facing component of the web tier. Therefore, both these options are incorrect. Reference: https://aws.amazon.com/blogs/aws/new-tls-termination-for-network-load-balancers/",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A company needs an Active Directory service to run directory-aware workloads in the AWS Cloud and it should also support configuring a trust relationship with any existing on-premises Microsoft Active Directory. Which AWS Directory Service is the best fit for this requirement?",
      "options": [
        "A. Active Directory Connector",
        "B. Simple Active Directory (Simple AD)",
        "C. AWS Transit Gateway",
        "D. AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD)"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD) AWS Directory Service lets you run Microsoft Active Directory (AD) as a managed service. AWS Directory Service for Microsoft Active Directory, also referred to as AWS Managed Microsoft AD, is powered by Windows Server 2012 R2. When you select and launch this directory type, it is created as a highly available pair of domain controllers connected to your virtual private cloud (VPC). With AWS Managed Microsoft AD, you can run directory-aware workloads in the AWS Cloud, including Microsoft SharePoint and custom .NET and SQL Server-based applications. You can also configure a trust relationship between AWS Managed Microsoft AD in the AWS Cloud and your existing on-premises Microsoft Active Directory, providing users and groups with access to resources in either domain, using single sign-on (SSO). AWS Managed Microsoft AD is your best choice if you need actual Active Directory features to support AWS applications or Windows workloads, including Amazon Relational Database Service for Microsoft SQL Server. It's also best if you want a standalone AD in the AWS Cloud that supports Office 365 or you need an LDAP directory to support your Linux applications. Incorrect options: Active Directory Connector - AD Connector is a directory gateway with which you can redirect directory requests to your on-premises Microsoft Active Directory without caching any information in the cloud. AD Connector is your best choice when you want to use your existing on-premises directory with compatible AWS services. Simple Active Directory (Simple AD) - Simple AD is a standalone directory in the cloud, where you create and manage user identities and manage access to applications. Simple AD provides a subset of the features offered by AWS Managed Microsoft AD. However, note that Simple AD does not support features such as multi-factor authentication (MFA), trust relationships with other domains, Active Directory Administrative Center, PowerShell support, Active Directory recycle bin, group managed service accounts, and schema extensions for POSIX and Microsoft applications. AWS Transit Gateway - AWS Transit Gateway connects VPCs and on-premises networks through a central hub. Transit Gateway is not an Active Directory service. References: https://docs.aws.amazon.com/directoryservice/latest/admin-guide/what_is.html https://docs.aws.amazon.com/directoryservice/latest/admin-guide/directory_simple_ad.html https://docs.aws.amazon.com/directoryservice/latest/admin-guide/directory_ad_connector.html",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A company has media files that need to be shared internally. Users are first authenticated using Active Directory and then they access files on a Microsoft Windows platform. The engineering manager wants to keep the same user permissions but wants the company to migrate the storage layer to AWS Cloud as the company is reaching its storage capacity limit on the on-premises infrastructure. What should a solutions architect recommend to meet this requirement?",
      "options": [
        "A. Create a corporate Amazon S3 bucket and move all media files",
        "B. Set up Amazon EFS and move all media files",
        "C. Set up Amazon FSx for Windows File Server and move all the media files",
        "D. Provision Amazon EC2 with Windows OS, attach multiple Amazon EBS volumes, and move all media files"
      ],
      "correct_answer": "C",
      "explanation": "Correct option: Set up Amazon FSx for Windows File Server and move all the media files Amazon FSx for Windows File Server provides fully managed, highly reliable, and scalable file storage that is accessible over the industry-standard Server Message Block (SMB) protocol. It is built on Windows Server, delivering a wide range of administrative features such as user quotas, end-user file restore, and Microsoft Active Directory (AD) integration. To support a wide spectrum of workloads, Amazon FSx provides high levels of throughput and IOPS and consistent sub-millisecond latencies. Amazon FSx file storage is accessible from Windows, Linux, and macOS compute instances and devices running on AWS or on-premises. Thousands of compute instances and devices can access a file system concurrently. Amazon FSx for Windows File Server supports Microsoft Active Directory (AD) integration so the same user permissions and access credentials can be used to access the files on FSx Windows File Server. Incorrect options: Create a corporate Amazon S3 bucket and move all media files - Amazon S3 is object-based storage and it does not support file storage. Hence S3 is not the correct option. Set up Amazon EFS and move all media files - Amazon EFS provides scalable file storage for use with Amazon EC2. You can use an EFS file system as a common data source for workloads and applications running on multiple instances. EFS is not compatible with the Windows platform, so this option is ruled out. Provision Amazon EC2 with Windows OS, attach multiple Amazon EBS volumes, and move all media files - Multi-attach Amazon EBS volumes are supported only for Nitro EC2 instances which are Linux-based. So this option is ruled out. Reference: https://aws.amazon.com/fsx/windows/",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An e-commerce company uses Amazon RDS MySQL DB to store the data. The analytics department at the company runs its reports on the same database. The engineering team has noticed sluggish performance on the database when the analytics reporting process is in progress. As an AWS Certified Solutions Architect - Associate, which of the following would you suggest as the MOST cost-optimal solution to improve the performance?",
      "options": [
        "A. Create a read-replica with the same compute capacity and the same storage capacity as the primary. Point the reporting queries to run against the read replica",
        "B. Create a read-replica with half compute capacity and half storage capacity as the primary. Point the reporting queries to run against the read replica",
        "C. Create a standby instance in a multi-AZ configuration with the same compute capacity and the same storage capacity as the primary. Point the reporting queries to run against the standby instance",
        "D. Create a standby instance in a multi-AZ configuration with half compute capacity and half storage capacity as the primary. Point the reporting queries to run against the standby instance"
      ],
      "correct_answer": "A",
      "explanation": "Correct option: Create a read-replica with the same compute capacity and the same storage capacity as the primary. Point the reporting queries to run against the read replica Amazon RDS uses the MariaDB, Microsoft SQL Server, MySQL, Oracle, and PostgreSQL DB engines' built-in replication functionality to create a special type of database instance called a read replica from a source database instance. The source database instance becomes the primary database instance. Updates made to the primary database instance are asynchronously copied to the read replica. You can reduce the load on your primary DB instance by routing read queries from your applications to the read replica. Amazon RDS Read Replicas: via - https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html via - https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html You can use read replicas to improve the performance of your Amazon RDS MySQL DB by handling business reporting or data warehousing scenarios where you might want business reporting queries to run against your read replica, rather than your production database instance. You can create up to five read replicas from one DB instance. For replication to operate effectively, each read replica should have the same amount of compute and storage resources as the source database instance. If you scale the source database instance, also scale the read replicas. via - https://docs.amazonaws.cn/en_us/AmazonRDS/latest/UserGuide/USER_MySQL.Replication.ReadReplicas.html Incorrect options: Create a read-replica with half compute capacity and half storage capacity as the primary. Point the reporting queries to run against the read replica - As mentioned in the explanation above, you should create a read-replica with the same compute capacity and the same storage capacity as the primary. Create a standby instance in a multi-AZ configuration with the same compute capacity and the same storage capacity as the primary. Point the reporting queries to run against the standby instance Create a standby instance in a multi-AZ configuration with half compute capacity and half storage capacity as the primary. Point the reporting queries to run against the standby instance Multi-AZ deployments are not a read scaling solution, so you cannot use a standby to serve read traffic. The standby is there just for failover. Hence both these options are incorrect. References: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html https://docs.amazonaws.cn/en_us/AmazonRDS/latest/UserGuide/USER_MySQL.Replication.ReadReplicas.html",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An e-commerce application uses a relational database that runs several queries that perform joins on multiple tables. The development team has found that these queries are slow and expensive, therefore these are a good candidate for caching. The application needs to use a caching service that supports multi-threading. As a solutions architect, which of the following services would you recommend for the given use case?",
      "options": [
        "A. Amazon ElastiCache for Redis",
        "B. Amazon DynamoDB Accelerator (DAX)",
        "C. AWS Global Accelerator",
        "D. Amazon ElastiCache for Memcached"
      ],
      "correct_answer": "D",
      "explanation": "Correct option: Amazon ElastiCache for Memcached Amazon ElastiCache is a web service that makes it easy to deploy, operate, and scale an in-memory data store and cache in the cloud. The service improves the performance of web applications by allowing you to retrieve information from fast, managed, in-memory data stores, instead of relying entirely on slower disk-based databases. Memcached is an open-source, distributed, in-memory key-value store that can retrieve data in milliseconds. Caching site information with Memcached can help you improve the performance and scalability of your site while controlling cost. Choose Memcached if the following apply to you: You need the simplest model possible. You need to run large nodes with multiple cores or threads (support for multi-threading). You need the ability to scale out and in, adding and removing nodes as demand on your system increases and decreases. You need to cache objects. via - https://aws.amazon.com/elasticache/redis-vs-memcached/ Incorrect options: Amazon ElastiCache for Redis - Redis, which stands for Remote Dictionary Server, is a fast, open-source, in-memory key-value data store for use as a database, cache, message broker, and queue. Redis now delivers sub-millisecond response times enabling millions of requests per second for real-time applications in Gaming, Ad-Tech, Financial Services, Healthcare, and IoT. Redis is a popular choice for caching, session management, gaming, leaderboards, real-time analytics, geospatial, ride-hailing, chat/messaging, media streaming, and pub/sub apps. Redis does not support multi-threading, so this option is not the right fit for the given use case. Amazon DynamoDB Accelerator (DAX) - Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for Amazon DynamoDB. DAX does not support relational databases. AWS Global Accelerator - AWS Global Accelerator is a networking service that helps you improve the availability and performance of the applications that you offer to your global users. This option has been added as a distractor, it has nothing to do with database caching. References: https://aws.amazon.com/caching/aws-caching/ https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/elasticache-use-cases.html https://aws.amazon.com/elasticache/redis-vs-memcached/",
      "reference": "Source: Practice Test #6 - AWS Certified Solutions Architect Associate"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 1,
      "question_text": "Which AWS service provides managed encryption keys for encrypting data at rest?",
      "options": [
        "A. AWS Shield",
        "B. AWS WAF",
        "C. AWS KMS",
        "D. AWS IAM"
      ],
      "correct_answer": "C",
      "explanation": "AWS Key Management Service (KMS) is a managed service that makes it easy for you to create and control the cryptographic keys used to encrypt your data. KMS integrates with most AWS services to encrypt data at rest.",
      "reference": "https://docs.aws.amazon.com/kms/latest/developerguide/overview.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 1,
      "question_text": "What is the primary purpose of AWS IAM?",
      "options": [
        "A. Monitor application performance",
        "B. Manage access to AWS services and resources",
        "C. Store data in the cloud",
        "D. Deploy applications automatically"
      ],
      "correct_answer": "B",
      "explanation": "AWS Identity and Access Management (IAM) is a web service that helps you securely control access to AWS resources. You use IAM to control who is authenticated and authorized to use resources.",
      "reference": "https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 1,
      "question_text": "Which feature allows you to require MFA for API calls made using IAM user credentials?",
      "options": [
        "A. IAM Roles",
        "B. IAM Policies with MFA condition",
        "C. AWS Organizations",
        "D. Security Groups"
      ],
      "correct_answer": "B",
      "explanation": "You can add MFA protection to API calls by adding conditions to IAM policies. The condition checks whether the user authenticated with an MFA device.",
      "reference": "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa_configure-api-require.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 1,
      "question_text": "What is the recommended way for an EC2 instance to access other AWS services securely?",
      "options": [
        "A. Store access keys in the application code",
        "B. Use IAM roles attached to the EC2 instance",
        "C. Store credentials in environment variables",
        "D. Use root account credentials"
      ],
      "correct_answer": "B",
      "explanation": "IAM roles for EC2 provide temporary credentials that are automatically rotated. This is more secure than storing long-term access keys on the instance.",
      "reference": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "A company needs to ensure that all S3 objects are encrypted at rest using keys managed by the customer. Which encryption option should they use?",
      "options": [
        "A. SSE-S3",
        "B. SSE-KMS with AWS managed key",
        "C. SSE-KMS with customer managed key (CMK)",
        "D. Client-side encryption only"
      ],
      "correct_answer": "C",
      "explanation": "SSE-KMS with a customer managed key (CMK) allows customers to create, manage, and audit the encryption keys used to encrypt S3 objects, providing full control over key management.",
      "reference": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "Which AWS service should be used to centrally manage security policies across multiple AWS accounts?",
      "options": [
        "A. AWS Config",
        "B. AWS Organizations with SCPs",
        "C. AWS IAM",
        "D. Amazon Inspector"
      ],
      "correct_answer": "B",
      "explanation": "AWS Organizations with Service Control Policies (SCPs) allows you to centrally manage and enforce security policies across multiple AWS accounts in your organization.",
      "reference": "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "A company wants to restrict S3 bucket access to only requests coming from their VPC. What should they configure?",
      "options": [
        "A. S3 ACLs",
        "B. IAM policies",
        "C. S3 bucket policy with VPC endpoint condition",
        "D. Security groups"
      ],
      "correct_answer": "C",
      "explanation": "You can use an S3 bucket policy with a condition that restricts access to requests from a specific VPC or VPC endpoint using the aws:sourceVpce or aws:sourceVpc condition keys.",
      "reference": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies-vpc-endpoint.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "Which service provides automated security assessment to help improve the security and compliance of applications deployed on AWS?",
      "options": [
        "A. AWS Trusted Advisor",
        "B. Amazon Inspector",
        "C. AWS Config",
        "D. Amazon GuardDuty"
      ],
      "correct_answer": "B",
      "explanation": "Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS by automatically assessing applications for vulnerabilities.",
      "reference": "https://docs.aws.amazon.com/inspector/latest/user/what-is-inspector.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 3,
      "question_text": "A company has a requirement to encrypt data before storing it in S3 and maintain complete control over the encryption process. The encryption must happen on the client side. Which approach should they use?",
      "options": [
        "A. SSE-S3",
        "B. SSE-KMS",
        "C. Client-side encryption with AWS KMS managed keys",
        "D. SSE-C"
      ],
      "correct_answer": "C",
      "explanation": "Client-side encryption with AWS KMS managed keys allows the client to encrypt data before uploading to S3, maintaining complete control over the encryption process while leveraging KMS for key management.",
      "reference": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingClientSideEncryption.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 3,
      "question_text": "A company needs to implement a solution that prevents accidental deletion of S3 objects and allows recovery of deleted objects for 90 days. Which combination of features should be enabled?",
      "options": [
        "A. S3 versioning and lifecycle policies",
        "B. S3 versioning and MFA Delete",
        "C. S3 Object Lock and versioning",
        "D. Cross-region replication and versioning"
      ],
      "correct_answer": "B",
      "explanation": "S3 versioning allows recovery of deleted objects by keeping previous versions. MFA Delete requires additional authentication to permanently delete object versions, preventing accidental deletion.",
      "reference": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/Versioning.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 3,
      "question_text": "A security team needs to detect and respond to potentially unauthorized or malicious activity in their AWS accounts. Which service should they use?",
      "options": [
        "A. AWS Config",
        "B. Amazon Inspector",
        "C. Amazon GuardDuty",
        "D. AWS Security Hub"
      ],
      "correct_answer": "C",
      "explanation": "Amazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior. It uses machine learning, anomaly detection, and integrated threat intelligence.",
      "reference": "https://docs.aws.amazon.com/guardduty/latest/ug/what-is-guardduty.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 3,
      "question_text": "An application requires database credentials that must be rotated automatically every 30 days. Which AWS service should be used to manage these credentials?",
      "options": [
        "A. AWS Systems Manager Parameter Store",
        "B. AWS Secrets Manager",
        "C. AWS KMS",
        "D. AWS IAM"
      ],
      "correct_answer": "B",
      "explanation": "AWS Secrets Manager enables automatic rotation of secrets, including database credentials. It can automatically rotate credentials for supported databases without requiring application changes.",
      "reference": "https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotating-secrets.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 1,
      "question_text": "Which AWS service provides a managed relational database with automatic failover to a standby replica?",
      "options": [
        "A. Amazon DynamoDB",
        "B. Amazon RDS Multi-AZ",
        "C. Amazon ElastiCache",
        "D. Amazon Redshift"
      ],
      "correct_answer": "B",
      "explanation": "Amazon RDS Multi-AZ deployments provide enhanced availability by automatically replicating data to a standby instance in a different Availability Zone and automatically failing over in case of an infrastructure failure.",
      "reference": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 1,
      "question_text": "What is the primary benefit of deploying resources across multiple Availability Zones?",
      "options": [
        "A. Lower costs",
        "B. Higher performance",
        "C. High availability and fault tolerance",
        "D. Easier management"
      ],
      "correct_answer": "C",
      "explanation": "Deploying resources across multiple Availability Zones provides high availability and fault tolerance because if one AZ experiences an outage, the application can continue operating from another AZ.",
      "reference": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 1,
      "question_text": "Which service distributes incoming application traffic across multiple targets?",
      "options": [
        "A. Amazon Route 53",
        "B. Elastic Load Balancing",
        "C. AWS Auto Scaling",
        "D. Amazon CloudFront"
      ],
      "correct_answer": "B",
      "explanation": "Elastic Load Balancing automatically distributes incoming application traffic across multiple targets, such as EC2 instances, containers, and IP addresses, in one or more Availability Zones.",
      "reference": "https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/what-is-load-balancing.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 1,
      "question_text": "Which S3 storage class provides the highest durability?",
      "options": [
        "A. S3 Standard",
        "B. S3 One Zone-IA",
        "C. S3 Glacier",
        "D. All S3 storage classes provide 99.999999999% durability"
      ],
      "correct_answer": "D",
      "explanation": "All Amazon S3 storage classes are designed to provide 99.999999999% (11 nines) durability of objects over a given year, except S3 One Zone-IA which stores data in a single AZ.",
      "reference": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "A company needs to deploy a highly available web application that can automatically scale based on demand. Which services should they use?",
      "options": [
        "A. EC2 instances with Elastic IP",
        "B. Auto Scaling group with Application Load Balancer",
        "C. Single EC2 instance with Reserved capacity",
        "D. AWS Lambda only"
      ],
      "correct_answer": "B",
      "explanation": "An Auto Scaling group with an Application Load Balancer provides both high availability (multiple instances across AZs) and automatic scaling based on demand.",
      "reference": "https://docs.aws.amazon.com/autoscaling/ec2/userguide/autoscaling-load-balancer.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "Which database service provides automatic replication across three Availability Zones with up to 15 read replicas?",
      "options": [
        "A. Amazon RDS MySQL",
        "B. Amazon Aurora",
        "C. Amazon DynamoDB",
        "D. Amazon DocumentDB"
      ],
      "correct_answer": "B",
      "explanation": "Amazon Aurora automatically replicates data across three Availability Zones and supports up to 15 low-latency read replicas, providing high availability and read scalability.",
      "reference": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "A company needs to implement a disaster recovery solution with an RPO of 1 hour and RTO of 4 hours. Which DR strategy is most appropriate?",
      "options": [
        "A. Backup and Restore",
        "B. Pilot Light",
        "C. Warm Standby",
        "D. Multi-Site Active/Active"
      ],
      "correct_answer": "B",
      "explanation": "Pilot Light strategy maintains a minimal version of the environment always running in the DR region. It can achieve RPO of hours and RTO of tens of minutes to hours, suitable for the given requirements.",
      "reference": "https://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads-on-aws/disaster-recovery-options-in-the-cloud.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "Which feature should be enabled to ensure S3 objects are replicated to another region for disaster recovery?",
      "options": [
        "A. S3 versioning",
        "B. S3 Transfer Acceleration",
        "C. S3 Cross-Region Replication",
        "D. S3 Lifecycle policies"
      ],
      "correct_answer": "C",
      "explanation": "S3 Cross-Region Replication (CRR) automatically replicates objects across S3 buckets in different AWS Regions, providing disaster recovery capabilities.",
      "reference": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 3,
      "question_text": "A company runs a critical application that requires near-zero RPO and RTO. The application uses a relational database. Which architecture should they implement?",
      "options": [
        "A. RDS Multi-AZ with automated backups",
        "B. Aurora Global Database with read replicas",
        "C. DynamoDB Global Tables",
        "D. RDS with Cross-Region Read Replicas"
      ],
      "correct_answer": "B",
      "explanation": "Aurora Global Database provides near-zero RPO through physical replication across regions with typical latency under 1 second, and RTO of less than 1 minute through fast failover to a secondary region.",
      "reference": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 3,
      "question_text": "A stateful application running on EC2 needs to maintain session data during scaling events. Which solution provides the best resilience?",
      "options": [
        "A. Store sessions on the EC2 instance EBS volume",
        "B. Use sticky sessions on the load balancer",
        "C. Store sessions in Amazon ElastiCache for Redis",
        "D. Store sessions in instance metadata"
      ],
      "correct_answer": "C",
      "explanation": "Storing sessions in ElastiCache for Redis provides a centralized, highly available session store that persists across scaling events and instance failures, unlike EBS volumes or sticky sessions.",
      "reference": "https://docs.aws.amazon.com/elasticache/latest/red-ug/elasticache-use-cases.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 3,
      "question_text": "An application needs to process messages exactly once, even if the processing system fails and restarts. Which SQS feature should be used?",
      "options": [
        "A. Standard Queue with visibility timeout",
        "B. FIFO Queue with deduplication",
        "C. Standard Queue with dead-letter queue",
        "D. FIFO Queue with message groups"
      ],
      "correct_answer": "B",
      "explanation": "SQS FIFO queues with deduplication provide exactly-once processing semantics by automatically filtering duplicate messages within a 5-minute deduplication interval.",
      "reference": "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues-exactly-once-processing.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 3,
      "question_text": "A company needs to implement a solution that allows an application to continue operating during an AWS region outage. Which combination of services should they use?",
      "options": [
        "A. Multi-AZ deployment with ALB",
        "B. Route 53 health checks with failover routing to a DR region",
        "C. CloudFront with multiple origins in the same region",
        "D. Global Accelerator with endpoints in the same region"
      ],
      "correct_answer": "B",
      "explanation": "Route 53 health checks with failover routing can detect when the primary region is unhealthy and automatically route traffic to a DR region, providing resilience against regional outages.",
      "reference": "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 1,
      "question_text": "Which service should be used to cache frequently accessed data to reduce database load?",
      "options": [
        "A. Amazon RDS",
        "B. Amazon ElastiCache",
        "C. Amazon S3",
        "D. Amazon EFS"
      ],
      "correct_answer": "B",
      "explanation": "Amazon ElastiCache is an in-memory caching service that can significantly reduce database load by caching frequently accessed data, improving application response times.",
      "reference": "https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/WhatIs.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 1,
      "question_text": "Which service provides a global content delivery network to reduce latency for users worldwide?",
      "options": [
        "A. AWS Global Accelerator",
        "B. Amazon CloudFront",
        "C. Amazon Route 53",
        "D. Elastic Load Balancing"
      ],
      "correct_answer": "B",
      "explanation": "Amazon CloudFront is a content delivery network (CDN) that caches content at edge locations worldwide, reducing latency for end users by serving content from locations closer to them.",
      "reference": "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 1,
      "question_text": "What is the maximum size of an item in Amazon DynamoDB?",
      "options": [
        "A. 1 KB",
        "B. 64 KB",
        "C. 400 KB",
        "D. 1 MB"
      ],
      "correct_answer": "C",
      "explanation": "The maximum item size in DynamoDB is 400 KB, including both attribute names and values. This limit ensures consistent performance for database operations.",
      "reference": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ServiceQuotas.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 1,
      "question_text": "Which EC2 instance type is optimized for compute-intensive workloads?",
      "options": [
        "A. R5 instances",
        "B. C5 instances",
        "C. M5 instances",
        "D. T3 instances"
      ],
      "correct_answer": "B",
      "explanation": "C5 instances are compute-optimized instances ideal for compute-intensive workloads such as batch processing, gaming servers, and scientific modeling.",
      "reference": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/compute-optimized-instances.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "An application needs to process millions of requests per second with single-digit millisecond latency. Which database service should be used?",
      "options": [
        "A. Amazon RDS",
        "B. Amazon DynamoDB with DAX",
        "C. Amazon Redshift",
        "D. Amazon Aurora"
      ],
      "correct_answer": "B",
      "explanation": "DynamoDB with DAX (DynamoDB Accelerator) provides in-memory caching that delivers microsecond response times for read-heavy workloads, supporting millions of requests per second.",
      "reference": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company needs to transfer 100TB of data to AWS with the fastest possible transfer time. Network bandwidth is limited. What should they use?",
      "options": [
        "A. AWS Direct Connect",
        "B. S3 Transfer Acceleration",
        "C. AWS Snowball Edge",
        "D. VPN connection"
      ],
      "correct_answer": "C",
      "explanation": "AWS Snowball Edge is ideal for transferring large amounts of data when network bandwidth is limited. It provides a physical device for offline data transfer.",
      "reference": "https://docs.aws.amazon.com/snowball/latest/developer-guide/whatisedge.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "Which EBS volume type provides the highest IOPS performance for I/O-intensive workloads?",
      "options": [
        "A. gp3",
        "B. io2 Block Express",
        "C. st1",
        "D. sc1"
      ],
      "correct_answer": "B",
      "explanation": "io2 Block Express volumes can deliver up to 256,000 IOPS and 4,000 MB/s throughput, providing the highest performance for I/O-intensive workloads like large databases.",
      "reference": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "An application running on EC2 needs to access S3 with the lowest possible latency. Which feature should be used?",
      "options": [
        "A. S3 Transfer Acceleration",
        "B. VPC Gateway Endpoint for S3",
        "C. CloudFront distribution",
        "D. AWS Direct Connect"
      ],
      "correct_answer": "B",
      "explanation": "A VPC Gateway Endpoint for S3 provides a private connection between your VPC and S3 without going through the internet, reducing latency and keeping traffic within the AWS network.",
      "reference": "https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A company needs to run a high-performance computing (HPC) workload that requires low-latency networking between instances. Which feature should they use?",
      "options": [
        "A. Placement groups - Spread",
        "B. Placement groups - Cluster",
        "C. Enhanced networking",
        "D. Multiple Elastic Network Interfaces"
      ],
      "correct_answer": "B",
      "explanation": "Cluster placement groups pack instances close together inside an Availability Zone, providing low-latency networking ideal for HPC workloads that require tight coupling between nodes.",
      "reference": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A data analytics application needs to query petabytes of data in S3 using standard SQL. Which service provides the best performance without managing infrastructure?",
      "options": [
        "A. Amazon EMR",
        "B. Amazon Athena",
        "C. Amazon Redshift",
        "D. AWS Glue"
      ],
      "correct_answer": "B",
      "explanation": "Amazon Athena is a serverless query service that enables direct SQL queries against S3 data without infrastructure management. It scales automatically and is ideal for ad-hoc analytics on large datasets.",
      "reference": "https://docs.aws.amazon.com/athena/latest/ug/what-is.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A real-time application needs to ingest and process millions of streaming data records per second. Which service should be used?",
      "options": [
        "A. Amazon SQS",
        "B. Amazon Kinesis Data Streams",
        "C. Amazon SNS",
        "D. AWS Step Functions"
      ],
      "correct_answer": "B",
      "explanation": "Amazon Kinesis Data Streams is designed for real-time streaming data ingestion and processing, capable of handling millions of records per second with sub-second latency.",
      "reference": "https://docs.aws.amazon.com/streams/latest/dev/introduction.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An application needs a shared file system accessible by multiple EC2 instances with POSIX compliance. Which service should be used?",
      "options": [
        "A. Amazon EBS Multi-Attach",
        "B. Amazon EFS",
        "C. Amazon S3",
        "D. Amazon FSx for Windows"
      ],
      "correct_answer": "B",
      "explanation": "Amazon EFS provides a fully managed, elastic, POSIX-compliant file system that can be mounted concurrently from multiple EC2 instances across multiple Availability Zones.",
      "reference": "https://docs.aws.amazon.com/efs/latest/ug/whatisefs.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "4",
      "difficulty": 1,
      "question_text": "Which EC2 pricing option offers the lowest cost for workloads that can be interrupted?",
      "options": [
        "A. On-Demand Instances",
        "B. Reserved Instances",
        "C. Spot Instances",
        "D. Dedicated Hosts"
      ],
      "correct_answer": "C",
      "explanation": "Spot Instances offer up to 90% discount compared to On-Demand prices for spare EC2 capacity that can be interrupted with 2-minute notice when AWS needs the capacity back.",
      "reference": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "4",
      "difficulty": 1,
      "question_text": "Which S3 storage class is the most cost-effective for infrequently accessed data that needs immediate access?",
      "options": [
        "A. S3 Standard",
        "B. S3 Standard-IA",
        "C. S3 Glacier",
        "D. S3 Glacier Deep Archive"
      ],
      "correct_answer": "B",
      "explanation": "S3 Standard-Infrequent Access (S3 Standard-IA) offers lower storage costs than S3 Standard while providing millisecond access to data, ideal for infrequently accessed data.",
      "reference": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "4",
      "difficulty": 1,
      "question_text": "Which service helps identify underutilized resources and provides cost optimization recommendations?",
      "options": [
        "A. AWS Budgets",
        "B. AWS Cost Explorer",
        "C. AWS Trusted Advisor",
        "D. AWS Config"
      ],
      "correct_answer": "C",
      "explanation": "AWS Trusted Advisor analyzes your AWS environment and provides recommendations for cost optimization, including identifying underutilized resources like idle EC2 instances.",
      "reference": "https://docs.aws.amazon.com/awssupport/latest/user/trusted-advisor.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "4",
      "difficulty": 1,
      "question_text": "What is the benefit of using AWS Lambda over EC2 for sporadic workloads?",
      "options": [
        "A. Higher performance",
        "B. Pay only for compute time used",
        "C. More control over infrastructure",
        "D. Better security"
      ],
      "correct_answer": "B",
      "explanation": "AWS Lambda charges only for the compute time consumed, making it cost-effective for sporadic workloads since you don't pay for idle time like with EC2 instances.",
      "reference": "https://docs.aws.amazon.com/lambda/latest/dg/welcome.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "4",
      "difficulty": 2,
      "question_text": "A company has predictable workloads running 24/7 for the next 3 years. Which EC2 pricing option provides the best cost savings?",
      "options": [
        "A. On-Demand Instances",
        "B. 3-year Reserved Instances with All Upfront payment",
        "C. Spot Instances",
        "D. Savings Plans"
      ],
      "correct_answer": "B",
      "explanation": "3-year Reserved Instances with All Upfront payment provide the maximum discount (up to 72%) for predictable, long-term workloads that run continuously.",
      "reference": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-reserved-instances.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "4",
      "difficulty": 2,
      "question_text": "Which S3 feature automatically moves objects to lower-cost storage classes based on access patterns?",
      "options": [
        "A. S3 Versioning",
        "B. S3 Intelligent-Tiering",
        "C. S3 Lifecycle policies",
        "D. S3 Replication"
      ],
      "correct_answer": "B",
      "explanation": "S3 Intelligent-Tiering automatically moves objects between access tiers based on changing access patterns, optimizing costs without performance impact or operational overhead.",
      "reference": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/intelligent-tiering.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "4",
      "difficulty": 2,
      "question_text": "A company wants to reduce data transfer costs between EC2 instances and S3. What should they implement?",
      "options": [
        "A. CloudFront distribution",
        "B. S3 Transfer Acceleration",
        "C. VPC Gateway Endpoint for S3",
        "D. AWS Direct Connect"
      ],
      "correct_answer": "C",
      "explanation": "VPC Gateway Endpoints for S3 are free and eliminate data transfer charges for traffic between EC2 instances in the VPC and S3, while keeping traffic within the AWS network.",
      "reference": "https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "4",
      "difficulty": 2,
      "question_text": "Which service allows you to set custom budgets and receive alerts when costs exceed thresholds?",
      "options": [
        "A. AWS Cost Explorer",
        "B. AWS Budgets",
        "C. AWS Pricing Calculator",
        "D. AWS Cost and Usage Report"
      ],
      "correct_answer": "B",
      "explanation": "AWS Budgets allows you to set custom cost and usage budgets and receive alerts via email or SNS when your actual or forecasted costs exceed your thresholds.",
      "reference": "https://docs.aws.amazon.com/cost-management/latest/userguide/budgets-managing-costs.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "4",
      "difficulty": 3,
      "question_text": "A company runs batch processing jobs that can be interrupted and restarted. Jobs typically run for 2-6 hours. Which combination provides the best cost optimization?",
      "options": [
        "A. On-Demand Instances with Auto Scaling",
        "B. Spot Instances with checkpointing",
        "C. Reserved Instances with scheduled scaling",
        "D. Lambda functions with Step Functions"
      ],
      "correct_answer": "B",
      "explanation": "Spot Instances with checkpointing provide up to 90% cost savings for interruptible workloads. Checkpointing allows jobs to save progress and resume after interruption.",
      "reference": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-interruptions.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "4",
      "difficulty": 3,
      "question_text": "A company needs to choose between RDS and Aurora for a production database. Which statement about cost is correct?",
      "options": [
        "A. Aurora is always cheaper than RDS",
        "B. RDS is always cheaper than Aurora",
        "C. Aurora can be more cost-effective due to better performance and reduced read replica needs",
        "D. Aurora and RDS have identical pricing"
      ],
      "correct_answer": "C",
      "explanation": "While Aurora has higher per-hour costs, it can be more cost-effective overall due to up to 5x better performance than MySQL RDS, potentially requiring fewer or smaller instances and read replicas.",
      "reference": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "4",
      "difficulty": 3,
      "question_text": "A company wants to optimize costs for archival data that must be retained for 7 years but is rarely accessed. Retrieval time of 12 hours is acceptable. Which storage should they use?",
      "options": [
        "A. S3 Standard-IA",
        "B. S3 Glacier",
        "C. S3 Glacier Deep Archive",
        "D. S3 One Zone-IA"
      ],
      "correct_answer": "C",
      "explanation": "S3 Glacier Deep Archive is the lowest-cost storage class, ideal for long-term archival where data retrieval within 12 hours is acceptable. It's up to 75% cheaper than Glacier.",
      "reference": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "4",
      "difficulty": 3,
      "question_text": "A company has multiple AWS accounts and wants to consolidate billing and receive volume discounts. What should they implement?",
      "options": [
        "A. AWS Cost Explorer across accounts",
        "B. AWS Organizations with consolidated billing",
        "C. Separate AWS Budgets per account",
        "D. AWS Compute Optimizer"
      ],
      "correct_answer": "B",
      "explanation": "AWS Organizations with consolidated billing combines usage across all accounts for volume pricing discounts and provides a single bill, maximizing cost savings from aggregated usage.",
      "reference": "https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/consolidated-billing.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "A company needs to allow third-party auditors temporary access to specific AWS resources. Which approach is most secure?",
      "options": [
        "A. Create IAM users for auditors",
        "B. Share root account credentials",
        "C. Use IAM roles with external ID for cross-account access",
        "D. Create access keys for auditors"
      ],
      "correct_answer": "C",
      "explanation": "IAM roles with external ID provide secure cross-account access for third parties. The external ID adds protection against confused deputy attacks and access is temporary.",
      "reference": "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-user_externalid.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 3,
      "question_text": "A company needs to implement a defense-in-depth strategy for their web application. Which combination of services provides the most comprehensive protection?",
      "options": [
        "A. Security Groups and NACLs only",
        "B. AWS WAF, Shield, and Security Groups",
        "C. AWS WAF, Shield Advanced, CloudFront, Security Groups, and NACLs",
        "D. Only AWS Shield Advanced"
      ],
      "correct_answer": "C",
      "explanation": "A defense-in-depth strategy uses multiple layers: CloudFront for edge protection, WAF for application-layer filtering, Shield Advanced for DDoS protection, and Security Groups/NACLs for network-level controls.",
      "reference": "https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-resiliency/aws-best-practices-ddos-resiliency.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 1,
      "question_text": "Which AWS service automatically backs up data across multiple facilities?",
      "options": [
        "A. Amazon EC2",
        "B. Amazon EBS",
        "C. Amazon S3",
        "D. All of the above with proper configuration"
      ],
      "correct_answer": "C",
      "explanation": "Amazon S3 automatically stores data across a minimum of three Availability Zones by default (except One Zone-IA), providing built-in redundancy without additional configuration.",
      "reference": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/DataDurability.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "A company needs to ensure their application can handle a sudden 10x increase in traffic. Which architecture approach should they use?",
      "options": [
        "A. Provision for peak capacity at all times",
        "B. Use Auto Scaling with predictive scaling",
        "C. Use spot instances only",
        "D. Manual scaling during peak times"
      ],
      "correct_answer": "B",
      "explanation": "Auto Scaling with predictive scaling uses machine learning to predict capacity needs and can scale proactively, handling sudden traffic increases while optimizing costs during normal periods.",
      "reference": "https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-predictive-scaling.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company needs to run a memory-intensive application that processes large datasets. Which EC2 instance type should they choose?",
      "options": [
        "A. C5 instances",
        "B. R5 instances",
        "C. T3 instances",
        "D. M5 instances"
      ],
      "correct_answer": "B",
      "explanation": "R5 instances are memory-optimized instances designed for memory-intensive applications such as high-performance databases, in-memory caching, and real-time big data analytics.",
      "reference": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/memory-optimized-instances.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "An application needs to serve static content globally with the lowest possible latency. Users are distributed worldwide. What is the optimal architecture?",
      "options": [
        "A. S3 bucket in a single region with Transfer Acceleration",
        "B. CloudFront distribution with S3 origin",
        "C. Multiple S3 buckets with Route 53 latency-based routing",
        "D. EC2 instances in multiple regions with ALB"
      ],
      "correct_answer": "B",
      "explanation": "CloudFront with S3 origin caches content at 400+ edge locations worldwide, providing the lowest latency for static content delivery to globally distributed users.",
      "reference": "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/GettingStarted.SimpleDistribution.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "4",
      "difficulty": 1,
      "question_text": "Which pricing model should a company choose for an application with unpredictable traffic patterns?",
      "options": [
        "A. Reserved Instances",
        "B. On-Demand Instances",
        "C. Spot Instances",
        "D. Dedicated Hosts"
      ],
      "correct_answer": "B",
      "explanation": "On-Demand Instances are best for unpredictable workloads as you pay by the hour or second with no upfront commitment, providing flexibility to scale up or down based on actual demand.",
      "reference": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-on-demand-instances.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "4",
      "difficulty": 2,
      "question_text": "A company runs development and test environments that are only used during business hours. How can they reduce costs?",
      "options": [
        "A. Use Reserved Instances",
        "B. Use Instance Scheduler to stop instances outside business hours",
        "C. Use Spot Instances only",
        "D. Move to larger instances"
      ],
      "correct_answer": "B",
      "explanation": "AWS Instance Scheduler automatically stops and starts EC2 and RDS instances based on schedules, reducing costs by up to 70% for instances that don't need to run 24/7.",
      "reference": "https://docs.aws.amazon.com/solutions/latest/instance-scheduler-on-aws/solution-overview.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 1,
      "question_text": "Which service should be used to protect a web application from SQL injection attacks?",
      "options": [
        "A. AWS Shield",
        "B. AWS WAF",
        "C. Amazon GuardDuty",
        "D. Security Groups"
      ],
      "correct_answer": "B",
      "explanation": "AWS WAF (Web Application Firewall) protects web applications from common web exploits including SQL injection by filtering malicious requests based on customizable rules.",
      "reference": "https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 3,
      "question_text": "A company needs to implement a multi-region active-active architecture for their application. Which database solution supports this requirement?",
      "options": [
        "A. RDS Multi-AZ",
        "B. Aurora Global Database (write forwarding)",
        "C. DynamoDB Global Tables",
        "D. RDS Read Replicas"
      ],
      "correct_answer": "C",
      "explanation": "DynamoDB Global Tables provide fully replicated, multi-region, multi-active database tables that allow read and write operations in any region with automatic conflict resolution.",
      "reference": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GlobalTables.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 1,
      "question_text": "Which AWS service provides block-level storage for EC2 instances?",
      "options": [
        "A. Amazon S3",
        "B. Amazon EFS",
        "C. Amazon EBS",
        "D. AWS Storage Gateway"
      ],
      "correct_answer": "C",
      "explanation": "Amazon Elastic Block Store (EBS) provides block-level storage volumes that can be attached to EC2 instances, functioning like raw, unformatted block devices.",
      "reference": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AmazonEBS.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company needs to migrate an on-premises Oracle database to AWS with minimal code changes. Which service should they use?",
      "options": [
        "A. Amazon DynamoDB",
        "B. Amazon RDS for Oracle",
        "C. Amazon Aurora",
        "D. Amazon Redshift"
      ],
      "correct_answer": "B",
      "explanation": "Amazon RDS for Oracle provides a managed Oracle database service that is compatible with existing Oracle applications, requiring minimal to no code changes for migration.",
      "reference": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Oracle.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "4",
      "difficulty": 3,
      "question_text": "A company wants to compare the costs of running their workload on different EC2 instance types. Which tool should they use?",
      "options": [
        "A. AWS Cost Explorer",
        "B. AWS Compute Optimizer",
        "C. AWS Pricing Calculator",
        "D. AWS Budgets"
      ],
      "correct_answer": "B",
      "explanation": "AWS Compute Optimizer analyzes your actual resource utilization and recommends optimal AWS resources including EC2 instance types, showing potential cost savings from rightsizing.",
      "reference": "https://docs.aws.amazon.com/compute-optimizer/latest/ug/what-is-compute-optimizer.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "A company needs to log all API calls made to their AWS account for security and compliance purposes. Which service should they use?",
      "options": [
        "A. Amazon CloudWatch",
        "B. AWS CloudTrail",
        "C. AWS Config",
        "D. VPC Flow Logs"
      ],
      "correct_answer": "B",
      "explanation": "AWS CloudTrail logs all API calls made to AWS services in your account, providing a complete audit trail for security analysis, resource change tracking, and compliance auditing.",
      "reference": "https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-user-guide.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 1,
      "question_text": "What happens when an EC2 instance in an Auto Scaling group fails a health check?",
      "options": [
        "A. Nothing, the instance continues running",
        "B. An alert is sent but the instance keeps running",
        "C. The instance is terminated and replaced",
        "D. The instance is stopped but not terminated"
      ],
      "correct_answer": "C",
      "explanation": "When an EC2 instance fails health checks in an Auto Scaling group, Auto Scaling terminates the unhealthy instance and launches a replacement to maintain the desired capacity.",
      "reference": "https://docs.aws.amazon.com/autoscaling/ec2/userguide/healthcheck.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 1,
      "question_text": "Which service enables running code without provisioning or managing servers?",
      "options": [
        "A. Amazon EC2",
        "B. Amazon ECS",
        "C. AWS Lambda",
        "D. AWS Batch"
      ],
      "correct_answer": "C",
      "explanation": "AWS Lambda is a serverless compute service that runs code in response to events and automatically manages the underlying compute resources, without requiring server provisioning.",
      "reference": "https://docs.aws.amazon.com/lambda/latest/dg/welcome.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "4",
      "difficulty": 2,
      "question_text": "A company wants to use a mix of On-Demand and Spot Instances for their web application. Which feature allows this while maintaining availability?",
      "options": [
        "A. Auto Scaling groups with mixed instances policy",
        "B. Spot Fleet only",
        "C. Reserved Instances",
        "D. Dedicated Hosts"
      ],
      "correct_answer": "A",
      "explanation": "Auto Scaling groups with mixed instances policy allow you to combine On-Demand and Spot Instances, maintaining availability by automatically replacing Spot interruptions with On-Demand if needed.",
      "reference": "https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-mixed-instances-groups.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You have launched an EC2 instance that will host a NodeJS application. After installing all the required software and configured your application, you noted down the EC2 instance public IPv4 so you can access it. Then, you stopped and then started your EC2 instance to complete the application configuration. After restart, you can't access the EC2 instance, and you found that the EC2 instance public IPv4 has been changed. What should you do to assign a fixed public IPv4 to your EC2 instance?",
      "options": [
        "A. Allocate an Elastic IP and assign it to your EC2 instance",
        "B. From inside your EC2 instance OS, change network configuration from DHCP to static and assign it a public IPv4",
        "C. Contact AWS Support and request a fixed public IPv4 to your EC2 instance",
        "D. This can't be done, you can only assign a fixed private IPv4 to your EC2 instance"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nElastic IP is a public IPv4 that you own as long as you want and you can attach it to one EC2 instance at a time.\n\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: EC2 SAA Level Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You have an application performing big data analysis hosted on a fleet of EC2 instances. You want to ensure your EC2 instances have the highest networking performance while communicating with each other. Which EC2 Placement Group should you choose?",
      "options": [
        "A. Spread Placement Group",
        "B. Cluster Placement Group",
        "C. Partition Placement Group"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nCluster Placement Groups place your EC2 instances next to each other which gives you high-performance computing and networking.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: EC2 SAA Level Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You have a critical application hosted on a fleet of EC2 instances in which you want to achieve maximum availability when there's an AZ failure. Which EC2 Placement Group should you choose?",
      "options": [
        "A. Cluster Placement Group",
        "B. Partition Placement Group",
        "C. Spread Placement Group"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nSpread Placement Group places your EC2 instances on different physical hardware across different AZs.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: EC2 SAA Level Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "Elastic Network Interface (ENI) can be attached to EC2 instances in another AZ.",
      "options": [
        "A. True",
        "B. False"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nElastic Network Interfaces (ENIs) are bounded to a specific AZ. You can not attach an ENI to an EC2 instance in a different AZ.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: EC2 SAA Level Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "The following are true regarding EC2 Hibernate, EXCEPT:",
      "options": [
        "A. EC2 Instance Root Volume must be an Instance Store volume",
        "B. Supports On-Demand and Reserved Instances",
        "C. EC2 Instance RAM must be less than 150GB",
        "D. EC2 Instance Root Volume type must be an EBS volume"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nTo enable EC2 Hibernate, the EC2 Instance Root Volume type must be an EBS volume and must be encrypted to ensure the protection of sensitive content.\n\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: EC2 SAA Level Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You need to move hundreds of Terabytes into Amazon S3, then process the data using a fleet of EC2 instances. You have a 1 Gbit/s broadband. You would like to move the data faster and possibly processing it while in transit. What do you recommend?",
      "options": [
        "A. Use your network",
        "B. Use Snowcone",
        "C. Use AWS Data Migration",
        "D. Use Snowball Edge"
      ],
      "correct_answer": "D",
      "explanation": "**Correct Answer: D**\n\n**Why D is correct:**\nSnowball Edge is the right answer as it comes with computing capabilities and allows you to pre-process the data while it's being moved into Snowball.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: AWS Storage Extras Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You want to expose virtually infinite storage for your tape backups. You want to keep the same software you're using and want an iSCSI compatible interface. What do you use?",
      "options": [
        "A. AWS Snowball",
        "B. AWS Storage Gateway - Tape Gateway",
        "C. AWS Storage Gateway - Volume Gateway",
        "D. AWS Storage Gateway - S3 File Gateway"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: AWS Storage Extras Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "Your EC2 Windows Servers need to share some data by having a Network File System mounted on them which respects the Windows security mechanisms and has integration with Microsoft Active Directory. What do you recommend?",
      "options": [
        "A. Amazon FSx for Windows (File Server)",
        "B. Amazon EFS",
        "C. Amazon FSx for Lustre",
        "D. S3 File Gateway"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: AWS Storage Extras Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You have hundreds of Terabytes that you want to migrate to AWS S3 as soon as possible. You tried to use your network bandwidth and it will take around 3 weeks to complete the upload process. What is the recommended approach to using in this situation?",
      "options": [
        "A. AWS Storage Gateway - Volume Gateway",
        "B. S3 Multi-part Upload",
        "C. AWS Snowball Edge",
        "D. AWS Data Migration Service"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: AWS Storage Extras Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You have a large dataset stored in S3 that you want to access from on-premises servers using the NFS or SMB protocol. Also, you want to authenticate access to these files through on-premises Microsoft AD. What would you use?",
      "options": [
        "A. AWS Storage Gateway - Volume Gateway",
        "B. AWS Storage Gateway - S3 File Gateway",
        "C. AWS Storage Gateway - Tape Gateway",
        "D. AWS Data Migration Service"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: AWS Storage Extras Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You are planning to migrate your company's infrastructure from on-premises to AWS Cloud. You have an on-premises Microsoft Windows File Server that you want to migrate. What is the most suitable AWS service you can use?",
      "options": [
        "A. Amazon FSx for Windows (File Server)",
        "B. AWS Storage Gateway - S3 File Gateway",
        "C. AWS Managed Microsoft AD"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: AWS Storage Extras Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You would like to have a distributed POSIX compliant file system that will allow you to maximize the IOPS in order to perform some High-Performance Computing (HPC) and genomics computational research. This file system has to easily scale to millions of IOPS. What do you recommend?",
      "options": [
        "A. EFS with Max. IO enabled",
        "B. Amazon FSx for Lustre",
        "C. Amazon S3 mounted on the EC2 instances",
        "D. EC2 Instance Store"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis is not meant for HPC. | This is not POSIX compliant. | This will give you the IOPS, but it is not a Distributed File System.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: AWS Storage Extras Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "Which deployment option in the FSx file system provides you with long-term storage that's replicated within AZ?",
      "options": [
        "A. Scratch File System",
        "B. Persistent File System"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nProvides long-term storage where data is replicated within the same AZ. Failed files were replaced within minutes.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: AWS Storage Extras Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "Which of the following protocols is NOT supported by AWS Transfer Family?",
      "options": [
        "A. File Transfer Protocol (FTP)",
        "B. File Transfer Protocol over SSL (FTPS)",
        "C. Transport Layer Security (TLS)",
        "D. Secure File Transfer Protocol (SFTP)"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nAWS Transfer Family is a managed service for file transfers into and out of S3 or EFS using the FTP protocol, thus TLS is not supported.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: AWS Storage Extras Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company uses a lot of files and data which is stored in an FSx for Windows File Server storage on AWS. Those files are currently used by the resources hosted on AWS. There’s a requirement for those files to be accessed on-premises with low latency. Which AWS service can help you achieve this?",
      "options": [
        "A. S3 File Gateway",
        "B. FSx for Windows File Server On-Premises",
        "C. FSx File Gateway",
        "D. Volume Gateway"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: AWS Storage Extras Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A Solutions Architect is working on planning the migration of a startup company from on-premises to AWS. Currently, their infrastructure consists of many servers and 30 TB of data hosted on a shared NFS storage. He has decided to use Amazon S3 to host the data. Which AWS service can efficiently migrate the data from on-premises to S3?",
      "options": [
        "A. AWS Storage Tape Gateway",
        "B. Amazon EBS",
        "C. AWS Transfer Family",
        "D. AWS DataSync"
      ],
      "correct_answer": "D",
      "explanation": "**Correct Answer: D**\n\n**Why D is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: AWS Storage Extras Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "Which AWS service is best suited to migrate a large amount of data from an S3 bucket to an EFS file system?",
      "options": [
        "A. AWS Snowball",
        "B. AWS DataSync",
        "C. AWS Transfer Family",
        "D. AWS Backup"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: AWS Storage Extras Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A Machine Learning company is working on a set of datasets that are hosted on S3 buckets. The company decided to release those datasets to the public to be useful for others in their research, but they don’t want to configure the S3 bucket to be public. And those datasets should be exposed over the FTP protocol. What can they do to do the requirement efficiently and with the least effort?",
      "options": [
        "A. Use AWS Transfer Family",
        "B. Create an EC2 instance with an FTP server installed then copy the data from S3 to the EC2 instance",
        "C. Use AWS Storage Gateway",
        "D. Copy the data from S3 to an EFS file system, then expose them over the FTP protocol"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: AWS Storage Extras Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "Amazon FSx for NetApp ONTAP is compatible with the following protocols, EXCEPT ………………",
      "options": [
        "A. NFS",
        "B. SMB",
        "C. FTP",
        "D. iSCSI"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: AWS Storage Extras Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "Which AWS service is best suited when migrating from an on-premises ZFS file system to AWS?",
      "options": [
        "A. Amazon FSx for OpenZFS",
        "B. Amazon FSx for NetApp ONTAP",
        "C. Amazon FSx for Windows File Server",
        "D. Amazon FSx for Luster"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: AWS Storage Extras Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company is running Amazon S3 File Gateway to host their data on S3 buckets and is able to mount them on-premises using SMB. The data currently is hosted on S3 Standard storage class and there is a requirement to reduce the costs for S3. So, they have decided to migrate some of those data to S3 Glacier. What is the most efficient way they can use to move the data to S3 Glacier automatically?",
      "options": [
        "A. Create a Lambda function to migrate data to S3 Glacier and periodically trigger it every day using Amazon EventBridge",
        "B. Use S3 Batch Operations to loop through S3 files and move them to S3 Glacier every day",
        "C. Use S3 Lifecycle Policy",
        "D. Use AWS DataSync to replicate data to S3 Glacier every day",
        "E. Configure S3 File Gateway to send the data to S3 Glacier directly"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why E is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: AWS Storage Extras Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You have on-premises sensitive files and documents that you want to regularly synchronize to AWS to keep another copy. Which AWS service can help you with that?",
      "options": [
        "A. AWS Database Migration Service",
        "B. Amazon EFS",
        "C. AWS DataSync"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nAWS DataSync is an online data transfer service that simplifies, automates, and accelerates moving data between on-premises storage systems and AWS Storage services, as well as between AWS Storage services.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: AWS Storage Extras Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "AWS DataSync supports the following locations, EXCEPT ....................",
      "options": [
        "A. Amazon S3",
        "B. Amazon EBS",
        "C. Amazon EFS",
        "D. Amazon FSx for Windows File Server"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: AWS Storage Extras Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "Which database helps you store relational datasets, with SQL language compatibility and the capability of processing transactions such as insert, update, and delete?",
      "options": [
        "A. Amazon DocumentDB",
        "B. Amazon RDS",
        "C. Amazon DynamoDB",
        "D. Amazon ElastiCache"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAmazon DynamoDB is a key-value, document, NoSQL database. It does not have SQL and does not store the data in a relational format, but can be used for transactional processing. | Amazon ElastiCache is a caching technology and it doesn't have SQL capabilities.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Databases in AWS Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "Which AWS service provides you with caching capability that is compatible with Redis API?",
      "options": [
        "A. Amazon RDS",
        "B. Amazon DynamoDB",
        "C. Amazon OpenSearch",
        "D. Amazon ElastiCache"
      ],
      "correct_answer": "D",
      "explanation": "**Correct Answer: D**\n\n**Why D is correct:**\nAmazon ElastiCache is a fully managed in-memory data store, compatible with Redis or Memcached.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Databases in AWS Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You want to migrate an on-premises MongoDB NoSQL database to AWS. You don't want to manage any database servers, so you want to use a managed NoSQL Serverless database, that provides you with high availability, durability, and reliability, and the capability to take your database global. Which database should you choose?",
      "options": [
        "A. Amazon RDS",
        "B. Amazon DynamoDB",
        "C. Amazon DocumentDB",
        "D. Amazon Aurora"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAmazon DynamoDB is a key-value, document, NoSQL database.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Databases in AWS Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You are looking to perform Online Transaction Processing (OLTP). You would like to use a database that has built-in auto-scaling capabilities and provides you with the maximum number of replicas for its underlying storage. What AWS service do you recommend?",
      "options": [
        "A. Amazon ElastiCache",
        "B. Amazon Neptune",
        "C. Amazon Aurora",
        "D. Amazon RDS"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nAmazon Aurora is a MySQL and PostgreSQL-compatible relational database. It features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 128TB per database instance. It delivers high performance and availability with up to 15 low-latency read replicas, point-in-time recovery, continuous backup to Amazon S3, and replication across 3 AZs.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Databases in AWS Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "As a Solutions Architect, a startup company asked you for help as they are working on an architecture for a social media website where users can be friends with each other, and like each other's posts. The company plan on performing some complicated queries such as \"What are the number of likes on the posts that have been posted by the friends of Mike?\". Which database do you recommend?",
      "options": [
        "A. Amazon RDS",
        "B. Amazon QLDB",
        "C. Amazon Neptune",
        "D. Amazon OpenSearch"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nAmazon Neptune is a fast, reliable, fully-managed graph database service that makes it easy to build and run applications that work with highly connected datasets.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Databases in AWS Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You have a set of files, 100MB each, that you want to store in a reliable and durable key-value store. Which AWS service do you recommend?",
      "options": [
        "A. Amazon Aurora",
        "B. Amazon S3",
        "C. Amazon DynamoDB",
        "D. Amazon ElastiCache"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAmazon S3 is indeed a key-value store! (where the key is the full path of the object in the bucket)\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Databases in AWS Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company has an on-premises website that uses ReactJS as its frontend, NodeJS as its backend, and MongoDB for the database. There are some issues with the self-hosted MongoDB database as there is a lot of maintenance required and they don’t have and can’t afford the resources or experience to handle those issues. So, a decision was made to migrate the website to AWS. They have decided to host the frontend ReactJS application in an S3 bucket and the NodeJS backend on a set of EC2 instances. Which AWS service can they use to migrate the MongoDB database that provides them with high scalability and availability without making any code changes?",
      "options": [
        "A. Amazon ElastiCache",
        "B. Amazon DocumentDB",
        "C. Amazon RDS for MongoDB",
        "D. Amazon Neptune"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Databases in AWS Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company using a self-hosted on-premises Apache Cassandra database which they want to migrate to AWS. Which AWS service can they use which provides them with a fully managed, highly available, and scalable Apache Cassandra database?",
      "options": [
        "A. Amazon DocumentDB",
        "B. Amazon DynamoDB",
        "C. Amazon Timestream",
        "D. Amazon Keyspaces"
      ],
      "correct_answer": "D",
      "explanation": "**Correct Answer: D**\n\n**Why D is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Databases in AWS Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A startup is working on developing a new project to reduce forest fires due to climate change. The startup is developing sensors that will be spread across the entire forest to make some readings such as temperature, humidity, and pressures which will help detect the forest fires before it happens. They are going to have thousands of sensors that are going to store a lot of readings each second. There is a requirement to store those readings and do fast analytics so they can predict if there is a fire. Which AWS service can they use to store those readings?",
      "options": [
        "A. Amazon Timestream",
        "B. Amazon Neptune",
        "C. Amazon S3",
        "D. Amazon ElastiCache"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Databases in AWS Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 3,
      "question_text": "You have strong regulatory requirements to only allow fully internally audited AWS services in production. You still want to allow your teams to experiment in a development environment while services are being audited. How can you best set this up?",
      "options": [
        "A. Provide the Dev team with a completely independent AWS account",
        "B. Apply a global IAM policy on your Prod account",
        "C. Create an AWS Organization and create two Prod and Dev OUs, then Apply an SCP on the Prod OU",
        "D. Create an AWS Config Rule"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: IAM Advanced Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 3,
      "question_text": "You are managing the AWS account for your company, and you want to give one of the developers access to read files from an S3 bucket. You have updated the bucket policy to this, but he still can't access the files in the bucket. What is the problem?{ \"Version\": \"2012-10-17\", \"Statement\": [{ \"Sid\": \"AllowsRead\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::123456789012:user/Dave\" }, \"Action\": \"s3:GetObject\", \"Resource\": \"arn:aws:s3:::static-files-bucket-xxx\" }]}",
      "options": [
        "A. Everything is okay, he just needs to logout and login again",
        "B. The bucket does not contain any files yet",
        "C. You should change the resource to arn:aws:s3:::static-files-bucket-xxx/*, because this is an object-level permission"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: IAM Advanced Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 3,
      "question_text": "You have 5 AWS Accounts that you manage using AWS Organizations. You want to restrict access to certain AWS services in each account. How should you do that?",
      "options": [
        "A. Using IAM Roles",
        "B. Using AWS Organizations SCP",
        "C. Using AWS Config"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: IAM Advanced Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 3,
      "question_text": "Which of the following IAM condition key you can use only to allow API calls to a specified AWS region?",
      "options": [
        "A. aws:RequiredRegion",
        "B. aws:SourceRegion",
        "C. aws:InitialRegion",
        "D. aws:RequestedRegion"
      ],
      "correct_answer": "D",
      "explanation": "**Correct Answer: D**\n\n**Why D is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: IAM Advanced Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 3,
      "question_text": "When configuring permissions for EventBridge to configure a Lambda function as a target you should use ………………….. but when you want to configure a Kinesis Data Streams as a target you should use …………………..",
      "options": [
        "A. Identity-Based Policy, Resource-based Policy",
        "B. Resource-Based Policy, Identity-based Policy",
        "C. Identity-Based Policy, Identity-Based Policy",
        "D. Resource-based Policy, Resource-based Policy"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: IAM Advanced Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "As part of your Disaster Recovery plan, you would like to have only the critical infrastructure up and running in AWS. You don't mind a longer Recovery Time Objective (RTO). Which DR strategy do you recommend?",
      "options": [
        "A. Backup and Restore",
        "B. Pilot Light",
        "C. Warm Standby",
        "D. Multi-Site"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nIf you're interested, read more about Disaster Recovery options in AWS here: https://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads-on-aws/disaster-recovery-options-in-the-cloud.html\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Disaster Recovery & Migration Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "You would like to get the Disaster Recovery strategy with the lowest Recovery Time Objective (RTO) and Recovery Point Objective (RPO), regardless of the cost. Which DR should you choose?",
      "options": [
        "A. Backup and Restore",
        "B. Pilot Light",
        "C. Warm Standby",
        "D. Multi-Site"
      ],
      "correct_answer": "D",
      "explanation": "**Correct Answer: D**\n\n**Why D is correct:**\nIf you're interested, read more about Disaster Recovery options in AWS here: https://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads-on-aws/disaster-recovery-options-in-the-cloud.html\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Disaster Recovery & Migration Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "Which of the following Disaster Recovery strategies has a potentially high Recovery Point Objective (RPO) and Recovery Time Objective (RTO)?",
      "options": [
        "A. Backup and Restore",
        "B. Pilot Light",
        "C. Warm Standby",
        "D. Multi-Site"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nIf you're interested, read more about Disaster Recovery options in AWS here: https://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads-on-aws/disaster-recovery-options-in-the-cloud.html\n\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Disaster Recovery & Migration Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "You want to make a Disaster Recovery plan where you have a scaled-down version of your system up and running, and when a disaster happens, it scales up quickly. Which DR strategy should you choose?",
      "options": [
        "A. Backup and Restore",
        "B. Pilot Light",
        "C. Warm Standby",
        "D. Multi-Site"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nIf you're interested, read more about Disaster Recovery options in AWS here: https://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads-on-aws/disaster-recovery-options-in-the-cloud.html\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Disaster Recovery & Migration Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "You have an on-premises Oracle database that you want to migrate to AWS, specifically to Amazon Aurora. How would you do the migration?",
      "options": [
        "A. Use AWS Schema Conversion Tool (AWS SCT) to convert the database schema, then use AWS Database Migration Service (AWS DMS) to migrate the data",
        "B. Use AWS Database Migration Service (AWS DMS) to convert the database schema, then use AWS Schema Conversion Tool (AWS SCT) to migrate the data"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Disaster Recovery & Migration Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "You are running many resources in AWS such as EC2 instances, EBS volumes, DynamoDB tables... You want an easy way to manage backups across all these AWS services from a single place. Which AWS offering makes this process easy?",
      "options": [
        "A. Amazon S3",
        "B. AWS Storage Gateway",
        "C. AWS Backup",
        "D. EC2 Snapshots"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nAWS Backup enables you to centralize and automate data protection across AWS services. It helps you support your regulatory compliance or business policies for data protection.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Disaster Recovery & Migration Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "A company planning to migrate its existing websites, applications, servers, virtual machines, and data to AWS. They want to do a lift-and-shift migration with minimum downtime and reduced costs. Which AWS service can help in this scenario?",
      "options": [
        "A. AWS Database Migration Service",
        "B. AWS Application Migration Service",
        "C. AWS Backup",
        "D. AWS Schema Conversion Tool"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Disaster Recovery & Migration Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "A company is using VMware on its on-premises data center to manage its infrastructure. There is a requirement to extend their data center and infrastructure to AWS but keep using the technology stack they are using which is VMware. Which AWS service can they use?",
      "options": [
        "A. VMware Cloud on AWS",
        "B. AWS DataSync",
        "C. AWS Application Migration Service",
        "D. AWS Application Discovery Service"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Disaster Recovery & Migration Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "A company is using RDS for MySQL as their main database but, lately they have been facing issues in managing the database, performance issues, and the scalability. And they have decided to use Aurora for MySQL instead for better performance, less complexity and less administrative tasks required. What is the best way and most cost-effective way to migrate from RDS for MySQL to Aurora for MySQL?",
      "options": [
        "A. Raise an AWS support ticket to do the migration as it is not supported",
        "B. Create a database dump from RDS from MySQL, store it in an S3 bucket, then restore it to Aurora for MySQL",
        "C. You can not migrate directly to Aurora for MySQL, you have to create a custom application to insert the data manually",
        "D. Create a snapshot from RDS for MySQL and restore it to Aurora for MySQL"
      ],
      "correct_answer": "D",
      "explanation": "**Correct Answer: D**\n\n**Why D is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Disaster Recovery & Migration Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "Which AWS service can you use to automate the backup across different AWS services such as RDS, DynamoDB, Aurora, and EFS file systems, and EBS volumes?",
      "options": [
        "A. Amazon S3 Lifecycle Policy",
        "B. AWS DataSync",
        "C. AWS Backup",
        "D. Amazon Glacier"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Disaster Recovery & Migration Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You would like to have a database that is efficient at performing analytical queries on large sets of columnar data. You would like to connect to this Data Warehouse using a reporting and dashboard tool such as Amazon QuickSight. Which AWS technology do you recommend?",
      "options": [
        "A. Amazon RDS",
        "B. Amazon S3",
        "C. Amazon Redshift",
        "D. Amazon Neptune"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Data & Analytics Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You have a lot of log files stored in an S3 bucket that you want to perform a quick analysis, if possible Serverless, to filter the logs and find users that attempted to make an unauthorized action. Which AWS service allows you to do so?",
      "options": [
        "A. Amazon DynamoDB",
        "B. Amazon Redshift",
        "C. S3 Glacier",
        "D. Amazon Athena"
      ],
      "correct_answer": "D",
      "explanation": "**Correct Answer: D**\n\n**Why D is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Data & Analytics Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "As a Solutions Architect, you have been instructed you to prepare a disaster recovery plan for a Redshift cluster. What should you do?",
      "options": [
        "A. Enable Multi-AZ",
        "B. Enable Automated Snapshots, then configure your Redshift cluster to automatically copy snapshots to another AWS region",
        "C. Take a snapshot then restore to a Redshift Global cluster"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Data & Analytics Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "Which feature in Redshift forces all COPY and UNLOAD traffic moving between your cluster and data repositories through your VPCs?",
      "options": [
        "A. Enhanced VPC Routing",
        "B. Improved VPC Routing",
        "C. Redshift Spectrum"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Data & Analytics Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You are running a gaming website that is using DynamoDB as its data store. Users have been asking for a search feature to find other gamers by name, with partial matches if possible. Which AWS technology do you recommend to implement this feature?",
      "options": [
        "A. Amazon DynamoDB",
        "B. Amazon Redshift",
        "C. Amazon OpenSearch Service",
        "D. Amazon Neptune"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Data & Analytics Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "An AWS service allows you to create, run, and monitor ETL (extract, transform, and load) jobs in a few clicks.",
      "options": [
        "A. AWS Glue",
        "B. Amazon Redshift",
        "C. Amazon RDS",
        "D. Amazon DynamoDB"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Data & Analytics Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company is using AWS to host its public websites and internal applications. Those different websites and applications generate a lot of logs and traces. There is a requirement to centrally store those logs and efficiently search and analyze those logs in real-time for detection of any errors and if there is a threat. Which AWS service can help them efficiently store and analyze logs?",
      "options": [
        "A. Amazon S3",
        "B. Amazon OpenSearch service",
        "C. Amazon ElastiCache",
        "D. Amazon QLDB"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Data & Analytics Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "……………………….. makes it easy and cost-effective for data engineers and analysts to run applications built using open source big data frameworks such as Apache Spark, Hive, or Presto without having to operate or manage clusters.",
      "options": [
        "A. AWS Lambda",
        "B. Amazon EMR",
        "C. Amazon Athena",
        "D. Amazon OpenSearch Service"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Data & Analytics Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "An e-commerce company has all its historical data such as orders, customers, revenues, and sales for the previous years hosted on a Redshift cluster. There is a requirement to generate some dashboards and reports indicating the revenues from the previous years and the total sales, so it will be easy to define the requirements for the next year. The DevOps team is assigned to find an AWS service that can help define those dashboards and have native integration with Redshift. Which AWS service is best suited?",
      "options": [
        "A. Amazon OpenSearch Service",
        "B. Amazon Athena",
        "C. Amazon QuickSight",
        "D. Amazon EMR"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Data & Analytics Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "Which AWS Glue feature allows you to save and track the data that has already been processed during a previous run of a Glue ETL job?",
      "options": [
        "A. Glue Job Bookmarks",
        "B. Glue Elastic Views",
        "C. Glue Streaming ETL",
        "D. Glue DataBrew"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Data & Analytics Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You are a DevOps engineer in a machine learning company which 3 TB of JSON files stored in an S3 bucket. There’s a requirement to do some analytics on those files using Amazon Athena and you have been tasked to find a way to convert those files’ format from JSON to Apache Parquet. Which AWS service is best suited?",
      "options": [
        "A. S3 Object Versioning",
        "B. Kinesis Data Streams",
        "C. Amazon MSK",
        "D. AWS Glue"
      ],
      "correct_answer": "D",
      "explanation": "**Correct Answer: D**\n\n**Why D is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Data & Analytics Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You have an on-premises application that is used together with an on-premises Apache Kafka to receive a stream of clickstream events from multiple websites. You have been tasked to migrate this application as soon as possible without any code changes. You decided to host the application on an EC2 instance. What is the best option you recommend to migrate Apache Kafka?",
      "options": [
        "A. Kinesis Data Streams",
        "B. AWS Glue",
        "C. Amazon MSK",
        "D. Kinesis Data Analytics"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Data & Analytics Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You have data stored in RDS, S3 buckets and you are using AWS Lake Formation as a data lake to collect, move and catalog data so you can do some analytics. You have a lot of big data and ML engineers in the company and you want to control access to part of the data as it might contain sensitive information. What can you use?",
      "options": [
        "A. Lake Formation Fine-grained Access Control",
        "B. Amazon Cognito",
        "C. AWS Shield",
        "D. S3 Object Lock"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Data & Analytics Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "Which AWS service is most appropriate when you want to perform real-time analytics on streams of data?",
      "options": [
        "A. Amazon SQS",
        "B. Amazon SNS",
        "C. Amazon Kinesis Data Analytics",
        "D. Amazon Kinesis Data Firehose"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nUse Kinesis Data Analytics with Kinesis Data Streams as the underlying source of data.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Data & Analytics Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You should use Amazon Transcribe to turn text into lifelike speech using deep learning.",
      "options": [
        "A. True",
        "B. False"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAmazon Transcribe is an AWS service that makes it easy for customers to convert speech-to-text. Amazon Polly is a service that turns text into lifelike speech.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Machine Learning Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company would like to implement a chatbot that will convert speech-to-text and recognize the customers' intentions. What service should it use?",
      "options": [
        "A. Transcribe",
        "B. Rekognition",
        "C. Connect",
        "D. Lex"
      ],
      "correct_answer": "D",
      "explanation": "**Correct Answer: D**\n\n**Why D is correct:**\nAmazon Lex is a service for building conversational interfaces into any application using voice and text. Lex provides the advanced deep learning functionalities of automatic speech recognition (ASR) for converting speech to text, and natural language understanding (NLU) to recognize the intent of the text, to enable you to build applications with highly engaging user experiences and lifelike conversational interactions.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Machine Learning Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You would like to find objects, people, text, or scenes in images and videos. What AWS service should you use?",
      "options": [
        "A. Rekognition",
        "B. Polly",
        "C. Kendra",
        "D. Lex"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nAmazon Rekognition makes it easy to add image and video analysis to your applications using proven, highly scalable, deep learning technology that requires no machine learning expertise to use.\n\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Machine Learning Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A start-up would like to rapidly create customized user experiences. Which AWS service can help?",
      "options": [
        "A. Personalize",
        "B. Kendra",
        "C. Connect"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nAmazon Personalize is a machine learning service that makes it easy for developers to create individualized recommendations for customers using their applications.\n\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Machine Learning Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A research team would like to group articles by topics using Natural Language Processing (NLP). Which service should they use?",
      "options": [
        "A. Translate",
        "B. Comprehend",
        "C. Lex",
        "D. Rekognition"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAmazon Comprehend is a natural language processing (NLP) service that uses machine learning to find meaning and insights in text.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Machine Learning Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company would like to convert its documents into different languages, with natural and accurate wording. What should they use?",
      "options": [
        "A. Transcribe",
        "B. Polly",
        "C. Translate",
        "D. WordTranslator"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nAmazon Translate is a neural machine translation service that delivers fast, high-quality, and affordable language translation.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Machine Learning Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A developer would like to build, train, and deploy a machine learning model quickly. Which service can he use?",
      "options": [
        "A. SageMaker",
        "B. Polly",
        "C. Comprehend",
        "D. Personalize"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nAmazon SageMaker is a fully managed service that provides every developer and data scientist with the ability to build, train, and deploy machine learning (ML) models quickly. SageMaker removes the heavy lifting from each step of the machine learning process to make it easier to develop high quality models.\n\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Machine Learning Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "Which AWS service makes it easy to convert speech-to-text?",
      "options": [
        "A. Connect",
        "B. Translate",
        "C. Transcribe",
        "D. Polly"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nAmazon Transcribe is an AWS service that makes it easy for customers to convert speech-to-text.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Machine Learning Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "Which of the following services is a document search service powered by machine learning?",
      "options": [
        "A. Translate",
        "B. Kendra",
        "C. Comprehend",
        "D. Polly"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAmazon Kendra is a highly accurate and easy to use enterprise search service that’s powered by machine learning.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Machine Learning Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company is managing an image and video sharing platform which is used by customers around the globe. The platform is running on AWS using an S3 bucket to host both images and videos and using CloudFront as the CDN to deliver content to customers all over the world with low latency. In the last couple of months, a lot of customers have complained that they have started to see inappropriate content on the platform which started to increase in the last week. It will be very expensive and time-consuming to manually approve those images and videos by employees before its published on the platform. There is a requirement to find a solution that can automatically detect inappropriate and offensive images and videos and give you the ability to set a minimum confidence threshold for items that will be flagged and allows for manual review. Which AWS service can fit the requirement?",
      "options": [
        "A. Amazon Polly",
        "B. Amazon Translate",
        "C. Amazon Lex",
        "D. Amazon Rekognition"
      ],
      "correct_answer": "D",
      "explanation": "**Correct Answer: D**\n\n**Why D is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Machine Learning Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "An online medical company that allows you to book an appointment with doctors using through a phone call is using AWS to host their infrastructure. They are using Amazon Connect and Amazon Lex to receive calls and create a workflow, book an appointment, and pay. According to the company’s policy, all calls must be recorded for review. But, there is a requirement to remove any Personally Identifiable Information (PII) from the call before it's saved. What do you recommend to use which helps in removing PII from calls?",
      "options": [
        "A. Amazon Polly",
        "B. Amazon Transcribe",
        "C. Amazon Recognition",
        "D. Amazon Translate"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Machine Learning Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "Amazon Polly allows you to turn text into speech. It has two important features. First is ……………….. which allows you to customize the pronunciation of words (e.g., “Amazon EC2” will be “Amazon Elastic Compute Cloud”). The second is ……………….. which allows you to emphasize words, including breathing sounds, whispering, and more.",
      "options": [
        "A. Speech Synthesis Markup Language (SSML), Pronunciation Lexicons",
        "B. Pronunciation Lexicons, Security Assertion Markup Language (SAML)",
        "C. Pronunciation Lexicons, Speech Synthesis Markup Language (SSML)",
        "D. Security Assertion Markup Language (SAML), Pronunciation Lexicons"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Machine Learning Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A medical company is in the process of implementing a solution to detect, extract, and analyze information from unstructured medical text like doctors’ notes, clinical trial reports, and radiology reports. Those documents are uploaded and stored on S3 buckets. According to the company’s regulations, the solution must be designed and implemented to keep patients’ privacy by identifying Protected Health Information (PHI) so the solution will be eligible with HIPAA. Which AWS service should you use?",
      "options": [
        "A. Amazon Comprehend Medical",
        "B. Amazon Rekognition",
        "C. Amazon Polly",
        "D. Amazon Translate"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Machine Learning Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You have multiple Docker-based applications hosted on-premises that you want to migrate to AWS. You don't want to provision or manage any infrastructure; you just want to run your containers on AWS. Which AWS service should you choose?",
      "options": [
        "A. Elastic Container Service (ECS) in EC2 Launch Mode",
        "B. Elastic Container Registry (ECR)",
        "C. AWS Fargate on ECS"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nAWS Fargate allows you to run your containers on AWS without managing any servers.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Containers on AWS Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "Amazon Elastic Container Service (ECS) has two Launch Types: .................. and ..................",
      "options": [
        "A. Amazon EC2 Launch Type and Fargate Launch Type",
        "B. Amazon EC2 Launch Type and EKS Launch Type",
        "C. Fargate Launch Type and EKS Launch Type"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Containers on AWS Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You have an application hosted on an ECS Cluster (EC2 Launch Type) where you want your ECS tasks to upload files to an S3 bucket. Which IAM Role for your ECS Tasks should you modify?",
      "options": [
        "A. EC2 Instance Profile",
        "B. ECS Task Role"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nECS Task Role is the IAM Role used by the ECS task itself. Use when your container wants to call other AWS services like S3, SQS, etc.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Containers on AWS Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You're planning to migrate a WordPress website running on Docker containers from on-premises to AWS. You have decided to run the application in an ECS Cluster, but you want your docker containers to access the same WordPress website content such as website files, images, videos, etc. What do you recommend to achieve this?",
      "options": [
        "A. Mount an EFS volume",
        "B. Mount an EBS volume",
        "C. Use an EC2 Instance Store"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nEFS volume can be shared between different EC2 instances and different ECS Tasks. It can be used as a persistent multi-AZ shared storage for your containers.\n\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Containers on AWS Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You are deploying an application on an ECS Cluster made of EC2 instances. Currently, the cluster is hosting one application that is issuing API calls to DynamoDB successfully. Upon adding a second application, which issues API calls to S3, you are getting authorization issues. What should you do to resolve the problem and ensure proper security?",
      "options": [
        "A. Edit the EC2 instance role to add permissions to S3",
        "B. Create an IAM task role for the new application",
        "C. Enable the Fargate mode",
        "D. Edit the S3 bucket policy to allow the ECS task"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis would allow the first application to access S3 as well, which is a security risk.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Containers on AWS Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You are migrating your on-premises Docker-based applications to Amazon ECS. You were using Docker Hub Container Image Library as your container image repository. Which is an alternative AWS service which is fully integrated with Amazon ECS?",
      "options": [
        "A. AWS Fargate",
        "B. Elastic Container Registry (ECR)",
        "C. Elastic Kubernetes Service (EKS)",
        "D. Amazon EC2"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAmazon ECR is a fully managed container registry that makes it easy to store, manage, share, and deploy your container images. ECR is fully integrated with Amazon ECS, allowing easy retrieval of container images from ECR while managing and running containers using ECS.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Containers on AWS Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "Amazon EKS supports the following node types, EXCEPT ………………..",
      "options": [
        "A. Managed Node Groups",
        "B. Self-Managed Nodes",
        "C. AWS Fargate",
        "D. AWS Lambda"
      ],
      "correct_answer": "D",
      "explanation": "**Correct Answer: D**\n\n**Why D is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Containers on AWS Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A developer has a running website and APIs on his local machine using containers and he wants to deploy both of them on AWS. The developer is new to AWS and doesn’t know much about different AWS services. Which of the following AWS services allows the developer to build and deploy the website and the APIs in the easiest way according to AWS best practices?",
      "options": [
        "A. AWS App Runner",
        "B. EC2 Instances + Application Load Balancer",
        "C. Amazon ECS",
        "D. AWS Fargate"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Containers on AWS Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You have just terminated an EC2 instance in us-east-1a, and its attached EBS volume is now available. Your teammate tries to attach it to an EC2 instance in us-east-1b but he can't. What is a possible cause for this?",
      "options": [
        "A. He's missing IAM permissions",
        "B. EBS volumes are locked to an AWS Region",
        "C. EBS volumes are locked to an Availability Zone"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nEBS Volumes are created for a specific AZ. It is possible to migrate them between different AZs using EBS Snapshots.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: EC2 Data Management Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You have launched an EC2 instance with two EBS volumes, Root volume type and the other EBS volume type to store the data. A month later you are planning to terminate the EC2 instance. What's the default behavior that will happen to each EBS volume?",
      "options": [
        "A. Both the root volume type and the EBS volume type will be deleted",
        "B. The Root volume type will be deleted and the EBS volume type will not be deleted",
        "C. The root volume type will not be deleted and the EBS volume type will be deleted",
        "D. Both the root volume type and the EBS volume type will not be deleted"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nBy default, the Root volume type will be deleted as its \"Delete On Termination\" attribute checked by default. Any other EBS volume types will not be deleted as its \"Delete On Termination\" attribute disabled by default.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: EC2 Data Management Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You can use an AMI in N.Virginia Region us-east-1 to launch an EC2 instance in any AWS Region.",
      "options": [
        "A. True",
        "B. False"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAMIs are built for a specific AWS Region, they're unique for each AWS Region. You can't launch an EC2 instance using an AMI in another AWS Region, but you can copy the AMI to the target AWS Region and then use it to create your EC2 instances.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: EC2 Data Management Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "Which of the following EBS volume types can be used as boot volumes when you create EC2 instances?",
      "options": [
        "A. gp2, gp3, st1, sc1",
        "B. gp2, gp3, io1, io2",
        "C. io1, io2, st1, sc1"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nWhen creating EC2 instances, you can only use the following EBS volume types as boot volumes: gp2, gp3, io1, io2, and Magnetic (Standard).\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: EC2 Data Management Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "What is EBS Multi-Attach?",
      "options": [
        "A. Attach the same EBS volume to multiple EC2 instances in multiple AZs",
        "B. Attach multiple EBS volumes in the same AZ to the same EC2 instance",
        "C. Attach the same EBS volume to multiple EC2 instances in the same AZ",
        "D. Attach multiple EBS volumes in multiple AZs to the same EC2 instance"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nUsing EBS Multi-Attach, you can attach the same EBS volume to multiple EC2 instances in the same AZ. Each EC2 instance has full read/write permissions.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: EC2 Data Management Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You would like to encrypt an unencrypted EBS volume attached to your EC2 instance. What should you do?",
      "options": [
        "A. Create an EBS snapshot of your EBS volume. Copy the snapshot and tick the option to encrypt the copied snapshot. Then, use the encrypted snapshot to create a new EBS volume",
        "B. Select your EBS volume, choose Edit Attributes, then tick the Encrypt using KMS option",
        "C. Create a new encrypted EBS volume, then copy data from your unencrypted EBS volume to the new EBS volume.",
        "D. Submit a request to AWS Support to encrypt your EBS volume"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nThis can be done but it'll take a lot of manual work, time, and will be costly especially if your EBS volume has a lot of data.\n\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: EC2 Data Management Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You have a fleet of EC2 instances distributes across AZs that process a large data set. What do you recommend to make the same data to be accessible as an NFS drive to all of your EC2 instances?",
      "options": [
        "A. Use EBS",
        "B. Use EFS",
        "C. Use an Instance Store"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nEFS is a network file system (NFS) that allows you to mount the same file system on EC2 instances that are in different AZs.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: EC2 Data Management Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You would like to have a high-performance local cache for your application hosted on an EC2 instance. You don't mind losing the cache upon the termination of your EC2 instance. Which storage mechanism do you recommend as a Solutions Architect?",
      "options": [
        "A. EBS",
        "B. EFS",
        "C. Instance Store"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nEC2 Instance Store provides the best disk I/O performance.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: EC2 Data Management Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You are running a high-performance database that requires an IOPS of 310,000 for its underlying storage. What do you recommend?",
      "options": [
        "A. Use an EBS gp2 drive",
        "B. Use an EBS io1 drive",
        "C. Use an EC2 Instance Store",
        "D. Use an EBS io2 Block Express drive"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nYou can run a database on an EC2 instance that uses an Instance Store, but you'll have a problem that the data will be lost if the EC2 instance is stopped (it can be restarted without problems). One solution is that you can set up a replication mechanism on another EC2 instance with an Instance Store to have a standby copy. Another solution is to set up backup mechanisms for your data. It's all up to you how you want to set up your architecture to validate your requirements. In this use case, it's around IOPS, so we have to choose an EC2 Instance Store.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: EC2 Data Management Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "Amazon RDS supports the following databases, EXCEPT:",
      "options": [
        "A. MongoDB",
        "B. MySQL",
        "C. MariaDB",
        "D. Microsoft SQL Server"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nRDS supports MySQL, PostgreSQL, MariaDB, Oracle, MS SQL Server, and Amazon Aurora.\n\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: RDS, Aurora, & ElastiCache Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "You're planning for a new solution that requires a MySQL database that must be available even in case of a disaster in one of the Availability Zones. What should you use?",
      "options": [
        "A. Create Read Replicas",
        "B. Enable Encryption",
        "C. Enable Multi-AZ"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nMulti-AZ helps when you plan a disaster recovery for an entire AZ going down. If you plan against an entire AWS Region going down, you should use backups and replication across AWS Regions.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: RDS, Aurora, & ElastiCache Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "We have an RDS database that struggles to keep up with the demand of requests from our website. Our million users mostly read news, and we don't post news very often. Which solution is NOT adapted to this problem?",
      "options": [
        "A. An ElastiCache Cluster",
        "B. RDS Multi-AZ",
        "C. RDS Read Replicas"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nBe very careful with the way you read questions at the exam. Here, the question is asking which solution is NOT adapted to this problem. ElastiCache and RDS Read Replicas do indeed help with scaling reads.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: RDS, Aurora, & ElastiCache Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "You have set up read replicas on your RDS database, but users are complaining that upon updating their social media posts, they do not see their updated posts right away. What is a possible cause for this?",
      "options": [
        "A. There must be a bug in your application",
        "B. Read Replicas have Asynchronous Replication, therefore it's likely your users will only read Eventual Consistency",
        "C. You should have setup Multi-AZ instead"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: RDS, Aurora, & ElastiCache Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "Which RDS (NOT Aurora) feature when used does not require you to change the SQL connection string?",
      "options": [
        "A. Multi-AZ",
        "B. Read Replicas"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nMulti-AZ keeps the same connection string regardless of which database is up.\n\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: RDS, Aurora, & ElastiCache Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "Your application running on a fleet of EC2 instances managed by an Auto Scaling Group behind an Application Load Balancer. Users have to constantly log back in and you don't want to enable Sticky Sessions on your ALB as you fear it will overload some EC2 instances. What should you do?",
      "options": [
        "A. Use your own custom Load Balancer on EC2 instances instead of using ALB",
        "B. Store session data in RDS",
        "C. Store session data in ElastiCache",
        "D. Store session data in a shared EBS volume"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nStoring Session Data in ElastiCache is a common pattern to ensuring different EC2 instances can retrieve your user's state if needed.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: RDS, Aurora, & ElastiCache Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "An analytics application is currently performing its queries against your main production RDS database. These queries run at any time of the day and slow down the RDS database which impacts your users' experience. What should you do to improve the users' experience?",
      "options": [
        "A. Setup a Read Replica",
        "B. Setup Multi-AZ",
        "C. Run the analytics queries at night"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nRead Replicas will help as your analytics application can now perform queries against it, and these queries won't impact the main production RDS database.\n\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: RDS, Aurora, & ElastiCache Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "You would like to ensure you have a replica of your database available in another AWS Region if a disaster happens to your main AWS Region. Which database do you recommend to implement this easily?",
      "options": [
        "A. RDS Read Replicas",
        "B. RDS Multi-AZ",
        "C. Aurora Read Replicas",
        "D. Aurora Global Database"
      ],
      "correct_answer": "D",
      "explanation": "**Correct Answer: D**\n\n**Why D is correct:**\nAurora Global Databases allows you to have an Aurora Replica in another AWS Region, with up to 5 secondary regions.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: RDS, Aurora, & ElastiCache Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "How can you enhance the security of your ElastiCache Redis Cluster by allowing users to access your ElastiCache Redis Cluster using their IAM Identities (e.g., Users, Roles)?",
      "options": [
        "A. Using Redis Authentication",
        "B. Using IAM Authentication",
        "C. Use Security Groups"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nUsing Redis Authentication, Redis requires a password before allowing clients to run commands. | Security Groups helps filter traffic going to your ElastiCache Redis Cluster instances but cannot help with application-level authentication.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: RDS, Aurora, & ElastiCache Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "Your company has a production Node.js application that is using RDS MySQL 5.6 as its database. A new application programmed in Java will perform some heavy analytics workload to create a dashboard on a regular hourly basis. What is the most cost-effective solution you can implement to minimize disruption for the main application?",
      "options": [
        "A. Enable Multi-AZ for the RDS database and run the analytics workload on the standby database",
        "B. Create a Read Replica in a different AZ and run the analytics workload on the replica database",
        "C. Create a Read Replica in a different AZ and run the analytics workload on the source database"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: RDS, Aurora, & ElastiCache Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "You would like to create a disaster recovery strategy for your RDS PostgreSQL database so that in case of a regional outage the database can be quickly made available for both read and write workloads in another AWS Region. The DR database must be highly available. What do you recommend?",
      "options": [
        "A. Create a Read Replica in the same region and enable Multi-AZ on the main database",
        "B. Create a Read Replica in a different region and enable Multi-AZ on the Read Replica",
        "C. Create a Read Replica in the same region and enable Multi-AZ on the Read Replica",
        "D. Enable Multi-Region option on the main database"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: RDS, Aurora, & ElastiCache Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "You have migrated the MySQL database from on-premises to RDS. You have a lot of applications and developers interacting with your database. Each developer has an IAM user in the company's AWS account. What is a suitable approach to give access to developers to the MySQL RDS DB instance instead of creating a DB user for each one?",
      "options": [
        "A. By default IAM users have access to your RDS database",
        "B. Use Amazon Cognito",
        "C. Enable IAM Database Authentication"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: RDS, Aurora, & ElastiCache Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "Which of the following statement is true regarding replication in both RDS Read Replicas and Multi-AZ?",
      "options": [
        "A. Read Replica uses Asynchronous Replication and Multi-AZ uses Asynchronous Replication",
        "B. Read Replica uses Asynchronous Replication and Multi-AZ uses Synchronous Replication",
        "C. Read Replica uses Synchronous Replication and Multi-AZ uses Synchronous Replication",
        "D. Read Replica uses Synchronous Replication and Multi-AZ uses Asynchronous Replication"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: RDS, Aurora, & ElastiCache Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "How do you encrypt an unencrypted RDS DB instance?",
      "options": [
        "A. Do it straight from AWS Console, select your RDS DB instance, choose Actions then Encrypt using KMS",
        "B. Do it straight from AWS Console, after stopping the RDS DB instance",
        "C. Create a snapshot of the unencrypted RDS DB instance, copy the snapshot and tick \"Enable encryption\", then restore the RDS DB instance from the encrypted snapshot"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: RDS, Aurora, & ElastiCache Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "For your RDS database, you can have up to ............ Read Replicas.",
      "options": [
        "A. 5",
        "B. 15",
        "C. 7"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: RDS, Aurora, & ElastiCache Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "Which RDS database technology does NOT support IAM Database Authentication?",
      "options": [
        "A. Oracle",
        "B. PostgreSQL",
        "C. MySQL"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: RDS, Aurora, & ElastiCache Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "You have an un-encrypted RDS DB instance and you want to create Read Replicas. Can you configure the RDS Read Replicas to be encrypted?",
      "options": [
        "A. No",
        "B. Yes"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nYou can not create encrypted Read Replicas from an unencrypted RDS DB instance.\n\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: RDS, Aurora, & ElastiCache Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "An application running in production is using an Aurora Cluster as its database. Your development team would like to run a version of the application in a scaled-down application with the ability to perform some heavy workload on a need-basis. Most of the time, the application will be unused. Your CIO has tasked you with helping the team to achieve this while minimizing costs. What do you suggest?",
      "options": [
        "A. Use an Aurora Global Database",
        "B. Use an RDS database",
        "C. Use Aurora Serverless",
        "D. Run Aurora on EC2, and write a script to shut down the EC2 instance at night"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: RDS, Aurora, & ElastiCache Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "How many Aurora Read Replicas can you have in a single Aurora DB Cluster?",
      "options": [
        "A. 5",
        "B. 10",
        "C. 15"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: RDS, Aurora, & ElastiCache Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "Amazon Aurora supports both .......................... databases.",
      "options": [
        "A. MySQL and MariaDB",
        "B. MySQL and PostgreSQL",
        "C. Oracle and MariaDB",
        "D. Oracle and MS SQL Server"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: RDS, Aurora, & ElastiCache Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "You work as a Solutions Architect for a gaming company. One of the games mandates that players are ranked in real-time based on their score. Your boss asked you to design then implement an effective and highly available solution to create a gaming leaderboard. What should you use?",
      "options": [
        "A. Use RDS for MySQL",
        "B. Use an Amazon Aurora",
        "C. Use ElastiCache for Memcached",
        "D. Use ElastiCache for Redis - Sorted Sets"
      ],
      "correct_answer": "D",
      "explanation": "**Correct Answer: D**\n\n**Why D is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: RDS, Aurora, & ElastiCache Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "You need full customization of an Oracle Database on AWS. You would like to benefit from using the AWS services. What do you recommend?",
      "options": [
        "A. RDS for Oracle",
        "B. RDS Custom for Oracle",
        "C. Deploy Oracle on EC2"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: RDS, Aurora, & ElastiCache Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "You need to store long-term backups for your Aurora database for disaster recovery and audit purposes. What do you recommend?",
      "options": [
        "A. Enable Automated Backups",
        "B. Perform On Demand Backups",
        "C. Use Aurora Database Cloning"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: RDS, Aurora, & ElastiCache Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "Your development team would like to perform a suite of read and write tests against your production Aurora database because they need access to production data as soon as possible. What do you advise?",
      "options": [
        "A. Create an Aurora Read Replica for them",
        "B. Do the test against the production database",
        "C. Make a DB Snapshot and Restore it into a new database",
        "D. Use the Aurora Cloning feature"
      ],
      "correct_answer": "D",
      "explanation": "**Correct Answer: D**\n\n**Why D is correct:**\nThis won't work for writes | This will impact your customers | This is slower than Aurora Cloning\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: RDS, Aurora, & ElastiCache Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "You have 100 EC2 instances connected to your RDS database and you see that upon a maintenance of the database, all your applications take a lot of time to reconnect to RDS, due to poor application logic. How do you improve this?",
      "options": [
        "A. Fix all the applications",
        "B. Disable Multi-AZ",
        "C. Enable Multi-AZ",
        "D. Use an RDS Proxy"
      ],
      "correct_answer": "D",
      "explanation": "**Correct Answer: D**\n\n**Why D is correct:**\nThis reduces the failover time by up to 66% and keeps connection actives for your applications\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: RDS, Aurora, & ElastiCache Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You're getting errors while trying to create a new S3 bucket named \"dev\". You're using a new AWS Account with no S3 buckets created before. And you double-checked and found that you have the correct IAM permissions to create S3 Buckets. What is a possible cause for this?",
      "options": [
        "A. Only AWS account root user can create S3 Buckets",
        "B. S3 bucket names must be globally unique and \"dev\" is already taken"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Amazon S3 Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You have enabled versioning in your S3 bucket which already contains a lot of files. Which version will the existing files have?",
      "options": [
        "A. 1",
        "B. 0",
        "C. -1",
        "D. null"
      ],
      "correct_answer": "D",
      "explanation": "**Correct Answer: D**\n\n**Why D is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Amazon S3 Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You have updated an S3 bucket policy to allow IAM users to read/write files in the S3 bucket, but one of the users complain that he can't perform a PutObject API call. What is a possible cause for this?",
      "options": [
        "A. The S3 bucket policy must be wrong",
        "B. The user is lacking permissions",
        "C. The IAM user must have an explicit DENY in the attached IAM Policy",
        "D. You need to contact AWS Support to lift this limit"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nExplicit DENY in an IAM Policy will take precedence over an S3 bucket policy.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Amazon S3 Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You want the content of an S3 bucket to be fully available in different AWS Regions. That will help your team perform data analysis at the lowest latency and cost possible. What S3 feature should you use?",
      "options": [
        "A. Amazon CloudFront Distributions",
        "B. S3 Versioning",
        "C. S3 Static Website Hosting",
        "D. S3 Replication"
      ],
      "correct_answer": "D",
      "explanation": "**Correct Answer: D**\n\n**Why D is correct:**\nS3 Replication allows you to replicate data from an S3 bucket to another in the same/different AWS Region.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Amazon S3 Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You have 3 S3 buckets. One source bucket A, and two destination buckets B and C in different AWS Regions. You want to replicate objects from bucket A to both bucket B and C. How would you achieve this?",
      "options": [
        "A. Configure replication from bucket A to bucket B, then from bucket A to bucket C",
        "B. Configure replication from bucket A to bucket B, then from bucket B to bucket C",
        "C. Configure replication from bucket A to bucket C, then from bucket C to bucket B"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nS3 Replication doesn't support \"chaining\" of replication. | S3 Replication doesn't support \"chaining\" of replication.\n\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Amazon S3 Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "Which of the following is NOT a Glacier Deep Archive retrieval mode?",
      "options": [
        "A. Expedited (1 - 5 minutes)",
        "B. Standard (12 hours)",
        "C. Bulk (48 hours)"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Amazon S3 Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "Which of the following is NOT a Glacier Flexible retrieval mode?",
      "options": [
        "A. Instant (10 seconds)",
        "B. Expedited (1 - 5 minutes)",
        "C. Standard (3 - 5 hours)",
        "D. Bulk (5 - 12 hours)"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Amazon S3 Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "Your client wants to make sure that file encryption is happening in S3, but he wants to fully manage the encryption keys and never store them in AWS. You recommend him to use ............................",
      "options": [
        "A. SSE-S3",
        "B. SSE-KMS",
        "C. SSE-C",
        "D. Client-Side Encryption"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nWith SSE-C, the encryption happens in AWS and you have full control over the encryption keys.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Amazon S3 Security Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "A company you're working for wants their data stored in S3 to be encrypted. They don't mind the encryption keys stored and managed by AWS, but they want to maintain control over the rotation policy of the encryption keys. You recommend them to use ....................",
      "options": [
        "A. SSE-S3",
        "B. SSE-KMS",
        "C. SSE-C",
        "D. Client-Side Encryption"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nWith SSE-KMS, the encryption happens in AWS, and the encryption keys are managed by AWS but you have full control over the rotation policy of the encryption key. Encryption keys stored in AWS.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Amazon S3 Security Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "Your company does not trust AWS for the encryption process and wants it to happen on the application. You recommend them to use ....................",
      "options": [
        "A. SSE-S3",
        "B. SSE-KMS",
        "C. SSE-C",
        "D. Client-Side Encryption"
      ],
      "correct_answer": "D",
      "explanation": "**Correct Answer: D**\n\n**Why D is correct:**\nWith Client-Side Encryption, you have to do the encryption yourself and you have full control over the encryption keys. You perform the encryption yourself and send the encrypted data to AWS. AWS does not know your encryption keys and cannot decrypt your data.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Amazon S3 Security Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "You have a website that loads files from an S3 bucket. When you try the URL of the files directly in your Chrome browser it works, but when a website with a different domain tries to load these files it doesn't. What's the problem?",
      "options": [
        "A. The Bucket policy is wrong",
        "B. The IAM policy is wrong",
        "C. CORS is wrong",
        "D. Encryption is wrong"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nCross-Origin Resource Sharing (CORS) defines a way for client web applications that are loaded in one domain to interact with resources in a different domain. To learn more about CORS, go here: https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Amazon S3 Security Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "An e-commerce company has its customers and orders data stored in an S3 bucket. The company’s CEO wants to generate a report to show the list of customers and the revenue for each customer. Customer data stored in files on the S3 bucket has sensitive information that we don’t want to expose in the report. How do you recommend the report can be created without exposing sensitive information?",
      "options": [
        "A. Use S3 Object Lambda to change the objects before they are retrieved by the report generator application",
        "B. Create another S3 bucket. Create a lambda function to process each file, remove the sensitive information, and then move them to the new S3 bucket",
        "C. Use S3 Object Lock to lock the sensitive information from being fetched by the report generator application"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Amazon S3 Security Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "You suspect that some of your employees try to access files in an S3 bucket that they don't have access to. How can you verify this is indeed the case without them noticing?",
      "options": [
        "A. Enable S3 Access Logs and analyze them using Athena",
        "B. Restrict their IAM policies and look at CloudTail logs",
        "C. Use a bucket policy"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nS3 Access Logs log all the requests made to S3 buckets and Amazon Athena can then be used to run serverless analytics on top of the log files.\n\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Amazon S3 Security Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "You are looking to provide temporary URLs to a growing list of federated users to allow them to perform a file upload on your S3 bucket to a specific location. What should you use?",
      "options": [
        "A. S3 CORS",
        "B. S3 Pre-Signed URL",
        "C. S3 Bucket Policies"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nS3 Pre-Signed URLs are temporary URLs that you generate to grant time-limited access to some actions in your S3 bucket.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Amazon S3 Security Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "For compliance reasons, your company has a policy mandate that database backups must be retained for 4 years. It shouldn't be possible to erase them. What do you recommend?",
      "options": [
        "A. Glacier Vaults with Vault Lock Policies",
        "B. EFS network drives with restrictive Linux permissions",
        "C. S3 with Bucket Policies"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Amazon S3 Security Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "You would like all your files in an S3 bucket to be encrypted by default. What is the optimal way of achieving this?",
      "options": [
        "A. Use a bucket policy that forces HTTPS connections",
        "B. Do nothing, Amazon S3 automatically encrypt new objects using Server-Side Encryption with S3-Managed Keys (SSE-S3)",
        "C. Enable Versioning"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis will force in-flight encryption but not at-rest encryption.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Amazon S3 Security Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "You have enabled versioning and want to be extra careful when it comes to deleting files on an S3 bucket. What should you enable to prevent accidental permanent deletions?",
      "options": [
        "A. Use a bucket policy",
        "B. Enable MFA Delete",
        "C. Encrypt the files",
        "D. Disable versioning"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nMFA Delete forces users to use MFA codes before deleting S3 objects. It's an extra level of security to prevent accidental deletions.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Amazon S3 Security Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "A company has its data and files stored on some S3 buckets. Some of these files need to be kept for a predefined period of time and protected from being overwritten and deletion according to company compliance policy. Which S3 feature helps you in doing this?",
      "options": [
        "A. S3 Object Lock - Retention Governance Mode",
        "B. S3 Versioning",
        "C. S3 Object Lock - Retention Compliance Mode",
        "D. S3 Glacier Vault Lock"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Amazon S3 Security Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "Which of the following S3 Object Lock configuration allows you to prevent an object or its versions from being overwritten or deleted indefinitely and gives you the ability to remove it manually?",
      "options": [
        "A. Retention Governance Mode",
        "B. Retention Compliance Mode",
        "C. Legal Hold"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Amazon S3 Security Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "As part of your Disaster Recovery strategy, you would like to make sure your entire infrastructure is code (IaC) so that you can easily re-deploy it in any AWS region. Which AWS service do you recommend?",
      "options": [
        "A. AWS CodePipeline",
        "B. AWS Elastic Beanstalk",
        "C. AWS CodeDeploy",
        "D. AWS CloudFormation"
      ],
      "correct_answer": "D",
      "explanation": "**Correct Answer: D**\n\n**Why D is correct:**\nAWS CloudFormation is the de-facto service in AWS for infrastructure as code (IaC). It enables you to create and provision AWS infrastructure deployments predictably and repeatedly.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Other Services Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "Which AWS service allows you to send marketing SMS and push notifications to a large number of customers with personalized messages?",
      "options": [
        "A. Amazon SNS",
        "B. Amazon PinPoint",
        "C. Amazon SES",
        "D. AWS Lambda"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Other Services Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "What is the most secure way to connect to an EC2 instance without exposing the SSH port 22?",
      "options": [
        "A. SSM Session Manager",
        "B. Site-to-Site VPN",
        "C. AWS Client VPN",
        "D. Bastion Host"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Other Services Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "Which AWS service allows you to run and schedule hundreds of thousands of computing jobs on AWS such as big data and complex analytics jobs?",
      "options": [
        "A. AWS Simple Batch Service",
        "B. Amazon EC2",
        "C. AWS Batch",
        "D. AWS Lambda"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Other Services Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "The company you are working on is using Salesforce and Slack internally. For archival and some analytics requirements, you have been tasked to transfer the data in both Salesforce and Slack to AWS in an S3 bucket. Which AWS service is best suited for this scenario?",
      "options": [
        "A. Amazon AppFlow",
        "B. AWS DataSync",
        "C. AWS Data Migration Service",
        "D. AWS Application Migration Service"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Other Services Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You have an e-commerce website and you are preparing for Black Friday which is the biggest sale of the year. You expect that your traffic will increase by 100x. Your website already using an SQS Standard Queue, and you're running a fleet of EC2 instances in an Auto Scaling Group to consume SQS messages. What should you do to prepare your SQS Queue?",
      "options": [
        "A. Contact AWS Support to pre-warm your SQS Standard Queue",
        "B. Enable Auto Scaling in your SQS queue",
        "C. Increase the capacity of the SQS queue",
        "D. Do nothing, SQS scales automatically"
      ],
      "correct_answer": "D",
      "explanation": "**Correct Answer: D**\n\n**Why D is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Messaging & Integration Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You have an SQS Queue where each consumer polls 10 messages at a time and finishes processing them in 1 minute. After a while, you noticed that the same SQS messages are received by different consumers resulting in your messages being processed more than once. What should you do to resolve this issue?",
      "options": [
        "A. Enable Long Polling",
        "B. Add DelaySeconds parameter to the messages when being produced",
        "C. Increase the Visibility Timeout",
        "D. Decrease the Visibility Timeout"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nSQS Visibility Timeout is a period of time during which Amazon SQS prevents other consumers from receiving and processing the message again. In Visibility Timeout, a message is hidden only after it is consumed from the queue. Increasing the Visibility Timeout gives more time to the consumer to process the message and prevent duplicate reading of the message. (default: 30 sec., min.: 0 sec., max.: 12 hours)\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Messaging & Integration Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "Which SQS Queue type allows your messages to be processed exactly once and in order?",
      "options": [
        "A. SQS Standard Queue",
        "B. SQS Dead Letter Queue",
        "C. SQS Delay Queue",
        "D. SQS FIFO Queue"
      ],
      "correct_answer": "D",
      "explanation": "**Correct Answer: D**\n\n**Why D is correct:**\nSQS FIFO (First-In-First-Out) Queues have all the capabilities of the SQS Standard Queue, plus the following two features. First, The order in which messages are sent and received are strictly preserved and a message is delivered once and remains available until a consumer process and deletes it. Second, duplicated messages are not introduced into the queue.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Messaging & Integration Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You have 3 different applications that you'd like to send them the same message. All 3 applications are using SQS. What is the best approach would you choose?",
      "options": [
        "A. Use SQS Replication Feature",
        "B. Use SNS + SQS Fan Out Pattern",
        "C. Send messages individually to 3 SQS queues"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis is a common pattern where only one message is sent to the SNS topic and then \"fan-out\" to multiple SQS queues. This approach has the following features: it's fully decoupled, no data loss, and you have the ability to add more SQS queues (more applications) over time.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Messaging & Integration Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You have a Kinesis data stream with 6 shards provisioned. This data stream usually receiving 5 MB/s of data and sending out 8 MB/s. Occasionally, your traffic spikes up to 2x and you get a ProvisionedThroughputExceeded exception. What should you do to resolve the issue?",
      "options": [
        "A. Add more Shards",
        "B. Enable Kinesis Replication",
        "C. Use SQS as a buffer to Kinesis"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nThe capacity limits of a Kinesis data stream are defined by the number of shards within the data stream. The limits can be exceeded by either data throughput or the number of reading data calls. Each shard allows for 1 MB/s incoming data and 2 MB/s outgoing data. You should increase the number of shards within your data stream to provide enough capacity.\n\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Messaging & Integration Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You have a website where you want to analyze clickstream data such as the sequence of clicks a user makes, the amount of time a user spends, and where the navigation begins and how it ends. You decided to use Amazon Kinesis, so you have configured the website to send these clickstream data all the way to a Kinesis data stream. While you checking the data sent to your Kinesis data stream, you found that the users' data is not ordered and the data for one individual user is spread across many shards. How would you fix this problem?",
      "options": [
        "A. There are too many shards, you should only use 1 shard",
        "B. You shouldn't use multiple consumers, only one and it should re-order data",
        "C. For each record sent to Kinesis add a partition key that represents the identity of the user"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nKinesis Data Stream uses the partition key associated with each data record to determine which shard a given data record belongs to. When you use the identity of each user as the partition key, this ensures the data for each user is ordered hence sent to the same shard.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Messaging & Integration Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You are running an application that produces a large amount of real-time data that you want to load into S3 and Redshift. Also, these data need to be transformed before being delivered to their destination. What is the best architecture would you choose?",
      "options": [
        "A. SQS + AWS Lambda",
        "B. SNS + HTTP Endpoint",
        "C. Kinesis Data Streams + Kinesis Data Firehose"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nThis is a perfect combo of technology for loading data near real-time data into S3 and Redshift. Kinesis Data Firehose supports custom data transformations using AWS Lambda.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Messaging & Integration Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "Which of the following is NOT a supported subscriber for AWS SNS?",
      "options": [
        "A. Amazon Kinesis Data Streams",
        "B. Amazon SQS",
        "C. HTTP(S) Endpoint",
        "D. AWS Lambda"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nNote: Kinesis Data Firehose is now supported, but not Kinesis Data Streams.\n\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Messaging & Integration Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "Which AWS service helps you when you want to send email notifications to your users?",
      "options": [
        "A. Amazon SQS with AWS Lambda",
        "B. Amazon SNS",
        "C. Amazon Kinesis"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nYou would be re-implementing a feature from SNS. | Amazon Kinesis on its own can not send emails. You would need to chain it with a consumer, and in the end, you would re-create something like SNS.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Messaging & Integration Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You're running many micro-services applications on-premises and they communicate using a message broker that supports MQTT protocol. You're planning to migrate these applications to AWS without re-engineering the applications and modifying the code. Which AWS service allows you to get a managed message broker that supports the MQTT protocol?",
      "options": [
        "A. Amazon SQS",
        "B. Amazon SNS",
        "C. Amazon Kinesis",
        "D. Amazon MQ"
      ],
      "correct_answer": "D",
      "explanation": "**Correct Answer: D**\n\n**Why D is correct:**\nAmazon MQ supports industry-standard APIs such as JMS and NMS, and protocols for messaging, including AMQP, STOMP, MQTT, and WebSocket.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Messaging & Integration Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "An e-commerce company is preparing for a big marketing promotion that will bring millions of transactions. Their website is hosted on EC2 instances in an Auto Scaling Group and they are using Amazon Aurora as their database. The Aurora database has a bottleneck and a lot of transactions have been failed in the last promotion they have made as they had a lot of transaction and the Aurora database wasn’t prepared to handle these too many transactions. What do you recommend to handle those transactions and prevent any failed transactions?",
      "options": [
        "A. Use SQS as a buffer to write to Aurora",
        "B. Host the website in AWS Fargate instead of EC2 instances",
        "C. Migrate Aurora to RDS for SQL Server"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Messaging & Integration Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company is using Amazon Kinesis Data Streams to ingest clickstream data and then do some analytical processes on it. There is a campaign in the next few days and the traffic is unpredictable which might grow up to 100x. What Kinesis Data Stream capacity mode do you recommend?",
      "options": [
        "A. Provisioned Mode",
        "B. On-demand Mode"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Messaging & Integration Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 1,
      "question_text": "Which EC2 Purchasing Option can provide you the biggest discount, but it is not suitable for critical jobs or databases?",
      "options": [
        "A. Convertible Reserved Instances",
        "B. Dedicated Hosts",
        "C. Spot Instances"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nSpot Instances are good for short workloads and this is the cheapest EC2 Purchasing Option. But, they are less reliable because you can lose your EC2 instance.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: EC2 Fundamentals Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 1,
      "question_text": "What should you use to control traffic in and out of EC2 instances?",
      "options": [
        "A. Network Access Control List (NACL)",
        "B. Security Groups",
        "C. IAM Policies"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nSecurity Groups operate at the EC2 instance level and can control traffic.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: EC2 Fundamentals Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 1,
      "question_text": "How long can you reserve an EC2 Reserved Instance?",
      "options": [
        "A. 1 or 3 years",
        "B. 2 or 4 years",
        "C. 6 months or 1 year",
        "D. Anytime between 1 and 3 years"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nEC2 Reserved Instances can be reserved for 1 or 3 years only.\n\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: EC2 Fundamentals Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 1,
      "question_text": "You would like to deploy a High-Performance Computing (HPC) application on EC2 instances. Which EC2 instance type should you choose?",
      "options": [
        "A. Storage Optimized",
        "B. Compute Optimized",
        "C. Memory Optimized",
        "D. General Purpose"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nCompute Optimized EC2 instances are great for compute-intensive workloads requiring high-performance processors (e.g., batch processing, media transcoding, high-performance computing, scientific modeling & machine learning, and dedicated gaming servers).\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: EC2 Fundamentals Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 1,
      "question_text": "Which EC2 Purchasing Option should you use for an application you plan to run on a server continuously for 1 year?",
      "options": [
        "A. Reserved Instances",
        "B. Spot Instances",
        "C. On-Demand Instances"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nReserved Instances are good for long workloads. You can reserve EC2 instances for 1 or 3 years.\n\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: EC2 Fundamentals Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 1,
      "question_text": "You are preparing to launch an application that will be hosted on a set of EC2 instances. This application needs some software installation and some OS packages need to be updated during the first launch. What is the best way to achieve this when you launch the EC2 instances?",
      "options": [
        "A. Connect to each EC2 instance using SSH, then install the required software and update your OS packages manually",
        "B. Write a bash script that installs the required software and updates to your OS, then contact AWS Support and provide them with the script. They will run it on your EC2 instances at launch",
        "C. Write a bash script that installs the required software and updates to your OS, then use this script in EC2 User Data when you launch your EC2 instances"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nEC2 User Data is used to bootstrap your EC2 instances using a bash script. This script can contain commands such as installing software/packages, download files from the Internet, or anything you want.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: EC2 Fundamentals Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 1,
      "question_text": "Which EC2 Instance Type should you choose for a critical application that uses an in-memory database?",
      "options": [
        "A. Compute Optimized",
        "B. Storage Optimized",
        "C. Memory Optimized",
        "D. General Purpose"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nMemory Optimized EC2 instances are great for workloads requiring large data sets in memory.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: EC2 Fundamentals Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 1,
      "question_text": "You have an e-commerce application with an OLTP database hosted on-premises. This application has popularity which results in its database has thousands of requests per second. You want to migrate the database to an EC2 instance. Which EC2 Instance Type should you choose to handle this high-frequency OLTP database?",
      "options": [
        "A. Compute Optimized",
        "B. Storage Optimized",
        "C. Memory Optimized",
        "D. General Purpose"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nStorage Optimized EC2 instances are great for workloads requiring high, sequential read/write access to large data sets on local storage.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: EC2 Fundamentals Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 1,
      "question_text": "Security Groups can be attached to only one EC2 instance.",
      "options": [
        "A. False",
        "B. True"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nSecurity Groups can be attached to multiple EC2 instances within the same AWS Region/VPC.\n\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: EC2 Fundamentals Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 1,
      "question_text": "You're planning to migrate on-premises applications to AWS. Your company has strict compliance requirements that require your applications to run on dedicated servers. You also need to use your own server-bound software license to reduce costs. Which EC2 Purchasing Option is suitable for you?",
      "options": [
        "A. Convertible Reserved Instances",
        "B. Dedicated Hosts",
        "C. Spot Instances"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nDedicated Hosts are good for companies with strong compliance needs or for software that have complicated licensing models. This is the most expensive EC2 Purchasing Option available.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: EC2 Fundamentals Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 1,
      "question_text": "You would like to deploy a database technology on an EC2 instance and the vendor license bills you based on the physical cores and underlying network socket visibility. Which EC2 Purchasing Option allows you to get visibility into them?",
      "options": [
        "A. Spot Instances",
        "B. On-Demand",
        "C. Dedicated Hosts",
        "D. Reserved Instances"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: EC2 Fundamentals Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 1,
      "question_text": "Spot Fleet is a set of Spot Instances and optionally ...............",
      "options": [
        "A. Reserved Instances",
        "B. On-Demand Instances",
        "C. Dedicated Hosts",
        "D. Dedicated Instances"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nSpot Fleet is a set of Spot Instances and optionally On-demand Instances. It allows you to automatically request Spot Instances with the lowest price.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: EC2 Fundamentals Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "You have an RDS DB instance that's configured to push its database logs to CloudWatch. You want to create a CloudWatch alarm if there's an Error found in the logs. How would you do that?",
      "options": [
        "A. Create a scheduled CloudWatch Event that triggers an AWS Lambda every 1 hour, scans the logs, and notify you through SNS topic",
        "B. Create a CloudWatch Logs Metric Filter that filter the logs for the keyword Error, then create a CloudWatch Alarm based on that Metric Filter",
        "C. Create an AWS Config Rule that monitors Error in your database logs and notify you through SNS topic"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Monitoring & Auditing Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "You have an application hosted on a fleet of EC2 instances managed by an Auto Scaling Group that you configured its minimum capacity to 2. Also, you have created a CloudWatch Alarm that is configured to scale in your ASG when CPU Utilization is below 60%. Currently, your application runs on 2 EC2 instances and has low traffic and the CloudWatch Alarm is in the ALARM state. What will happen?",
      "options": [
        "A. One EC2 instance will be terminated and the ASG desired and minimum capacity will go to 1",
        "B. The CloudWatch Alarm will remain in ALARM state but never decrease the number of EC2 instances in the ASG",
        "C. The CloudWatch Alarm will be detached from my ASG",
        "D. The CloudWatch Alarm will go in OK state"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThe number of EC2 instances in an ASG can not go below the minimum capacity, even if the CloudWatch alarm would in theory trigger an EC2 instance termination.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Monitoring & Auditing Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "How would you monitor your EC2 instance memory usage in CloudWatch?",
      "options": [
        "A. Enable EC2 Detailed Monitoring",
        "B. By default, the EC2 instance pushes memory usage to CloudWatch",
        "C. Use the Unified CloudWatch Agent to push memory usage as a custom metric to CloudWatch"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Monitoring & Auditing Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "You have made a configuration change and would like to evaluate the impact of it on the performance of your application. Which AWS service should you use?",
      "options": [
        "A. Amazon CloudWatch",
        "B. AWS CloudTrail"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nAmazon CloudWatch is a monitoring service that allows you to monitor your applications, respond to system-wide performance changes, optimize resource utilization, and get a unified view of operational health. It is used to monitor your applications' performance and metrics.\n\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Monitoring & Auditing Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "Someone has terminated an EC2 instance in your AWS account last week, which was hosting a critical database that contains sensitive data. Which AWS service helps you find who did that and when?",
      "options": [
        "A. CloudWatch Metrics",
        "B. CloudWatch Alarms",
        "C. CloudWatch Events",
        "D. AWS CloudTrail"
      ],
      "correct_answer": "D",
      "explanation": "**Correct Answer: D**\n\n**Why D is correct:**\nAWS CloudTrail allows you to log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. It provides the event history of your AWS account activity, audit API calls made through the AWS Management Console, AWS SDKs, AWS CLI. So, the EC2 instance termination API call will appear here. You can use CloudTrail to detect unusual activity in your AWS accounts.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Monitoring & Auditing Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "You have CloudTrail enabled for your AWS Account in all AWS Regions. What should you use to detect unusual activity in your AWS Account?",
      "options": [
        "A. CloudTrail Data Events",
        "B. CloudTrail Insights",
        "C. CloudTrail Management Events"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Monitoring & Auditing Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "One of your teammates terminated an EC2 instance 4 months ago which has critical data. You don't know who made this so you are going to review all API calls within this period using CloudTrail. You already have CloudTrail set up and configured to send logs to the S3 bucket. What should you do to find out who made this?",
      "options": [
        "A. Use CloudTrail Event History in CloudTrail Console",
        "B. Analyze CloudTrail logs in S3 bucket using Amazon Athena"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nYou can use the CloudTrail Console to view the last 90 days of recorded API activity. For events older than 90 days, use Athena to analyze CloudTrail logs stored in S3.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Monitoring & Auditing Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "You are running a website on a fleet of EC2 instances with OS that has a known vulnerability on port 84. You want to continuously monitor your EC2 instances if they have port 84 exposed. How should you do this?",
      "options": [
        "A. Setup CloudWatch Metrics",
        "B. Setup CloudTrail Trails",
        "C. Setup Config Rules",
        "D. Schedule a CloudWatch Event to trigger a Lambda function to scan your EC2 instances"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Monitoring & Auditing Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "You would like to evaluate the compliance of your resource's configurations over time. Which AWS service will you choose?",
      "options": [
        "A. AWS Config",
        "B. Amazon CloudWatch",
        "C. AWS CloudTrail"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Monitoring & Auditing Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "Someone changed the configuration of a resource and made it non-compliant. Which AWS service is responsible for logging who made modifications to resources?",
      "options": [
        "A. Amazon CloudWatch",
        "B. AWS CloudTrail",
        "C. AWS Config"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Monitoring & Auditing Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "You have enabled AWS Config to monitor Security Groups if there's unrestricted SSH access to any of your EC2 instances. Which AWS Config feature can you use to automatically re-configure your Security Groups to their correct state?",
      "options": [
        "A. AWS Config Remediations",
        "B. AWS Config Rules",
        "C. AWS Config Notifications"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Monitoring & Auditing Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "You are running a critical website on a set of EC2 instances with a tightened Security Group that has restricted SSH access. You have enabled AWS Config in your AWS Region and you want to be notified via email when someone modified your EC2 instances' Security Group. Which AWS Config feature helps you do this?",
      "options": [
        "A. AWS Config Remediations",
        "B. AWS Config Rules",
        "C. AWS Config Notifications"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Monitoring & Auditing Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "…………………………. is a CloudWatch feature that allows you to send CloudWatch metrics in near real-time to S3 bucket (through Kinesis Data Firehose) and 3rd party destinations (e.g., Splunk, Datadog, …).",
      "options": [
        "A. CloudWatch Metric Stream",
        "B. CloudWatch Log Stream",
        "C. CloudWatch Metric Filter",
        "D. CloudWatch Log Group"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Monitoring & Auditing Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "A DevOps engineer is working for a company and managing its infrastructure and resources on AWS. There was a sudden spike in traffic for the main application for the company which was not normal in this period of the year. The application is hosted on a couple of EC2 instances in private subnets and is fronted by an Application Load Balancer in a public subnet. To detect if this is normal traffic or an attack, the DevOps engineer enabled the VPC Flow Logs for the subnets and stored those logs in CloudWatch Log Group. The DevOps wants to analyze those logs and find out the top IP addresses making requests against the website to check if there is an attack. Which of the following can help the DevOps engineer to analyze those logs?",
      "options": [
        "A. CloudWatch Metric Stream",
        "B. CloudWatch Alarm",
        "C. CloudWatch Contributor Insights",
        "D. CloudWatch Metric Filter"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Monitoring & Auditing Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "A company is developing a Serverless application on AWS using Lambda, DynamoDB, and Cognito. A junior developer joined a few weeks ago and accidentally deleted one of the DynamoDB tables in the dev AWS account which contained important data. The CTO asks you to prevent this from happening again and there must be a notification system to monitor if there is an attempt to make such deletion actions for the DynamoDB tables. What would you do?",
      "options": [
        "A. Assign developers to a certain IAM group which prevents deletion of DynamoDB tables. Configure EventBridge to capture any DeleteTable API calls through S3 and send a notification using KMS",
        "B. Assign developers to a certain IAM group which prevents deletion of DynamoDB tables. Configure EventBridge to capture any DeleteTable API calls through CloudTrail and send a notification using SNS",
        "C. Assign developers to a certain IAM group which prevents deletion of DynamoDB tables. Configure EventBridge to capture any DeleteTable API calls through CloudTrail and send a notification using KMS"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Monitoring & Auditing Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "A company has a running Serverless application on AWS which uses EventBridge as an inter-communication channel between different services within the application. There is a requirement to use the events in the prod environment in the dev environment to make some tests. The tests will be done every 6 months, so the events need to be stored and used later on. What is the most efficient and cost-effective way to store EventBridge events and use them later?",
      "options": [
        "A. Use EventBridge Archive and Replay feature",
        "B. Create a Lambda function to store the EventBridge events in an S3 bucket for later usage",
        "C. Configure EventBridge to store events in a DynamoDB table"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Monitoring & Auditing Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You have created a Lambda function that typically will take around 1 hour to process some data. The code works fine when you run it locally on your machine, but when you invoke the Lambda function it fails with a \"timeout\" error after 3 seconds. What should you do?",
      "options": [
        "A. Configure your Lambda's timeout to 25 minutes",
        "B. Configure your Lambda's memory to 10 GB",
        "C. Run your code somewhere else (e.g., EC2 instance)"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nLambda's maximum execution time is 15 minutes. You can run your code somewhere else such as an EC2 instance or use Amazon ECS.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Serverless Overview Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "Before you create a DynamoDB table, you need to provision the EC2 instance the DynamoDB table will be running on.",
      "options": [
        "A. True",
        "B. False"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nDynamoDB is serverless with no servers to provision, patch, or manage and no software to install, maintain or operate. It automatically scales tables up and down to adjust for capacity and maintain performance. It provides both provisioned (specify RCU & WCU) and on-demand (pay for what you use) capacity modes.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Serverless Overview Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You have provisioned a DynamoDB table with 10 RCUs and 10 WCUs. A month later you want to increase the RCU to handle more read traffic. What should you do?",
      "options": [
        "A. Increase RCU and keep WCU the same",
        "B. You need to increase both RCU and WCU",
        "C. Increase RCU and decrease WCU"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nRCU and WCU are decoupled, so you can increase/decrease each value separately.\n\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Serverless Overview Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You have an e-commerce website where you are using DynamoDB as your database. You are about to enter the Christmas sale and you have a few items which are very popular and you expect that they will be read often. Unfortunately, last year due to the huge traffic you had the ProvisionedThroughputExceededException exception. What would you do to prevent this error from happening again?",
      "options": [
        "A. Increase the RCU to a very high value",
        "B. Create a DAX Cluster",
        "C. Migrate the database away from DynamoDB for the time of the sale"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nDynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to 10x performance improvement. It caches the most frequently used data, thus offloading the heavy reads on hot keys off your DynamoDB table, hence preventing the \"ProvisionedThroughputExceededException\" exception.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Serverless Overview Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You have developed a mobile application that uses DynamoDB as its datastore. You want to automate sending welcome emails to new users after they sign up. What is the most efficient way to achieve this?",
      "options": [
        "A. Schedule a Lambda function to run every minute using CloudWatch Events, scan the entire table looking for new users",
        "B. Enable SNS and DynamoDB integration",
        "C. Enable DynamoDB Streams and configure it to invoke a Lambda function to send emails"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nDynamoDB Streams allows you to capture a time-ordered sequence of item-level modifications in a DynamoDB table. It's integrated with AWS Lambda so that you create triggers that automatically respond to events in real-time.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Serverless Overview Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "To create a serverless API, you should integrate Amazon API Gateway with ......................",
      "options": [
        "A. EC2 Instance",
        "B. Elastic Load Balancing",
        "C. AWS Lambda"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nEC2 instances are not serverless. | An ELB does not help create an API on its own.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Serverless Overview Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "When you are using an Edge-Optimized API Gateway, your API Gateway lives in CloudFront Edge Locations across all AWS Regions.",
      "options": [
        "A. False",
        "B. True"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nAn Edge-Optimized API Gateway is best for geographically distributed clients. API requests are routed to the nearest CloudFront Edge Location which improves latency. The API Gateway still lives in one AWS Region.\n\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Serverless Overview Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You are running an application in production that is leveraging DynamoDB as its datastore and is experiencing smooth sustained usage. There is a need to make the application run in development mode as well, where it will experience the unpredictable volume of requests. What is the most cost-effective solution that you recommend?",
      "options": [
        "A. Use Provisioned Capacity Mode with Auto Scaling enabled for both development and production",
        "B. Use Provisioned Capacity Mode with Auto Scaling enabled for production and use On-Demand Capacity Mode for development",
        "C. Use Provisioned Capacity Mode with Auto Scaling enabled for development and use On-Demand Capacity Mode for production",
        "D. Use On-Demand Capacity Mode for both development and production"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis will work, but it would be extremely expensive as On-Demand Capacity Mode is much more expensive than Provisioned Capacity Mode. As we know production has a smooth sustained curve, Provisioned Capacity Mode with Auto Scaling enabled would be perfect.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Serverless Overview Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You have an application that is served globally using CloudFront Distribution. You want to authenticate users at the CloudFront Edge Locations instead of authentication requests go all the way to your origins. What should you use to satisfy this requirement?",
      "options": [
        "A. Lambda@Edge",
        "B. API Gateway",
        "C. DynamoDB",
        "D. AWS Global Accelerator"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nLambda@Edge is a feature of CloudFront that lets you run code closer to your users, which improves performance and reduces latency.\n\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Serverless Overview Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "The maximum size of an item in a DynamoDB table is ...................",
      "options": [
        "A. 1 MB",
        "B. 500 KB",
        "C. 400 KB",
        "D. 400 MB"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Serverless Overview Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "Which AWS service allows you to build Serverless workflows using AWS services (e.g., Lambda) and supports human approval?",
      "options": [
        "A. AWS Lambda",
        "B. Amazon ECS",
        "C. AWS Step Functions",
        "D. AWS Storage Gateway"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Serverless Overview Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company has a serverless application on AWS which consists of Lambda, DynamoDB, and Step Functions. In the last month, there are an increase in the number of requests against the application which results in an increase in DynamoDB costs, and requests started to be throttled. After further investigation, it shows that the majority of requests are read requests against some queries in the DynamoDB table. What do you recommend to prevent throttles and reduce costs efficiently?",
      "options": [
        "A. Use an EC2 instance with Redis installed and place it between the Lambda function and the DynamoDB table",
        "B. Migrate from DynamoDB to Aurora and use ElastiCache to cache the most requested read data",
        "C. Migrate from DynamoDB to S3 and use CloudFront to cache the most requested read data",
        "D. Use DynamoDB Accelerator (DAX) to cache the most requested read data"
      ],
      "correct_answer": "D",
      "explanation": "**Correct Answer: D**\n\n**Why D is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Serverless Overview Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You are a DevOps engineer in a football company that has a website that is backed by a DynamoDB table. The table stores viewers’ feedback for football matches. You have been tasked to work with the analytics team to generate reports on the viewers’ feedback. The analytics team wants the data in DynamoDB in json format and hosted in an S3 bucket to start working on it and create the reports. What is the best and most cost-effective way to convert DynamoDB data to json files?",
      "options": [
        "A. Select DynamoDB table then select Export to S3",
        "B. Create a Lambda function to read DynamoDB data, convert them to json files, then store the files in S3 bucket",
        "C. Use AWS Transfer Family",
        "D. Use AWS DataSync"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Serverless Overview Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A website is currently in the development process and it is going to be hosted on AWS. There is a requirement to store user sessions for users logged in to the website with an automatic expiry and deletion of expired user sessions. Which of the following AWS services are best suited for this use case?",
      "options": [
        "A. Store users’ sessions in an S3 bucket and enable S3 Lifecycle Policy",
        "B. Store users’ sessions locally in an EC2 instance",
        "C. Store users’ sessions in a DynamoDB table and enable TTL",
        "D. Store users’ sessions in an EFS file system"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Serverless Overview Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You have a mobile application and would like to give your users access to their own personal space in the S3 bucket. How do you achieve that?",
      "options": [
        "A. Generate IAM user credentials for each of your application's users",
        "B. Use Amazon Cognito Identity Federation",
        "C. Use SAML Identity Federation",
        "D. Use a Bucket Policy to make your bucket public"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAmazon Cognito can be used to federate mobile user accounts and provide them with their own IAM permissions, so they can be able to access their own personal space in the S3 bucket.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Serverless Overview Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You are developing a new web and mobile application that will be hosted on AWS and currently, you are working on developing the login and signup page. The application backend is serverless and you are using Lambda, DynamoDB, and API Gateway. Which of the following is the best and easiest approach to configure the authentication for your backend?",
      "options": [
        "A. Store users’ credentials in a DynamoDB table encrypted using KMS",
        "B. Store users’ credentials in an S3 bucket encrypted using KMS",
        "C. Use Cognito User Pools",
        "D. Store users’ credentials in AWS Secrets Manager"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Serverless Overview Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You are running a mobile application where you want each registered user to upload/download images to/from his own folder in the S3 bucket. Also, you want to give your users to sign-up and sign in using their social media accounts (e.g., Facebook). Which AWS service should you choose?",
      "options": [
        "A. AWS Identity and Access Management (IAM)",
        "B. AWS IAM Identity Center",
        "C. Amazon Cognito",
        "D. Amazon CloudFront"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nAmazon Cognito lets you add user sign-up, sign-in, and access control to your web and mobile apps quickly and easily. Amazon Cognito scales to millions of users and supports sign-in with social identity providers, such as Apple, Facebook, Google, and Amazon, and enterprise identity providers via SAML 2.0 and OpenID Connect.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Serverless Overview Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "Scaling an EC2 instance from r4.large to r4.4xlarge is called .....................",
      "options": [
        "A. Horizontal Scalability",
        "B. Vertical Scalability"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: High Availability & Scalability Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "Running an application on an Auto Scaling Group that scales the number of EC2 instances in and out is called .....................",
      "options": [
        "A. Horizontal Scalability",
        "B. Vertical Scalability"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: High Availability & Scalability Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "Elastic Load Balancers provide a .......................",
      "options": [
        "A. static IPv4 we can use in our application",
        "B. static DNS name we can use in our application",
        "C. static IPv6 we can use in our application"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nOnly Network Load Balancer provides both static DNS name and static IP. While, Application Load Balancer provides a static DNS name but it does NOT provide a static IP. The reason being that AWS wants your Elastic Load Balancer to be accessible using a static endpoint, even if the underlying infrastructure that AWS manages changes.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: High Availability & Scalability Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "You are running a website on 10 EC2 instances fronted by an Elastic Load Balancer. Your users are complaining about the fact that the website always asks them to re-authenticate when they are moving between website pages. You are puzzled because it's working just fine on your machine and in the Dev environment with 1 EC2 instance. What could be the reason?",
      "options": [
        "A. Your website must have an issue when hosted on multiple EC2 instances",
        "B. The EC2 instances log out users as they can't see their IP addresses, instead, they receive ELB IP addresses.",
        "C. The Elastic Load Balancer does not have Sticky Sessions enabled"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nELB Sticky Session feature ensures traffic for the same client is always redirected to the same target (e.g., EC2 instance). This helps that the client does not lose his session data.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: High Availability & Scalability Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "You are using an Application Load Balancer to distribute traffic to your website hosted on EC2 instances. It turns out that your website only sees traffic coming from private IPv4 addresses which are in fact your Application Load Balancer's IP addresses. What should you do to get the IP address of clients connected to your website?",
      "options": [
        "A. Modify your website's frontend so that users send their IP in every request",
        "B. Modify your website's backend to get the client IP address from the X-Forwarded-For header",
        "C. Modify your website's backend to get the client IP address from the X-Forwarded-Port header",
        "D. Modify your website's backend to get the client IP address from the X-Forwarded-Proto header"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nWhen using an Application Load Balancer to distribute traffic to your EC2 instances, the IP address you'll receive requests from will be the ALB's private IP addresses. To get the client's IP address, ALB adds an additional header called \"X-Forwarded-For\" contains the client's IP address.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: High Availability & Scalability Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "You hosted an application on a set of EC2 instances fronted by an Elastic Load Balancer. A week later, users begin complaining that sometimes the application just doesn't work. You investigate the issue and found that some EC2 instances crash from time to time. What should you do to protect users from connecting to the EC2 instances that are crashing?",
      "options": [
        "A. Enable ELB Health Checks",
        "B. Enable ELB Stickiness",
        "C. Enable SSL Termination",
        "D. Enable Cross-Zone Load Balancing"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nWhen you enable ELB Health Checks, your ELB won't send traffic to unhealthy (crashed) EC2 instances.\n\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: High Availability & Scalability Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "You are working as a Solutions Architect for a company and you are required to design an architecture for a high-performance, low-latency application that will receive millions of requests per second. Which type of Elastic Load Balancer should you choose?",
      "options": [
        "A. Application Load Balancer",
        "B. Classic Load Balancer",
        "C. Network Load Balancer"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nNetwork Load Balancer provides the highest performance and lowest latency if your application needs it.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: High Availability & Scalability Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "Application Load Balancers support the following protocols, EXCEPT:",
      "options": [
        "A. HTTP",
        "B. HTTPS",
        "C. TCP",
        "D. WebSocket"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nApplication Load Balancers support HTTP, HTTPS and WebSocket\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: High Availability & Scalability Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "Application Load Balancers can route traffic to different Target Groups based on the following, EXCEPT:",
      "options": [
        "A. Client's Location (Geography)",
        "B. Hostname",
        "C. Request URL Path",
        "D. Source IP Address"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nALBs can route traffic to different Target Groups based on URL Path, Hostname, HTTP Headers, and Query Strings.\n\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: High Availability & Scalability Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "Registered targets in a Target Groups for an Application Load Balancer can be one of the following, EXCEPT:",
      "options": [
        "A. EC2 Instances",
        "B. Network Load Balancer",
        "C. Private IP Addresses",
        "D. Lambda Functions"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: High Availability & Scalability Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "For compliance purposes, you would like to expose a fixed static IP address to your end-users so that they can write firewall rules that will be stable and approved by regulators. What type of Elastic Load Balancer would you choose?",
      "options": [
        "A. Application Load Balancer with an Elastic IP attached to it",
        "B. Network Load Balancer",
        "C. Classic Load Balancer"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nNetwork Load Balancer has one static IP address per AZ and you can attach an Elastic IP address to it. Application Load Balancers and Classic Load Balancers have a static DNS name.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: High Availability & Scalability Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "You want to create a custom application-based cookie in your Application Load Balancer. Which of the following you can use as a cookie name?",
      "options": [
        "A. AWSALBAPP",
        "B. APPUSERC",
        "C. AWSALBTG",
        "D. AWSALB"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThe following cookie names are reserved by the ELB (AWSALB, AWSALBAPP, AWSALBTG).\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: High Availability & Scalability Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "You have a Network Load Balancer that distributes traffic across a set of EC2 instances in us-east-1. You have 2 EC2 instances in us-east-1b AZ and 5 EC2 instances in us-east-1e AZ. You have noticed that the CPU utilization is higher in the EC2 instances in us-east-1b AZ. After more investigation, you noticed that the traffic is equally distributed across the two AZs. How would you solve this problem?",
      "options": [
        "A. Enable Cross-Zone Load Balancing",
        "B. Enable Sticky Sessions",
        "C. Enable ELB Health Checks",
        "D. Enable SSL Termination"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nWhen Cross-Zone Load Balancing is enabled, ELB distributes traffic evenly across all registered EC2 instances in all AZs.\n\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: High Availability & Scalability Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "Which feature in both Application Load Balancers and Network Load Balancers allows you to load multiple SSL certificates on one listener?",
      "options": [
        "A. TLS Termination",
        "B. Server Name Indication (SNI)",
        "C. SSL Security Policies",
        "D. Host Headers"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: High Availability & Scalability Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "You have an Application Load Balancer that is configured to redirect traffic to 3 Target Groups based on the following hostnames: users.example.com, api.external.example.com, and checkout.example.com. You would like to configure HTTPS for each of these hostnames. How do you configure the ALB to make this work?",
      "options": [
        "A. Use an HTTP to HTTPS redirect rule",
        "B. Use a security group SSL certificate",
        "C. Use Server Name Indication (SNI)"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nServer Name Indication (SNI) allows you to expose multiple HTTPS applications each with its own SSL certificate on the same listener. Read more here: https://aws.amazon.com/blogs/aws/new-application-load-balancer-sni/\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: High Availability & Scalability Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "You have an application hosted on a set of EC2 instances managed by an Auto Scaling Group that you configured both desired and maximum capacity to 3. Also, you have created a CloudWatch Alarm that is configured to scale out your ASG when CPU Utilization reaches 60%. Your application suddenly received huge traffic and is now running at 80% CPU Utilization. What will happen?",
      "options": [
        "A. Nothing",
        "B. The desired capacity will go up to 4 and the maximum capacity will stay at 3",
        "C. The desired capacity will go up to 4 and the maximum capacity will stay at 4"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nThe Auto Scaling Group can't go over the maximum capacity (you configured) during scale-out events.\n\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: High Availability & Scalability Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "You have an Auto Scaling Group fronted by an Application Load Balancer. You have configured the ASG to use ALB Health Checks, then one EC2 instance has just been reported unhealthy. What will happen to the EC2 instance?",
      "options": [
        "A. The ASG will keep the instance running and re-start the application",
        "B. The ASG will detach the EC2 instance and leave it running",
        "C. The ASG will terminate the EC2 instance"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nYou can configure the Auto Scaling Group to determine the EC2 instances' health based on Application Load Balancer Health Checks instead of EC2 Status Checks (default). When an EC2 instance fails the ALB Health Checks, it is marked unhealthy and will be terminated while the ASG launches a new EC2 instance.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: High Availability & Scalability Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "Your boss asked you to scale your Auto Scaling Group based on the number of requests per minute your application makes to your database. What should you do?",
      "options": [
        "A. Create a CloudWatch custom metric then create a CloudWatch Alarm on this metric to scale your ASG",
        "B. You politely tell him it's impossible",
        "C. Enable Detailed Monitoring then create a CloudWatch Alarm to scale your ASG"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nThere's no CloudWatch Metric for \"requests per minute\" for backend-to-database connections. You need to create a CloudWatch Custom Metric, then create a CloudWatch Alarm.\n\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: High Availability & Scalability Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "An application is deployed with an Application Load Balancer and an Auto Scaling Group. Currently, you manually scale the ASG and you would like to define a Scaling Policy that will ensure the average number of connections to your EC2 instances is around 1000. Which Scaling Policy should you use?",
      "options": [
        "A. Simple Scaling Policy",
        "B. Step Scaling Policy",
        "C. Target Tracking Policy",
        "D. Scheduled Scaling Policy"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: High Availability & Scalability Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "You have an ASG and a Network Load Balancer. The application on your ASG supports the HTTP protocol and is integrated with the Load Balancer health checks. You are currently using the TCP health checks. You would like to migrate to using HTTP health checks, what do you do?",
      "options": [
        "A. Migrate to an Application Load Balancer",
        "B. Migrate the health check to HTTP"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nthe NLB supports HTTP health checks as well as TCP and HTTPS\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: High Availability & Scalability Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "You have a website hosted in EC2 instances in an Auto Scaling Group fronted by an Application Load Balancer. Currently, the website is served over HTTP, and you have been tasked to configure it to use HTTPS. You have created a certificate in ACM and attached it to the Application Load Balancer. What you can do to force users to access the website using HTTPS instead of HTTP?",
      "options": [
        "A. Send an email to all customers to use HTTPS instead of HTTP",
        "B. Configure the Application Load Balancer to redirect HTTP to HTTPS",
        "C. Configure the DNS record to redirect HTTP to HTTPS"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: High Availability & Scalability Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "What is a proper definition of an IAM Role?",
      "options": [
        "A. IAM Users in multiple User Groups",
        "B. An IAM entity that defines a password policy for IAM Users",
        "C. An IAM entity that defines a set of permissions for making requests to AWS services, and will be used by an AWS service",
        "D. Permissions assigned to IAM Users to perform actions"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nSome AWS services need to perform actions on your behalf. To do so, you assign permissions to AWS services with IAM Roles.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: IAM & AWS CLI Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "Which of the following is an IAM Security Tool?",
      "options": [
        "A. IAM Credentials Report",
        "B. IAM Root Account Manager",
        "C. IAM Services Report",
        "D. IAM Security Advisor"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nIAM Credentials report lists all your AWS Account's IAM Users and the status of their various credentials.\n\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: IAM & AWS CLI Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "Which answer is INCORRECT regarding IAM Users?",
      "options": [
        "A. IAM Users can belong to multiple User Groups",
        "B. IAM Users don't have to belong to a User Group",
        "C. IAM Policies can be attached directly to IAM Users",
        "D. IAM Users access AWS services using root account credentials"
      ],
      "correct_answer": "D",
      "explanation": "**Correct Answer: D**\n\n**Why D is correct:**\nIAM Users access AWS services using their own credentials (username & password or Access Keys).\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: IAM & AWS CLI Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "Which of the following is an IAM best practice?",
      "options": [
        "A. Create several IAM Users for one physical person",
        "B. Don't use the root user account",
        "C. Share your AWS account credentials with your colleague, so (s)he can perform a task for you",
        "D. Do not enable MFA for easier access"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nUse the root account only to create your first IAM User and a few account/service management tasks. For everyday tasks, use an IAM User.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: IAM & AWS CLI Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "What are IAM Policies?",
      "options": [
        "A. A set of policies that defines how AWS accounts interact with each other",
        "B. JSON documents that define a set of permissions for making requests to AWS services, and can be used by IAM Users, User Groups, and IAM Roles",
        "C. A set of policies that define a password for IAM Users",
        "D. A set of policies defined by AWS that show how customers interact with AWS"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: IAM & AWS CLI Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "Which principle should you apply regarding IAM Permissions?",
      "options": [
        "A. Grant most privilege",
        "B. Grant more permissions if your employee asks you to",
        "C. Grant least privilege",
        "D. Restrict root account permissions"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nThat's right! Don't give more permissions than the user needs.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: IAM & AWS CLI Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "What should you do to increase your root account security?",
      "options": [
        "A. Remove permissions from the root account",
        "B. Only access AWS services through AWS Command Line Interface (CLI)",
        "C. Don't create IAM Users, only access your AWS account using the root account",
        "D. Enable Multi-Factor Authentication (MFA)"
      ],
      "correct_answer": "D",
      "explanation": "**Correct Answer: D**\n\n**Why D is correct:**\nWhen you enable MFA, this adds another layer of security. Even if your password is stolen, lost, or hacked your account is not compromised.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: IAM & AWS CLI Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "IAM User Groups can contain IAM Users and other User Groups.",
      "options": [
        "A. True",
        "B. False"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: IAM & AWS CLI Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "An IAM policy consists of one or more statements. A statement in an IAM Policy consists of the following, EXCEPT:",
      "options": [
        "A. Effect",
        "B. Principal",
        "C. Version",
        "D. Action",
        "E. Resource"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nA statement in an IAM Policy consists of Sid, Effect, Principal, Action, Resource, and Condition. Version is part of the IAM Policy itself, not the statement.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why E is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: IAM & AWS CLI Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "To enable In-flight Encryption (In-Transit Encryption), we need to have ........................",
      "options": [
        "A. an HTTP endpoint with an SSL certificate",
        "B. an HTTPS endpoint with an SSL certificate",
        "C. a TCP endpoint"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nIn-flight Encryption = HTTPS, and HTTPS can not be enabled without an SSL certificate.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: AWS Security & Encryption Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "Server-Side Encryption means that the data is sent encrypted to the server.",
      "options": [
        "A. True",
        "B. False"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nServer-Side Encryption means the server will encrypt the data for us. We don't need to encrypt it beforehand.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: AWS Security & Encryption Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "In Server-Side Encryption, where do the encryption and decryption happen?",
      "options": [
        "A. Both Encryption and Decryption happen on the server",
        "B. Both Encryption and Decryption happen on the client",
        "C. Encryption happens on the server and Decryption happens on the client",
        "D. Encryption happens on the client and Decryption happens on the server"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nIn Server-Side Encryption, we can't do encryption/decryption ourselves as we don't have access to the corresponding encryption key.\n\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: AWS Security & Encryption Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "In Client-Side Encryption, the server must know our encryption scheme before we can upload the data.",
      "options": [
        "A. False",
        "B. True"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nWith Client-Side Encryption, the server doesn't need to know any information about the encryption scheme being used, as the server will not perform any encryption or decryption operations.\n\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: AWS Security & Encryption Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "You need to create KMS Keys in AWS KMS before you are able to use the encryption features for EBS, S3, RDS ...",
      "options": [
        "A. True",
        "B. False"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nYou can use the AWS Managed Service keys in KMS, therefore we don't need to create our own KMS keys.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: AWS Security & Encryption Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "AWS KMS supports both symmetric and asymmetric KMS keys.",
      "options": [
        "A. True",
        "B. False"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nKMS keys can be symmetric or asymmetric. A symmetric KMS key represents a 256-bit key used for encryption and decryption. An asymmetric KMS key represents an RSA key pair used for encryption and decryption or signing and verification, but not both. Or it represents an elliptic curve (ECC) key pair used for signing and verification.\n\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: AWS Security & Encryption Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "When you enable Automatic Rotation on your KMS Key, the backing key is rotated every .................",
      "options": [
        "A. 90 days",
        "B. 1 year",
        "C. 2 years",
        "D. 3 years"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: AWS Security & Encryption Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "You have an AMI that has an encrypted EBS snapshot using KMS CMK. You want to share this AMI with another AWS account. You have shared the AMI with the desired AWS account, but the other AWS account still can't use it. How would you solve this problem?",
      "options": [
        "A. The other AWS account needs to logout and login again to refresh its credentials",
        "B. You need to share the KMS CMK used to encrypt the AMI with the other AWS account",
        "C. You can't share an AMI that has an encrypted EBS snapshot"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: AWS Security & Encryption Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "You have created a Customer-managed CMK in KMS that you use to encrypt both S3 buckets and EBS snapshots. Your company policy mandates that your encryption keys be rotated every 6 months. What should you do?",
      "options": [
        "A. Re-configure your KMS CMK and enable Automatic Key Rotation, and configure the Retention Period with 180 days",
        "B. Use AWS Managed Keys as they are automatically rotated by AWS every 3 months",
        "C. Rotate the KMS CMK manually. Create a new KMS CMK and use Key Aliases to reference the new KMS CMK. Keep the old KMS CMK so you can decrypt the old data"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nIt is better to use Automatic Key Rotation and configure Retention Period\n\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: AWS Security & Encryption Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "What should you use to control access to your KMS CMKs?",
      "options": [
        "A. KMS Key Policies",
        "B. KMS IAM Policy",
        "C. AWS GuardDuty",
        "D. KMS Access Control List (KMS ACL)"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: AWS Security & Encryption Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "You have a Lambda function used to process some data in the database. You would like to give your Lambda function access to the database password. Which of the following options is the most secure?",
      "options": [
        "A. Embed it in the code",
        "B. Have it as a plaintext environment variable",
        "C. Have it as an encrypted environment variable and decrypt it at runtime"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nThis is the most secure solution amongst these options.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: AWS Security & Encryption Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "You have a secret value that you use for encryption purposes, and you want to store and track the values of this secret over time. Which AWS service should you use?",
      "options": [
        "A. AWS KMS Versioning feature",
        "B. SSM Parameter Store",
        "C. Amazon S3"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nSSM Parameters Store can be used to store secrets and has built-in version tracking capability. Each time you edit the value of a parameter, SSM Parameter Store creates a new version of the parameter and retains the previous versions. You can view the details, including the values, of all versions in a parameter's history.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: AWS Security & Encryption Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "Your user-facing website is a high-risk target for DDoS attacks and you would like to get 24/7 support in case they happen and AWS bill reimbursement for the incurred costs during the attack. What AWS service should you use?",
      "options": [
        "A. AWS WAF",
        "B. AWS Shield Advanced",
        "C. AWS Shield",
        "D. AWS DDoS OpsTeam"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: AWS Security & Encryption Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "You would like to externally maintain the configuration values of your main database, to be picked up at runtime by your application. What's the best place to store them to maintain control and version history?",
      "options": [
        "A. Amazon DynamoDB",
        "B. Amazon S3",
        "C. Amazon EBS",
        "D. SSM Parameter Store"
      ],
      "correct_answer": "D",
      "explanation": "**Correct Answer: D**\n\n**Why D is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: AWS Security & Encryption Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "AWS GuardDuty scans the following data sources, EXCEPT ................",
      "options": [
        "A. CloudTrail Logs",
        "B. VPC Flow Logs",
        "C. DNS Logs",
        "D. CloudWatch Logs"
      ],
      "correct_answer": "D",
      "explanation": "**Correct Answer: D**\n\n**Why D is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: AWS Security & Encryption Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "You have a website hosted on a fleet of EC2 instances fronted by an Application Load Balancer. What should you use to protect your website from common web application attacks (e.g., SQL Injection)?",
      "options": [
        "A. AWS Shield",
        "B. AWS WAF",
        "C. AWS Security Hub",
        "D. AWS GuardDuty"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: AWS Security & Encryption Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "You would like to analyze OS vulnerabilities from within EC2 instances. You need these analyses to occur weekly and provide you with concrete recommendations in case vulnerabilities are found. Which AWS service should you use?",
      "options": [
        "A. AWS Shield",
        "B. Amazon GuardDuty",
        "C. Amazon Inspector",
        "D. AWS Config"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: AWS Security & Encryption Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "What is the most suitable AWS service for storing RDS DB passwords which also provides you automatic rotation?",
      "options": [
        "A. AWS Secrets Manager",
        "B. AWS KMS",
        "C. AWS SSM Parameter Store"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: AWS Security & Encryption Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "Which AWS service allows you to centrally manage EC2 Security Groups and AWS Shield Advanced across all AWS accounts in your AWS Organization?",
      "options": [
        "A. AWS Shield",
        "B. AWS GuardDuty",
        "C. AWS Config",
        "D. AWS Firewall Manager"
      ],
      "correct_answer": "D",
      "explanation": "**Correct Answer: D**\n\n**Why D is correct:**\nAWS Firewall Manager is a security management service that allows you to centrally configure and manage firewall rules across your accounts and applications in AWS Organizations. It is integrated with AWS Organizations so you can enable AWS WAF rules, AWS Shield Advanced protection, security groups, AWS Network Firewall rules, and Amazon Route 53 Resolver DNS Firewall rules.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: AWS Security & Encryption Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "Which AWS service helps you protect your sensitive data stored in S3 buckets?",
      "options": [
        "A. Amazon GuardDuty",
        "B. Amazon Shield",
        "C. Amazon Macie",
        "D. AWS KMS"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nAmazon Macie is a fully managed data security service that uses Machine Learning to discover and protect your sensitive data stored in S3 buckets. It automatically provides an inventory of S3 buckets including a list of unencrypted buckets, publicly accessible buckets, and buckets shared with other AWS accounts. It allows you to identify and alert you to sensitive data, such as Personally Identifiable Information (PII).\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: AWS Security & Encryption Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "An online-payment company is using AWS to host its infrastructure. The frontend is created using VueJS and is hosted on an S3 bucket and the backend is developed using PHP and is hosted on EC2 instances in an Auto Scaling Group. As their customers are worldwide, they use both CloudFront and Aurora Global database to implement multi-region deployments to provide the lowest latency and provide availability, and resiliency. A new feature required which gives customers the ability to store data encrypted on the database and this data must not be disclosed even by the company admins. The data should be encrypted on the client side and stored in an encrypted format. What do you recommend to implement this?",
      "options": [
        "A. Using Aurora Client-side Encryption and KMS Multi-region Keys",
        "B. Using Lambda Client-side Encryption and KMS Multi-region Keys",
        "C. Using Aurora Client-side Encryption and CloudHSM",
        "D. Using Lambda Client-side Encryption and CloudHSM"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: AWS Security & Encryption Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "You have an S3 bucket that is encrypted with SSE-KMS. You have been tasked to replicate the objects to a target bucket in the same AWS region but with a different KMS Key. You have configured the S3 replication, the target bucket, and the target KMS key and it is still not working. What is missing to make the S3 replication work?",
      "options": [
        "A. This is not a supported feature",
        "B. You have to raise a support ticket for AWS to start this replication process for you",
        "C. You have to configure permissions for both Source KMS Key kms:Decrypt and Target KMS Key kms:Encrypt to be used by the S3 Replication Service",
        "D. The source KMS Key and the target KMS key must be the same"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: AWS Security & Encryption Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "You have generated a public certificate using LetsEncrypt and uploaded it to the ACM so you can use and attach to an Application Load Balancer that forwards traffic to EC2 instances. As this certificate is generated outside of AWS, it does not support the automatic renewal feature. How would you be notified 30 days before this certificate expires so you can manually generate a new one?",
      "options": [
        "A. Configure ACM to send notifications by linking it to 3rd party certificate provider LetsEncrypt",
        "B. Configure EventBridge for Daily Expiration Events from ACM to invoke SNS notifications to your email",
        "C. Configure EventBridge for Monthly Expiration Events from ACM to invoke SNS notifications to your email",
        "D. Configure CloudWatch Alarms for Daily Expiration Events from ACM to invoke SNS notifications to your email"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: AWS Security & Encryption Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "You have created the main Edge-Optimized API Gateway in us-west-2 AWS region. This main Edge-Optimized API Gateway forwards traffic to the second level API Gateway in ap-southeast-1. You want to secure the main API Gateway by attaching an ACM certificate to it. Which AWS region are you going to create the ACM certificate in?",
      "options": [
        "A. us-east-1",
        "B. us-west-2",
        "C. ap-southeast-1",
        "D. Both us-east-1 and us-west-2 works"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nAs the Edge-Optimized API Gateway is using a custom AWS managed CloudFront distribution behind the scene to route requests across the globe through CloudFront Edge locations, the ACM certificate must be created in us-east-1.\n\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: AWS Security & Encryption Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "You are managing an AWS Organization with multiple AWS accounts. Each account has a separate application with different resources. You want an easy way to manage Security Groups and WAF Rules across those accounts as there was a security incident the last week and you want to tighten up your resources. Which AWS service can help you to do so?",
      "options": [
        "A. AWS Guard Duty",
        "B. Amazon Shield",
        "C. Amazon Inspector",
        "D. AWS Firewall Manager"
      ],
      "correct_answer": "D",
      "explanation": "**Correct Answer: D**\n\n**Why D is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: AWS Security & Encryption Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "You have purchased mycoolcompany.com on Amazon Route 53 Registrar and would like the domain to point to your Elastic Load Balancer my-elb-1234567890.us-west-2.elb.amazonaws.com. Which Route 53 Record type must you use here?",
      "options": [
        "A. CNAME",
        "B. Alias"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nYou can't create a CNAME record that has the same name as the top node of the DNS namespace (Zone Apex), in our case \"mycoolcompany.com.\"\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Route 53 Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "You have deployed a new Elastic Beanstalk environment and would like to direct 5% of your production traffic to this new environment. This allows you to monitor for CloudWatch metrics and ensuring that there're no bugs exist with your new environment. Which Route 53 Routing Policy allows you to do so?",
      "options": [
        "A. Simple",
        "B. Weighted",
        "C. Latency",
        "D. Failover"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nWeighted Routing Policy allows you to redirect part of the traffic based on weight (e.g., percentage). It's a common use case to send part of traffic to a new version of your application.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Route 53 Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "You have updated a Route 53 Record's myapp.mydomain.com value to point to a new Elastic Load Balancer, but it looks like users are still redirected to the old ELB. What is a possible cause for this behavior?",
      "options": [
        "A. Because of the Alias record",
        "B. Because of the CNAME record",
        "C. Because of the TTL",
        "D. Because of Route 53 Health Checks"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nEach DNS record has a TTL (Time To Live) which orders clients for how long to cache these values and not overload the DNS Resolver with DNS requests. The TTL value should be set to strike a balance between how long the value should be cached vs. how many requests should go to the DNS Resolver.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Route 53 Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "You have an application that's hosted in two different AWS Regions us-west-1 and eu-west-2. You want your users to get the best possible user experience by minimizing the response time from application servers to your users. Which Route 53 Routing Policy should you choose?",
      "options": [
        "A. Multi Value",
        "B. Weighted",
        "C. Latency",
        "D. Geolocation"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nLatency Routing Policy will evaluate the latency between your users and AWS Regions, and help them get a DNS response that will minimize their latency (e.g. response time)\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Route 53 Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "You have a legal requirement that people in any country but France should NOT be able to access your website. Which Route 53 Routing Policy helps you in achieving this?",
      "options": [
        "A. Latency",
        "B. Simple",
        "C. Multi Value",
        "D. Geolocation"
      ],
      "correct_answer": "D",
      "explanation": "**Correct Answer: D**\n\n**Why D is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Route 53 Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "You have purchased a domain on GoDaddy and would like to use Route 53 as the DNS Service Provider. What should you do to make this work?",
      "options": [
        "A. Request for a domain transfer",
        "B. Create a Private Hosted Zone and update the 3rd party Registrar NS records",
        "C. Create a Public Hosted Zone and update the Route 53 NS records",
        "D. Create a Public Hosted Zone and update the 3rd party Registrar NS records"
      ],
      "correct_answer": "D",
      "explanation": "**Correct Answer: D**\n\n**Why D is correct:**\nPublic Hosted Zones are meant to be used for people requesting your website through the Internet. Finally, NS records must be updated on the 3rd party Registrar.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Route 53 Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "Which of the following are NOT valid Route 53 Health Checks?",
      "options": [
        "A. Health Check that monitor SQS Queue",
        "B. Health Check that monitors an Endpoint",
        "C. Health Check that monitors other Health Checks",
        "D. Health Check that monitor CloudWatch Alarms"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Route 53 Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "What does this CIDR 10.0.4.0/28 correspond to?",
      "options": [
        "A. 10.0.4.0 to 10.0.4.15",
        "B. 10.0.4.0 to 10.0.32.0",
        "C. 10.0.4.0 to 10.0.4.28",
        "D. 10.0.0.0 to 10.0.16.0"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\n/28 means 16 IPs (=2^(32-28) = 2^4), means only the last digit can change.\n\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: VPC Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "You have a corporate network of size 10.0.0.0/8 and a satellite office of size 192.168.0.0/16. Which CIDR is acceptable for your AWS VPC if you plan on connecting your networks later on?",
      "options": [
        "A. 172.16.0.0/12",
        "B. 172.16.0.0/16",
        "C. 10.0.16.0/16",
        "D. 192.168.4.0/18"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nCIDR not should overlap, and the max CIDR size in AWS is /16.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: VPC Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "You plan on creating a subnet and want it to have at least capacity for 28 EC2 instances. What's the minimum size you need to have for your subnet?",
      "options": [
        "A. /28",
        "B. /27",
        "C. /26",
        "D. /25"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: VPC Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "Security Groups operate at the ................. level while NACLs operate at the ................. level.",
      "options": [
        "A. EC2 instance, Subnet",
        "B. Subnet, EC2 instance"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: VPC Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "You have attached an Internet Gateway to your VPC, but your EC2 instances still don't have access to the internet. What is NOT a possible issue?",
      "options": [
        "A. Route Tables are missing entries",
        "B. The EC2 instances don't have public IPs",
        "C. The Security Group does not allow traffic in",
        "D. The NACL does not allow network traffic out"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nSecurity groups are stateful and if traffic can go out, then it can go back in.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: VPC Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "You would like to provide Internet access to your EC2 instances in private subnets with IPv4 while making sure this solution requires the least amount of administration and scales seamlessly. What should you use?",
      "options": [
        "A. NAT Instances with Source/Destination Check flag off",
        "B. Egress Only Internet Gateway",
        "C. NAT Gateway"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nThis means creating a NAT EC2 Instance and although this would work, it would require some integration and would not scale seamlessly. | This is for IPv6 only.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: VPC Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "VPC Peering has been enabled between VPC A and VPC B, and the route tables have been updated for VPC A. But, the EC2 instances cannot communicate. What is the likely issue?",
      "options": [
        "A. Check the NACL",
        "B. Check the Route Tables in VPC B",
        "C. Check the EC2 instance attached Security Groups",
        "D. Check if DNS Resolution is enabled"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nRoute tables must be updated in both VPCs that are peered.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: VPC Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "You have set up a Direct Connect connection between your corporate data center and your VPC A in your AWS account. You need to access VPC B in another AWS region from your corporate datacenter as well. What should you do?",
      "options": [
        "A. Enable VPC Peering",
        "B. Use a Customer Gateway",
        "C. Use a Direct Connect Gateway",
        "D. Set up a NAT Gateway"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nThis is the main use case of Direct Connect Gateways.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: VPC Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "When using VPC Endpoints, what are the only two AWS services that have a Gateway Endpoint available?",
      "options": [
        "A. Amazon S3 & Amazon SQS",
        "B. Amazon SQS & DynamoDB",
        "C. Amazon S3 & DynamoDB"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nThese two services have a VPC Gateway Endpoint (remember it), all the other ones have an Interface endpoint (powered by Private Link - means a private IP).\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: VPC Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "AWS reserves 5 IP addresses each time you create a new subnet in a VPC. When you create a subnet with CIDR 10.0.0.0/24, the following IP addresses are reserved, EXCEPT ....................",
      "options": [
        "A. 10.0.0.1",
        "B. 10.0.0.2",
        "C. 10.0.0.3",
        "D. 10.0.0.4"
      ],
      "correct_answer": "D",
      "explanation": "**Correct Answer: D**\n\n**Why D is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: VPC Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "You have 3 VPCs A, B, and C. You want to establish a VPC Peering connection between all the 3 VPCs. What should you do?",
      "options": [
        "A. As VPC Peering supports Transitive Peering, so you need to establish 2 VPC Peering connections (A-B, B-C)",
        "B. Establish 3 VPC Peering connections (A-B, A-C, B-C)"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: VPC Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "How can you capture information about IP traffic inside your VPCs?",
      "options": [
        "A. Enable VPC Flow Logs",
        "B. Enable VPC Traffic Mirroring",
        "C. Enable CloudWatch Traffic Logs"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nVPC Flow Logs is a VPC feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC.\n\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: VPC Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "If you want a 500 Mbps Direct Connect connection between your corporate datacenter to AWS, you would choose a .................. connection.",
      "options": [
        "A. Dedicated",
        "B. Hosted"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nHosted Direct Connect connection supports 50Mbps, 500Mbps, up to 10Gbps.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: VPC Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "When you set up an AWS Site-to-Site VPN connection between your corporate on-premises datacenter and VPCs in AWS Cloud, what are the two major components you want to configure for this connection?",
      "options": [
        "A. Customer Gateway and NAT Gateway",
        "B. Internet Gateway and Customer Gateway",
        "C. Virtual Private Gateway and Internet Gateway",
        "D. Virtual Private Gateway and Customer Gateway"
      ],
      "correct_answer": "D",
      "explanation": "**Correct Answer: D**\n\n**Why D is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: VPC Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "Your company has several on-premises sites across the USA. These sites are currently linked using private connections, but your private connections provider has been recently quite unstable, making your IT architecture partially offline. You would like to create a backup connection that will use the public Internet to link your on-premises sites, that you can failover in case of issues with your provider. What do you recommend?",
      "options": [
        "A. VPC Peering",
        "B. AWS VPN CloudHub",
        "C. Direct Connect",
        "D. AWS PrivateLink"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAWS VPN CloudHub allows you to securely communicate with multiple sites using AWS VPN. It operates on a simple hub-and-spoke model that you can use with or without a VPC.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: VPC Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "You need to set up a dedicated connection between your on-premises corporate datacenter and AWS Cloud. This connection must be private, consistent, and traffic must not travel through the Internet. Which AWS service should you use?",
      "options": [
        "A. Site-to-Site VPN",
        "B. AWS PrivateLink",
        "C. AWS Direct Connect",
        "D. Amazon EventBridge"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nThis would not be suitable, as traffic travels through the public Internet.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: VPC Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "Using a Direct Connect connection, you can access both public and private AWS resources.",
      "options": [
        "A. True",
        "B. False"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: VPC Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "You want to scale up an AWS Site-to-Site VPN connection throughput, established between your on-premises data and AWS Cloud, beyond a single IPsec tunnel's maximum limit of 1.25 Gbps. What should you do?",
      "options": [
        "A. Use 2 Virtual Private Gateways",
        "B. Use Direct Connect Gateway",
        "C. Use Transit Gateway"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: VPC Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "You have a VPC in your AWS account that runs in a dual-stack mode. You are continuously trying to launch an EC2 instance, but it fails. After further investigation, you have found that you are no longer have IPv4 addresses available. What should you do?",
      "options": [
        "A. Modify your VPC to run in IPv6 mode only",
        "B. Modify your VPC to run in IPv4 mode only",
        "C. Add an additional IPv4 CIDR to your VPC"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: VPC Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "A web application backend is hosted on EC2 instances in private subnets fronted by an Application Load Balancer in public subnets. There is a requirement to give some of the developers access to the backend EC2 instances but without exposing the backend EC2 instances to the Internet. You have created a bastion host EC2 instance in the public subnet and configured the backend EC2 instances Security Group to allow traffic from the bastion host. Which of the following is the best configuration for bastion host Security Group to make it secure?",
      "options": [
        "A. Allow traffic only on port 80 from the company’s public CIDR",
        "B. Allow traffic only on port 22 from the company’s public CIDR",
        "C. Allow traffic only on port 22 from the company’s private CIDR",
        "D. Allow traffic only on port 80 from the company’s private CIDR"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: VPC Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "A company has set up a Direct Connect connection between their corporate data center to AWS. There is a requirement to prepare a cost-effective secure backup connection in case there are issues with this Direct Connect connection. What is the most cost effective and secure solution you recommend?",
      "options": [
        "A. Setup another Direct Connect connection to the same AWS region",
        "B. Setup another Direct Connect connection to a different AWS region",
        "C. Setup a Site-to-Site VPN connection as a backup"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: VPC Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "Which AWS service allows you to protect and control traffic in your VPC from layer 3 to layer 7?",
      "options": [
        "A. AWS Network Firewall",
        "B. Amazon Guard Duty",
        "C. Amazon Inspector",
        "D. Amazon Shield"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: VPC Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "A web application hosted on a fleet of EC2 instances managed by an Auto Scaling Group. You are exposing this application through an Application Load Balancer. Both the EC2 instances and the ALB are deployed on a VPC with the following CIDR 192.168.0.0/18. How do you configure the EC2 instances' security group to ensure only the ALB can access them on port 80?",
      "options": [
        "A. Add an Inbound Rule with port 80 and 0.0.0.0/0 as the source",
        "B. Add an Inbound Rule with port 80 and 192.168.0.0/18 as the source",
        "C. Add an Inbound Rule with port 80 and ALB's Security Group as the source",
        "D. Load an SSL certificate on the ALB"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nThis is the most secure way of ensuring only the ALB can access the EC2 instances. Referencing by security groups in rules is an extremely powerful rule and many questions at the exam rely on it. Make sure you fully master the concepts behind it!\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: VPC Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A startup company plans to run its application on AWS. As a solutions architect, the company hired you to design and implement a fully Serverless REST API. Which technology stack do you recommend?",
      "options": [
        "A. API Gateway + AWS Lambda",
        "B. Application Load Balancer + EC2",
        "C. Elastic Container Service (ECS) + Elastic Block Store (EBS)",
        "D. Amazon CloudFront + S3"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Serverless Solutions Architecture Discussions Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "The following AWS services have an out of the box caching feature, EXCEPT .................",
      "options": [
        "A. API Gateway",
        "B. Lambda",
        "C. DynamoDB"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nLambda does not have an out-of-the-box caching feature.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Serverless Solutions Architecture Discussions Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You have a lot of static files stored in an S3 bucket that you want to distribute globally to your users. Which AWS service should you use?",
      "options": [
        "A. S3 Cross-Region Replication",
        "B. Amazon CloudFront",
        "C. Amazon Route 53",
        "D. API Gateway"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAmazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds. This is a perfect use case for Amazon CloudFront.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Serverless Solutions Architecture Discussions Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You have created a DynamoDB table in ap-northeast-1 and would like to make it available in eu-west-1, so you decided to create a DynamoDB Global Table. What needs to be enabled first before you create a DynamoDB Global Table?",
      "options": [
        "A. DynamoDB Streams",
        "B. DynamoDB DAX",
        "C. DynamoDB Versioning",
        "D. DynamoDB Backups"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nDynamoDB Streams enable DynamoDB to get a changelog and use that changelog to replicate data across replica tables in other AWS Regions.\n\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Serverless Solutions Architecture Discussions Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You have configured a Lambda function to run each time an item is added to a DynamoDB table using DynamoDB Streams. The function is meant to insert messages into the SQS queue for further long processing jobs. Each time the Lambda function is invoked, it seems able to read from the DynamoDB Stream but it isn't able to insert the messages into the SQS queue. What do you think the problem is?",
      "options": [
        "A. Lambda can't be used to insert messages into the SQS queue, use an EC2 instance instead",
        "B. The Lambda Execution IAM Role is missing permissions",
        "C. The Lambda security group must allow outbound access to SQS",
        "D. The SQS security group must be edited to allow AWS Lambda"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAWS Lambda is a perfect fit with SQS. | Security Groups can not point to SQS queues. | Security Groups can not be attached to SQS queues.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Serverless Solutions Architecture Discussions Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You would like to create an architecture for a micro-services application whose sole purpose is to encode videos stored in an S3 bucket and store the encoded videos back into an S3 bucket. You would like to make this micro-services application reliable and has the ability to retry upon failures. Each video may take over 25 minutes to be processed. The services used in the architecture should be asynchronous and should have the capability to be stopped for a day and resume the next day from the videos that haven't been encoded yet. Which of the following AWS services would you recommend in this scenario?",
      "options": [
        "A. Amazon S3 + AWS Lambda",
        "B. Amazon SNS + Amazon EC2",
        "C. Amazon SQS + Amazon EC2",
        "D. Amazon SQS + AWS Lambda"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nAmazon SQS allows you to retain messages for days and process them later, while we can take down our EC2 instances.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Serverless Solutions Architecture Discussions Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You are running a photo-sharing website where your images are downloaded from all over the world. Every month you publish a master pack of beautiful mountain images that are over 15 GB in size. The content is currently hosted on an Elastic File System (EFS) file system and distributed by an Application Load Balancer and a set of EC2 instances. Each month, you are experiencing very high traffic which increases the load on your EC2 instances and increases network costs. What do you recommend to reduce EC2 load and network costs without refactoring your website?",
      "options": [
        "A. Hosts the master pack into S3",
        "B. Enable Application Load Balancer Caching",
        "C. Scale up the EC2 instances",
        "D. Create a CloudFront Distribution"
      ],
      "correct_answer": "D",
      "explanation": "**Correct Answer: D**\n\n**Why D is correct:**\nAmazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds. Amazon CloudFront can be used in front of an Application Load Balancer.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Serverless Solutions Architecture Discussions Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "An AWS service allows you to capture gigabytes of data per second in real-time and deliver these data to multiple consuming applications, with a replay feature.",
      "options": [
        "A. Kinesis Data Streams",
        "B. Amazon S3",
        "C. Amazon MQ"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nAmazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. It can continuously capture gigabytes of data per second from hundreds of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events.\n\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Serverless Solutions Architecture Discussions Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "4",
      "difficulty": 2,
      "question_text": "Which AWS Service analyzes your AWS account and gives recommendations for cost optimization, performance, security, fault tolerance, and service limits?",
      "options": [
        "A. AWS Trusted Advisor",
        "B. AWS CloudTrail",
        "C. AWS Identity and Access Management (AWS IAM)",
        "D. AWS CloudFormation"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nAWS Trusted Advisor provides recommendations that help you follow AWS best practices. It evaluates your account by using checks. These checks identify ways to optimize your AWS infrastructure, improve security and performance, reduce costs, and monitor service quotas.\n\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: WhitePapers & Architectures Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "4",
      "difficulty": 2,
      "question_text": "You are working on a Serverless application where you want to process objects uploaded to an S3 bucket. You have configured S3 Events on your S3 bucket to invoke a Lambda function every time an object has been uploaded. You want to ensure that events that can't be processed are sent to a Dead Letter Queue (DLQ) for further processing. Which AWS service should you use to set up the DLQ?",
      "options": [
        "A. S3 Events",
        "B. SNS Topic",
        "C. Lambda Function"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nThe Lambda function's invocation is \"asynchronous\", so the DLQ has to be set on the Lambda function side.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: More Solution Architectures Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "4",
      "difficulty": 2,
      "question_text": "As a Solutions Architect, you have created an architecture for a company that includes the following AWS services: CloudFront, Web Application Firewall (AWS WAF), AWS Shield, Application Load Balancer, and EC2 instances managed by an Auto Scaling Group. Sometimes the company receives malicious requests and wants to block these IP addresses. According to your architecture, Where should you do it?",
      "options": [
        "A. CloudFront",
        "B. AWS WAF",
        "C. AWS Shield",
        "D. ALB Security Group",
        "E. EC2 Security Group",
        "F. NACL"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why E is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: More Solution Architectures Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "4",
      "difficulty": 2,
      "question_text": "You have a set of Linux EC2 instances deployed in a Cluster Placement Group in order to perform High-Performance Computing (HPC). You would like to maximize network performance between your EC2 instances. What should you use?",
      "options": [
        "A. Elastic Fabric Adapter (EFA)",
        "B. Elastic Network Interface (ENI)",
        "C. Elastic Network Adapter (ENA)",
        "D. FSx for Lustre"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: More Solution Architectures Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "4",
      "difficulty": 2,
      "question_text": "Your website TriangleSunglasses.com is hosted on a fleet of EC2 instances managed by an Auto Scaling Group and fronted by an Application Load Balancer. Your ASG has been configured to scale on-demand based on the traffic going to your website. To reduce costs, you have configured the ASG to scale based on the traffic going through the ALB. To make the solution highly available, you have updated your ASG and set the minimum capacity to 2. How can you further reduce the costs while respecting the requirements?",
      "options": [
        "A. Remove the ALB and use an Elastic IP instead",
        "B. Reserve two EC2 instances",
        "C. Reduce the minimum capacity to 1",
        "D. Reduce the minimum capacity to 0"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis is the way to save further costs as we will run 2 EC2 instances no matter what.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Classic Solutions Architecture Discussions Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "4",
      "difficulty": 2,
      "question_text": "Which of the following will NOT help us while designing a STATELESS application tier?",
      "options": [
        "A. Store session data in Amazon RDS",
        "B. Store session data in Amazon ElastiCache",
        "C. Store session data in the client HTTP cookies",
        "D. Store session data on EBS volumes"
      ],
      "correct_answer": "D",
      "explanation": "**Correct Answer: D**\n\n**Why D is correct:**\nEBS volumes are created in a specific AZ and can only be attached to one EC2 instance at a time.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Classic Solutions Architecture Discussions Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "4",
      "difficulty": 2,
      "question_text": "You want to install software updates on 100s of Linux EC2 instances that you manage. You want to store these updates on shared storage which should be dynamically loaded on the EC2 instances and shouldn't require heavy operations. What do you suggest?",
      "options": [
        "A. Store the software updates on EBS and sync them using data replication software from one master in each AZ",
        "B. Store the software updates on EFS and mount EFS as a network drive at startup",
        "C. Package the software updates as an EBS snapshot and create EBS volumes for each new software update",
        "D. Store the software updates on Amazon RDS"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nEFS is a network file system (NFS) that allows you to mount the same file system to 100s of EC2 instances. Storing software updates on an EFS allows each EC2 instance to access them.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Classic Solutions Architecture Discussions Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "4",
      "difficulty": 2,
      "question_text": "As a Solutions Architect, you're planning to migrate a complex ERP software suite to AWS Cloud. You're planning to host the software on a set of Linux EC2 instances managed by an Auto Scaling Group. The software traditionally takes over an hour to set up on a Linux machine. How do you recommend you speed up the installation process when there's a scale-out event?",
      "options": [
        "A. Use a Golden AMI",
        "B. Bootstrap using EC2 User Data",
        "C. Store the application in Amazon RDS",
        "D. Retrieve the application setup files from EFS"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nGolden AMI is an image that contains all your software installed and configured so that future EC2 instances can boot up quickly from that AMI.\n\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why C is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Classic Solutions Architecture Discussions Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "4",
      "difficulty": 2,
      "question_text": "You're developing an application and would like to deploy it to Elastic Beanstalk with minimal cost. You should run it in ..................",
      "options": [
        "A. Single Instance Mode",
        "B. High Availability Mode"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nThe question mentions that you're still in the development stage and you want to reduce costs. Single Instance Mode will create one EC2 instance and one Elastic IP.\n\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Classic Solutions Architecture Discussions Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "4",
      "difficulty": 2,
      "question_text": "You're deploying your application to an Elastic Beanstalk environment but you notice that the deployment process is painfully slow. After reviewing the logs, you found that your dependencies are resolved on each EC2 instance each time you deploy. How can you speed up the deployment process with minimal impact?",
      "options": [
        "A. Remove some dependencies in your code",
        "B. Place the dependencies in Amazon EFS",
        "C. Create a Golden AMI that contains the dependencies and use that image to launch the EC2 instances"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nGolden AMI is an image that contains all your software, dependencies, and configurations, so that future EC2 instances can boot up quickly from that AMI.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Classic Solutions Architecture Discussions Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "How can you be notified when there's an object uploaded to your S3 bucket?",
      "options": [
        "A. S3 Select",
        "B. S3 Access Logs",
        "C. S3 Event Notifications",
        "D. S3 Analytics"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Amazon S3 Advanced Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "You have an S3 bucket that has S3 Versioning enabled. This S3 bucket has a lot of objects, and you would like to remove old object versions to reduce costs. What's the best approach to automate the deletion of these old object versions?",
      "options": [
        "A. S3 Lifecycle Rules - Transition Actions",
        "B. S3 Lifecycle Rules - Expiration Actions",
        "C. S3 Access Logs"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Amazon S3 Advanced Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "How can you automate the transition of S3 objects between their different tiers?",
      "options": [
        "A. AWS Lambda",
        "B. CloudWatch Events",
        "C. S3 Lifecycle Rules"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Amazon S3 Advanced Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "While you're uploading large files to an S3 bucket using Multi-part Upload, there are a lot of unfinished parts stored in the S3 bucket due to network issues. You are not using these unfinished parts and they cost you money. What is the best approach to remove these unfinished parts?",
      "options": [
        "A. Use AWS Lambda to loop on each old/unfinished part and delete them",
        "B. Request AWS Support to help you delete old/unfinished parts",
        "C. Use an S3 Lifecycle Policy to automate old/unfinished parts deletion"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nThis would work but it includes a lot of manual work and will cost you more money.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Amazon S3 Advanced Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "You are looking to get recommendations for S3 Lifecycle Rules. How can you analyze the optimal number of days to move objects between different storage tiers?",
      "options": [
        "A. S3 Inventory",
        "B. S3 Analytics",
        "C. S3 Lifecycle Rules Advisor"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Amazon S3 Advanced Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "You are looking to build an index of your files in S3, using Amazon RDS PostgreSQL. To build this index, it is necessary to read the first 250 bytes of each object in S3, which contains some metadata about the content of the file itself. There are over 100,000 files in your S3 bucket, amounting to 50 TB of data. How can you build this index efficiently?",
      "options": [
        "A. Use the RDS Import feature to load the data from S3 to PostgreSQL, and run a SQL query to build the index",
        "B. Create an application that will traverse the S3 bucket, read all the files one by one, extract the first 250 bytes, and store that information in RDS",
        "C. Create an application that will traverse the S3 bucket, issue a Byte Range Fetch for the first 250 bytes, and store that information in RDS",
        "D. Create an application that will traverse the S3 bucket, use Athena to get the first 250 bytes, and store that information in RDS"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Amazon S3 Advanced Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "You have a large dataset stored on-premises that you want to upload to the S3 bucket. The dataset is divided into 10 GB files. You have good bandwidth but your Internet connection isn't stable. What is the best way to upload this dataset to S3 and ensure that the process is fast and avoid any problems with the Internet connection?",
      "options": [
        "A. Use Multi-part Upload Only",
        "B. Use S3 Select & Use S3 Transfer Acceleration",
        "C. Use S3 Multi-part Upload & S3 Transfer Acceleration"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Amazon S3 Advanced Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A company is preparing for compliance and regulatory review on its infrastructure on AWS. Currently, they have their files stored on S3 buckets encrypted using S3 Default Encryption, which must be encrypted using KMS as required for compliance and regulatory review. Which S3 feature allows them to encrypt all files in their S3 buckets in the most efficient and cost-effective way?",
      "options": [
        "A. S3 Access Points",
        "B. S3 Cross-Region Replication",
        "C. S3 Batch Operations",
        "D. S3 Lifecycle Rules"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: Amazon S3 Advanced Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "You have a 25 GB file that you're trying to upload to S3 but you're getting errors. What is a possible solution for this?",
      "options": [
        "A. The file size limit on S3 is 5 GB",
        "B. Update your bucket policy to allow the larger file",
        "C. Use Multi-Part upload when uploading files larger than 5GB",
        "D. Encrypt the file"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nMulti-Part Upload is recommended as soon as the file is over 100 MB.\n\n**Why A is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why B is incorrect:**\nThis option does not meet the requirements specified in the question.\n**Why D is incorrect:**\nThis option does not meet the requirements specified in the question.",
      "reference": "Source: Amazon S3 Advanced Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You have a CloudFront Distribution that serves your website hosted on a fleet of EC2 instances behind an Application Load Balancer. All your clients are from the United States, but you found that some malicious requests are coming from other countries. What should you do to only allow users from the US and block other countries?",
      "options": [
        "A. Use CloudFront Geo Restriction",
        "B. Use Origin Access Control",
        "C. Set up a security group and attach it to your CloudFront Distribution",
        "D. Use a Route 53 Latency record and attach it to CloudFront"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: CloudFront & AWS Global Accelerator Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "You have a static website hosted on an S3 bucket. You have created a CloudFront Distribution that points to your S3 bucket to better serve your requests and improve performance. After a while, you noticed that users can still access your website directly from the S3 bucket. You want to enforce users to access the website only through CloudFront. How would you achieve that?",
      "options": [
        "A. Send an email to your clients and tell them to not use the S3 endpoint",
        "B. Configure your CloudFront Distribution and create an Origin Access Control (OAC), then update your S3 Bucket Policy to only accept requests from your CloudFront Distribution.",
        "C. Use S3 Access Points to redirect clients to CloudFront"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: CloudFront & AWS Global Accelerator Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "What does this S3 bucket policy do?{ \"Version\": \"2012-10-17\", \"Id\": \"Mystery policy\", \"Statement\": [{ \"Sid\": \"What could it be?\", \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"cloudfront.amazonaws.com\" }, \"Action\": \"s3:GetObject\", \"Resource\": \"arn:aws:s3:::examplebucket/*\", \"Condition\": { \"StringEquals\": { \"AWS:SourceArn\": \"arn:aws:cloudfront::123456789012:distribution/EDFDVBD6EXAMPLE\" } } }]}",
      "options": [
        "A. Forces GetObject request to be encrypted if coming from CloudFront",
        "B. Only allows the S3 bucket content to be accessed from your CloudFront Distribution",
        "C. Only allows GetObject type of request on the S3 bucket from anybody"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: CloudFront & AWS Global Accelerator Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A WordPress website is hosted in a set of EC2 instances in an EC2 Auto Scaling Group and fronted by a CloudFront Distribution which is configured to cache the content for 3 days. You have released a new version of the website and want to release it immediately to production without waiting for 3 days for the cached content to be expired. What is the easiest and most efficient way to solve this?",
      "options": [
        "A. Open a support ticket with AWS Support to remove the CloudFront Cache",
        "B. CloudFront Cache Invalidation",
        "C. EC2 Cache Invalidation"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: CloudFront & AWS Global Accelerator Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company is deploying a media-sharing website to AWS. They are going to use CloudFront to deliver the content with low latency to their customers where they are located in both US and Europe only. After a while there a huge costs for CloudFront. Which CloudFront feature allows you to decrease costs by targeting only US and Europe?",
      "options": [
        "A. CloudFront Cache Invalidation",
        "B. CloudFront Price Classes",
        "C. CloudFront Cache Behavior",
        "D. Origin Access Control"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why A is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: CloudFront & AWS Global Accelerator Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company is migrating a web application to AWS Cloud and they are going to use a set of EC2 instances in an EC2 Auto Scaling Group. The web application is made of multiple components so they will need a host-based routing feature to route to specific web application components. This web application is used by many customers and therefore the web application must have a static IP address so it can be whitelisted by the customers’ firewalls. As the customers are distributed around the world, the web application must also provide low latency to all customers. Which AWS service can help you to assign a static IP address and provide low latency across the globe?",
      "options": [
        "A. AWS Global Accelerator + Application Load Balancer",
        "B. Amazon CloudFront",
        "C. Network Load Balancer",
        "D. Application Load Balancer"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nThis option correctly addresses the requirements stated in the question.\n\n**Why B is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why C is incorrect:**\nThis option does not meet the specific requirements of the scenario.\n**Why D is incorrect:**\nThis option does not meet the specific requirements of the scenario.",
      "reference": "Source: CloudFront & AWS Global Accelerator Quiz"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "A company is planning to deploy a new application on AWS. The application must comply with data residency requirements that mandate all data must remain within a specific country. The company also wants to minimize latency for users located in that country. Which factors should the solutions architect consider when selecting an AWS Region? (Choose TWO)",
      "options": [
        "A. The number of Availability Zones in the Region",
        "B. Compliance with local data sovereignty laws",
        "C. The age of the AWS Region",
        "D. Proximity to the end users",
        "E. The number of services available in the Region compared to us-east-1"
      ],
      "correct_answer": "B, D",
      "explanation": "**Correct Answers: B and D**\n\n**Why B is correct (Compliance):**\nData residency and sovereignty requirements are critical factors when choosing a Region. Some countries require that certain types of data never leave their borders. AWS Regions are isolated from each other, and data does not automatically replicate across Regions unless explicitly configured.\n\n**Why D is correct (Proximity to users):**\nSelecting a Region close to your end users reduces network latency and improves application performance. This is especially important for real-time applications.\n\n**Why A is incorrect:**\nWhile the number of AZs affects high availability design, all Regions have at least 3 AZs, which is sufficient for most HA requirements. This is not a primary selection criterion.\n\n**Why C is incorrect:**\nThe age of a Region has no bearing on its capabilities or suitability for workloads.\n\n**Why E is incorrect:**\nWhile service availability varies by Region, most common services are available in all Regions. This is typically a secondary consideration after compliance and latency.",
      "reference": "https://docs.aws.amazon.com/whitepapers/latest/aws-overview/global-infrastructure.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "A solutions architect is designing a highly available application that requires the lowest possible latency between application tiers. The application consists of a web tier and a database tier. Which architecture should the solutions architect recommend?",
      "options": [
        "A. Deploy the web tier across multiple Regions and the database in a single Availability Zone",
        "B. Deploy both tiers in a single Availability Zone within the same VPC",
        "C. Deploy the web tier across multiple Availability Zones and use Amazon RDS Multi-AZ for the database",
        "D. Deploy the web tier in one Availability Zone and the database tier in another Availability Zone"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nThis architecture provides both high availability and low latency:\n- Web tier across multiple AZs provides fault tolerance\n- RDS Multi-AZ provides automatic failover for the database\n- All components remain within the same Region, ensuring low latency between tiers\n- AZs within a Region are connected with high-bandwidth, low-latency networking\n\n**Why A is incorrect:**\nCross-Region deployment introduces significant latency between the web and database tiers, and having the database in a single AZ creates a single point of failure.\n\n**Why B is incorrect:**\nWhile this provides the lowest latency, deploying everything in a single AZ creates a single point of failure. If that AZ experiences an outage, the entire application becomes unavailable.\n\n**Why D is incorrect:**\nThis provides some redundancy but doesn't make the web tier highly available. If the AZ hosting the web tier fails, the application becomes unavailable.",
      "reference": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 1,
      "question_text": "A company wants to ensure their application remains available even if an entire data center fails. Which AWS concept provides this level of fault isolation?",
      "options": [
        "A. AWS Regions",
        "B. Availability Zones",
        "C. Edge Locations",
        "D. Local Zones"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAvailability Zones (AZs) are distinct locations within an AWS Region that are engineered to be isolated from failures in other AZs. Each AZ consists of one or more discrete data centers with redundant power, networking, and connectivity. Deploying across multiple AZs protects against data center failures.\n\n**Why A is incorrect:**\nRegions provide geographic isolation and are useful for disaster recovery scenarios, but they are a broader concept. The question specifically asks about data center-level fault isolation.\n\n**Why C is incorrect:**\nEdge Locations are used by CloudFront for content caching and are not designed for application hosting or fault isolation.\n\n**Why D is incorrect:**\nLocal Zones are extensions of Regions for latency-sensitive applications in specific geographic areas. They don't specifically address fault isolation at the data center level.",
      "reference": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "A company is planning to migrate 50 servers from their on-premises data center to AWS. The servers run various applications including web servers, application servers, and databases. The company wants a centralized way to track the migration progress of all servers. Which AWS service should the solutions architect recommend?",
      "options": [
        "A. AWS Database Migration Service",
        "B. AWS Migration Hub",
        "C. AWS Application Discovery Service",
        "D. AWS Server Migration Service"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAWS Migration Hub provides a single location to track the progress of application migrations across multiple AWS and partner solutions. It allows you to:\n- View the migration status of all your servers\n- Track migration progress across different migration tools\n- Get a unified view of your migration portfolio\n\n**Why A is incorrect:**\nAWS DMS is specifically for database migrations. It migrates databases to AWS but doesn't provide centralized tracking for server migrations.\n\n**Why C is incorrect:**\nAWS Application Discovery Service helps you plan migrations by collecting information about your on-premises servers. It's a discovery tool, not a migration tracking tool.\n\n**Why D is incorrect:**\nAWS Server Migration Service (now replaced by AWS Application Migration Service) performs the actual migration but doesn't provide the centralized tracking dashboard that Migration Hub offers.",
      "reference": "https://docs.aws.amazon.com/migrationhub/latest/ug/whatishub.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "A company needs to migrate their on-premises Oracle database to Amazon Aurora PostgreSQL. The database contains 500 GB of data and the company wants to minimize downtime during migration. Which AWS service should be used?",
      "options": [
        "A. AWS Snowball",
        "B. AWS DataSync",
        "C. AWS Database Migration Service with Schema Conversion Tool",
        "D. Amazon S3 Transfer Acceleration"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nAWS Database Migration Service (DMS) combined with the Schema Conversion Tool (SCT) is designed for heterogeneous database migrations (Oracle to PostgreSQL). Key benefits:\n- SCT converts the database schema from Oracle to PostgreSQL\n- DMS performs continuous data replication with minimal downtime\n- Supports ongoing replication until cutover\n- Handles the complexity of migrating between different database engines\n\n**Why A is incorrect:**\nAWS Snowball is for large-scale data transfers and is not suitable for database migrations that require schema conversion and continuous replication.\n\n**Why B is incorrect:**\nAWS DataSync is for file-based data transfers between on-premises storage and AWS. It doesn't handle database migrations or schema conversion.\n\n**Why D is incorrect:**\nS3 Transfer Acceleration speeds up uploads to S3. It's not a database migration tool and cannot handle schema conversion.",
      "reference": "https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Introduction.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 3,
      "question_text": "A company wants to migrate their on-premises VMware virtual machines to AWS with minimal changes to the existing infrastructure. They need to maintain their current operations and management practices during the initial migration phase. Which migration strategy and AWS service combination should the solutions architect recommend?",
      "options": [
        "A. Replatform strategy using AWS Elastic Beanstalk",
        "B. Rehost strategy using AWS Application Migration Service",
        "C. Refactor strategy using AWS Lambda",
        "D. Repurchase strategy using Amazon WorkSpaces"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThe rehost strategy (lift-and-shift) with AWS Application Migration Service is ideal when:\n- Minimal changes to existing infrastructure are required\n- Current operations need to be maintained\n- Quick migration is needed\nAWS Application Migration Service automates the conversion and migration of servers to run natively on AWS.\n\n**Why A is incorrect:**\nReplatforming involves making optimizations during migration (like moving to managed services). Elastic Beanstalk would require application changes and doesn't directly migrate VMs.\n\n**Why C is incorrect:**\nRefactoring involves re-architecting applications, often using serverless. This requires significant changes and doesn't meet the requirement for minimal changes.\n\n**Why D is incorrect:**\nRepurchasing means replacing with a different product (SaaS). WorkSpaces is a desktop virtualization service, not a server migration solution.",
      "reference": "https://docs.aws.amazon.com/mgn/latest/ug/what-is-application-migration-service.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "A company has multiple AWS accounts and wants to allow developers in the Development account to access an S3 bucket in the Production account. The developers should assume a role in the Production account to access the bucket. Which combination of steps should the solutions architect take? (Choose TWO)",
      "options": [
        "A. Create an IAM role in the Development account with permissions to access the S3 bucket",
        "B. Create an IAM role in the Production account with a trust policy that allows the Development account to assume it",
        "C. Attach a resource-based policy to the S3 bucket that allows access from the Development account's VPC",
        "D. Grant the developers in the Development account permission to call sts:AssumeRole on the Production account role",
        "E. Create an IAM user in the Production account for each developer"
      ],
      "correct_answer": "B, D",
      "explanation": "**Correct Answers: B and D**\n\n**Why B is correct:**\nThe IAM role must be created in the Production account (where the resource exists). The role needs:\n- Permissions to access the S3 bucket\n- A trust policy specifying that the Development account can assume this role\n\n**Why D is correct:**\nDevelopers in the Development account need permission to call sts:AssumeRole for the role ARN in the Production account. Without this permission, they cannot assume the cross-account role.\n\n**Why A is incorrect:**\nCreating a role in the Development account doesn't grant access to resources in the Production account. Cross-account access requires a role in the account containing the resource.\n\n**Why C is incorrect:**\nVPC-based policies don't apply to cross-account access scenarios. IAM roles are the correct mechanism for cross-account access.\n\n**Why E is incorrect:**\nCreating individual IAM users violates security best practices. IAM roles should be used for cross-account access as they provide temporary credentials.",
      "reference": "https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "A solutions architect needs to ensure that an EC2 instance can access objects in an S3 bucket without storing long-term credentials on the instance. What is the MOST secure way to accomplish this?",
      "options": [
        "A. Store AWS access keys in environment variables on the EC2 instance",
        "B. Create an IAM user and store the credentials in the application configuration file",
        "C. Attach an IAM role with the necessary S3 permissions to the EC2 instance",
        "D. Use the root account credentials in the application"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nIAM roles for EC2 instances provide:\n- Temporary security credentials automatically rotated by AWS\n- No need to store long-term credentials on the instance\n- Credentials available through the instance metadata service\n- Automatic credential refresh before expiration\n\n**Why A is incorrect:**\nEnvironment variables still contain long-term credentials that could be exposed through various attacks or logging. They don't rotate automatically.\n\n**Why B is incorrect:**\nStoring credentials in configuration files is a security risk. If the file is exposed or the instance is compromised, the credentials are compromised.\n\n**Why D is incorrect:**\nUsing root account credentials is a severe security violation. Root credentials should never be used for applications and should be protected with MFA.",
      "reference": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "An IAM user has two policies attached. Policy A explicitly allows s3:GetObject on a specific bucket. Policy B explicitly denies s3:* on all S3 resources. What is the effective permission for this user when accessing the S3 bucket?",
      "options": [
        "A. The user can access the bucket because explicit allows take precedence",
        "B. The user cannot access the bucket because explicit denies take precedence",
        "C. The user can access only the specific bucket mentioned in Policy A",
        "D. The policies cancel each other out, resulting in implicit deny"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAWS IAM policy evaluation follows this logic:\n1. By default, all requests are implicitly denied\n2. An explicit allow overrides the implicit deny\n3. An explicit deny ALWAYS overrides any allows\n\nSince Policy B has an explicit deny for s3:*, the user cannot access any S3 resources, regardless of what Policy A allows.\n\n**Why A is incorrect:**\nExplicit allows do NOT take precedence over explicit denies. This is a common misconception. Denies always win.\n\n**Why C is incorrect:**\nThe explicit deny in Policy B applies to ALL S3 resources (s3:* on *), which includes the specific bucket in Policy A.\n\n**Why D is incorrect:**\nThe policies don't cancel out. The explicit deny in Policy B takes effect, resulting in denied access (not implicit deny, but explicit deny).",
      "reference": "https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 1,
      "question_text": "Which type of IAM entity should be used to grant AWS service permissions to make API calls on your behalf?",
      "options": [
        "A. IAM User",
        "B. IAM Group",
        "C. IAM Role",
        "D. IAM Policy"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nIAM Roles are designed for delegation and can be assumed by:\n- AWS services (like EC2, Lambda)\n- Users from other AWS accounts\n- Federated users\n\nService roles allow AWS services to perform actions on your behalf using temporary credentials.\n\n**Why A is incorrect:**\nIAM Users are meant for people or applications that need long-term credentials. Services should not use IAM users because roles provide better security through temporary credentials.\n\n**Why B is incorrect:**\nIAM Groups are collections of users for easier permission management. Groups cannot be assumed by services.\n\n**Why D is incorrect:**\nIAM Policies define permissions but are not entities that can be assumed. Policies are attached to users, groups, or roles to grant permissions.",
      "reference": "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "A company has a web application deployed in us-east-1 and eu-west-1. They want to route users to the Region that provides the lowest latency. Which Route 53 routing policy should the solutions architect configure?",
      "options": [
        "A. Simple routing policy",
        "B. Weighted routing policy",
        "C. Latency-based routing policy",
        "D. Geolocation routing policy"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nLatency-based routing directs users to the AWS Region that provides the lowest latency. Route 53:\n- Measures latency between users and AWS Regions\n- Routes traffic to the Region with the lowest latency\n- Automatically adjusts based on network conditions\n\n**Why A is incorrect:**\nSimple routing returns all values in random order. It doesn't consider latency or user location.\n\n**Why B is incorrect:**\nWeighted routing distributes traffic based on assigned weights (percentages). It's useful for load distribution but doesn't optimize for latency.\n\n**Why D is incorrect:**\nGeolocation routing routes based on the geographic location of users, not latency. A user might be geographically closer to one Region but have lower latency to another due to network topology.",
      "reference": "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-latency.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "A company needs to configure DNS for their domain to point to an Application Load Balancer. They want to use a root domain (example.com) without the www prefix. Which Route 53 record type should be used?",
      "options": [
        "A. CNAME record pointing to the ALB DNS name",
        "B. A record with Alias pointing to the ALB",
        "C. MX record pointing to the ALB DNS name",
        "D. TXT record with the ALB IP address"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nRoute 53 Alias records:\n- Can be created for the zone apex (root domain like example.com)\n- Work with AWS resources like ALBs, CloudFront, S3 websites\n- Are free for AWS resource queries\n- Automatically update when the underlying resource IP changes\n\n**Why A is incorrect:**\nCNAME records CANNOT be used for the zone apex (root domain) according to DNS RFC standards. CNAMEs can only be used for subdomains like www.example.com.\n\n**Why C is incorrect:**\nMX records are for mail servers, not for pointing domains to load balancers.\n\n**Why D is incorrect:**\nTXT records store text information for various purposes (like SPF, domain verification). They cannot route traffic to a load balancer. Also, ALBs don't have static IP addresses.",
      "reference": "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "A solutions architect is designing a VPC with public and private subnets. EC2 instances in the private subnet need to download software updates from the internet but should not be directly accessible from the internet. Which solution should the architect implement?",
      "options": [
        "A. Attach an Internet Gateway to the private subnet",
        "B. Configure a NAT Gateway in the public subnet and update the private subnet route table",
        "C. Assign public IP addresses to the instances in the private subnet",
        "D. Create a VPC peering connection to another VPC with internet access"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nNAT Gateway allows instances in private subnets to:\n- Initiate outbound connections to the internet\n- Download updates, patches, and software\n- Remain inaccessible from inbound internet traffic\n\nConfiguration requires:\n- NAT Gateway in a public subnet with an Elastic IP\n- Route in private subnet route table pointing 0.0.0.0/0 to the NAT Gateway\n\n**Why A is incorrect:**\nInternet Gateways don't attach to subnets; they attach to VPCs. Even with an IGW, private instances need NAT for outbound-only internet access.\n\n**Why C is incorrect:**\nAssigning public IPs would make instances directly accessible from the internet, violating the security requirement.\n\n**Why D is incorrect:**\nVPC peering connects two VPCs but doesn't provide internet access. You cannot route through another VPC's IGW.",
      "reference": "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "A company has a VPC with CIDR block 10.0.0.0/16. They need to create subnets in two Availability Zones with at least 200 available IP addresses per subnet. Which subnet CIDR blocks would meet this requirement?",
      "options": [
        "A. 10.0.1.0/24 and 10.0.2.0/24",
        "B. 10.0.1.0/25 and 10.0.2.0/25",
        "C. 10.0.1.0/26 and 10.0.2.0/26",
        "D. 10.0.1.0/28 and 10.0.2.0/28"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nA /24 subnet provides 256 IP addresses. AWS reserves 5 IPs per subnet, leaving 251 available IPs, which exceeds the 200 requirement.\n\nSubnet size calculations:\n- /24 = 256 IPs - 5 reserved = 251 available\n- /25 = 128 IPs - 5 reserved = 123 available\n- /26 = 64 IPs - 5 reserved = 59 available\n- /28 = 16 IPs - 5 reserved = 11 available\n\n**Why B is incorrect:**\n/25 provides only 123 available IPs after AWS reservations, which is less than 200.\n\n**Why C is incorrect:**\n/26 provides only 59 available IPs, far below the requirement.\n\n**Why D is incorrect:**\n/28 provides only 11 available IPs, far below the requirement.\n\n**AWS Reserved IPs (5 per subnet):**\n- Network address\n- VPC router\n- DNS server\n- Reserved for future use\n- Broadcast address",
      "reference": "https://docs.aws.amazon.com/vpc/latest/userguide/configure-subnets.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 3,
      "question_text": "A company has two VPCs: VPC-A (10.0.0.0/16) in us-east-1 and VPC-B (10.1.0.0/16) in eu-west-1. They need private connectivity between these VPCs. The traffic must not traverse the public internet. Which solution should the architect recommend?",
      "options": [
        "A. Configure VPC peering between VPC-A and VPC-B",
        "B. Create an Internet Gateway in each VPC and use public IPs",
        "C. Set up AWS Site-to-Site VPN between the VPCs",
        "D. Use VPC endpoints to connect the VPCs"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nVPC peering:\n- Creates a private connection between two VPCs\n- Traffic stays on the AWS global network (never traverses public internet)\n- Works across Regions (inter-Region VPC peering)\n- The CIDR blocks don't overlap (10.0.0.0/16 and 10.1.0.0/16), which is required\n\n**Why B is incorrect:**\nUsing public IPs and Internet Gateways would route traffic through the public internet, which violates the requirement.\n\n**Why C is incorrect:**\nSite-to-Site VPN is designed for connecting on-premises networks to AWS, not for VPC-to-VPC connectivity. While technically possible, VPC peering is simpler and more efficient.\n\n**Why D is incorrect:**\nVPC endpoints connect VPCs to AWS services (like S3, DynamoDB). They don't provide VPC-to-VPC connectivity.",
      "reference": "https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company is running a production web application on EC2 instances behind an Application Load Balancer. During a recent scaling event, it took too long for new instances to start serving traffic because they needed to download and install application dependencies. How can the solutions architect reduce the instance launch time?",
      "options": [
        "A. Use larger EC2 instance types",
        "B. Create a custom AMI with the application and dependencies pre-installed",
        "C. Increase the instance store volume size",
        "D. Enable enhanced networking on the instances"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nCustom AMIs (Amazon Machine Images):\n- Include the OS, application code, and dependencies\n- Significantly reduce instance launch time\n- Eliminate the need to download and install software at boot\n- Are a best practice for production auto-scaling environments\n\n**Why A is incorrect:**\nLarger instance types don't reduce the time needed to download and install software. They provide more resources but don't address the software installation delay.\n\n**Why C is incorrect:**\nInstance store is ephemeral storage and doesn't persist across stops/starts. It doesn't help with faster application deployment.\n\n**Why D is incorrect:**\nEnhanced networking improves network performance but doesn't reduce the time to install application dependencies.",
      "reference": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company needs to host a stateless web application that experiences unpredictable traffic patterns with occasional spikes. The application must scale automatically based on demand while minimizing costs during low-traffic periods. Which EC2 purchasing option should the solutions architect recommend for the Auto Scaling group?",
      "options": [
        "A. On-Demand Instances only",
        "B. Reserved Instances only",
        "C. Spot Instances only",
        "D. A combination of On-Demand Instances for baseline and Spot Instances for scaling"
      ],
      "correct_answer": "D",
      "explanation": "**Correct Answer: D**\n\n**Why D is correct:**\nCombining On-Demand and Spot Instances provides:\n- On-Demand for baseline capacity: reliable, always available\n- Spot Instances for additional capacity during spikes: up to 90% cost savings\n- Auto Scaling can be configured to use mixed instance policies\n- Stateless applications can tolerate Spot interruptions\n\n**Why A is incorrect:**\nOn-Demand only is more expensive than necessary for variable workloads. You pay full price even during spikes when Spot could be used.\n\n**Why B is incorrect:**\nReserved Instances require commitment and don't scale with unpredictable traffic. They're best for steady-state baseline workloads.\n\n**Why C is incorrect:**\nSpot only is risky for production workloads. Spot Instances can be interrupted with 2-minute notice, potentially affecting application availability during interruptions.",
      "reference": "https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-mixed-instances-groups.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "An EC2 instance in a public subnet cannot connect to the internet despite having a public IP address. The VPC has an Internet Gateway attached. What is the MOST likely cause?",
      "options": [
        "A. The security group is blocking outbound traffic",
        "B. The route table does not have a route to the Internet Gateway",
        "C. The instance needs an Elastic IP instead of a public IP",
        "D. The NACL is allowing all traffic"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nFor internet connectivity, the subnet's route table must have:\n- A route for 0.0.0.0/0 pointing to the Internet Gateway\nWithout this route, traffic destined for the internet has no path to follow.\n\nCommon checklist for internet access:\n1. VPC has IGW attached ✓ (stated in question)\n2. Instance has public IP ✓ (stated in question)\n3. Route table has route to IGW ← Most likely missing\n4. Security group allows traffic\n5. NACL allows traffic\n\n**Why A is incorrect:**\nSecurity groups are stateful and allow all outbound traffic by default. Even if modified, they would typically be a secondary consideration after routing.\n\n**Why C is incorrect:**\nBoth public IPs (auto-assigned) and Elastic IPs work for internet connectivity. The type of public IP doesn't matter.\n\n**Why D is incorrect:**\nNACLs allowing all traffic would not cause connectivity issues. Blocking traffic would cause issues, but the default NACL allows all traffic.",
      "reference": "https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company stores millions of log files in S3. The files are frequently accessed for the first 30 days, occasionally accessed for the next 60 days, and rarely accessed after 90 days but must be retained for 7 years for compliance. Which S3 lifecycle configuration provides the MOST cost-effective solution?",
      "options": [
        "A. Keep all data in S3 Standard for 7 years",
        "B. Transition to S3 Standard-IA after 30 days, S3 Glacier after 90 days",
        "C. Transition to S3 One Zone-IA after 30 days, delete after 90 days",
        "D. Store all data directly in S3 Glacier Deep Archive"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis lifecycle policy matches access patterns with appropriate storage classes:\n- Days 0-30: S3 Standard (frequent access)\n- Days 30-90: S3 Standard-IA (occasional access, lower cost)\n- Days 90+: S3 Glacier (archival, lowest cost for long-term retention)\n\nCost optimization through tiering while meeting the 7-year retention requirement.\n\n**Why A is incorrect:**\nKeeping all data in S3 Standard is the most expensive option. Data accessed rarely should be in cheaper storage classes.\n\n**Why C is incorrect:**\n- One Zone-IA has lower availability (single AZ)\n- Deleting after 90 days violates the 7-year retention requirement\n\n**Why D is incorrect:**\nGlacier Deep Archive has retrieval times of 12-48 hours. This is unsuitable for files that are frequently accessed in the first 30 days.",
      "reference": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "A company wants to allow a partner organization to upload files to their S3 bucket without giving them AWS credentials. The files should be uploaded over HTTPS and the upload capability should be time-limited. Which solution should the architect recommend?",
      "options": [
        "A. Create an IAM user for the partner and share the access keys",
        "B. Enable public access on the S3 bucket",
        "C. Generate presigned URLs for the partner to use for uploads",
        "D. Configure S3 bucket ACLs to allow partner account access"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nS3 presigned URLs:\n- Allow temporary access without AWS credentials\n- Can be configured for PUT operations (uploads)\n- Have configurable expiration times\n- Are transmitted over HTTPS by default\n- Don't require the partner to have an AWS account\n\n**Why A is incorrect:**\nSharing IAM credentials:\n- Is a security anti-pattern\n- Provides long-term access\n- Requires credential management and rotation\n- Credentials could be leaked or misused\n\n**Why B is incorrect:**\nPublic access would allow anyone to access the bucket, not just the partner. This is a significant security risk.\n\n**Why D is incorrect:**\nACLs require the partner to have an AWS account and would provide ongoing access rather than time-limited access.",
      "reference": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/PresignedUrlUploadObject.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "A developer needs to programmatically manage IAM users from their local workstation. They have installed the AWS CLI. What must be configured before the developer can successfully create IAM users using the CLI?",
      "options": [
        "A. AWS Region and VPC ID",
        "B. Access key ID and secret access key with appropriate IAM permissions",
        "C. EC2 instance ID and security group",
        "D. S3 bucket name and object key"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nTo use AWS CLI, you need:\n1. Access Key ID - identifies the IAM user/role\n2. Secret Access Key - authenticates API requests\n3. IAM permissions - the credentials must have iam:CreateUser permission\n\nConfigure using: `aws configure` command\n\n**Why A is incorrect:**\nRegion is needed but VPC ID is not required for IAM operations. IAM is a global service.\n\n**Why C is incorrect:**\nEC2 instance ID and security groups are for EC2 management, not CLI configuration.\n\n**Why D is incorrect:**\nS3 bucket and object information are for S3 operations, not CLI authentication configuration.",
      "reference": "https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "4",
      "difficulty": 2,
      "question_text": "A solutions architect needs to automate the upload of thousands of files to S3 as part of a data pipeline. The script will run on an EC2 instance. Which approach is MOST secure for authenticating the AWS CLI commands?",
      "options": [
        "A. Store access keys in a configuration file on the EC2 instance",
        "B. Set access keys as environment variables on the EC2 instance",
        "C. Attach an IAM role to the EC2 instance with S3 permissions",
        "D. Hardcode the access keys in the automation script"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nIAM roles for EC2:\n- Provide temporary credentials automatically\n- Credentials are automatically rotated\n- No credential storage required\n- AWS CLI automatically uses instance metadata credentials\n- Most secure approach for EC2-based automation\n\n**Why A is incorrect:**\nConfiguration files with credentials:\n- Contain long-term credentials\n- Can be accidentally committed to source control\n- Require manual rotation\n\n**Why B is incorrect:**\nEnvironment variables:\n- Still contain long-term credentials\n- Visible to other processes on the instance\n- Can be leaked through logs or crash dumps\n\n**Why D is incorrect:**\nHardcoding credentials is the worst practice:\n- Credentials in source code are easily leaked\n- Cannot be rotated without code changes\n- Major security vulnerability",
      "reference": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "4",
      "difficulty": 1,
      "question_text": "A company is running production workloads on AWS and requires 24/7 access to AWS support engineers via phone for critical system outages. Which is the MINIMUM AWS Support plan that meets this requirement?",
      "options": [
        "A. Basic Support",
        "B. Developer Support",
        "C. Business Support",
        "D. Enterprise Support"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nBusiness Support includes:\n- 24/7 phone, chat, and email access to Cloud Support Engineers\n- < 1 hour response time for production system down\n- AWS Trusted Advisor full checks\n- Third-party software support\n\n**Why A is incorrect:**\nBasic Support:\n- Only customer service access\n- No technical support from engineers\n- Documentation and forums only\n\n**Why B is incorrect:**\nDeveloper Support:\n- Business hours email access only\n- No phone support\n- Designed for non-production workloads\n\n**Why D is incorrect:**\nEnterprise Support provides 24/7 access but is not the minimum required. It's more expensive and includes additional features like a Technical Account Manager (TAM).",
      "reference": "https://aws.amazon.com/premiumsupport/plans/"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "A company is running a critical application on an EC2 instance. When the instance is stopped and started, the application becomes unavailable because the public IP address changes. How can the solutions architect ensure the instance maintains a consistent public IP address?",
      "options": [
        "A. Use an Auto Scaling group with minimum capacity of 1",
        "B. Associate an Elastic IP address with the instance",
        "C. Place the instance in a private subnet",
        "D. Enable detailed monitoring on the instance"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nElastic IP addresses:\n- Are static, public IPv4 addresses\n- Remain associated with your account until released\n- Stay attached to an instance across stops/starts\n- Can be quickly remapped to another instance if needed\n\n**Why A is incorrect:**\nAuto Scaling doesn't provide a static IP address. New instances would still get random public IPs unless combined with an Elastic IP or load balancer.\n\n**Why C is incorrect:**\nPrivate subnets don't have public IPs at all. This would make the application inaccessible from the internet.\n\n**Why D is incorrect:**\nDetailed monitoring provides additional CloudWatch metrics but doesn't affect IP addressing.",
      "reference": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "A company's application uses an Amazon RDS MySQL database. The database experiences heavy read traffic during business hours. The application team wants to improve read performance without modifying the application code. Which solution should the architect recommend?",
      "options": [
        "A. Enable RDS Multi-AZ deployment",
        "B. Create an RDS Read Replica and configure the application to use it",
        "C. Increase the RDS instance size",
        "D. Create an RDS Read Replica and use Route 53 weighted routing"
      ],
      "correct_answer": "D",
      "explanation": "**Correct Answer: D**\n\n**Why D is correct:**\nRead Replicas with Route 53 weighted routing:\n- Create Read Replica for read traffic offloading\n- Route 53 weighted routing can distribute read traffic\n- Application doesn't need code changes (same DNS endpoint)\n- Improves read performance by distributing load\n\n**Why A is incorrect:**\nMulti-AZ provides high availability but not read scaling. The standby cannot be used for read queries.\n\n**Why B is incorrect:**\nCreating a Read Replica helps, but the application would need code changes to direct reads to the replica endpoint.\n\n**Why C is incorrect:**\nIncreasing instance size (vertical scaling) provides more resources but has limits and doesn't address the root cause. It's also more expensive than read replicas.",
      "reference": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "An application requires a relational database with automatic failover, minimal downtime, and the ability to scale reads across multiple Availability Zones. Which AWS database solution should the architect recommend?",
      "options": [
        "A. Amazon RDS MySQL with Multi-AZ",
        "B. Amazon Aurora MySQL",
        "C. Amazon DynamoDB",
        "D. Amazon Redshift"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAmazon Aurora provides:\n- Automatic failover with < 30 seconds failover time\n- Up to 15 Read Replicas across AZs\n- Shared storage layer for faster replication\n- 6-way replication across 3 AZs\n- Auto-scaling storage up to 128 TB\n\n**Why A is incorrect:**\nRDS MySQL Multi-AZ:\n- Only one standby (not used for reads)\n- Read Replicas require separate configuration\n- Slower replication than Aurora\n- Longer failover time than Aurora\n\n**Why C is incorrect:**\nDynamoDB is a NoSQL database, not relational. The question specifically requires a relational database.\n\n**Why D is incorrect:**\nRedshift is a data warehouse for analytics workloads, not an OLTP transactional database.",
      "reference": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/CHAP_AuroraOverview.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company's web application frequently queries the same product data from an RDS database. The database team has identified that 80% of queries are repeated reads. How can the architect improve application performance and reduce database load?",
      "options": [
        "A. Enable RDS Performance Insights",
        "B. Implement Amazon ElastiCache as a caching layer",
        "C. Convert to Amazon Aurora Serverless",
        "D. Enable RDS Storage Auto Scaling"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAmazon ElastiCache provides:\n- In-memory caching for frequently accessed data\n- Sub-millisecond response times\n- Reduces database load by caching repeated queries\n- Supports Redis or Memcached\n\nFor read-heavy workloads with repeated queries, caching is ideal.\n\n**Why A is incorrect:**\nPerformance Insights provides monitoring and analysis but doesn't improve performance directly.\n\n**Why C is incorrect:**\nAurora Serverless helps with variable workloads but doesn't specifically address caching repeated read queries.\n\n**Why D is incorrect:**\nStorage Auto Scaling handles storage capacity, not read performance or repeated query patterns.",
      "reference": "https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/WhatIs.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company wants to process images uploaded to S3. The processing takes approximately 2 minutes per image and runs a few times per day. The company wants to minimize operational overhead and costs. Which compute solution should the architect recommend?",
      "options": [
        "A. Amazon EC2 On-Demand Instance running continuously",
        "B. Amazon EC2 Reserved Instance",
        "C. AWS Lambda triggered by S3 events",
        "D. Amazon ECS with Fargate"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nAWS Lambda is ideal because:\n- Pay only for execution time (not idle time)\n- No servers to manage (serverless)\n- S3 event triggers supported natively\n- 2-minute processing fits within Lambda's 15-minute timeout\n- Minimal operational overhead\n\n**Why A is incorrect:**\nRunning EC2 continuously for workloads that run a few times per day is wasteful and expensive. You'd pay 24/7 for hours of actual use.\n\n**Why B is incorrect:**\nReserved Instances are for steady-state workloads. This workload is sporadic, making Lambda more cost-effective.\n\n**Why D is incorrect:**\nECS with Fargate has more overhead than Lambda for simple event-driven processing. It's better suited for containerized applications or longer-running processes.",
      "reference": "https://docs.aws.amazon.com/lambda/latest/dg/with-s3.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company hosts a static website on S3 and wants to reduce latency for users globally while protecting the S3 bucket from direct public access. Which solution should the architect implement?",
      "options": [
        "A. Enable S3 Transfer Acceleration",
        "B. Create a CloudFront distribution with Origin Access Control",
        "C. Enable S3 Cross-Region Replication",
        "D. Use Route 53 latency-based routing to multiple S3 buckets"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nCloudFront with Origin Access Control (OAC):\n- Caches content at edge locations globally\n- Reduces latency for users worldwide\n- OAC restricts S3 access to CloudFront only\n- S3 bucket can remain private\n- Provides DDoS protection through AWS Shield\n\n**Why A is incorrect:**\nS3 Transfer Acceleration speeds up uploads to S3, not content delivery to users.\n\n**Why C is incorrect:**\nCross-Region Replication copies data to another Region but doesn't provide caching or restrict direct S3 access.\n\n**Why D is incorrect:**\nMultiple S3 buckets would require maintaining duplicate content. This doesn't restrict direct S3 access and is more complex to manage.",
      "reference": "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company is building a microservices architecture where one service produces messages that multiple consumer services need to process independently. Each message must be delivered to all consumers. Which AWS messaging solution should the architect use?",
      "options": [
        "A. Amazon SQS Standard queue with multiple consumers",
        "B. Amazon SNS topic with SQS queue subscriptions for each consumer",
        "C. Amazon SQS FIFO queue",
        "D. Amazon Kinesis Data Streams"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nSNS + SQS fan-out pattern:\n- SNS topic receives messages from producer\n- Each consumer has its own SQS queue subscribed to the topic\n- Messages are delivered to ALL subscribers (fan-out)\n- Consumers process independently at their own pace\n- Provides decoupling between producer and consumers\n\n**Why A is incorrect:**\nSQS Standard queue: each message is delivered to only ONE consumer (competing consumers pattern). Doesn't meet the requirement for all consumers to receive each message.\n\n**Why C is incorrect:**\nFIFO queues maintain order but still deliver each message to only one consumer.\n\n**Why D is incorrect:**\nKinesis is designed for streaming data at scale. While it can support multiple consumers, the SNS+SQS pattern is simpler for standard fan-out use cases.",
      "reference": "https://docs.aws.amazon.com/sns/latest/dg/sns-sqs-as-subscriber.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "A company's application experiences predictable traffic increases every Monday at 9 AM when users start their workday. The current Auto Scaling configuration uses target tracking based on CPU utilization, but instances take 5 minutes to warm up. This causes poor performance every Monday morning. How can the architect improve the scaling response?",
      "options": [
        "A. Decrease the target CPU utilization threshold",
        "B. Add a scheduled scaling action for Monday mornings",
        "C. Enable detailed monitoring for faster metrics",
        "D. Increase the maximum capacity of the Auto Scaling group"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nScheduled scaling:\n- Scales out proactively before the traffic increase\n- Instances are ready before users arrive at 9 AM\n- Complements target tracking for predictable patterns\n- Eliminates warm-up time impact during known peaks\n\n**Why A is incorrect:**\nLowering the threshold makes scaling more aggressive but still reactive. Instances still need warm-up time.\n\n**Why C is incorrect:**\nDetailed monitoring provides 1-minute metrics instead of 5-minute, but doesn't solve the warm-up time issue.\n\n**Why D is incorrect:**\nIncreasing maximum capacity doesn't help if scaling isn't triggered proactively.",
      "reference": "https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-scheduled-scaling.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A gaming application needs to store player session data with the following requirements: single-digit millisecond latency, automatic scaling, and no database management overhead. Which AWS database service should the architect choose?",
      "options": [
        "A. Amazon RDS with provisioned IOPS",
        "B. Amazon Aurora Serverless",
        "C. Amazon DynamoDB",
        "D. Amazon Neptune"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nAmazon DynamoDB provides:\n- Single-digit millisecond latency at any scale\n- Automatic scaling with on-demand capacity mode\n- Fully managed (no database administration)\n- Ideal for session data and gaming use cases\n- Built-in DAX for microsecond latency if needed\n\n**Why A is incorrect:**\nRDS requires management (patching, scaling decisions) and doesn't guarantee single-digit millisecond latency at scale.\n\n**Why B is incorrect:**\nAurora Serverless is relational and doesn't provide the same low-latency guarantees as DynamoDB for key-value access patterns.\n\n**Why D is incorrect:**\nNeptune is a graph database for relationship-heavy data. Overkill for session data storage.",
      "reference": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "A company needs to monitor their AWS environment for suspicious activity such as cryptocurrency mining, unauthorized access attempts, and compromised instances. Which AWS service should the architect enable?",
      "options": [
        "A. Amazon Inspector",
        "B. AWS Config",
        "C. Amazon GuardDuty",
        "D. AWS CloudTrail"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nAmazon GuardDuty:\n- Threat detection service for continuous monitoring\n- Uses machine learning to detect anomalies\n- Analyzes CloudTrail, VPC Flow Logs, and DNS logs\n- Detects cryptocurrency mining, compromised instances\n- Identifies unauthorized access patterns\n- No agents or software to deploy\n\n**Why A is incorrect:**\nInspector assesses EC2 instances for vulnerabilities and compliance. It doesn't detect active threats or suspicious behavior.\n\n**Why B is incorrect:**\nAWS Config tracks resource configurations and compliance. It doesn't detect threats or malicious activity.\n\n**Why D is incorrect:**\nCloudTrail logs API activity but doesn't analyze for threats. GuardDuty uses CloudTrail as a data source.",
      "reference": "https://docs.aws.amazon.com/guardduty/latest/ug/what-is-guardduty.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "A company needs to protect their web application from common web exploits like SQL injection and cross-site scripting (XSS). The application is hosted behind an Application Load Balancer. Which AWS service should the architect use?",
      "options": [
        "A. AWS Shield Standard",
        "B. AWS WAF",
        "C. Amazon GuardDuty",
        "D. Network ACLs"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAWS WAF (Web Application Firewall):\n- Protects against common web exploits\n- Filters SQL injection and XSS attacks\n- Integrates with ALB, CloudFront, API Gateway\n- Customizable rules based on request patterns\n- AWS Managed Rules available for common threats\n\n**Why A is incorrect:**\nAWS Shield protects against DDoS attacks, not application-layer attacks like SQL injection.\n\n**Why C is incorrect:**\nGuardDuty detects threats but doesn't block them. WAF actively filters and blocks malicious requests.\n\n**Why D is incorrect:**\nNACLs filter traffic at the network level (IP/port). They cannot inspect HTTP request content for SQL injection or XSS.",
      "reference": "https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "A company wants to reduce their on-premises backup storage costs by using AWS. Their backup application writes data using the iSCSI protocol. The company wants to maintain low-latency access to recent backups while storing older backups in S3. Which AWS service should the architect recommend?",
      "options": [
        "A. AWS DataSync",
        "B. AWS Storage Gateway - Volume Gateway (Cached mode)",
        "C. AWS Storage Gateway - File Gateway",
        "D. Amazon S3 with Transfer Acceleration"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nVolume Gateway (Cached mode):\n- Presents iSCSI block storage to applications\n- Caches frequently accessed data locally\n- Stores full data in S3\n- Recent backups available with low latency\n- Older data retrieved from S3 as needed\n\n**Why A is incorrect:**\nDataSync is for data transfer between storage systems. It doesn't provide iSCSI interface or local caching.\n\n**Why C is incorrect:**\nFile Gateway uses NFS/SMB protocols, not iSCSI. The backup application requires iSCSI.\n\n**Why D is incorrect:**\nS3 Transfer Acceleration speeds up uploads but doesn't provide iSCSI interface or local caching.",
      "reference": "https://docs.aws.amazon.com/storagegateway/latest/vgw/StorageGatewayConcepts.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "4",
      "difficulty": 2,
      "question_text": "A company is running a steady-state workload on EC2 instances 24/7. They want to reduce costs by committing to a 1-year term but need flexibility to change instance families if requirements change. Which EC2 purchasing option should the architect recommend?",
      "options": [
        "A. Standard Reserved Instances",
        "B. Convertible Reserved Instances",
        "C. Spot Instances",
        "D. Dedicated Hosts"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nConvertible Reserved Instances:\n- Provide significant discount over On-Demand\n- Allow exchanging for different instance types, families, or OS\n- Flexibility to adapt as requirements change\n- 1-year or 3-year terms available\n\n**Why A is incorrect:**\nStandard Reserved Instances cannot be exchanged for different instance families. They offer higher discount but less flexibility.\n\n**Why C is incorrect:**\nSpot Instances are not suitable for 24/7 steady workloads due to potential interruptions.\n\n**Why D is incorrect:**\nDedicated Hosts are for compliance requirements (licensing, regulations), not cost optimization. They're typically more expensive.",
      "reference": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ri-convertible-exchange.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company needs to collect and process streaming data from thousands of IoT sensors in real-time. The data needs to be stored in S3 for later analysis and also processed immediately for anomaly detection. Which combination of AWS services should the architect use?",
      "options": [
        "A. Amazon SQS and AWS Lambda",
        "B. Amazon Kinesis Data Streams with Kinesis Data Firehose and Lambda",
        "C. Amazon SNS and Amazon S3",
        "D. AWS IoT Core and Amazon DynamoDB"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis combination provides:\n- Kinesis Data Streams: Real-time ingestion from IoT sensors\n- Multiple consumers can read the same stream\n- Lambda: Real-time processing for anomaly detection\n- Kinesis Data Firehose: Automatic delivery to S3 for storage\n- Built for high-throughput streaming data\n\n**Why A is incorrect:**\nSQS doesn't support multiple consumers reading the same message. Once processed, messages are deleted.\n\n**Why C is incorrect:**\nSNS is for pub/sub messaging, not high-volume streaming data ingestion. Doesn't provide continuous delivery to S3.\n\n**Why D is incorrect:**\nWhile IoT Core can receive sensor data, DynamoDB alone doesn't address the S3 storage requirement or real-time processing architecture.",
      "reference": "https://docs.aws.amazon.com/streams/latest/dev/introduction.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company has a content management application running on multiple EC2 instances that need to share files simultaneously. The instances are in multiple Availability Zones and need POSIX-compliant file access. Which storage solution should the architect recommend?",
      "options": [
        "A. Amazon S3 with S3 Select",
        "B. Amazon EBS with Multi-Attach",
        "C. Amazon EFS",
        "D. Amazon FSx for Lustre"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nAmazon EFS provides:\n- POSIX-compliant file system\n- Shared access from multiple EC2 instances\n- Works across multiple Availability Zones\n- Automatic scaling (no capacity planning needed)\n- Standard file system operations (mount, read, write)\n\n**Why A is incorrect:**\nS3 is object storage, not a file system. It doesn't provide POSIX file semantics or standard mount capabilities.\n\n**Why B is incorrect:**\nEBS Multi-Attach only works within a single Availability Zone and only with io1/io2 volumes. Doesn't meet the multi-AZ requirement.\n\n**Why D is incorrect:**\nFSx for Lustre is designed for high-performance computing workloads. Overkill for a content management application.",
      "reference": "https://docs.aws.amazon.com/efs/latest/ug/whatisefs.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "A company's application uses RDS database credentials that need to be rotated every 90 days according to security policy. The credentials should be rotated automatically without application downtime. Which AWS service should the architect use?",
      "options": [
        "A. AWS Systems Manager Parameter Store",
        "B. AWS Secrets Manager",
        "C. AWS Certificate Manager",
        "D. AWS KMS"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAWS Secrets Manager:\n- Built-in automatic rotation for RDS credentials\n- Rotates without application downtime\n- Lambda functions handle rotation process\n- Applications retrieve current credentials automatically\n- Supports RDS, Redshift, DocumentDB native rotation\n\n**Why A is incorrect:**\nParameter Store can store secrets but doesn't have built-in automatic rotation. You'd need to build custom rotation logic.\n\n**Why C is incorrect:**\nCertificate Manager manages SSL/TLS certificates, not database credentials.\n\n**Why D is incorrect:**\nKMS manages encryption keys, not database credentials or their rotation.",
      "reference": "https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotating-secrets.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "A company has 50 AWS accounts and wants to ensure that no one can launch EC2 instances in any Region except us-east-1 and eu-west-1. This policy must be enforced across all accounts. Which solution should the architect implement?",
      "options": [
        "A. IAM policies in each account denying EC2 in other Regions",
        "B. AWS Organizations SCP denying EC2 in non-approved Regions",
        "C. VPC endpoint policies restricting Region access",
        "D. AWS Config rules with auto-remediation"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nService Control Policies (SCPs):\n- Apply to all accounts in the Organization\n- Cannot be bypassed by account administrators\n- Centrally managed in the management account\n- Can deny services in specific Regions\n- Enforced at the Organizations level\n\n**Why A is incorrect:**\nIAM policies in each account:\n- Require deployment to all 50 accounts\n- Account admins could modify or remove them\n- Not centrally managed\n\n**Why C is incorrect:**\nVPC endpoint policies control access to AWS services through VPC endpoints. They don't restrict Region access.\n\n**Why D is incorrect:**\nAWS Config can detect violations and remediate, but SCPs prevent the action entirely. Prevention is better than detection/remediation.",
      "reference": "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company wants to be notified when their RDS database CPU utilization exceeds 80% for 5 consecutive minutes. They also want to automatically scale up the instance when this occurs. Which combination of AWS services should the architect use?",
      "options": [
        "A. CloudWatch Alarms, SNS, and Lambda",
        "B. CloudWatch Events, SQS, and EC2 Auto Scaling",
        "C. CloudTrail, CloudWatch Logs, and SNS",
        "D. AWS Config, Lambda, and SNS"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\n- CloudWatch Alarm: Monitor CPU > 80% for 5 minutes\n- SNS: Send notification to team\n- Lambda: Trigger RDS instance modification to scale up\n\nThis provides both notification and automatic remediation.\n\n**Why B is incorrect:**\nEC2 Auto Scaling doesn't work with RDS instances. CloudWatch Events (now EventBridge) is for event-driven automation, not metric monitoring.\n\n**Why C is incorrect:**\nCloudTrail logs API calls, not performance metrics. This doesn't address CPU monitoring.\n\n**Why D is incorrect:**\nAWS Config tracks configuration changes, not performance metrics like CPU utilization.",
      "reference": "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "A company is deploying a microservices application where different URL paths should route to different target groups. For example, /api/* should go to API servers and /web/* should go to web servers. Which type of load balancer should the architect use?",
      "options": [
        "A. Classic Load Balancer",
        "B. Network Load Balancer",
        "C. Application Load Balancer",
        "D. Gateway Load Balancer"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nApplication Load Balancer:\n- Operates at Layer 7 (HTTP/HTTPS)\n- Supports path-based routing (/api/*, /web/*)\n- Routes requests to different target groups based on URL path\n- Also supports host-based routing\n- Ideal for microservices architectures\n\n**Why A is incorrect:**\nClassic Load Balancer is legacy and doesn't support path-based routing.\n\n**Why B is incorrect:**\nNetwork Load Balancer operates at Layer 4 (TCP/UDP). It cannot inspect URL paths.\n\n**Why D is incorrect:**\nGateway Load Balancer is for deploying third-party virtual appliances (firewalls, etc.), not for application routing.",
      "reference": "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company has a gaming application deployed in us-east-1 and ap-southeast-1. They want to provide static IP addresses for their application and ensure users are routed to the nearest healthy Region with the lowest latency. Which AWS service should the architect use?",
      "options": [
        "A. Amazon CloudFront",
        "B. AWS Global Accelerator",
        "C. Route 53 with latency-based routing",
        "D. Elastic Load Balancer"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAWS Global Accelerator provides:\n- Two static anycast IP addresses\n- Routes traffic through AWS global network\n- Automatic failover to healthy endpoints\n- Latency-based routing to nearest Region\n- Consistent performance via AWS backbone\n\n**Why A is incorrect:**\nCloudFront is for content caching. Doesn't provide static IPs and is optimized for cacheable content, not real-time gaming.\n\n**Why C is incorrect:**\nRoute 53 latency routing works but doesn't provide static IP addresses. DNS changes require TTL expiration.\n\n**Why D is incorrect:**\nELB works within a single Region. It doesn't provide global routing or static IPs for multi-Region deployments.",
      "reference": "https://docs.aws.amazon.com/global-accelerator/latest/dg/what-is-global-accelerator.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company is building a serverless REST API that needs to authenticate requests using OAuth 2.0 tokens and transform incoming requests before sending them to Lambda functions. Which AWS service provides these capabilities?",
      "options": [
        "A. AWS AppSync",
        "B. Amazon API Gateway",
        "C. Application Load Balancer",
        "D. Amazon CloudFront"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAmazon API Gateway provides:\n- OAuth 2.0/JWT token validation with authorizers\n- Request/response transformation using mapping templates\n- Native Lambda integration\n- REST API creation and management\n- Rate limiting and throttling\n\n**Why A is incorrect:**\nAppSync is for GraphQL APIs, not REST APIs.\n\n**Why C is incorrect:**\nALB can route to Lambda but doesn't provide request transformation or OAuth 2.0 token validation.\n\n**Why D is incorrect:**\nCloudFront is a CDN for content delivery. It doesn't provide API management features.",
      "reference": "https://docs.aws.amazon.com/apigateway/latest/developerguide/welcome.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "A company requires a disaster recovery solution with an RPO of 1 hour and RTO of 4 hours. They want to minimize costs during normal operations. The production environment runs on EC2 instances with RDS databases. Which DR strategy should the architect recommend?",
      "options": [
        "A. Multi-site active-active",
        "B. Warm standby",
        "C. Pilot light",
        "D. Backup and restore"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nPilot Light strategy:\n- Core components (databases) running in DR Region\n- EC2 instances stopped (minimal cost)\n- Data replication maintains RPO < 1 hour\n- Can scale up quickly to meet RTO < 4 hours\n- Lower cost than warm standby during normal operations\n\n**Why A is incorrect:**\nMulti-site active-active has lowest RTO/RPO but highest cost. Overkill for these requirements.\n\n**Why B is incorrect:**\nWarm standby runs a scaled-down version continuously. Higher cost than pilot light for similar RPO/RTO.\n\n**Why D is incorrect:**\nBackup and restore typically has RTO measured in days, not hours. Doesn't meet the 4-hour RTO requirement.",
      "reference": "https://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads-on-aws/disaster-recovery-options-in-the-cloud.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company needs to coordinate multiple Lambda functions that process an order: validate payment, update inventory, and send confirmation. Each step depends on the previous step's success. The workflow should handle failures and retries automatically. Which AWS service should the architect use?",
      "options": [
        "A. Amazon SQS with Lambda triggers",
        "B. AWS Step Functions",
        "C. Amazon EventBridge",
        "D. AWS Lambda Destinations"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAWS Step Functions:\n- Orchestrates multi-step workflows\n- Visual workflow designer\n- Built-in error handling and retries\n- Maintains state between steps\n- Supports sequential, parallel, and conditional execution\n- Ideal for order processing workflows\n\n**Why A is incorrect:**\nSQS with Lambda can chain functions but requires custom error handling logic. No built-in workflow visualization or state management.\n\n**Why C is incorrect:**\nEventBridge is for event routing, not workflow orchestration. Doesn't maintain state between steps.\n\n**Why D is incorrect:**\nLambda Destinations handle async invocation results but don't provide workflow orchestration or retry logic.",
      "reference": "https://docs.aws.amazon.com/step-functions/latest/dg/welcome.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 3,
      "question_text": "A company runs a three-tier web application. The application tier must access the database tier but should not be directly accessible from the internet. The web tier needs to be publicly accessible. All tiers must be highly available. How should the solutions architect design the VPC architecture?",
      "options": [
        "A. Place all tiers in public subnets across multiple AZs with security groups",
        "B. Place web tier in public subnets, application and database tiers in private subnets, all across multiple AZs",
        "C. Place all tiers in private subnets with a NAT Gateway for internet access",
        "D. Place web and application tiers in public subnets, database tier in a private subnet"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nThis design provides:\n- Web tier in public subnets: accessible from internet via ALB\n- Application tier in private subnets: not directly accessible from internet\n- Database tier in private subnets: protected from internet access\n- Multiple AZs for high availability across all tiers\n- Security through network isolation\n\n**Why A is incorrect:**\nPlacing application and database in public subnets exposes them unnecessarily to the internet.\n\n**Why C is incorrect:**\nThe web tier needs to be publicly accessible. Placing it in private subnets would require complex configurations.\n\n**Why D is incorrect:**\nThe application tier should not be in public subnets per the requirements.",
      "reference": "https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Scenario2.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "4",
      "difficulty": 2,
      "question_text": "A company wants to analyze their AWS spending and identify opportunities for cost optimization. They need recommendations for underutilized resources and Reserved Instance purchases. Which AWS tool provides these insights?",
      "options": [
        "A. AWS Cost Explorer",
        "B. AWS Budgets",
        "C. AWS Cost and Usage Report",
        "D. AWS Trusted Advisor"
      ],
      "correct_answer": "D",
      "explanation": "**Correct Answer: D**\n\n**Why D is correct:**\nAWS Trusted Advisor provides:\n- Cost optimization recommendations\n- Identifies underutilized EC2 instances\n- Reserved Instance optimization suggestions\n- Idle load balancers and unused EBS volumes\n- Checks across multiple categories\n\n**Why A is incorrect:**\nCost Explorer visualizes spending but doesn't provide specific optimization recommendations for underutilized resources.\n\n**Why B is incorrect:**\nAWS Budgets sets spending alerts but doesn't analyze utilization or recommend optimizations.\n\n**Why C is incorrect:**\nCost and Usage Report provides detailed billing data but doesn't include optimization recommendations.",
      "reference": "https://docs.aws.amazon.com/awssupport/latest/user/trusted-advisor.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "A company needs to allow instances in a private subnet to download patches from the internet while preventing any inbound connections from the internet. Which solution should the architect implement?",
      "options": [
        "A. Internet Gateway with restrictive security groups",
        "B. NAT Gateway in a public subnet",
        "C. VPC endpoint for patch services",
        "D. Transit Gateway with internet access"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nNAT Gateway enables instances in private subnets to connect to the internet for outbound traffic (like downloading patches) while preventing unsolicited inbound connections. NAT Gateway must be placed in a public subnet with an Elastic IP.\n\n**Why A is incorrect:**\nAn Internet Gateway alone would require instances to have public IPs, making them potentially accessible from the internet.\n\n**Why C is incorrect:**\nVPC endpoints provide private connectivity to AWS services, not to the general internet for patch downloads.\n\n**Why D is incorrect:**\nTransit Gateway connects VPCs and on-premises networks but doesn't inherently provide NAT functionality.",
      "reference": "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "A company wants their EC2 instances to access S3 without traffic going over the public internet. Which solution provides the MOST secure and cost-effective approach?",
      "options": [
        "A. NAT Gateway with S3 bucket policy",
        "B. VPC Gateway Endpoint for S3",
        "C. AWS PrivateLink for S3",
        "D. Internet Gateway with S3 bucket policy"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nVPC Gateway Endpoints for S3:\n- Keep traffic within the AWS network (not over internet)\n- Free to use (no data processing charges)\n- Simple to configure via route tables\n- Secure by default\n\n**Why A is incorrect:**\nNAT Gateway routes traffic through the internet and incurs data processing charges.\n\n**Why C is incorrect:**\nPrivateLink (Interface Endpoints) works but costs money per hour and per GB. Gateway Endpoints are free.\n\n**Why D is incorrect:**\nInternet Gateway would route traffic over the public internet.",
      "reference": "https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 3,
      "question_text": "A company has a VPC with CIDR 10.0.0.0/16. They need to connect this VPC to their on-premises network (192.168.0.0/16) using AWS Site-to-Site VPN. After configuration, on-premises servers cannot reach EC2 instances in the VPC. What is the MOST likely cause?",
      "options": [
        "A. The VPC CIDR overlaps with the on-premises CIDR",
        "B. The route table for the subnet does not have a route to the Virtual Private Gateway",
        "C. The security group does not allow ICMP traffic",
        "D. The VPN connection is using the wrong encryption algorithm"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nFor VPN connectivity to work, the VPC route table must have:\n- A route for the on-premises CIDR (192.168.0.0/16)\n- Target: Virtual Private Gateway (VGW)\nWithout this route, return traffic from EC2 cannot reach on-premises.\n\n**Why A is incorrect:**\nThe CIDRs don't overlap (10.0.0.0/16 vs 192.168.0.0/16 are different ranges).\n\n**Why C is incorrect:**\nSecurity groups could cause issues, but routing is the more fundamental requirement to check first.\n\n**Why D is incorrect:**\nIf encryption was wrong, the VPN tunnel wouldn't establish at all.",
      "reference": "https://docs.aws.amazon.com/vpn/latest/s2svpn/VPNRoutingTypes.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "A security team requires that all traffic between application servers and database servers within a VPC be encrypted. Both tiers are in private subnets. Which is the simplest solution?",
      "options": [
        "A. Configure TLS/SSL encryption on the database connections",
        "B. Create a VPN connection between the subnets",
        "C. Use VPC traffic mirroring with encryption",
        "D. Deploy a NAT Gateway between the subnets"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nEnabling TLS/SSL on database connections:\n- Encrypts data in transit between application and database\n- Supported by RDS, Aurora, and most databases\n- Simple configuration change\n- No infrastructure changes required\n\n**Why B is incorrect:**\nVPN between subnets within the same VPC is unnecessary complexity.\n\n**Why C is incorrect:**\nTraffic mirroring is for monitoring/analysis, not encryption.\n\n**Why D is incorrect:**\nNAT Gateway is for outbound internet access, not for inter-subnet encryption.",
      "reference": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.SSL.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "A company has multiple VPCs that need to communicate with each other and with on-premises networks. They want a hub-and-spoke model to simplify network management. Which AWS service should the architect use?",
      "options": [
        "A. VPC Peering",
        "B. AWS Transit Gateway",
        "C. AWS Direct Connect",
        "D. VPC Endpoints"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAWS Transit Gateway:\n- Acts as a central hub for connecting VPCs and on-premises networks\n- Supports hub-and-spoke topology\n- Simplifies management with centralized routing\n- Scales to thousands of VPCs\n- Supports transitive routing\n\n**Why A is incorrect:**\nVPC Peering is point-to-point and doesn't support transitive routing. Each VPC pair needs a separate peering connection.\n\n**Why C is incorrect:**\nDirect Connect provides dedicated connectivity to on-premises but doesn't act as a hub for VPC-to-VPC communication.\n\n**Why D is incorrect:**\nVPC Endpoints connect to AWS services, not to other VPCs.",
      "reference": "https://docs.aws.amazon.com/vpc/latest/tgw/what-is-transit-gateway.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "An application in VPC-A (10.1.0.0/16) needs to access resources in VPC-B (10.2.0.0/16). Both VPCs are in the same Region and same AWS account. What is the simplest way to enable this connectivity?",
      "options": [
        "A. Create an Internet Gateway in each VPC",
        "B. Create a VPC Peering connection",
        "C. Set up AWS Site-to-Site VPN between the VPCs",
        "D. Use AWS PrivateLink"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nVPC Peering:\n- Direct, private connection between two VPCs\n- No overlapping CIDRs (10.1.0.0/16 and 10.2.0.0/16)\n- Traffic stays within AWS network\n- Simple to configure\n- No additional hardware or gateways needed\n\n**Why A is incorrect:**\nInternet Gateways route traffic over the internet, which is unnecessary and less secure.\n\n**Why C is incorrect:**\nSite-to-Site VPN is for connecting to on-premises networks, not VPC-to-VPC within AWS.\n\n**Why D is incorrect:**\nPrivateLink is for accessing services, not for general VPC-to-VPC connectivity.",
      "reference": "https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "A Network ACL is configured with the following inbound rules: Rule 100 - Allow HTTP from 0.0.0.0/0, Rule 200 - Deny all traffic. An HTTP request arrives from IP 203.0.113.50. What happens to this request?",
      "options": [
        "A. The request is denied because Rule 200 denies all traffic",
        "B. The request is allowed because Rule 100 is evaluated first",
        "C. The request is denied because explicit deny takes precedence",
        "D. The request is allowed because HTTP is a standard protocol"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nNACLs evaluate rules in order by rule number (lowest first):\n1. Rule 100 (Allow HTTP) is evaluated first\n2. The request matches Rule 100 and is allowed\n3. Rule 200 is never evaluated for this request\n\n**Why A is incorrect:**\nRule 200 is only evaluated if no earlier rule matches. Rule 100 matches first.\n\n**Why C is incorrect:**\nNACLs don't use the same evaluation logic as IAM. Rules are evaluated in numerical order, and the first match applies.\n\n**Why D is incorrect:**\nThe protocol type doesn't automatically allow traffic; rules must explicitly permit it.",
      "reference": "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "What is the difference between Security Groups and Network ACLs in a VPC?",
      "options": [
        "A. Security Groups are stateless; NACLs are stateful",
        "B. Security Groups operate at instance level; NACLs operate at subnet level",
        "C. Security Groups only allow rules; NACLs only deny rules",
        "D. Security Groups are evaluated after NACLs"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\n- Security Groups: Operate at the instance (ENI) level, stateful\n- NACLs: Operate at the subnet level, stateless\n\nKey differences:\n- SGs are stateful (return traffic auto-allowed), NACLs are stateless\n- SGs only have allow rules, NACLs have allow and deny rules\n- SGs evaluate all rules, NACLs evaluate in order\n\n**Why A is incorrect:**\nIt's the opposite: Security Groups are stateful, NACLs are stateless.\n\n**Why C is incorrect:**\nNACLs support both allow and deny rules. Security Groups only support allow rules.\n\n**Why D is incorrect:**\nTraffic flows through NACL first (subnet boundary), then Security Group (instance boundary).",
      "reference": "https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Security.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company needs to run a batch processing job that can tolerate interruptions. The job typically runs for 2-3 hours and processes data that can be restarted if interrupted. Which EC2 purchasing option provides the best cost savings?",
      "options": [
        "A. On-Demand Instances",
        "B. Reserved Instances",
        "C. Spot Instances",
        "D. Dedicated Hosts"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nSpot Instances:\n- Up to 90% discount compared to On-Demand\n- Ideal for fault-tolerant, interruptible workloads\n- Batch processing can checkpoint and restart\n- Best choice when cost is priority and interruptions are acceptable\n\n**Why A is incorrect:**\nOn-Demand is more expensive and doesn't offer the significant savings available for interruptible workloads.\n\n**Why B is incorrect:**\nReserved Instances require commitment and are for steady-state workloads, not batch jobs.\n\n**Why D is incorrect:**\nDedicated Hosts are the most expensive option, used for licensing or compliance requirements.",
      "reference": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company runs a memory-intensive application that requires 256 GB of RAM. They need consistent performance and the lowest hourly cost. Which EC2 instance family should the architect recommend?",
      "options": [
        "A. C5 (Compute Optimized)",
        "B. R5 (Memory Optimized)",
        "C. T3 (General Purpose with burstable CPU)",
        "D. I3 (Storage Optimized)"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nR5 (Memory Optimized) instances:\n- Designed for memory-intensive workloads\n- High memory-to-CPU ratio\n- Cost-effective for applications needing large amounts of RAM\n- Available in sizes up to 768 GB RAM\n\n**Why A is incorrect:**\nC5 instances are compute-optimized with lower memory-to-CPU ratios. You'd pay for CPU you don't need.\n\n**Why C is incorrect:**\nT3 instances are burstable and not suitable for sustained high-memory workloads. They have limited memory options.\n\n**Why D is incorrect:**\nI3 instances are optimized for storage I/O, not memory-intensive workloads.",
      "reference": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/memory-optimized-instances.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "An application requires dedicated hardware for compliance reasons. The company needs to track per-socket licensing for their software. Which EC2 option should the architect use?",
      "options": [
        "A. Dedicated Instances",
        "B. Dedicated Hosts",
        "C. Reserved Instances",
        "D. Spot Instances"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nDedicated Hosts:\n- Physical server dedicated to your use\n- Visibility into sockets and physical cores\n- Required for per-socket or per-core licensing (Windows Server, SQL Server, Oracle)\n- Helps meet compliance requirements\n\n**Why A is incorrect:**\nDedicated Instances run on dedicated hardware but don't provide socket/core visibility for licensing.\n\n**Why C is incorrect:**\nReserved Instances are a pricing model, not a hardware isolation option.\n\n**Why D is incorrect:**\nSpot Instances don't guarantee dedicated hardware.",
      "reference": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-hosts-overview.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "An Auto Scaling group needs to maintain application availability during instance launches. New instances take 5 minutes to complete initialization before they can serve traffic. How can the architect ensure the load balancer doesn't send traffic to instances before they're ready?",
      "options": [
        "A. Increase the health check interval",
        "B. Configure a health check grace period",
        "C. Use step scaling policies",
        "D. Enable instance protection"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nHealth check grace period:\n- Prevents Auto Scaling from terminating instances during initialization\n- Gives instances time to complete startup before health checks start\n- Set to 300 seconds (5 minutes) for this scenario\n- Instance won't receive traffic until healthy\n\n**Why A is incorrect:**\nHealth check interval is how often checks occur, not how long to wait before starting checks.\n\n**Why C is incorrect:**\nStep scaling policies define how to scale, not how to handle instance initialization.\n\n**Why D is incorrect:**\nInstance protection prevents scale-in termination, not health check handling.",
      "reference": "https://docs.aws.amazon.com/autoscaling/ec2/userguide/healthcheck.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company needs to capture the exact state of an EC2 instance including the operating system, applications, and configurations. They want to use this to launch identical instances in another Region. What should they create?",
      "options": [
        "A. EBS Snapshot",
        "B. Amazon Machine Image (AMI)",
        "C. EC2 Instance template",
        "D. CloudFormation template"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAmazon Machine Image (AMI):\n- Captures OS, applications, configurations, and data volumes\n- Can be copied to other Regions\n- Used to launch identical instances\n- Includes block device mappings\n\n**Why A is incorrect:**\nEBS Snapshots capture volume data but not the full instance configuration (instance type, networking, etc.).\n\n**Why C is incorrect:**\nLaunch Templates define instance configuration but don't capture the actual OS state and installed applications.\n\n**Why D is incorrect:**\nCloudFormation templates define infrastructure as code but don't capture the runtime state of an instance.",
      "reference": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company wants to run a high-performance computing (HPC) workload that requires low-latency, high-throughput communication between instances. Which EC2 feature should the architect use?",
      "options": [
        "A. Enhanced Networking with Elastic Network Adapter (ENA)",
        "B. Elastic Fabric Adapter (EFA)",
        "C. Elastic IP addresses",
        "D. Multiple network interfaces"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nElastic Fabric Adapter (EFA):\n- Designed specifically for HPC workloads\n- Provides OS-bypass capabilities for lower latency\n- Supports MPI (Message Passing Interface)\n- Enables high-throughput, low-latency inter-instance communication\n\n**Why A is incorrect:**\nENA provides enhanced networking but doesn't have OS-bypass capabilities needed for HPC.\n\n**Why C is incorrect:**\nElastic IPs are static public IP addresses, unrelated to inter-instance performance.\n\n**Why D is incorrect:**\nMultiple ENIs don't specifically improve HPC performance like EFA does.",
      "reference": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "An EC2 instance needs to be stopped and started on a schedule to save costs. The instance has data on its root EBS volume that must be preserved. What happens to the data when the instance is stopped?",
      "options": [
        "A. Data is deleted when the instance is stopped",
        "B. Data is preserved on the EBS volume",
        "C. Data is automatically backed up to S3",
        "D. Data is moved to instance store"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nEBS volumes persist independently from the EC2 instance:\n- Data on EBS root volumes is preserved when instances are stopped\n- The volume remains attached and data is intact\n- When instance starts again, data is available\n- This is different from instance store (ephemeral)\n\n**Why A is incorrect:**\nEBS data is not deleted when stopping an instance. Only termination can delete the root volume (if configured).\n\n**Why C is incorrect:**\nNo automatic S3 backup occurs. You must explicitly create snapshots.\n\n**Why D is incorrect:**\nData doesn't move to instance store. Instance store is separate ephemeral storage.",
      "reference": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Stop_Start.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "A company runs an RDS MySQL database for their e-commerce application. During peak shopping periods, read queries cause performance issues. Write operations are acceptable. How can the architect improve read performance with minimal application changes?",
      "options": [
        "A. Enable Multi-AZ deployment",
        "B. Create Read Replicas",
        "C. Increase instance size",
        "D. Enable Enhanced Monitoring"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nRDS Read Replicas:\n- Offload read traffic from the primary database\n- Scale read capacity horizontally\n- Applications can direct reads to replica endpoints\n- Asynchronous replication from primary\n\n**Why A is incorrect:**\nMulti-AZ provides high availability but the standby cannot serve read traffic.\n\n**Why C is incorrect:**\nIncreasing instance size helps but is expensive and has limits. Read Replicas provide horizontal scaling.\n\n**Why D is incorrect:**\nEnhanced Monitoring provides visibility but doesn't improve performance.",
      "reference": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "A company's RDS database must meet a recovery point objective (RPO) of 5 minutes. Which RDS feature should be configured?",
      "options": [
        "A. Automated backups with a backup window",
        "B. Manual DB snapshots",
        "C. Read Replicas in another AZ",
        "D. Multi-AZ deployment"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nRDS Automated Backups:\n- Continuous backup of transaction logs to S3\n- Point-in-time recovery to any second within retention period\n- RPO is effectively the last 5 minutes (transaction log backup frequency)\n- Retention period: 1-35 days\n\n**Why B is incorrect:**\nManual snapshots are taken at specific points in time. RPO would be the time since the last snapshot.\n\n**Why C is incorrect:**\nRead Replicas are for read scaling, not backup/recovery. They use async replication.\n\n**Why D is incorrect:**\nMulti-AZ provides high availability (low RTO), not backup recovery (RPO).",
      "reference": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 3,
      "question_text": "A company is migrating from on-premises PostgreSQL to Amazon Aurora PostgreSQL. They need zero downtime during the cutover. Which migration approach should the architect recommend?",
      "options": [
        "A. Use pg_dump and pg_restore during a maintenance window",
        "B. Use AWS DMS with ongoing replication until cutover",
        "C. Create an Aurora Read Replica from the on-premises database",
        "D. Use AWS Snowball to transfer the database files"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAWS DMS with ongoing replication:\n- Performs initial full load\n- Captures and replicates changes (CDC - Change Data Capture)\n- Keeps target in sync until cutover\n- Cutover involves pointing application to Aurora (minimal downtime)\n\n**Why A is incorrect:**\npg_dump/pg_restore requires downtime during export and import.\n\n**Why C is incorrect:**\nAurora Read Replica from on-premises isn't directly supported. You need DMS for migration.\n\n**Why D is incorrect:**\nSnowball is for large data transfers, not for live database migration with replication.",
      "reference": "https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.PostgreSQL.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "An Aurora MySQL database needs to scale automatically based on application demand. During peak hours, the database needs more capacity, but during off-hours, it should scale down to save costs. Which Aurora feature should be used?",
      "options": [
        "A. Aurora Multi-Master",
        "B. Aurora Global Database",
        "C. Aurora Serverless v2",
        "D. Aurora Parallel Query"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nAurora Serverless v2:\n- Automatically scales capacity up and down\n- Scales in fine-grained increments based on demand\n- Scales down during low-traffic periods to save costs\n- No capacity planning required\n\n**Why A is incorrect:**\nMulti-Master allows multiple write nodes but doesn't automatically scale based on demand.\n\n**Why B is incorrect:**\nGlobal Database provides cross-Region replication, not automatic scaling.\n\n**Why D is incorrect:**\nParallel Query optimizes analytical queries but doesn't provide automatic scaling.",
      "reference": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless-v2.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "A company stores sensitive financial documents in S3. They require that all objects be encrypted at rest using keys they manage and can audit. Which S3 encryption option should they use?",
      "options": [
        "A. SSE-S3",
        "B. SSE-KMS with customer managed key",
        "C. SSE-C",
        "D. Client-side encryption"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nSSE-KMS with customer managed key (CMK):\n- Customer controls the encryption key\n- Key usage is logged in CloudTrail for auditing\n- Can set key policies and rotation schedules\n- Meets requirements for key management and auditing\n\n**Why A is incorrect:**\nSSE-S3 uses AWS-managed keys. Customer cannot manage or audit key usage.\n\n**Why C is incorrect:**\nSSE-C requires customer to provide keys with each request. More operational overhead.\n\n**Why D is incorrect:**\nClient-side encryption works but adds complexity. SSE-KMS is simpler and meets requirements.",
      "reference": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company needs to transfer 100 TB of data from on-premises storage to S3. Their network connection is 1 Gbps. What is the FASTEST way to complete this transfer?",
      "options": [
        "A. Use S3 Transfer Acceleration",
        "B. Use AWS DataSync",
        "C. Use AWS Snowball Edge",
        "D. Use multi-part upload directly to S3"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nAWS Snowball Edge:\n- Physical device shipped to customer\n- 100 TB transfer over 1 Gbps would take ~10 days minimum\n- Snowball can be loaded locally and shipped back in days\n- Ideal for large data transfers when network is the bottleneck\n\nCalculation: 100 TB over 1 Gbps = ~9-10 days continuous transfer\n\n**Why A is incorrect:**\nTransfer Acceleration speeds up uploads but is still limited by network bandwidth.\n\n**Why B is incorrect:**\nDataSync optimizes transfer but cannot exceed network bandwidth limits.\n\n**Why D is incorrect:**\nMulti-part upload is still constrained by network bandwidth.",
      "reference": "https://docs.aws.amazon.com/snowball/latest/developer-guide/whatissnowball.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "4",
      "difficulty": 2,
      "question_text": "A company stores log files in S3. Files older than 30 days are rarely accessed but must be retained for 7 years. Which S3 storage class transition provides the lowest cost?",
      "options": [
        "A. S3 Standard → S3 Glacier Instant Retrieval after 30 days",
        "B. S3 Standard → S3 Glacier Deep Archive after 30 days",
        "C. S3 Standard → S3 Standard-IA after 30 days",
        "D. S3 Standard → S3 One Zone-IA after 30 days"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nS3 Glacier Deep Archive:\n- Lowest cost storage class\n- 12-48 hour retrieval time (acceptable for rarely accessed data)\n- Ideal for long-term archival\n- Up to 95% lower cost than S3 Standard\n\n**Why A is incorrect:**\nGlacier Instant Retrieval has faster access but higher cost than Deep Archive.\n\n**Why C is incorrect:**\nStandard-IA is more expensive than Glacier classes for archival data.\n\n**Why D is incorrect:**\nOne Zone-IA has lower availability and is more expensive than Deep Archive.",
      "reference": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "A company wants to ensure that S3 objects cannot be deleted or overwritten for a specific retention period to meet compliance requirements. Which S3 feature should be enabled?",
      "options": [
        "A. S3 Versioning",
        "B. S3 Object Lock",
        "C. S3 Cross-Region Replication",
        "D. S3 MFA Delete"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nS3 Object Lock:\n- Write-once-read-many (WORM) model\n- Prevents object deletion or modification for retention period\n- Supports Governance and Compliance modes\n- Required for regulatory compliance (SEC, FINRA)\n\n**Why A is incorrect:**\nVersioning preserves deleted objects but doesn't prevent deletion.\n\n**Why C is incorrect:**\nCRR copies objects to another Region but doesn't prevent deletion in source.\n\n**Why D is incorrect:**\nMFA Delete requires MFA to delete but doesn't enforce retention periods.",
      "reference": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A static website is hosted on S3 and distributed via CloudFront. Users report that updates to the website are not immediately visible. What should the architect do to ensure users see the latest content?",
      "options": [
        "A. Disable S3 versioning",
        "B. Create a CloudFront invalidation",
        "C. Enable S3 Transfer Acceleration",
        "D. Change the S3 storage class"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nCloudFront Invalidation:\n- Forces CloudFront to fetch fresh content from origin\n- Removes cached objects from edge locations\n- Users receive updated content on next request\n- Can invalidate specific files or all files (*)\n\n**Why A is incorrect:**\nVersioning manages object versions, unrelated to CloudFront caching.\n\n**Why C is incorrect:**\nTransfer Acceleration speeds up uploads, doesn't affect caching.\n\n**Why D is incorrect:**\nStorage class affects cost and retrieval time, not caching.",
      "reference": "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Invalidation.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "An order processing system must ensure that each order is processed exactly once and in the order received. Which SQS queue type should be used?",
      "options": [
        "A. Standard Queue",
        "B. FIFO Queue",
        "C. Dead Letter Queue",
        "D. Delay Queue"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nSQS FIFO Queue:\n- First-In-First-Out ordering guaranteed\n- Exactly-once processing (deduplication)\n- Messages processed in strict order\n- Required for order-sensitive workflows\n\n**Why A is incorrect:**\nStandard queues provide at-least-once delivery and best-effort ordering. Messages might be delivered out of order or duplicated.\n\n**Why C is incorrect:**\nDead Letter Queue handles failed messages, not ordering or deduplication.\n\n**Why D is incorrect:**\nDelay Queue delays initial delivery but doesn't guarantee ordering.",
      "reference": "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company needs to send notifications to multiple subscribers whenever a new order is placed. Some subscribers use HTTP endpoints, others use email, and some use SQS queues. Which AWS service should the architect use?",
      "options": [
        "A. Amazon SQS",
        "B. Amazon SNS",
        "C. Amazon EventBridge",
        "D. AWS Lambda"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAmazon SNS (Simple Notification Service):\n- Pub/sub messaging for multiple subscribers\n- Supports multiple protocol types (HTTP, Email, SQS, Lambda, SMS)\n- Single message published to topic reaches all subscribers\n- Decouples publishers from subscribers\n\n**Why A is incorrect:**\nSQS is point-to-point, not pub/sub. One message goes to one consumer.\n\n**Why C is incorrect:**\nEventBridge is for event-driven architectures with rules-based routing. SNS is simpler for basic pub/sub.\n\n**Why D is incorrect:**\nLambda is compute, not a messaging service.",
      "reference": "https://docs.aws.amazon.com/sns/latest/dg/welcome.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company needs to collect custom application metrics from their EC2 instances and create alarms based on these metrics. Which approach should the architect recommend?",
      "options": [
        "A. Enable detailed monitoring on EC2",
        "B. Use CloudWatch agent to publish custom metrics",
        "C. Use VPC Flow Logs",
        "D. Enable AWS Config"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nCloudWatch Agent:\n- Collects custom application metrics\n- Can collect memory, disk, and application-specific metrics\n- Publishes to CloudWatch Metrics\n- Enables creating alarms on custom metrics\n\n**Why A is incorrect:**\nDetailed monitoring provides more frequent EC2 metrics (1-minute) but not custom application metrics.\n\n**Why C is incorrect:**\nVPC Flow Logs capture network traffic metadata, not application metrics.\n\n**Why D is incorrect:**\nAWS Config tracks resource configurations, not application metrics.",
      "reference": "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A Lambda function needs to access resources in a VPC (RDS database). After configuring VPC access, the function can no longer reach the internet to call external APIs. How can the architect enable internet access?",
      "options": [
        "A. Attach an Elastic IP to the Lambda function",
        "B. Configure a NAT Gateway in the VPC",
        "C. Enable Lambda Provisioned Concurrency",
        "D. Increase the Lambda function timeout"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nWhen Lambda runs in a VPC:\n- It uses private IP addresses\n- No direct internet access\n- Requires NAT Gateway for outbound internet access\n- Lambda should be in private subnet, NAT in public subnet\n\n**Why A is incorrect:**\nLambda functions cannot have Elastic IPs attached directly.\n\n**Why C is incorrect:**\nProvisioned Concurrency reduces cold starts, unrelated to network access.\n\n**Why D is incorrect:**\nTimeout settings don't affect network connectivity.",
      "reference": "https://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "An application needs to maintain user session state while distributing traffic across multiple EC2 instances. The architect wants to ensure all requests from a user go to the same instance. Which ELB feature should be configured?",
      "options": [
        "A. Cross-zone load balancing",
        "B. Connection draining",
        "C. Sticky sessions",
        "D. Health checks"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nSticky Sessions (Session Affinity):\n- Binds user sessions to specific target instances\n- Uses cookies to track session-instance mapping\n- Ensures requests from same user go to same instance\n- Useful for stateful applications\n\n**Why A is incorrect:**\nCross-zone distributes traffic evenly across AZs but doesn't maintain session affinity.\n\n**Why B is incorrect:**\nConnection draining ensures in-flight requests complete during deregistration, unrelated to sessions.\n\n**Why D is incorrect:**\nHealth checks monitor instance health, not session management.",
      "reference": "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/sticky-sessions.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company's web application experiences variable traffic patterns with sudden spikes. They want to ensure the application can handle increased load automatically. Which combination provides automatic scaling with minimal latency?",
      "options": [
        "A. EC2 with manual capacity management",
        "B. EC2 Auto Scaling with target tracking policy",
        "C. Single large EC2 instance",
        "D. EC2 Reserved Instances only"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nEC2 Auto Scaling with target tracking:\n- Automatically adjusts capacity based on demand\n- Target tracking maintains desired metric (e.g., CPU at 50%)\n- Scales out quickly during traffic spikes\n- Scales in during low demand to save costs\n\n**Why A is incorrect:**\nManual capacity management cannot respond quickly to sudden spikes.\n\n**Why C is incorrect:**\nA single instance cannot scale and creates a single point of failure.\n\n**Why D is incorrect:**\nReserved Instances are a pricing model, not a scaling solution.",
      "reference": "https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A global application needs to route users to the nearest AWS Region to minimize latency. Which AWS service should be used?",
      "options": [
        "A. Amazon CloudFront",
        "B. AWS Global Accelerator",
        "C. Amazon Route 53 with simple routing",
        "D. Elastic Load Balancing"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAWS Global Accelerator:\n- Uses AWS global network for optimal routing\n- Routes users to nearest healthy endpoint\n- Provides static anycast IP addresses\n- Reduces latency by 60% or more vs internet routing\n\n**Why A is incorrect:**\nCloudFront caches content at edge but doesn't route to multiple Regions based on proximity.\n\n**Why C is incorrect:**\nSimple routing doesn't consider user location or latency.\n\n**Why D is incorrect:**\nELB operates within a single Region, not globally.",
      "reference": "https://docs.aws.amazon.com/global-accelerator/latest/dg/what-is-global-accelerator.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "An application requires sub-millisecond latency for database reads. The data is relatively static and can be cached. Which AWS service should be used to improve read performance?",
      "options": [
        "A. Amazon RDS with Read Replicas",
        "B. Amazon ElastiCache",
        "C. Amazon Redshift",
        "D. Amazon DynamoDB"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAmazon ElastiCache:\n- In-memory caching (Redis or Memcached)\n- Sub-millisecond latency\n- Reduces database load\n- Perfect for caching static or frequently accessed data\n\n**Why A is incorrect:**\nRead Replicas improve read scaling but don't provide sub-millisecond latency.\n\n**Why C is incorrect:**\nRedshift is for analytics/warehousing, not low-latency caching.\n\n**Why D is incorrect:**\nDynamoDB provides single-digit millisecond latency, not sub-millisecond.",
      "reference": "https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/WhatIs.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company needs to process large amounts of streaming data in real-time with sub-second latency. Which AWS service is designed for this use case?",
      "options": [
        "A. Amazon SQS",
        "B. Amazon Kinesis Data Streams",
        "C. Amazon S3",
        "D. AWS Batch"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAmazon Kinesis Data Streams:\n- Real-time data streaming\n- Sub-second latency for data processing\n- Can handle gigabytes per second\n- Multiple consumers can process same data\n\n**Why A is incorrect:**\nSQS is for message queuing, not real-time streaming with multiple consumers.\n\n**Why C is incorrect:**\nS3 is object storage, not designed for real-time streaming.\n\n**Why D is incorrect:**\nAWS Batch is for batch processing, not real-time streaming.",
      "reference": "https://docs.aws.amazon.com/streams/latest/dev/introduction.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A media company needs to deliver video content globally with low latency. Which AWS service should they use?",
      "options": [
        "A. Amazon S3 with public access",
        "B. Amazon CloudFront",
        "C. AWS Direct Connect",
        "D. Amazon EC2 in multiple Regions"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAmazon CloudFront:\n- Content Delivery Network (CDN) with global edge locations\n- Caches content close to viewers\n- Reduces latency for video delivery\n- Supports live and on-demand video streaming\n\n**Why A is incorrect:**\nS3 alone doesn't provide edge caching or global distribution.\n\n**Why C is incorrect:**\nDirect Connect is for dedicated network connections, not content delivery.\n\n**Why D is incorrect:**\nEC2 in multiple Regions adds complexity and cost without the edge caching benefits.",
      "reference": "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "An application needs high IOPS storage for a relational database. Which EBS volume type provides the highest IOPS?",
      "options": [
        "A. gp3 (General Purpose SSD)",
        "B. io2 Block Express",
        "C. st1 (Throughput Optimized HDD)",
        "D. sc1 (Cold HDD)"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nio2 Block Express:\n- Up to 256,000 IOPS per volume\n- Highest IOPS available in EBS\n- 99.999% durability\n- Designed for mission-critical applications\n\n**Why A is incorrect:**\ngp3 provides up to 16,000 IOPS, suitable for most workloads but not highest IOPS.\n\n**Why C is incorrect:**\nst1 is optimized for throughput, not IOPS. Max 500 IOPS.\n\n**Why D is incorrect:**\nsc1 is for infrequently accessed data. Max 250 IOPS.",
      "reference": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company needs a fully managed file system that supports the NFS protocol and can be accessed by multiple EC2 instances simultaneously. Which AWS service should they use?",
      "options": [
        "A. Amazon EBS",
        "B. Amazon EFS",
        "C. Amazon S3",
        "D. AWS Storage Gateway"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAmazon EFS (Elastic File System):\n- Fully managed NFS file system\n- Shared access from multiple EC2 instances\n- Automatic scaling\n- Regional service with Multi-AZ redundancy\n\n**Why A is incorrect:**\nEBS volumes can only be attached to one instance at a time (except io1/io2 Multi-Attach).\n\n**Why C is incorrect:**\nS3 is object storage with its own API, not NFS-compatible.\n\n**Why D is incorrect:**\nStorage Gateway bridges on-premises to AWS, not a native shared file system.",
      "reference": "https://docs.aws.amazon.com/efs/latest/ug/whatisefs.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "An application uses DynamoDB and needs consistent single-digit millisecond performance at any scale. The table receives millions of requests per second. Which DynamoDB feature ensures this performance?",
      "options": [
        "A. DynamoDB Streams",
        "B. DynamoDB Auto Scaling",
        "C. DynamoDB On-Demand capacity",
        "D. DynamoDB Accelerator (DAX)"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nDynamoDB On-Demand:\n- Automatically scales to handle any traffic level\n- No capacity planning required\n- Maintains consistent single-digit millisecond performance\n- Pay per request pricing\n\n**Why A is incorrect:**\nStreams capture data changes but don't affect read/write performance.\n\n**Why B is incorrect:**\nAuto Scaling adjusts provisioned capacity but has lag time during sudden spikes.\n\n**Why D is incorrect:**\nDAX provides microsecond latency for cached reads, but the question asks about any scale performance.",
      "reference": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A web application uses Application Load Balancer and experiences uneven traffic distribution across targets. Some instances are overloaded while others are underutilized. What should the architect enable?",
      "options": [
        "A. Sticky sessions",
        "B. Cross-zone load balancing",
        "C. Connection draining",
        "D. Slow start mode"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nCross-zone load balancing:\n- Distributes traffic evenly across all targets in all AZs\n- Prevents uneven distribution when AZs have different numbers of targets\n- Enabled by default for ALB\n- Ensures balanced load across instances\n\n**Why A is incorrect:**\nSticky sessions bind users to specific instances, which can cause uneven distribution.\n\n**Why C is incorrect:**\nConnection draining handles deregistration gracefully, not traffic distribution.\n\n**Why D is incorrect:**\nSlow start gradually increases traffic to new targets, not overall distribution.",
      "reference": "https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company needs to run containerized applications with automatic scaling. They want AWS to manage the underlying infrastructure. Which service should they use?",
      "options": [
        "A. Amazon ECS on EC2",
        "B. Amazon ECS on Fargate",
        "C. Amazon EC2 with Docker installed",
        "D. AWS Elastic Beanstalk with Docker"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAmazon ECS on Fargate:\n- Serverless container platform\n- No infrastructure management required\n- Automatic scaling\n- Pay only for resources used by containers\n\n**Why A is incorrect:**\nECS on EC2 requires managing EC2 instances.\n\n**Why C is incorrect:**\nEC2 with Docker requires full infrastructure management.\n\n**Why D is incorrect:**\nElastic Beanstalk provides some abstraction but still manages EC2 instances underneath.",
      "reference": "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/AWS_Fargate.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A data lake stores petabytes of data in S3. Analysts need to run ad-hoc SQL queries against this data without loading it into a database. Which AWS service should they use?",
      "options": [
        "A. Amazon RDS",
        "B. Amazon Redshift",
        "C. Amazon Athena",
        "D. Amazon DynamoDB"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nAmazon Athena:\n- Serverless query service\n- Queries data directly in S3 using SQL\n- No infrastructure to manage\n- Pay per query based on data scanned\n\n**Why A is incorrect:**\nRDS requires loading data into a database.\n\n**Why B is incorrect:**\nRedshift requires loading data into the data warehouse.\n\n**Why D is incorrect:**\nDynamoDB is NoSQL and requires data loading.",
      "reference": "https://docs.aws.amazon.com/athena/latest/ug/what-is.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "An API Gateway receives thousands of requests per second. To reduce backend load, responses that don't change frequently should be cached. Which feature should be enabled?",
      "options": [
        "A. API Gateway throttling",
        "B. API Gateway caching",
        "C. API Gateway usage plans",
        "D. API Gateway authorizers"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAPI Gateway Caching:\n- Caches endpoint responses\n- Reduces backend calls\n- Configurable TTL (time-to-live)\n- Improves latency and reduces load\n\n**Why A is incorrect:**\nThrottling limits request rate but doesn't cache responses.\n\n**Why C is incorrect:**\nUsage plans manage API access quotas, not caching.\n\n**Why D is incorrect:**\nAuthorizers handle authentication/authorization, not caching.",
      "reference": "https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company runs a machine learning inference workload that requires GPU instances. The workload is unpredictable and can vary from zero to thousands of requests per minute. Which compute option provides the best cost efficiency?",
      "options": [
        "A. Reserved EC2 GPU instances",
        "B. On-Demand EC2 GPU instances",
        "C. AWS Lambda with GPU",
        "D. Amazon SageMaker Serverless Inference"
      ],
      "correct_answer": "D",
      "explanation": "**Correct Answer: D**\n\n**Why D is correct:**\nSageMaker Serverless Inference:\n- Scales from zero to handle spikes\n- Pay only when inference is running\n- No idle capacity costs\n- Automatic scaling\n\n**Why A is incorrect:**\nReserved instances require upfront commitment and pay for idle time.\n\n**Why B is incorrect:**\nOn-Demand instances run continuously, paying for idle time.\n\n**Why C is incorrect:**\nLambda doesn't support GPU instances.",
      "reference": "https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 3,
      "question_text": "A company's application has a microservices architecture with services deployed across multiple AWS accounts. They need service-to-service communication without exposing services to the public internet. Which solution provides this securely?",
      "options": [
        "A. Internet Gateway with security groups",
        "B. AWS PrivateLink",
        "C. VPC Peering with public subnets",
        "D. NAT Gateway"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAWS PrivateLink:\n- Private connectivity between VPCs and services\n- Traffic stays on AWS network\n- Works across accounts\n- No internet exposure\n\n**Why A is incorrect:**\nInternet Gateway exposes services to the public internet.\n\n**Why C is incorrect:**\nVPC Peering with public subnets doesn't provide the same level of security.\n\n**Why D is incorrect:**\nNAT Gateway provides outbound internet access, not private service connectivity.",
      "reference": "https://docs.aws.amazon.com/vpc/latest/privatelink/what-is-privatelink.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company needs to run SQL queries against structured data stored in S3. They want the fastest query performance for interactive analytics. Which approach should they use?",
      "options": [
        "A. Amazon Athena with data in JSON format",
        "B. Amazon Athena with data in Parquet format",
        "C. Amazon S3 Select",
        "D. AWS Glue ETL jobs"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAthena with Parquet:\n- Parquet is columnar format optimized for analytics\n- Only reads columns needed for query\n- Reduces data scanned and improves performance\n- Supports compression for faster reads\n\n**Why A is incorrect:**\nJSON is row-based and requires scanning entire files.\n\n**Why C is incorrect:**\nS3 Select has limited SQL support compared to Athena.\n\n**Why D is incorrect:**\nGlue ETL is for transformation, not interactive queries.",
      "reference": "https://docs.aws.amazon.com/athena/latest/ug/columnar-storage.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A Lambda function processes events from an SQS queue. During peak times, messages back up in the queue. How can the architect improve throughput?",
      "options": [
        "A. Increase Lambda timeout",
        "B. Increase Lambda reserved concurrency",
        "C. Decrease SQS visibility timeout",
        "D. Use SQS long polling"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nIncreasing reserved concurrency:\n- Allows more Lambda instances to run simultaneously\n- Processes more messages in parallel\n- Reduces queue backlog faster\n- Each instance processes messages independently\n\n**Why A is incorrect:**\nTimeout affects individual execution time, not parallelism.\n\n**Why C is incorrect:**\nDecreasing visibility timeout could cause duplicate processing.\n\n**Why D is incorrect:**\nLong polling reduces empty responses but doesn't increase throughput.",
      "reference": "https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company needs to transcode video files uploaded to S3. Each video takes about 10 minutes to process. Which compute option is most appropriate?",
      "options": [
        "A. AWS Lambda",
        "B. Amazon EC2 with Auto Scaling",
        "C. AWS Batch",
        "D. Amazon ECS on Fargate"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nAWS Batch:\n- Designed for batch processing workloads\n- Automatically provisions compute resources\n- Supports jobs running for minutes to hours\n- Cost-effective using Spot Instances\n\n**Why A is incorrect:**\nLambda has a 15-minute timeout, risky for 10-minute jobs with variable processing times.\n\n**Why B is incorrect:**\nEC2 with Auto Scaling works but requires more management than Batch.\n\n**Why D is incorrect:**\nFargate works but Batch is purpose-built for batch processing.",
      "reference": "https://docs.aws.amazon.com/batch/latest/userguide/what-is-batch.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A gaming application needs to store player session data with microsecond latency. Data is accessed by session ID. Which database should be used?",
      "options": [
        "A. Amazon RDS MySQL",
        "B. Amazon DynamoDB with DAX",
        "C. Amazon Aurora",
        "D. Amazon ElastiCache Redis"
      ],
      "correct_answer": "D",
      "explanation": "**Correct Answer: D**\n\n**Why D is correct:**\nElastiCache Redis:\n- In-memory data store\n- Microsecond latency\n- Supports key-value lookups by session ID\n- Ideal for session data caching\n\n**Why A is incorrect:**\nRDS provides millisecond latency, not microsecond.\n\n**Why B is incorrect:**\nDynamoDB with DAX provides microsecond latency for cached reads but is more complex for simple session storage.\n\n**Why C is incorrect:**\nAurora provides millisecond latency, not microsecond.",
      "reference": "https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/WhatIs.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "An e-commerce site needs to display product recommendations in real-time based on user behavior. Which AWS service is designed for this?",
      "options": [
        "A. Amazon Personalize",
        "B. Amazon Comprehend",
        "C. Amazon Rekognition",
        "D. Amazon Polly"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nAmazon Personalize:\n- Machine learning service for personalized recommendations\n- Real-time recommendations based on user interactions\n- No ML expertise required\n- Supports e-commerce product recommendations\n\n**Why B is incorrect:**\nComprehend is for natural language processing (text analysis).\n\n**Why C is incorrect:**\nRekognition is for image and video analysis.\n\n**Why D is incorrect:**\nPolly is for text-to-speech conversion.",
      "reference": "https://docs.aws.amazon.com/personalize/latest/dg/what-is-personalize.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A web application behind an Application Load Balancer experiences slow response times during traffic spikes. The EC2 instances have high CPU utilization. What should be done to improve performance?",
      "options": [
        "A. Enable ALB access logs",
        "B. Add more instances to the target group",
        "C. Configure ALB idle timeout",
        "D. Enable ALB deletion protection"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAdding instances to target group:\n- Distributes load across more instances\n- Reduces CPU utilization per instance\n- Improves response times\n- Should be done via Auto Scaling for automation\n\n**Why A is incorrect:**\nAccess logs help with troubleshooting but don't improve performance.\n\n**Why C is incorrect:**\nIdle timeout affects connection duration, not processing capacity.\n\n**Why D is incorrect:**\nDeletion protection prevents accidental deletion, unrelated to performance.",
      "reference": "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "A company needs to manage encryption keys for their applications. They require automatic key rotation and audit logging. Which AWS service should they use?",
      "options": [
        "A. AWS Secrets Manager",
        "B. AWS KMS",
        "C. AWS Certificate Manager",
        "D. AWS CloudHSM"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAWS KMS (Key Management Service):\n- Manages encryption keys\n- Automatic key rotation available\n- Integrated with CloudTrail for audit logging\n- Used by most AWS services for encryption\n\n**Why A is incorrect:**\nSecrets Manager stores secrets (credentials), not encryption keys.\n\n**Why C is incorrect:**\nCertificate Manager handles SSL/TLS certificates, not encryption keys.\n\n**Why D is incorrect:**\nCloudHSM provides dedicated hardware but doesn't have automatic rotation.",
      "reference": "https://docs.aws.amazon.com/kms/latest/developerguide/overview.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "An application needs to store database credentials securely and rotate them automatically. Which AWS service provides this capability?",
      "options": [
        "A. AWS Systems Manager Parameter Store",
        "B. AWS Secrets Manager",
        "C. AWS KMS",
        "D. Amazon S3 with encryption"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAWS Secrets Manager:\n- Stores credentials and secrets securely\n- Automatic rotation for RDS, Redshift, DocumentDB\n- Built-in Lambda functions for rotation\n- Encrypted using KMS\n\n**Why A is incorrect:**\nParameter Store can store secrets but doesn't have built-in automatic rotation.\n\n**Why C is incorrect:**\nKMS manages encryption keys, not application secrets.\n\n**Why D is incorrect:**\nS3 is not designed for credential management.",
      "reference": "https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "A company wants to enforce MFA for all IAM users accessing the AWS Console. How should this be implemented?",
      "options": [
        "A. Create an SCP in AWS Organizations",
        "B. Create an IAM policy requiring MFA",
        "C. Enable MFA on the root account only",
        "D. Configure AWS Config rules"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nIAM policy with MFA condition:\n- Denies access unless MFA is authenticated\n- Uses aws:MultiFactorAuthPresent condition\n- Can be attached to users or groups\n- Enforces MFA at the policy level\n\n**Why A is incorrect:**\nSCPs work at the organization level but are better for restricting services, not enforcing MFA.\n\n**Why C is incorrect:**\nThis doesn't enforce MFA for all IAM users.\n\n**Why D is incorrect:**\nConfig rules can check compliance but don't enforce MFA.",
      "reference": "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa_configure-api-require.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "A company needs to protect their web application from SQL injection and cross-site scripting (XSS) attacks. Which AWS service should they use?",
      "options": [
        "A. AWS Shield",
        "B. AWS WAF",
        "C. Amazon GuardDuty",
        "D. AWS Network Firewall"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAWS WAF (Web Application Firewall):\n- Protects against SQL injection\n- Protects against XSS attacks\n- Customizable rules\n- Integrates with CloudFront, ALB, API Gateway\n\n**Why A is incorrect:**\nShield protects against DDoS attacks, not application-layer attacks.\n\n**Why C is incorrect:**\nGuardDuty detects threats but doesn't block attacks.\n\n**Why D is incorrect:**\nNetwork Firewall operates at network layer, not application layer.",
      "reference": "https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "A company needs to monitor their AWS accounts for malicious activity and unauthorized behavior. Which service provides continuous security monitoring?",
      "options": [
        "A. AWS CloudTrail",
        "B. Amazon GuardDuty",
        "C. AWS Config",
        "D. Amazon Inspector"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAmazon GuardDuty:\n- Continuous security monitoring\n- Threat detection using ML\n- Analyzes CloudTrail, VPC Flow Logs, DNS logs\n- Detects account compromise, reconnaissance, etc.\n\n**Why A is incorrect:**\nCloudTrail logs API calls but doesn't analyze for threats.\n\n**Why C is incorrect:**\nConfig tracks resource configurations, not security threats.\n\n**Why D is incorrect:**\nInspector assesses EC2 instances for vulnerabilities, not account-wide threats.",
      "reference": "https://docs.aws.amazon.com/guardduty/latest/ug/what-is-guardduty.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "A company requires that all EBS volumes be encrypted with customer-managed keys. How can they enforce this across all accounts in their organization?",
      "options": [
        "A. Enable EBS encryption by default in each account",
        "B. Use an SCP to deny ec2:CreateVolume without encryption",
        "C. Create an AWS Config rule",
        "D. Use IAM policies in each account"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nService Control Policies (SCPs):\n- Apply to all accounts in the organization\n- Can deny actions that don't meet requirements\n- Enforced at the organization level\n- Cannot be overridden by account admins\n\n**Why A is incorrect:**\nDefault encryption must be set per account and can be disabled.\n\n**Why C is incorrect:**\nConfig rules detect but don't prevent non-compliant resources.\n\n**Why D is incorrect:**\nIAM policies must be maintained in each account separately.",
      "reference": "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "A company needs to ensure their application remains available if an entire AWS Region fails. Which architecture approach should they use?",
      "options": [
        "A. Multi-AZ deployment",
        "B. Multi-Region deployment with Route 53 failover",
        "C. Auto Scaling across multiple AZs",
        "D. Reserved Instances in multiple AZs"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nMulti-Region with Route 53 failover:\n- Application deployed in multiple Regions\n- Route 53 health checks detect Region failure\n- Automatic failover to healthy Region\n- Provides Region-level resilience\n\n**Why A is incorrect:**\nMulti-AZ protects against AZ failure, not Region failure.\n\n**Why C is incorrect:**\nAuto Scaling within a Region doesn't protect against Region failure.\n\n**Why D is incorrect:**\nReserved Instances are a pricing model, not an availability solution.",
      "reference": "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "An application must recover from disasters with an RTO of 4 hours and RPO of 1 hour. Which disaster recovery strategy is most appropriate?",
      "options": [
        "A. Backup and Restore",
        "B. Pilot Light",
        "C. Warm Standby",
        "D. Multi-Site Active/Active"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nWarm Standby:\n- Scaled-down version running in DR Region\n- Can scale up quickly during disaster\n- RTO: hours, RPO: minutes to hours\n- Good balance of cost and recovery time\n\n**Why A is incorrect:**\nBackup/Restore has longer RTO (hours to days).\n\n**Why B is incorrect:**\nPilot Light has longer RTO as infrastructure must be scaled up.\n\n**Why D is incorrect:**\nMulti-Site is more expensive than needed for these requirements.",
      "reference": "https://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads-on-aws/disaster-recovery-options-in-the-cloud.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "An S3 bucket contains critical data that must be protected against accidental deletion. Which S3 features should be enabled? (Choose TWO)",
      "options": [
        "A. S3 Versioning",
        "B. S3 Transfer Acceleration",
        "C. MFA Delete",
        "D. S3 Intelligent-Tiering"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A and C**\n\n**Why A is correct:**\nS3 Versioning:\n- Preserves all versions of objects\n- Deleted objects can be recovered\n- Protects against accidental overwrites\n\n**Why C is correct:**\nMFA Delete:\n- Requires MFA to permanently delete versions\n- Adds extra protection against malicious deletion\n\n**Why B is incorrect:**\nTransfer Acceleration speeds up uploads, not deletion protection.\n\n**Why D is incorrect:**\nIntelligent-Tiering optimizes costs, not deletion protection.",
      "reference": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/Versioning.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "A company runs a critical database on RDS. They need automatic failover with minimal data loss if the primary instance fails. Which feature should be enabled?",
      "options": [
        "A. Read Replicas",
        "B. Multi-AZ deployment",
        "C. Automated backups",
        "D. Enhanced monitoring"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nMulti-AZ deployment:\n- Synchronous replication to standby\n- Automatic failover (1-2 minutes)\n- No data loss during failover\n- Standby in different AZ\n\n**Why A is incorrect:**\nRead Replicas use async replication and don't provide automatic failover.\n\n**Why C is incorrect:**\nAutomated backups are for recovery, not automatic failover.\n\n**Why D is incorrect:**\nEnhanced monitoring provides metrics, not failover capability.",
      "reference": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "An application uses SQS for message processing. If a message cannot be processed after multiple attempts, where should it be sent for analysis?",
      "options": [
        "A. Another SQS queue",
        "B. Dead-letter queue (DLQ)",
        "C. SNS topic",
        "D. S3 bucket"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nDead-letter queue (DLQ):\n- Captures messages that fail processing\n- Configured with maxReceiveCount\n- Allows analysis of failed messages\n- Prevents poison messages from blocking queue\n\n**Why A is incorrect:**\nRegular queue doesn't provide special handling for failed messages.\n\n**Why C is incorrect:**\nSNS is for notifications, not storing failed messages.\n\n**Why D is incorrect:**\nS3 is not integrated with SQS for failed message handling.",
      "reference": "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "A Lambda function occasionally fails due to timeout when calling an external API. How can the architect improve reliability?",
      "options": [
        "A. Increase Lambda memory",
        "B. Implement retry logic with exponential backoff",
        "C. Decrease Lambda timeout",
        "D. Use synchronous invocation"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nRetry with exponential backoff:\n- Retries failed requests with increasing delays\n- Handles transient failures gracefully\n- Prevents overwhelming the external API\n- Standard reliability pattern\n\n**Why A is incorrect:**\nMemory affects CPU allocation, not external API reliability.\n\n**Why C is incorrect:**\nDecreasing timeout would cause more failures.\n\n**Why D is incorrect:**\nInvocation type doesn't affect external API reliability.",
      "reference": "https://docs.aws.amazon.com/general/latest/gr/api-retries.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company needs to process clickstream data from their website in real-time and load it into Redshift for analytics. Which architecture is most appropriate?",
      "options": [
        "A. Kinesis Data Streams → Lambda → Redshift",
        "B. Kinesis Data Firehose → Redshift",
        "C. SQS → Lambda → Redshift",
        "D. SNS → Lambda → Redshift"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nKinesis Data Firehose to Redshift:\n- Fully managed delivery stream\n- Native Redshift integration\n- Automatic batching and loading\n- No code required\n\n**Why A is incorrect:**\nMore complex, requires custom Lambda code.\n\n**Why C is incorrect:**\nSQS is for message queuing, not streaming analytics.\n\n**Why D is incorrect:**\nSNS is for notifications, not data streaming.",
      "reference": "https://docs.aws.amazon.com/firehose/latest/dev/create-destination.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "An application needs to query DynamoDB with complex filters on non-key attributes. Scans are slow and expensive. What should be created?",
      "options": [
        "A. Local Secondary Index",
        "B. Global Secondary Index",
        "C. DAX cluster",
        "D. Read Replica"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nGlobal Secondary Index (GSI):\n- Allows queries on non-key attributes\n- Has its own partition and sort keys\n- Avoids expensive table scans\n- Can be created on existing tables\n\n**Why A is incorrect:**\nLSI must be created at table creation and uses same partition key.\n\n**Why C is incorrect:**\nDAX is for caching, doesn't help with query patterns.\n\n**Why D is incorrect:**\nDynamoDB doesn't have read replicas like RDS.",
      "reference": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company runs Windows file shares that need to be migrated to AWS while maintaining SMB protocol compatibility. Which service should they use?",
      "options": [
        "A. Amazon EFS",
        "B. Amazon FSx for Windows File Server",
        "C. Amazon S3",
        "D. AWS Storage Gateway"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nFSx for Windows File Server:\n- Native Windows file system\n- Full SMB protocol support\n- Active Directory integration\n- NTFS permissions\n\n**Why A is incorrect:**\nEFS uses NFS protocol, not SMB.\n\n**Why C is incorrect:**\nS3 is object storage, not a file system.\n\n**Why D is incorrect:**\nStorage Gateway is for hybrid scenarios, FSx is fully managed.",
      "reference": "https://docs.aws.amazon.com/fsx/latest/WindowsGuide/what-is.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "An image processing application needs to resize images immediately after they're uploaded to S3. Which integration should be used?",
      "options": [
        "A. S3 Event Notification to Lambda",
        "B. S3 Replication",
        "C. S3 Batch Operations",
        "D. S3 Lifecycle Policy"
      ],
      "correct_answer": "A",
      "explanation": "**Correct Answer: A**\n\n**Why A is correct:**\nS3 Event Notification to Lambda:\n- Triggers automatically on object upload\n- Lambda processes image immediately\n- Serverless, scales automatically\n- Event-driven architecture\n\n**Why B is incorrect:**\nReplication copies objects but doesn't process them.\n\n**Why C is incorrect:**\nBatch Operations is for bulk processing existing objects.\n\n**Why D is incorrect:**\nLifecycle policies manage storage class transitions, not processing.",
      "reference": "https://docs.aws.amazon.com/lambda/latest/dg/with-s3.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company needs to run Apache Spark workloads for big data processing. They want a managed service that handles cluster provisioning. Which service should they use?",
      "options": [
        "A. Amazon EC2",
        "B. Amazon EMR",
        "C. AWS Glue",
        "D. Amazon Kinesis"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAmazon EMR (Elastic MapReduce):\n- Managed Hadoop/Spark clusters\n- Handles provisioning and configuration\n- Supports Spark, Hive, Presto, etc.\n- Scalable and cost-effective\n\n**Why A is incorrect:**\nEC2 requires manual cluster management.\n\n**Why C is incorrect:**\nGlue is serverless ETL, different use case than EMR.\n\n**Why D is incorrect:**\nKinesis is for streaming, not batch Spark processing.",
      "reference": "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-what-is-emr.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "An API receives traffic from clients worldwide. To reduce latency for all users, where should the API be deployed?",
      "options": [
        "A. Single Region with CloudFront",
        "B. Multiple Regions with Route 53 latency-based routing",
        "C. Single Region with larger instance sizes",
        "D. Multiple AZs within one Region"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nMulti-Region with latency-based routing:\n- API deployed in Regions close to users\n- Route 53 directs users to lowest-latency Region\n- Reduces network latency globally\n- Improves response times for all users\n\n**Why A is incorrect:**\nCloudFront caches content but API responses are often dynamic.\n\n**Why C is incorrect:**\nLarger instances don't reduce network latency.\n\n**Why D is incorrect:**\nMultiple AZs in one Region don't help global latency.",
      "reference": "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-latency.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company needs to search through large amounts of text data including documents, emails, and support tickets. Which AWS service provides enterprise search capabilities?",
      "options": [
        "A. Amazon Elasticsearch Service",
        "B. Amazon Kendra",
        "C. Amazon CloudSearch",
        "D. Amazon Comprehend"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAmazon Kendra:\n- Intelligent enterprise search\n- Natural language queries\n- Machine learning powered\n- Connectors for various data sources\n\n**Why A is incorrect:**\nElasticsearch requires more configuration and expertise.\n\n**Why C is incorrect:**\nCloudSearch is basic search, less intelligent than Kendra.\n\n**Why D is incorrect:**\nComprehend analyzes text, doesn't provide search.",
      "reference": "https://docs.aws.amazon.com/kendra/latest/dg/what-is-kendra.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A monolithic application is being re-architected to microservices. Services need to communicate asynchronously. Which pattern should be implemented?",
      "options": [
        "A. Direct HTTP calls between services",
        "B. Event-driven architecture with SNS/SQS",
        "C. Shared database",
        "D. FTP file exchange"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nEvent-driven with SNS/SQS:\n- Loose coupling between services\n- Asynchronous communication\n- Services can evolve independently\n- Better fault tolerance\n\n**Why A is incorrect:**\nDirect calls create tight coupling.\n\n**Why C is incorrect:**\nShared database creates coupling and scaling issues.\n\n**Why D is incorrect:**\nFTP is outdated and not cloud-native.",
      "reference": "https://docs.aws.amazon.com/whitepapers/latest/microservices-on-aws/microservices-on-aws.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company needs to run Kubernetes workloads without managing the control plane. Which AWS service should they use?",
      "options": [
        "A. Amazon ECS",
        "B. Amazon EKS",
        "C. AWS Fargate",
        "D. AWS App Runner"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAmazon EKS (Elastic Kubernetes Service):\n- Managed Kubernetes service\n- AWS manages the control plane\n- Compatible with Kubernetes ecosystem\n- Can run on EC2 or Fargate\n\n**Why A is incorrect:**\nECS is AWS's proprietary container orchestration, not Kubernetes.\n\n**Why C is incorrect:**\nFargate is a compute option, not an orchestration service.\n\n**Why D is incorrect:**\nApp Runner is for simple containerized apps, not full Kubernetes.",
      "reference": "https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company's data warehouse queries are slow when analyzing recent data mixed with historical data. Which Redshift feature can improve performance?",
      "options": [
        "A. Redshift Spectrum",
        "B. Concurrency Scaling",
        "C. Workload Management (WLM)",
        "D. Result caching"
      ],
      "correct_answer": "D",
      "explanation": "**Correct Answer: D**\n\n**Why D is correct:**\nResult caching:\n- Caches query results\n- Identical queries return cached results immediately\n- Reduces query execution time\n- Automatic and enabled by default\n\n**Why A is incorrect:**\nSpectrum queries S3 data, doesn't speed up regular queries.\n\n**Why B is incorrect:**\nConcurrency Scaling handles more concurrent queries.\n\n**Why C is incorrect:**\nWLM manages query priorities, doesn't cache results.",
      "reference": "https://docs.aws.amazon.com/redshift/latest/dg/c_challenges_achieving_high_performance_queries.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "4",
      "difficulty": 2,
      "question_text": "A company runs development environments that are only used during business hours (8am-6pm). How can they reduce costs?",
      "options": [
        "A. Use Reserved Instances",
        "B. Use AWS Instance Scheduler",
        "C. Use Dedicated Hosts",
        "D. Use Larger instance sizes"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAWS Instance Scheduler:\n- Automatically starts/stops instances on schedule\n- Runs instances only during business hours\n- Can reduce costs by 50-70%\n- Works with EC2 and RDS\n\n**Why A is incorrect:**\nReserved Instances require 24/7 commitment.\n\n**Why C is incorrect:**\nDedicated Hosts are more expensive.\n\n**Why D is incorrect:**\nLarger instances cost more, not less.",
      "reference": "https://docs.aws.amazon.com/solutions/latest/instance-scheduler-on-aws/welcome.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "4",
      "difficulty": 2,
      "question_text": "A company has unpredictable workloads with occasional traffic spikes. They want to optimize costs without Reserved Instance commitment. Which pricing model should they consider?",
      "options": [
        "A. On-Demand Instances only",
        "B. Savings Plans",
        "C. Spot Instances for everything",
        "D. Dedicated Instances"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nSavings Plans:\n- Flexible commitment ($/hour)\n- Applies to any instance type/family\n- Up to 72% savings vs On-Demand\n- More flexible than Reserved Instances\n\n**Why A is incorrect:**\nOn-Demand is most expensive with no discounts.\n\n**Why C is incorrect:**\nSpot can be interrupted, not suitable for all workloads.\n\n**Why D is incorrect:**\nDedicated Instances are more expensive.",
      "reference": "https://docs.aws.amazon.com/savingsplans/latest/userguide/what-is-savings-plans.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "4",
      "difficulty": 2,
      "question_text": "A company stores large amounts of data in S3 but doesn't know the access patterns. Which storage class automatically optimizes costs based on access?",
      "options": [
        "A. S3 Standard",
        "B. S3 Standard-IA",
        "C. S3 Intelligent-Tiering",
        "D. S3 Glacier"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nS3 Intelligent-Tiering:\n- Automatically moves objects between tiers\n- No retrieval fees\n- Monitors access patterns\n- Optimizes costs without manual intervention\n\n**Why A is incorrect:**\nStandard doesn't automatically optimize costs.\n\n**Why B is incorrect:**\nStandard-IA requires knowing access patterns upfront.\n\n**Why D is incorrect:**\nGlacier requires manual lifecycle policies.",
      "reference": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/intelligent-tiering.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "4",
      "difficulty": 2,
      "question_text": "A company wants to analyze their AWS spending and receive recommendations for cost optimization. Which AWS service provides this?",
      "options": [
        "A. AWS Budgets",
        "B. AWS Cost Explorer",
        "C. AWS Trusted Advisor",
        "D. AWS Cost and Usage Report"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nAWS Trusted Advisor:\n- Provides cost optimization recommendations\n- Identifies idle resources\n- Suggests Reserved Instance purchases\n- Checks for underutilized resources\n\n**Why A is incorrect:**\nBudgets tracks spending against limits but doesn't recommend optimizations.\n\n**Why B is incorrect:**\nCost Explorer visualizes costs but limited recommendations.\n\n**Why D is incorrect:**\nCUR provides detailed billing data, not recommendations.",
      "reference": "https://docs.aws.amazon.com/awssupport/latest/user/trusted-advisor.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "4",
      "difficulty": 2,
      "question_text": "A company has multiple AWS accounts and wants consolidated billing with volume discounts. Which service should they use?",
      "options": [
        "A. AWS Cost Explorer",
        "B. AWS Organizations",
        "C. AWS Budgets",
        "D. AWS Control Tower"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAWS Organizations:\n- Consolidated billing across accounts\n- Volume discounts aggregated\n- Shared Reserved Instances\n- Centralized cost management\n\n**Why A is incorrect:**\nCost Explorer analyzes costs but doesn't consolidate billing.\n\n**Why C is incorrect:**\nBudgets sets spending alerts, not consolidated billing.\n\n**Why D is incorrect:**\nControl Tower sets up accounts but uses Organizations for billing.",
      "reference": "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_introduction.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company needs to reduce cold start latency for their Lambda functions that serve an API. Which feature should they enable?",
      "options": [
        "A. Lambda Layers",
        "B. Lambda Provisioned Concurrency",
        "C. Lambda Reserved Concurrency",
        "D. Lambda Destinations"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nLambda Provisioned Concurrency:\n- Keeps functions initialized and ready\n- Eliminates cold start latency\n- Consistent response times\n- Ideal for latency-sensitive APIs\n\n**Why A is incorrect:**\nLayers share code but don't reduce cold starts.\n\n**Why C is incorrect:**\nReserved Concurrency limits concurrent executions, doesn't reduce cold starts.\n\n**Why D is incorrect:**\nDestinations route async results, unrelated to cold starts.",
      "reference": "https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A web application needs to maintain WebSocket connections for real-time updates. Which AWS service supports WebSocket APIs?",
      "options": [
        "A. Application Load Balancer only",
        "B. Amazon API Gateway",
        "C. Amazon CloudFront",
        "D. AWS AppSync only"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAmazon API Gateway:\n- Native WebSocket API support\n- Manages persistent connections\n- Integrates with Lambda for message handling\n- Scales automatically\n\n**Why A is incorrect:**\nALB supports WebSocket but needs backend to manage connections.\n\n**Why C is incorrect:**\nCloudFront doesn't manage WebSocket APIs directly.\n\n**Why D is incorrect:**\nAppSync supports real-time but API Gateway has broader WebSocket support.",
      "reference": "https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-websocket-api.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "An application needs to process millions of short-lived tasks per day. Each task takes less than 15 minutes. Which compute option is most cost-effective?",
      "options": [
        "A. EC2 On-Demand instances",
        "B. AWS Lambda",
        "C. EC2 Reserved Instances",
        "D. Amazon ECS on EC2"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAWS Lambda:\n- Pay only for execution time\n- No idle costs\n- Automatic scaling\n- Perfect for short tasks under 15 minutes\n\n**Why A is incorrect:**\nEC2 On-Demand has idle time costs between tasks.\n\n**Why C is incorrect:**\nReserved Instances require commitment and have idle costs.\n\n**Why D is incorrect:**\nECS on EC2 requires instance management and has idle costs.",
      "reference": "https://docs.aws.amazon.com/lambda/latest/dg/welcome.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company needs to run graphics-intensive workloads requiring GPU access. Which EC2 instance family should they use?",
      "options": [
        "A. M5 (General Purpose)",
        "B. C5 (Compute Optimized)",
        "C. P4 or G5 (Accelerated Computing)",
        "D. R5 (Memory Optimized)"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nP4 and G5 instances:\n- GPU-equipped instances\n- P4 for ML training\n- G5 for graphics rendering and inference\n- Support NVIDIA GPUs\n\n**Why A is incorrect:**\nM5 is general purpose without GPU.\n\n**Why B is incorrect:**\nC5 is compute optimized but without GPU.\n\n**Why D is incorrect:**\nR5 is memory optimized without GPU.",
      "reference": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/accelerated-computing-instances.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A distributed application needs to coordinate tasks across multiple components. Which AWS service provides workflow orchestration?",
      "options": [
        "A. Amazon SQS",
        "B. AWS Step Functions",
        "C. Amazon SNS",
        "D. Amazon EventBridge"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAWS Step Functions:\n- Visual workflow orchestration\n- Coordinates multiple services\n- Handles retries and error handling\n- State machines for complex flows\n\n**Why A is incorrect:**\nSQS is message queuing, not orchestration.\n\n**Why C is incorrect:**\nSNS is pub/sub notification, not workflow.\n\n**Why D is incorrect:**\nEventBridge routes events but doesn't orchestrate workflows.",
      "reference": "https://docs.aws.amazon.com/step-functions/latest/dg/welcome.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company needs to reduce database read load. Queries are repetitive and data doesn't change frequently. What should be implemented?",
      "options": [
        "A. Database sharding",
        "B. Query result caching with ElastiCache",
        "C. Multi-AZ deployment",
        "D. Increased IOPS"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nElastiCache for query caching:\n- Caches frequent query results\n- Reduces database load\n- Sub-millisecond response times\n- Perfect for repetitive queries\n\n**Why A is incorrect:**\nSharding distributes data but adds complexity.\n\n**Why C is incorrect:**\nMulti-AZ provides HA, not read performance.\n\n**Why D is incorrect:**\nIncreased IOPS helps but caching is more effective for repetitive queries.",
      "reference": "https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/WhatIs.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "An application needs to process events from multiple sources and route them to different targets based on rules. Which AWS service should be used?",
      "options": [
        "A. Amazon SQS",
        "B. Amazon SNS",
        "C. Amazon EventBridge",
        "D. AWS Lambda"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nAmazon EventBridge:\n- Event bus for routing events\n- Rules-based routing to targets\n- Supports multiple event sources\n- Schema registry for event discovery\n\n**Why A is incorrect:**\nSQS is point-to-point queuing, not event routing.\n\n**Why B is incorrect:**\nSNS is pub/sub without complex routing rules.\n\n**Why D is incorrect:**\nLambda processes events but doesn't route them.",
      "reference": "https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-what-is.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A data pipeline needs to transform CSV files into Parquet format for analytics. Which serverless service should be used?",
      "options": [
        "A. Amazon EMR",
        "B. AWS Glue",
        "C. Amazon Kinesis",
        "D. AWS Batch"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAWS Glue:\n- Serverless ETL service\n- Native Parquet conversion\n- Automatic schema discovery\n- No infrastructure management\n\n**Why A is incorrect:**\nEMR requires cluster management.\n\n**Why C is incorrect:**\nKinesis is for streaming, not batch ETL.\n\n**Why D is incorrect:**\nBatch requires custom code for transformation.",
      "reference": "https://docs.aws.amazon.com/glue/latest/dg/what-is-glue.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company wants to automatically scale DynamoDB table capacity during traffic spikes without manual intervention. Which feature should they enable?",
      "options": [
        "A. DynamoDB Streams",
        "B. DynamoDB Auto Scaling",
        "C. DynamoDB DAX",
        "D. DynamoDB Global Tables"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nDynamoDB Auto Scaling:\n- Automatically adjusts throughput capacity\n- Based on actual traffic patterns\n- Maintains performance during spikes\n- Reduces costs during low traffic\n\n**Why A is incorrect:**\nStreams capture data changes, not scaling.\n\n**Why C is incorrect:**\nDAX is for caching, not capacity scaling.\n\n**Why D is incorrect:**\nGlobal Tables provide multi-Region replication, not auto scaling.",
      "reference": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A website serves both static and dynamic content. Static content should be cached at edge locations. Which architecture is recommended?",
      "options": [
        "A. S3 static website only",
        "B. CloudFront with S3 for static and ALB for dynamic",
        "C. EC2 instances for everything",
        "D. API Gateway for static content"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nCloudFront with multiple origins:\n- S3 origin for static content (cached at edge)\n- ALB origin for dynamic content\n- Path-based routing\n- Best performance and cost efficiency\n\n**Why A is incorrect:**\nS3 website doesn't handle dynamic content.\n\n**Why C is incorrect:**\nEC2 for static content is inefficient and not cached globally.\n\n**Why D is incorrect:**\nAPI Gateway is for APIs, not static content serving.",
      "reference": "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/DownloadDistS3AndCustomOrigins.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "A company needs to ensure all API calls to their AWS account are logged for compliance. Which service should be enabled?",
      "options": [
        "A. Amazon CloudWatch Logs",
        "B. AWS CloudTrail",
        "C. AWS Config",
        "D. VPC Flow Logs"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAWS CloudTrail:\n- Logs all API calls to AWS services\n- Records who, what, when, where\n- Required for compliance auditing\n- Supports multi-Region trails\n\n**Why A is incorrect:**\nCloudWatch Logs stores application logs, not API audit logs.\n\n**Why C is incorrect:**\nConfig tracks resource configurations, not API calls.\n\n**Why D is incorrect:**\nFlow Logs capture network traffic, not API calls.",
      "reference": "https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-user-guide.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "A company wants to prevent accidental deletion of an S3 bucket containing critical data. Which feature should be enabled?",
      "options": [
        "A. S3 Versioning",
        "B. S3 Object Lock",
        "C. S3 Bucket Policy denying delete",
        "D. Enable MFA Delete and add bucket policy"
      ],
      "correct_answer": "D",
      "explanation": "**Correct Answer: D**\n\n**Why D is correct:**\nMFA Delete with bucket policy:\n- MFA required for bucket deletion\n- Bucket policy can deny DeleteBucket\n- Multiple layers of protection\n- Strongest protection against accidental deletion\n\n**Why A is incorrect:**\nVersioning protects objects, not bucket deletion.\n\n**Why B is incorrect:**\nObject Lock protects objects, not the bucket itself.\n\n**Why C is incorrect:**\nPolicy alone can be modified; MFA adds extra protection.",
      "reference": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/MultiFactorAuthenticationDelete.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "An application running on EC2 needs to access S3 buckets. How should credentials be provided securely?",
      "options": [
        "A. Store access keys in environment variables",
        "B. Use an IAM role attached to the EC2 instance",
        "C. Store credentials in a configuration file",
        "D. Hard-code credentials in the application"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nIAM Role for EC2:\n- No long-term credentials to manage\n- Automatically rotated temporary credentials\n- Best security practice\n- AWS SDK automatically uses role credentials\n\n**Why A is incorrect:**\nEnvironment variables with access keys can be leaked.\n\n**Why C is incorrect:**\nConfiguration files can be accessed if compromised.\n\n**Why D is incorrect:**\nHard-coded credentials are extremely insecure.",
      "reference": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "A company needs to restrict access to specific S3 objects based on the requesting user's department tag in IAM. Which feature enables this?",
      "options": [
        "A. S3 Bucket Policies",
        "B. S3 ACLs",
        "C. Attribute-Based Access Control (ABAC)",
        "D. S3 Object Ownership"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nAttribute-Based Access Control:\n- Uses tags for access decisions\n- IAM policies with condition keys\n- Scales with organization\n- Department tag can control S3 access\n\n**Why A is incorrect:**\nBucket policies don't use IAM user tags directly.\n\n**Why B is incorrect:**\nACLs are legacy and don't support tag-based access.\n\n**Why D is incorrect:**\nObject Ownership controls ACL behavior, not tag-based access.",
      "reference": "https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction_attribute-based-access-control.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "A company needs to protect their DDoS-sensitive application. Which AWS service provides advanced DDoS protection?",
      "options": [
        "A. AWS WAF",
        "B. AWS Shield Advanced",
        "C. Security Groups",
        "D. Network ACLs"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAWS Shield Advanced:\n- Enhanced DDoS protection\n- Layer 3, 4, and 7 protection\n- 24/7 DDoS Response Team access\n- Cost protection during attacks\n\n**Why A is incorrect:**\nWAF protects against application attacks but not all DDoS types.\n\n**Why C is incorrect:**\nSecurity groups don't protect against DDoS attacks.\n\n**Why D is incorrect:**\nNACLs provide basic filtering but not DDoS protection.",
      "reference": "https://docs.aws.amazon.com/waf/latest/developerguide/shield-chapter.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "A company needs to scan EC2 instances for security vulnerabilities and compliance violations. Which service should they use?",
      "options": [
        "A. Amazon GuardDuty",
        "B. Amazon Inspector",
        "C. AWS Config",
        "D. AWS Trusted Advisor"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAmazon Inspector:\n- Automated vulnerability scanning\n- Checks for software vulnerabilities\n- Network exposure analysis\n- Compliance assessment\n\n**Why A is incorrect:**\nGuardDuty detects threats but doesn't scan for vulnerabilities.\n\n**Why C is incorrect:**\nConfig checks resource configurations, not vulnerabilities.\n\n**Why D is incorrect:**\nTrusted Advisor provides best practice recommendations, not vulnerability scanning.",
      "reference": "https://docs.aws.amazon.com/inspector/latest/user/what-is-inspector.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "A company needs to centrally manage security findings across multiple AWS accounts. Which service provides this capability?",
      "options": [
        "A. AWS CloudTrail",
        "B. AWS Security Hub",
        "C. Amazon Macie",
        "D. AWS Config"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAWS Security Hub:\n- Centralized security view\n- Aggregates findings from multiple services\n- Works across accounts with Organizations\n- Security standards compliance checks\n\n**Why A is incorrect:**\nCloudTrail logs API calls but doesn't aggregate security findings.\n\n**Why C is incorrect:**\nMacie focuses on S3 data security, not central aggregation.\n\n**Why D is incorrect:**\nConfig tracks resources but doesn't aggregate security findings.",
      "reference": "https://docs.aws.amazon.com/securityhub/latest/userguide/what-is-securityhub.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "A company needs to discover and protect sensitive data like PII stored in S3. Which AWS service is designed for this?",
      "options": [
        "A. Amazon Inspector",
        "B. Amazon Macie",
        "C. AWS Config",
        "D. Amazon GuardDuty"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAmazon Macie:\n- Discovers sensitive data in S3\n- Uses ML to identify PII, PHI, financial data\n- Alerts on data security issues\n- Helps with compliance\n\n**Why A is incorrect:**\nInspector scans EC2 for vulnerabilities, not S3 data.\n\n**Why C is incorrect:**\nConfig tracks configurations, not data content.\n\n**Why D is incorrect:**\nGuardDuty detects threats, not sensitive data.",
      "reference": "https://docs.aws.amazon.com/macie/latest/user/what-is-macie.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "A company needs their web application to remain available even if an entire Availability Zone fails. How should the architecture be designed?",
      "options": [
        "A. Deploy to a single AZ with Auto Scaling",
        "B. Deploy across multiple AZs with a load balancer",
        "C. Use a larger instance in one AZ",
        "D. Enable detailed monitoring"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nMulti-AZ with load balancer:\n- Distributes traffic across AZs\n- Continues if one AZ fails\n- Load balancer routes to healthy targets\n- Best practice for high availability\n\n**Why A is incorrect:**\nSingle AZ fails entirely if that AZ goes down.\n\n**Why C is incorrect:**\nLarger instance doesn't provide AZ redundancy.\n\n**Why D is incorrect:**\nMonitoring doesn't provide fault tolerance.",
      "reference": "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-getting-started.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "An Auto Scaling group launches instances but they fail health checks and get terminated immediately. What is the likely issue?",
      "options": [
        "A. Instance type is too small",
        "B. Health check grace period is too short",
        "C. Maximum capacity is reached",
        "D. Scaling cooldown is too long"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nHealth check grace period:\n- Time to wait before starting health checks\n- If too short, instance terminated before ready\n- Must allow time for application startup\n- Common issue with slow-starting apps\n\n**Why A is incorrect:**\nSmall instance type wouldn't cause immediate termination.\n\n**Why C is incorrect:**\nMax capacity prevents new launches, not termination.\n\n**Why D is incorrect:**\nCooldown affects scaling actions, not health checks.",
      "reference": "https://docs.aws.amazon.com/autoscaling/ec2/userguide/healthcheck.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "A company wants to replicate their S3 bucket to another Region for disaster recovery. Which feature should they enable?",
      "options": [
        "A. S3 Versioning only",
        "B. S3 Cross-Region Replication",
        "C. S3 Transfer Acceleration",
        "D. S3 Lifecycle policies"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nS3 Cross-Region Replication (CRR):\n- Automatically replicates objects to another Region\n- Provides geographic redundancy\n- Supports disaster recovery\n- Requires versioning enabled\n\n**Why A is incorrect:**\nVersioning alone doesn't replicate to another Region.\n\n**Why C is incorrect:**\nTransfer Acceleration speeds uploads, not replication.\n\n**Why D is incorrect:**\nLifecycle policies manage storage classes, not cross-Region replication.",
      "reference": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "A company's application must continue operating even if the primary database fails. The failover must be automatic with minimal data loss. Which RDS configuration meets this requirement?",
      "options": [
        "A. Single-AZ with automated backups",
        "B. Multi-AZ deployment",
        "C. Read Replicas only",
        "D. Single-AZ with manual snapshots"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nMulti-AZ deployment:\n- Synchronous replication (no data loss)\n- Automatic failover (1-2 minutes)\n- Standby takes over if primary fails\n- No manual intervention required\n\n**Why A is incorrect:**\nSingle-AZ requires manual recovery and has data loss risk.\n\n**Why C is incorrect:**\nRead Replicas are async and require manual promotion.\n\n**Why D is incorrect:**\nManual snapshots require intervention and have higher RPO.",
      "reference": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "A company needs to ensure their EBS volumes survive the failure of the underlying hardware. How are EBS volumes protected by default?",
      "options": [
        "A. EBS volumes replicate across multiple AZs",
        "B. EBS volumes replicate within the same AZ",
        "C. EBS volumes are backed up to S3 automatically",
        "D. EBS volumes have no built-in redundancy"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nEBS replication within AZ:\n- Automatically replicates within the AZ\n- Protects against hardware failure\n- 99.999% availability SLA\n- Data not replicated across AZs automatically\n\n**Why A is incorrect:**\nEBS does not replicate across AZs by default.\n\n**Why C is incorrect:**\nAutomated backups are not enabled by default for EBS.\n\n**Why D is incorrect:**\nEBS does have built-in redundancy within the AZ.",
      "reference": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "An application processes messages from SQS. If processing fails, the message should be retried. After multiple failures, it should be moved to a separate queue. How should this be implemented?",
      "options": [
        "A. Configure message retention period",
        "B. Configure a dead-letter queue with redrive policy",
        "C. Configure visibility timeout only",
        "D. Configure long polling"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nDead-letter queue with redrive policy:\n- Automatically moves failed messages\n- maxReceiveCount sets retry limit\n- Preserves failed messages for analysis\n- Prevents poison messages from blocking queue\n\n**Why A is incorrect:**\nRetention period keeps messages but doesn't handle failures.\n\n**Why C is incorrect:**\nVisibility timeout allows retries but doesn't isolate failures.\n\n**Why D is incorrect:**\nLong polling reduces empty responses, unrelated to failure handling.",
      "reference": "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "A company wants automatic recovery of EC2 instances that fail status checks. Which feature should be configured?",
      "options": [
        "A. Auto Scaling group",
        "B. EC2 Auto Recovery",
        "C. CloudWatch Alarms with SNS",
        "D. AWS Systems Manager"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nEC2 Auto Recovery:\n- Automatically recovers impaired instances\n- Triggered by system status check failure\n- Restarts instance on new hardware\n- Maintains instance ID, IP, and EBS volumes\n\n**Why A is incorrect:**\nAuto Scaling terminates and launches new instances.\n\n**Why C is incorrect:**\nCloudWatch Alarms alert but don't auto-recover.\n\n**Why D is incorrect:**\nSystems Manager can automate but requires custom setup.",
      "reference": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recover.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "A company wants to automatically redirect traffic away from an unhealthy endpoint in Route 53. Which feature enables this?",
      "options": [
        "A. Simple routing policy",
        "B. Weighted routing policy",
        "C. Failover routing policy with health checks",
        "D. Geolocation routing policy"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nFailover routing with health checks:\n- Primary and secondary endpoints\n- Health checks monitor primary\n- Automatic failover to secondary when primary unhealthy\n- Designed for disaster recovery\n\n**Why A is incorrect:**\nSimple routing doesn't support health check failover.\n\n**Why B is incorrect:**\nWeighted routing distributes traffic, doesn't failover.\n\n**Why D is incorrect:**\nGeolocation routes by location, not health status.",
      "reference": "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-configuring.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "An Aurora MySQL database needs to handle read-heavy workloads during peak hours. Which feature allows horizontal scaling of read capacity?",
      "options": [
        "A. Aurora Multi-Master",
        "B. Aurora Read Replicas",
        "C. Aurora Serverless",
        "D. Aurora Global Database"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAurora Read Replicas:\n- Up to 15 read replicas\n- Same storage layer as primary\n- Millisecond replication lag\n- Horizontal read scaling\n\n**Why A is incorrect:**\nMulti-Master allows multiple write nodes, not read scaling.\n\n**Why C is incorrect:**\nServerless scales compute but doesn't add read endpoints.\n\n**Why D is incorrect:**\nGlobal Database is for cross-Region, not read scaling.",
      "reference": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company needs to store documents with flexible schema and query by document attributes. Which AWS database service should they use?",
      "options": [
        "A. Amazon RDS",
        "B. Amazon DynamoDB",
        "C. Amazon DocumentDB",
        "D. Amazon Neptune"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nAmazon DocumentDB:\n- Document database (MongoDB compatible)\n- Flexible JSON documents\n- Rich query capabilities on document attributes\n- Fully managed\n\n**Why A is incorrect:**\nRDS is relational with fixed schema.\n\n**Why B is incorrect:**\nDynamoDB is key-value/wide column, limited query flexibility.\n\n**Why D is incorrect:**\nNeptune is a graph database for relationships.",
      "reference": "https://docs.aws.amazon.com/documentdb/latest/developerguide/what-is.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company needs to run graph queries on highly connected data (social network). Which AWS database service should they use?",
      "options": [
        "A. Amazon DynamoDB",
        "B. Amazon DocumentDB",
        "C. Amazon Neptune",
        "D. Amazon Redshift"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nAmazon Neptune:\n- Purpose-built graph database\n- Supports Property Graph and RDF\n- Optimized for relationship queries\n- Ideal for social networks, recommendations\n\n**Why A is incorrect:**\nDynamoDB is key-value, not optimized for graph traversals.\n\n**Why B is incorrect:**\nDocumentDB is for documents, not graph relationships.\n\n**Why D is incorrect:**\nRedshift is for analytics, not graph queries.",
      "reference": "https://docs.aws.amazon.com/neptune/latest/userguide/intro.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company needs a fully managed time-series database for IoT sensor data with millions of data points per second. Which service should they use?",
      "options": [
        "A. Amazon RDS",
        "B. Amazon Timestream",
        "C. Amazon DynamoDB",
        "D. Amazon Redshift"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAmazon Timestream:\n- Purpose-built for time-series data\n- Handles trillions of events per day\n- Automatic data tiering\n- Built-in time-series analytics\n\n**Why A is incorrect:**\nRDS is not optimized for time-series workloads.\n\n**Why C is incorrect:**\nDynamoDB can store time-series but lacks specialized features.\n\n**Why D is incorrect:**\nRedshift is for analytics, not high-speed time-series ingestion.",
      "reference": "https://docs.aws.amazon.com/timestream/latest/developerguide/what-is-timestream.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company needs to store 500 TB of data with low-latency access. Individual files are up to 100 GB. Which storage option is most suitable?",
      "options": [
        "A. Amazon EBS",
        "B. Amazon S3",
        "C. Amazon EFS",
        "D. Instance store"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAmazon S3:\n- Unlimited storage capacity\n- Supports objects up to 5 TB\n- Low-latency access\n- Cost-effective for large data\n\n**Why A is incorrect:**\nEBS volume maximum is 64 TB, and it's block storage.\n\n**Why C is incorrect:**\nEFS is file storage with higher cost for this scale.\n\n**Why D is incorrect:**\nInstance store is ephemeral and limited in size.",
      "reference": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A Linux HPC workload requires shared file storage with sub-millisecond latency. Which AWS storage service is optimized for this?",
      "options": [
        "A. Amazon EFS",
        "B. Amazon FSx for Lustre",
        "C. Amazon S3",
        "D. Amazon EBS"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nFSx for Lustre:\n- High-performance file system\n- Sub-millisecond latency\n- Designed for HPC workloads\n- Can integrate with S3\n\n**Why A is incorrect:**\nEFS has higher latency than Lustre for HPC.\n\n**Why C is incorrect:**\nS3 is object storage, not file storage.\n\n**Why D is incorrect:**\nEBS is block storage, not shared file storage.",
      "reference": "https://docs.aws.amazon.com/fsx/latest/LustreGuide/what-is.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company wants to accelerate uploads to an S3 bucket from users around the world. Which feature should be enabled?",
      "options": [
        "A. S3 Cross-Region Replication",
        "B. S3 Transfer Acceleration",
        "C. S3 Multipart Upload",
        "D. CloudFront with S3 origin"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nS3 Transfer Acceleration:\n- Uses CloudFront edge locations\n- Uploads routed through nearest edge\n- Up to 500% faster for global users\n- Optimized network path to S3\n\n**Why A is incorrect:**\nCRR replicates after upload, doesn't accelerate upload.\n\n**Why C is incorrect:**\nMultipart improves large file uploads but not network path.\n\n**Why D is incorrect:**\nCloudFront accelerates downloads, not uploads.",
      "reference": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/transfer-acceleration.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A company needs to process video files and generate thumbnails. Videos are uploaded to S3. Which architecture provides automatic, scalable processing?",
      "options": [
        "A. EC2 polling S3 for new objects",
        "B. S3 Event → Lambda → MediaConvert",
        "C. AWS Batch scheduled jobs",
        "D. Manual processing workflow"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nEvent-driven architecture:\n- S3 triggers Lambda on upload\n- Lambda invokes MediaConvert\n- Serverless and automatic\n- Scales with upload volume\n\n**Why A is incorrect:**\nPolling is inefficient and requires managing EC2.\n\n**Why C is incorrect:**\nScheduled jobs don't process immediately on upload.\n\n**Why D is incorrect:**\nManual processing doesn't scale automatically.",
      "reference": "https://docs.aws.amazon.com/mediaconvert/latest/ug/what-is.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "An application needs to handle traffic spikes of 10x normal load. The spikes are unpredictable and last 5-10 minutes. Which architecture handles this best?",
      "options": [
        "A. EC2 instances sized for peak load",
        "B. Auto Scaling with step scaling policy",
        "C. Lambda with API Gateway",
        "D. EC2 Reserved Instances"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nLambda with API Gateway:\n- Scales instantly to thousands of concurrent executions\n- No pre-warming needed\n- Pay only for execution time\n- Handles unpredictable spikes naturally\n\n**Why A is incorrect:**\nSizing for peak wastes resources during normal load.\n\n**Why B is incorrect:**\nStep scaling has lag time during sudden spikes.\n\n**Why D is incorrect:**\nReserved Instances are a pricing model, not scaling solution.",
      "reference": "https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "A company needs to deploy the same application stack across multiple AWS accounts. What is the best approach?",
      "options": [
        "A. Manual deployment in each account",
        "B. AWS CloudFormation StackSets",
        "C. Copy AMIs between accounts",
        "D. Use AWS Organizations only"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nCloudFormation StackSets:\n- Deploy stacks across multiple accounts/Regions\n- Central management\n- Consistent infrastructure\n- Integrates with AWS Organizations\n\n**Why A is incorrect:**\nManual deployment is error-prone and doesn't scale.\n\n**Why C is incorrect:**\nAMI copying doesn't deploy complete infrastructure.\n\n**Why D is incorrect:**\nOrganizations manages accounts but doesn't deploy infrastructure.",
      "reference": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "A company needs to provide temporary access to S3 objects for external partners without creating IAM users. Which method should be used?",
      "options": [
        "A. Make the bucket public",
        "B. Generate S3 presigned URLs",
        "C. Create IAM users for each partner",
        "D. Use S3 ACLs"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nS3 Presigned URLs:\n- Time-limited access to specific objects\n- No IAM credentials needed\n- Generated using existing credentials\n- Secure and temporary\n\n**Why A is incorrect:**\nPublic bucket exposes all objects to everyone.\n\n**Why C is incorrect:**\nCreating IAM users for external partners is management overhead.\n\n**Why D is incorrect:**\nACLs can't provide temporary access.",
      "reference": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/ShareObjectPreSignedURL.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "4",
      "difficulty": 2,
      "question_text": "A company runs stateless web servers with predictable steady-state usage. They want to reduce costs. Which EC2 purchasing option provides the best savings?",
      "options": [
        "A. On-Demand Instances",
        "B. Spot Instances",
        "C. Standard Reserved Instances (3-year)",
        "D. Scheduled Reserved Instances"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\n3-year Standard Reserved Instances:\n- Up to 72% discount vs On-Demand\n- Predictable steady-state workload matches RI model\n- Long-term commitment provides best savings\n- Can pay all upfront for maximum discount\n\n**Why A is incorrect:**\nOn-Demand is most expensive for steady workloads.\n\n**Why B is incorrect:**\nSpot can be interrupted; not suitable for web servers.\n\n**Why D is incorrect:**\nScheduled RIs are for recurring patterns, not steady-state.",
      "reference": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-reserved-instances.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "3",
      "difficulty": 2,
      "question_text": "A mobile app backend needs a GraphQL API. Which AWS service provides managed GraphQL?",
      "options": [
        "A. Amazon API Gateway",
        "B. AWS AppSync",
        "C. Amazon CloudFront",
        "D. AWS Lambda"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nAWS AppSync:\n- Managed GraphQL service\n- Real-time subscriptions\n- Offline support\n- Integrates with DynamoDB, Lambda, etc.\n\n**Why A is incorrect:**\nAPI Gateway supports REST and WebSocket, not native GraphQL.\n\n**Why C is incorrect:**\nCloudFront is a CDN, not an API service.\n\n**Why D is incorrect:**\nLambda can implement GraphQL but requires custom code.",
      "reference": "https://docs.aws.amazon.com/appsync/latest/devguide/what-is-appsync.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "2",
      "difficulty": 2,
      "question_text": "A company needs their Aurora database to be accessible from another Region with low-latency reads. Which feature should they use?",
      "options": [
        "A. Aurora Read Replicas",
        "B. Aurora Multi-Master",
        "C. Aurora Global Database",
        "D. Aurora Serverless"
      ],
      "correct_answer": "C",
      "explanation": "**Correct Answer: C**\n\n**Why C is correct:**\nAurora Global Database:\n- Cross-Region replication\n- Sub-second replication lag\n- Local read access in secondary Regions\n- Supports disaster recovery\n\n**Why A is incorrect:**\nRead Replicas are within the same Region.\n\n**Why B is incorrect:**\nMulti-Master is for multiple write nodes in same Region.\n\n**Why D is incorrect:**\nServerless scales capacity, not cross-Region.",
      "reference": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database.html"
    },
    {
      "exam_type": "SAA-C03",
      "domain": "1",
      "difficulty": 2,
      "question_text": "A company needs to grant cross-account access to an S3 bucket. The bucket is in Account A, and users in Account B need access. What is the recommended approach?",
      "options": [
        "A. Make the bucket public",
        "B. Use bucket policy with Account B's ARN",
        "C. Copy data to Account B",
        "D. Create IAM users in Account A"
      ],
      "correct_answer": "B",
      "explanation": "**Correct Answer: B**\n\n**Why B is correct:**\nBucket policy for cross-account:\n- Grants access to Account B principals\n- Uses Principal with Account B ARN\n- Account B users assume role or use IAM to access\n- Maintains data in single location\n\n**Why A is incorrect:**\nPublic access exposes data to everyone.\n\n**Why C is incorrect:**\nCopying creates duplicate data and sync issues.\n\n**Why D is incorrect:**\nCreating users in Account A for external access is complex.",
      "reference": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-walkthroughs-managing-access-example2.html"
    }
  ]
}